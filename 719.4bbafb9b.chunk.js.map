{"version":3,"file":"719.4bbafb9b.chunk.js","mappings":"gBAAIA,ECCAC,EADAC,ECAAC,E,uQCWJ,MAEMC,IAP2B,oBAAtBC,mBACPC,gBAAgBD,oBAIyBE,aAAaC,QAAQ,YAGvC,UAIdC,EACY,IAAgBL,GAAkBK,QAE9CC,EACY,IAAgBN,GAAkBM,QAE9CC,EACkB,IAAgBP,GAAkBO,cAEpDC,EACgB,IAAgBR,GAAkBQ,YAElDC,EAEX,IAAgBT,GAAkBS,gBAEvBC,EACkB,IAAgBV,GAAkBU,cAE3DC,EAAoB,GAAGD,OAEhBE,EAAwB,GAAGD,QAe3BE,GAVe,IAAgBb,GAAkBc,WAGhC,IAAgBd,GAAkBe,aAO9B,MAIrB,WAAEC,GAAe,IAAgBhB,E,+DCpD9C,MAAMiB,EAAgC,CACpCC,QAAS,CACPC,SAAU,IAASC,QACnBN,WAAY,OACZC,aAAc,WACdT,QAASe,EAAQC,IAAIC,OACjB,4CACA,mCACJlB,QAASgB,EAAQC,IAAIC,OACjB,4CACA,mCACJhB,cAAec,EAAQC,IAAIC,OACvB,oDACA,2CACJf,YAAa,gDACbC,gBAAiB,8CACjBC,cAAe,UACfM,WAAY,iCAEd,cAAe,CACbG,SAAU,IAASK,YACnBV,WAAY,QACZC,aAAc,cACdT,QAAS,wCACTD,QAAS,uCACTE,cAAe,+CACfC,YAAa,oDACbC,gBAAiB,kDACjBC,cAAe,QACfM,WAAY,sCAIhB,K,6JCrCO,MAAMS,EAAgB,IAAIC,OAC/B,IAAI,uBACJ,KAKWC,EAAoB,uBAYpBC,GAVyB,IAAIF,OACxC,IAAI,uBACJ,KAGmC,IAAIA,OACvC,IAAI,8BACJ,KAG4B,4BAYjBG,EAAe,yB,qECjCrB,IAAKC,EAAL,CAAKA,IACVA,EAAA,GAAK,KACLA,EAAA,KAAO,OACPA,EAAA,IAAM,MAHIA,GAAL,CAAKA,GAAA,G,oDCiCAC,EAAL,CAAKA,IACVA,EAAAA,EAAA,KAAO,GAAP,OACAA,EAAAA,EAAA,IAAM,IAAN,MACAA,EAAAA,EAAA,OAAS,IAAT,SACAA,EAAAA,EAAA,KAAO,IAAP,OACAA,EAAAA,EAAA,OAAS,GAAT,SALUA,GAAL,CAAKA,GAAA,I,sBC1BZ,SAASC,EAAoBC,GAC3B,MAAO,CACLC,OAAQC,OAAOC,iBAKb,IAAIC,GAAO,EACX,MAAQA,GAAM,CAEZ,MAAMC,EAAU,IAAIC,SAA4BC,IAE9CP,EAAKQ,UAAaC,IACG,OAAfA,EAAMC,MACRN,GAAO,EACPG,EAAQ,OAERA,EAAQE,EAAMC,KAChB,CACD,IAGGC,QAAcN,EAEN,OAAVM,UACIA,EAEV,CACF,EAEJ,CAEA,MAAMC,EAGF,CACFC,UAAYC,GACVA,GAAOA,EAAIC,QAAsD,mBAArCD,EAAIC,OAAOb,OAAOC,eAChDa,UAAUF,GACR,QAAY,IAARA,EACF,MAAO,CAAC,KAAM,IAEhB,MAAM,OAAEC,KAAWE,GAASH,GACtB,MAAEI,EAAK,MAAEC,GAAU,IAAIC,eAY7B,OAXIL,GACF,WAEE,gBAAiBJ,KAASI,EACxBG,EAAMG,YAAYV,GAEpBO,EAAMG,YAAY,MAElBH,EAAMI,OACP,EARD,GAUK,CAAC,IAAKL,EAAMjB,KAAMmB,GAAS,CAACA,GACrC,EACAI,YAAYC,GACV,IAAKA,EACH,OAEF,MAAM,KAAExB,KAASiB,GAASO,EAE1B,MAAO,IACFP,EACHF,OAAQhB,EAAoBC,GAEhC,G,gDC9DuD,oBAAjByB,cAEgBrC,EAAQC,IAAIC,OAGpE,SAASoC,IACP,KAAiBC,IAAI,cAAef,GACpC,KAAiBe,IAAI,aAAc,CACjCd,UAAYF,GACHA,aAAiBiB,EAAA,EAE1BL,YAAcZ,GACL,IAAIiB,EAAA,GAAqBC,IAC9B,MAAMC,EAAS,KACZC,IAAI,SACJR,YAAYZ,GAEfmB,EACGE,WACC,QAAM,CACJlE,KAAOA,GAAkB+D,EAAS/D,KAAKA,GACvCmE,MAAQA,GAAmBJ,EAASI,MAAMA,GAC1CC,SAAU,IAAML,EAASK,cAG5BC,MAAMC,GACLP,EAASQ,KAAI,KACXD,EAAaE,cACbR,EAAO,OAAe,KAEzB,IAGPd,UAAYL,GACH,KAAiBoB,IAAI,SAAUf,UAAU,CAC9CgB,UAAYH,GACVlB,EAAMqB,UAAU,CACdlE,KAAOA,GAAkB+D,EAAS/D,KAAKA,GAAMqE,OAC7CF,MAAQA,GAAmBJ,EAASI,MAAMA,GAAOE,OACjDD,SAAU,IAAML,EAASK,WAAWC,aAM9C,KAAiBR,IAAI,eAAgB,CACnCd,UAAYF,GACHA,aAAiB4B,EAAA,GAE1BhB,YAAcZ,GACL,IAAI4B,EAAA,IAAa,KACtB,MAAMT,EAAS,KACZC,IAAI,SACJR,YAAYZ,GAEfmB,EAAOQ,cAAcH,MAAK,KACxBL,EAAO,OAAe,GACtB,IAGNd,UAAYL,GACH,KAAiBoB,IAAI,SAAUf,UAAU,CAC9CsB,YAAa,IAAM3B,EAAM2B,iBAIjC,CAWA,SAASE,EAAgBC,GACvB,MAAMC,EAAgB,CACpBC,IAAK,CAAEC,SAAUC,EAAQF,KACzBV,MAAO,CAAEW,SAAUC,EAAQZ,OAC3Ba,KAAM,CAAEF,SAAUC,EAAQC,OAEtBC,EAAqBC,IACzB,MAAM,SAAEJ,GAAaF,EAAcM,GAEnCN,EAAcM,GAAQJ,SAAWC,EAAQG,GAEzCH,EAAQG,GAAU,IAAIC,KACpBL,EAASM,MAAML,EAASI,GACxB,MAAME,EAAmBF,EAAKG,KAAKC,GAtBzC,SAAuBvC,GACrB,IACE,OAAOwC,KAAKC,UAAUzC,EACxB,CAAE,MAAOmB,GACP,OAAOuB,OAAO1C,EAChB,CACF,CAgBiD2C,CAAcJ,KAEzDZ,EAAOpB,YAAY,CAAEqC,KAAM,UAAWV,SAAQC,KAAME,GAAmB,CACxE,EAGHQ,OAAOC,KAAKlB,GAAemB,SAASb,GAClCD,EAAkBC,IAEtB,C,gDC/GO,SAASc,EAAWnD,GAGzB,OAAOoD,QAAQpD,EAAMqD,MAAM,wBAC7B,CCkBO,IAAKC,EAAL,CAAKA,IACVA,EAAAA,EAAA,aAAe,GAAf,eACAA,EAAAA,EAAA,SAAW,GAAX,WACAA,EAAAA,EAAA,KAAO,GAAP,OAHUA,GAAL,CAAKA,GAAA,IAiEAC,EAAL,CAAKA,IACVA,EAAAA,EAAA,QAAU,GAAV,UACAA,EAAAA,EAAA,UAAY,GAAZ,YACAA,EAAAA,EAAA,KAAO,GAAP,OACAA,EAAAA,EAAA,OAAS,GAAT,QAJUA,GAAL,CAAKA,GAAA,IAOAC,EAAL,CAAKA,IACVA,EAAAA,EAAA,SAAW,GAAX,WACAA,EAAAA,EAAA,UAAY,GAAZ,YAFUA,GAAL,CAAKA,GAAA,I,WC5BZ,MAAMC,EAA2B,CAC/BC,KAAM,CACJC,WAAW,EACX5D,KAAM,GACNuB,WAAO,GAETsC,MAAO,CAAC,EACRC,QAAS,CACPC,YAAa,CACXC,MAAO,EACPC,UAAW,EACXC,QAAS,KAKf,SAASC,EAAcC,GACjBA,EAAKC,YAAcd,EAAUe,MAAQF,EAAKG,KAAKC,KACjDJ,EAAKC,UAAYd,EAAUkB,UAG7B,MAAM,KAAEF,GAASH,EAEXM,EAAuB,CAC3BC,UAAW,IAAIC,KAAKL,EAAKI,WAAWE,cAGpCC,gBACEV,EAAKU,iBACLV,EAAKW,MACLX,EAAKG,KAAKS,kBACVZ,EAAKG,KAAKQ,MACVX,EAAKG,KAAKO,gBAEZG,KAAMb,EAAKa,MAAQV,EAAKU,KAExBC,YAAad,EAAKe,GAElBpB,YAAaK,EAAKL,aAAe,GAGnC,OAAQK,EAAKC,WACX,KAAKd,EAAUe,KACf,KAAKf,EAAU6B,aAAc,CAC3B,MAAMb,EAAOH,EAAKG,MACZ,KAAEvB,GAASuB,EAEjB,IAAIc,EAAOjB,EAAKkB,QAEhB,GAAa,gCAATtC,EAAwC,CAE1CqC,EADcd,EAAKtE,MACNsF,WACf,MAAO,GAAa,qCAATvC,EAA6C,CAGtDqC,EAFcd,EAAKtE,MAENuF,OAAO,GAAGC,OACzB,CAEAxC,OAAOyC,OAAOhB,EAAW,CACvB1B,OACAqC,OACAd,KAAMH,EAAKG,KAAKtE,QAGlB,KACF,CAEA,KAAKsD,EAAUkB,SAAU,CACvB,MAAMF,EAAOH,EAAKG,KAElBtB,OAAOyC,OAAOhB,EAAW,CACvB1B,KAAM,mCACNqC,KAAMd,EAAKoB,OACXpB,KAAMA,EACNqB,SAAS,IAGX,KACF,CAEA,QAGE,MAAO,CAAC,EAGZ,OAAOlB,CACT,CAEA,MAAMmB,GAAe,QACnB,sBACAtG,MAAOuG,UACcA,EAAUC,WACjBrD,IAAIyB,KAId6B,GAAe,QACnB,sBACAzG,OAAS4F,KAAIW,eAGX,GAFiB1C,EAAW+B,GAEd,CAeZ,aAdoBW,EAAUG,SAASd,IACVzC,KAAK0B,IAChC,GAAuB,IAAnBA,EAAKO,UAIT,OAAOR,EAAc,IAChBC,EACHe,KACAd,UAAWd,EAAUkB,SACrBF,KAAMH,GACN,IAGkB8B,OAAO7C,QAC/B,CAaA,aAXmByC,EAAUK,eAAehB,IACjBzC,KAAK0B,IAC9B,MAAMC,EAAYD,EAAKI,GAAKjB,EAAUkB,SAAWlB,EAAUe,KAC3D,OAAOH,EAAc,IAChBC,EACHC,YACAc,KACAZ,KAAMH,GACN,GAGgB,IAIlBgC,GAAa,QACjB,oBACA7G,OAAS4F,KAAIW,cACJA,EAAUM,WAAWjB,KAI1BkB,EAAyB,CAC7BlB,GAAI,GACJvB,WAAW,EACX5D,KAAM,GACNuB,WAAO,EACPwC,YAAa,GAGf,SAASuC,EAAqBhC,EAAYiC,GAOxC,OANqBjC,EAAKtE,KAAKwG,OAAO,GAEDC,MAAMC,GAClCA,EAAI5B,kBAAoByB,EAAWzB,iBAI9C,CAEA,MAAM0B,GAAQ,QAAY,CACxBG,KAAM,QACNjD,eACAkD,SAAU,CAERC,gBAAiB,CACfC,QAAS,CAACC,EAAOC,KACFA,EAAOC,QAEf9D,SAAS+D,IACZ,MAAQhC,YAAaC,GAAO+B,EAEvBH,EAAMlD,MAAMsB,KACf4B,EAAMlD,MAAMsB,GAAM,IAAKkB,IAGzB,MAAM/B,EAAOyC,EAAMlD,MAAMsB,GAEzBlC,OAAOyC,OAAOpB,EAAM,CAClBa,KAEApB,YAAamD,EAAQnD,aAAe,IAGjCuC,EAAqBhC,EAAM4C,KAC9B5C,EAAKtE,KAAOsE,EAAKtE,KAAKmH,OAAOD,GAC/B,IAGFV,EAAMY,aAAaC,eAAeN,EAAM,EAE1CO,QAAUtH,IACD,CACLiH,QAASjH,EAAK0C,IAAIyB,MAKxBoD,aACER,EACAC,GAEA,MAAM,GAAE7B,EAAE,KAAEf,GAAS4C,EAAOC,QACfF,EAAMlD,MAAMsB,GAEpBnF,KAAKwH,KAAK,IACVpD,EACHG,KAAMH,EAAKG,KACXkD,OAAQ,YAGV,MAAMC,EAAUX,EAAMpD,KAAK3D,KAAKkG,QAAQ9B,GAASA,IAASe,IAC1DuC,EAAQC,QAAQxC,GAChB4B,EAAMpD,KAAK3D,KAAO0H,CACpB,EAEAE,gBACEb,EACAC,GAMA,MAAM,OAAEa,EAAM,OAAEC,EAAM,UAAEC,GAAcf,EAAOC,QAGvC7C,EAFO2C,EAAMlD,MAAMgE,GAEP7H,KAAKgI,MAAM5D,GAASA,EAAKU,kBAAoBgD,IAE3D1D,IACE2D,SACK3D,EAAKqD,OAEZrD,EAAKqD,OAAS,QAGpB,EACAJ,eAAeN,GACb,MAmBMkB,EAnBmBhF,OAAOC,KAAK6D,EAAMlD,OAAOqE,QAKhD,CAACC,EAAKhD,KACN,MAAMb,EAAOyC,EAAMlD,MAAMsB,GAGzB,IAAKb,EAAKtE,KAAKoI,OACb,OAAOD,EAGT,MAAME,EAAU/D,EAAKtE,KAAKsE,EAAKtE,KAAKoI,OAAS,GAG7C,OAFAD,EAAIX,KAAK,CAAErC,KAAIkD,YAERF,CAAG,GACT,IAE6BG,MAAK,CAACC,EAAGC,IAErC5D,KAAK6D,MAAMD,EAAEH,QAAQ1D,WAAaC,KAAK6D,MAAMF,EAAEF,QAAQ1D,aAI3DoC,EAAMpD,KAAK3D,KAAOiI,EAAOvF,KAAKgG,GAAMA,EAAEvD,IACxC,EACAwD,MAAK,IACIjF,GAIXkF,cAAgBC,IACdA,EAAQC,QAAQjD,EAAakD,SAAUhC,IACrCA,EAAMpD,KAAKC,WAAY,CAAI,IAG7BiF,EAAQC,QAAQjD,EAAamD,WAAW,CAACjC,EAAOC,KAC9CD,EAAMpD,KAAKC,WAAY,EAEvB,MAAM8D,EAAsC,GAE5CV,EAAOC,QAAQ9D,SAAS+D,IACtB,MAAQhC,YAAaC,GAAO+B,EAEvBH,EAAMlD,MAAMsB,KACf4B,EAAMlD,MAAMsB,GAAM,IAAKkB,IAGzB,MAAM/B,EAAOyC,EAAMlD,MAAMsB,GAEzBlC,OAAOyC,OAAOpB,EAAM,CAClBa,KAEApB,YAAamD,EAAQnD,aAAe,IAGjCuC,EAAqBhC,EAAM4C,KAC9B5C,EAAKtE,KAAOsE,EAAKtE,KAAKmH,OAAOD,IAG/BQ,EAAQF,KAAKrC,EAAG,IAGlB4B,EAAMpD,KAAK3D,KAAO0H,CAAO,IAE3BmB,EAAQC,QAAQjD,EAAaoD,UAAU,CAAClC,EAAOC,KAC7C,EAAQzF,MAAMyF,GAEdD,EAAMpD,KAAKC,WAAY,EACvBmD,EAAMpD,KAAKpC,MAAQyF,EAAOzF,MAAM2F,OAAO,IAGzC2B,EAAQC,QAAQ9C,EAAa+C,SAAS,CAAChC,EAAOC,KAC5C,MAAM,GAAE7B,GAAO6B,EAAOzC,KAAK5B,IAEtBoE,EAAMlD,MAAMsB,KACf4B,EAAMlD,MAAMsB,GAAM,IAAKkB,IAIzBU,EAAMlD,MAAMsB,GAAIvB,WAAY,CAAI,IAGlCiF,EAAQC,QAAQ9C,EAAagD,WAAW,CAACjC,EAAOC,KAC9C,MAAM,GAAE7B,GAAO6B,EAAOzC,KAAK5B,IACrB2B,EAAOyC,EAAMlD,MAAMsB,GACzBb,EAAKV,WAAY,EAEjBU,EAAKa,GAAKA,EAEVb,EAAKtE,KAAOgH,EAAOC,OAAO,IAE5B4B,EAAQC,QAAQ9C,EAAaiD,UAAU,CAAClC,EAAOC,KAC7C,EAAQzF,MAAMyF,GAEd,MAAM1C,EAAOyC,EAAMlD,MAAMmD,EAAOzC,KAAK5B,IAAIwC,IACzCb,EAAKV,WAAY,EACjBU,EAAK/C,MAAQyF,EAAOzF,MAAM2F,OAAO,IAKnC2B,EAAQC,QAAQ1C,EAAW4C,WAAW,CAACjC,EAAOC,KAC5C,MAAM,GAAE7B,GAAO6B,EAAOzC,KAAK5B,IACrB2B,EAAOyC,EAAMlD,MAAMsB,GAEnBV,EAAWrB,EAAW+B,IAEtB,YAAEpB,GAAgBO,EAExByC,EAAMjD,QAAQC,YAAYC,OAASD,EAC/BU,EACFsC,EAAMjD,QAAQC,YAAYE,WAAaF,EAEvCgD,EAAMjD,QAAQC,YAAYG,SAAWH,EAGvCO,EAAKP,YAAc,CAAC,GACpB,KA8BO,aAAEwD,EAAY,gBAAEK,EAAe,gBAAEf,EAAiB8B,MAAK,KA1BzC,SACxB5B,GAAqBA,EAAMmC,MAAMrF,QACjCA,IACC,IAAIsF,EAAsB,EACtBC,EAAoB,EAExBnG,OAAOoG,OAAOxF,GAAOV,SAAQ,EAAGgC,KAAIpB,kBACjBX,EAAW+B,GAG1BgE,GAAuBpF,EAEvBqF,GAAqBrF,CACvB,IAKF,MAAO,CACLC,MAHYmF,EAAsBC,EAIlCnF,UAAWkF,EACXjF,QAASkF,EACV,IAKH5C,EAAM8C,SAOO9C,EAAa,QC7crB,MAAM,EACH,CACN+C,OAAQ,SACRC,eAAgB,iBCkBpB,MAAM,EAA2B,CAC/BC,UAAW,CACTC,M,SAAO,GAAOC,uBAAuBC,OAEvCC,eAAgB,CACdlD,KAAM,KACNmD,QAAS,MAEXC,SAAU,MAUZ,SAASC,EAAmBjD,GAC1B,MAAM,eAAE8C,EAAc,SAAEE,GAAahD,EAErC8C,GACErM,aAAayM,QACX,EAAwBV,OACxB3G,KAAKC,UAAU,CACb,CAACgH,EAAelD,MAAOkD,EAAeC,WAG5CC,GACEvM,aAAayM,QACX,EAAwBT,eACxB5G,KAAKC,UAAUkH,GAErB,CAEA,MAAM,GAAQ,QAAY,CACxBpD,KAAM,SACNjD,aAAY,EACZkD,SAAU,CACRsD,kBAAmB,CACjBnD,GAEEE,SAAWN,OAAMmD,eAGnB/C,EAAM8C,eAAiB,CACrBlD,OACAmD,QAASA,GAAW/C,EAAMgD,WAAWpD,IAAS,MAGhDqD,EAAmBjD,EAAM,EAE3BoD,YAAa,CAACpD,GAASE,cACrBF,EAAMgD,SAAW9C,EAEjB+C,EAAmBjD,EAAM,EAE3BqD,uBAAwB,CAACrD,GAASE,cAChCF,EAAM0C,UAAUC,MAAQzC,CAAO,EAIjCoD,cAAe,CAACtD,GAASE,cACnBF,EAAMgD,UACR9G,OAAOC,KAAK6D,EAAMgD,UAAU5G,SAASmH,IACnCrH,OAAOC,KAAK6D,EAAMgD,SAASO,IAAanH,SAASoH,IAC/C,GAAIxD,EAAMgD,SAASO,GAAYC,GAAYC,SAAWvD,EAAS,CAO7D,UANOF,EAAMgD,SAASO,GAAYC,GAEqB,IAAnDtH,OAAOC,KAAK6D,EAAMgD,SAASO,IAAalC,eACnCrB,EAAMgD,SAASO,GAGpBvD,EAAM8C,gBAAgBC,SAASW,OAAOD,SAAWvD,EAAS,CAC5D,MAEMyD,EAFUzH,OAAO0H,QAAQ5D,EAAMgD,UAEV/B,MACzB,EAAE,CAAE/H,KAAWA,EAAMwK,OAAOD,SAI5BzD,EAAM8C,eADJa,EACqB,CACrB/D,KAAM+D,EAAW,GACjBZ,QAASY,EAAW,IAGC,CACrB/D,KAAM,KACNmD,QAAS,KAGf,CAEAE,EAAmBjD,EACrB,IACA,GAEN,MAQO,kBACXmD,EAAiB,YACjBC,EAAW,uBACXC,EAAsB,cACtBC,GACE,EAAMf,QAEK,EAAa,QAGrB,MCvIMsB,EAAoB,oBCgEjC,MApDA,MAGEC,cACEC,KAAKC,QAAU,IAAIC,iBDjBc,wBCkBnC,CAEOC,kBACLtE,EACAc,EACAP,GAEA4D,KAAKC,QAAQpK,YAAY,CACvBqC,KAAM,iBACN/C,MAAO,CAAE0G,OAAMc,SAAQP,YAE3B,CAEOgE,sBAAsBC,EAAsBpE,GAEjD+D,KAAKC,QAAQpK,YAAY,CAAEqC,KAAM,aAAc/C,MAAO,CAAEkL,QAAOpE,UACjE,CAEOqE,wBAAwBD,EAAepE,GAE5C+D,KAAKC,QAAQpK,YAAY,CACvBqC,KAAM,gBACN/C,MAAO,CAAEkL,QAAOpE,UAEpB,CAEOsE,gBAAgBC,GAEjBA,EAAUlD,OAAS,GACrB0C,KAAKC,QAAQpK,YAAYkG,EAAgByE,GAE7C,CAEOC,sBAAsB5E,EAAcmD,GACzCgB,KAAKC,QAAQpK,YACXuJ,EAAkB,CAChBvD,OACAmD,YAGN,CAEA0B,KAAK9E,GACHoE,KAAKC,QAAQpK,YAAY+F,EAC3B,G,iECtDK,MAAM+E,EAAkB,CAC7B9E,EACA+E,KAGO,CACLC,WAAY,CACVlE,EACAP,EACA0E,KAGAF,EAAWR,sBAAsBvE,EAAM,CACrCc,SACAP,UACA0E,WACAlM,KAAM,CAAC,SAAU,QAAS,UAAU+G,MAAMoF,GAAMA,IAAMpE,KACtD,IClBRlI,eAAeuM,EACbC,EACAC,EACAC,EAAY,IAEZ,IAAIC,EAAQ,GAEZ,gBAAiB9H,KAAQ2H,EACvBG,EAAM1E,KAAKpD,GACP8H,EAAM9D,SAAW6D,UACbD,EAAaE,GACnBA,EAAQ,IAIRA,EAAM9D,OAAS,SACX4D,EAAaE,EAEvB,CA4BO3M,eAAgB4M,GACrBC,EACAC,GAEA,IAAIC,EAAS,EACb,OAAa,CAEX,MAAMP,QAAcK,EAAc,IAAKC,EAAQC,WAE/C,GAAqB,IAAjBP,EAAM3D,OACR,YAGI2D,EAENO,GAAUP,EAAM3D,MAClB,CACF,CC7DO,MAUDmE,GAVkC,MACtC,MAAMxB,EAAU,IAAIC,iBAAiBJ,GAErC,MAAO,CACL4B,QAAU9F,IACRqE,EAAQpK,YAAY+F,EAAI,EAE3B,EAGe+F,GAELC,GAAgCnN,MAAOoN,IAClD,MAAMC,QAAuBC,GAA4BF,GAczD,OAZIC,GACFL,GAAUC,QAAQ,CAChBxJ,KAAM,OACNhD,KAAM,CACJmF,GAAIwH,EAAQG,IACZ9M,KAAM4M,EACNG,QAAStJ,EAAiBuJ,UAC1BC,SAAU7N,EAAc8N,YAKrBN,CAAc,E,gBCnClB,MAAMO,GAAY,iDAEZC,GAAa,iDCUbC,GAAyB,CAACF,GAAWC,I,gBCwBlD,MAaaP,GAA8BtN,MAAOoN,IAChD,MAAOW,EAAatN,QAZIT,OAAOoN,IAC/B,MAAMW,EAAcX,GAASpI,MAAM+I,aAAe,GAGlD,MAAoB,SAAhBA,EACK,CAACA,EAAaX,EAAQY,aAGxB,CAACD,OAAa,EAAU,EAIGE,CAAkBb,GAEpD,IAAIc,EAA8B,SAAhBH,KAA4BtN,EAM9C,OAJAyN,EACEA,KACEzN,EAAMsD,MAAM,SAAoBtD,EAAMsD,MAAM,QAEzCmK,EAAczN,OAAO,GA0P9B,OAvPA,MA6BE6K,YAAY6C,GACV,GAjBF,KAAQC,UAAYlC,EAAgB,WAAY,IAAI,GAEpD,KAAQmC,YAAc,IAAIC,EAAA,EACxB,IAAIC,MAcCJ,EAAKK,uBACR,MAAM,IAAIC,MAAM,yCAGlBlD,KAAKiD,uBAAyBL,EAAKK,uBAEnCL,EAAKO,cAAc3M,WAAW4M,IAC5BpD,KAAKoD,aAAeA,EAEhBpD,KAAKqD,MAAMC,KAAO,GACpBtD,KAAK8C,YAAYxQ,KAAK0N,KAAKqD,MAC7B,IAGFT,EAAKW,YACFC,MACC,EAAAC,EAAA,IAAOtO,QAAoB,IAAVA,KAElBqB,WAAU/B,MAAOiP,IAChB1D,KAAK0D,GAAKA,QACJ1D,KAAK2D,eAAe,IAG9B3D,KAAK4D,gBAAiB,EAAAC,EAAA,GAAc,CAClCjB,EAAKW,YACLX,EAAKkB,gBACJN,MACD,EAAA5L,EAAA,IAAI,EAAEmM,EAAYC,OAAoBA,KAAkBD,IAE5D,CApDYE,eACV,QAASjE,KAAKoD,YAChB,CAUWC,YACT,OAAOrD,KAAK8C,YAAYoB,UAC1B,CAIWC,YACT,OAAOnE,KAAKoE,MACd,CAkCA,0BAAkC/J,EAAiB8H,GACjD,OAAOnC,KAAKiD,uBAAuB5I,EAAI8H,GACpCxL,MAAKlC,OAASkI,SAAQpH,cACS,cAAXoH,IACCpH,WAIdqM,GAA8BrM,IAC7B,KAER8O,OAAM,KAAM,GACjB,CAEA,oBAA4BrC,EAAkBsC,GAC5C,IAGE,UAFsBtE,KAAK0D,GAAIa,eAAevC,GAEhC,CACZ,MAAMwC,QAAYxE,KAAKoD,aAAcqB,gBAAgBH,SAEhCtE,KAAK0D,GAAIgB,aAAa1C,EAAKwC,EAClD,CAEA,OAAO,CACT,CAAE,MAAOG,GAEP,OADA,GAAQlO,MAAM,wBAAwBuL,OAASsC,KAASK,EAAIC,aACrD,CACT,CACF,CAEA,uBAA+BC,GAG7B,MAAM1D,EAAY0D,EAAavH,OAE/B0C,KAAK6C,UAAUhC,WACb,cACA,oBAAoBM,KAAaA,YAAoBnB,KAAKqD,MAAMC,mBAGlE,IAAI1F,EAAIuD,QACFrM,QAAQgQ,IACZD,EAAajN,KAAInD,MAAO6E,IACtB,MAAM,GAAEe,EAAE,QAAE4H,EAAO,KAAE/M,GAASoE,EAE9B,IAAIyL,EAAajQ,QAAQC,SAAQ,GASjC,OAPIkN,IAAYtJ,EAAiBuJ,WAAahN,EAC5C6P,EAAa/E,KAAKgF,cAAc3K,EAAInF,GAC3B+M,IAAYtJ,EAAiBgB,WACtCoL,EAAa/E,KAAKiF,oBAAoB5K,EAAI/F,EAAc8N,SAInD2C,EAAWpO,MAAKlC,MAAOc,IACxBA,QACIyK,KAAK0D,GAAIwB,gBAAgB,CAAE7K,KAAI4H,kBAE/BjC,KAAK0D,GAAIyB,gBAAgB,CAC7B9K,KACA4H,UACAtF,OAAQjE,EAAgBjC,QAI5B,MAAM4M,EAAQrD,KAAK8C,YAAY3N,MAC/BkO,EAAM+B,OAAO/K,GACbuD,IACAoC,KAAK8C,YAAYxQ,KAAK+Q,GAEtBrD,KAAK6C,UAAUhC,WACb,cACA,oBAAoBM,EAAYvD,KAAKuD,YACnCnB,KAAKqD,MAAMC,kBAEd,GACD,IAGR,CAEA+B,QACE,MAAMC,EAAUtF,KAAK4D,eAAeJ,MAClC,EAAA+B,EAAA,IAAKC,GAAM,GAAQrO,IAAI,8BAA8BqO,QACrD,EAAApK,EAAA,IAAQqK,IAAoC,IAAlBA,KAC1B,EAAAC,EAAA,IAAS,IAAM1F,KAAK8C,eAEpB,EAAA1H,EAAA,IAAQoK,GAAMA,EAAElC,KAAO,KACvB,EAAAoC,EAAA,IAAUrC,IACR,MAAMxK,EAAO,IAAIwK,EAAM9E,UAMjB4C,EAtLW,IAkLMtI,EAAKuC,QACzBwC,GAAMA,EAAEjB,SAAWjE,EAAgBiN,YACpCrI,OAIIsI,EAAiBhI,GACrBA,EAAEqE,UAAYtJ,EAAiBgB,UAC9BiE,EAAEqE,UAAYtJ,EAAiBuJ,WAAalC,KAAKiE,SAEpD,GAAI9C,EAAY,EAAG,CACjB,MAAM0D,EAAehM,EAClBuC,QACEwC,GAAMA,EAAEjB,SAAWjE,EAAgBuF,SAAW2H,EAAchI,KAE9DJ,MAAK,CAACC,EAAGC,IACDD,EAAE0E,SAAWzE,EAAEyE,WAEvBzG,MAAM,EAAGyF,GAEZ,GAAI0D,EAAavH,OAAS,EAWxB,OAVAuH,EAAaxM,SAASuF,IACpByF,EAAMlN,IAAIyH,EAAEvD,GAAI,IACXuD,EACHjB,OAAQjE,EAAgBiN,WACxB,IAGJ3F,KAAK8C,YAAYxQ,KAAK+Q,GAEtBrD,KAAK6C,UAAUhC,WAAW,cAAe,eAClCb,KAAK6F,iBAAiBhB,EAEjC,CAEA,OAAO,GAAK,KAahB,OATA7E,KAAKoE,OAASkB,EAAQ9B,MAAK,EAAAsC,EAAA,MAE3B9F,KAAKoE,OAAO5N,UAAU,CACpBlE,KAAOiD,IACLyK,KAAK6C,UAAUhC,WAAW,SAAS,EAErCpK,MAAQkO,GAAQ3E,KAAK6C,UAAUhC,WAAW,QAAS8D,EAAIC,cAGlD5E,IACT,CAEA,mBACE+F,EACA9D,EACAE,GAEA,OAAOnB,EACL+E,GACCA,GACC/F,KAAK0B,QACHqE,EAAKnO,KAAKoK,IAAQ,CAChB3H,GAAI2H,EACJG,WACAF,gBD/QyB,ICoRnC,CAEA,cAAqBhB,GACnB,GAAqB,IAAjBA,EAAM3D,OACR,aAGmB0C,KAAK0D,GAAIsC,aAAa/E,GAA3C,MAEMoC,EAAQrD,KAAK8C,YAAY3N,MAE/B8L,EAAM5I,SAASiB,GACb+J,EAAMlN,IAAImD,EAAKe,GAAI,IAAKf,EAAMqD,OAAQjE,EAAgBuF,YAExD+B,KAAK8C,YAAYxQ,KAAK+Q,EACxB,CAEA,sBACE,MAAMA,QAAcrD,KAAK0D,GAAIuC,aAAa,CACxCC,SAAU,CAACxN,EAAgBuF,WAC1BtH,MAAMsK,GAAU,IAAI+B,IAAI/B,EAAMrJ,KAAK0B,GAAS,CAACA,EAAKe,GAAIf,QAEzD0G,KAAK8C,YAAYxQ,KAAK,IAAI0Q,IAAI,IAAIK,KAAUrD,KAAKqD,QACnD,G,uEClTK,SAAS8C,GAAcC,EAAgB9I,EAAS,KACrD,OAAO8I,EAAO9I,OAASA,EAAS,GAAG8I,EAAO1K,MAAM,EAAG4B,QAAe8I,CACpE,CA0BA,MAAMC,GAAqB,uBClBpB,MA0BMC,GAAgC,CAC3CzL,EACA0L,KAEA,MAAM,iBACJrM,EAAgB,MAChBsM,EACAC,aAAa,KACXtM,EACAuM,OAAO,UAAE7M,EAAS,OAAE8M,GAAQ,QAC5BC,GACD,KACD1O,EAAI,MACJ/C,GACEoR,EACJ,MAAO,CACLtM,KAAMC,EACNsM,QACAtO,OACA2B,WAAW,EAAAgN,GAAA,IAAgBhN,GAE3BM,OACAhF,QACAyR,UACA/L,SACAiM,YAAaH,EACd,EAoCUI,GAA0B,EACrCxM,OACAb,KACAmB,SACAhB,YACAK,uBACF,CACEK,OACAb,KACAmB,SACAhB,WAAW,EAAAgN,GAAA,IAAgBhN,GAC3BG,gBAAiBE,ICxDZ,SAAS8M,GACdC,EACAC,GAEA,OAAOzS,SAAUgD,KACf,GAAIyP,EAAOC,QACT,MAAM,IAAIC,aAAa,6BAA8B,cAEvD,OAAOH,KAAQxP,EAAK,CAExB,C,+BC4NO,IAusHK4P,GAAL,CAAKA,IAEVA,EAAA,IAAM,MAENA,EAAA,cAAgB,kBAEhBA,EAAA,aAAe,iBAEfA,EAAA,KAAO,OAEPA,EAAA,eAAiB,mBAEjBA,EAAA,cAAgB,kBAZNA,GAAL,CAAKA,IAAA,IAorLwB,KAAG;;;;;;;;;MAgCH,KAAG;;;;;;;;MAyCF,KAAG;;;;;;;;;;;;;;;MAiDV,KAAG;;;;;;;;;;;;;;MAkDK,KAAG;;;;;;;;MAyClC,MAAMC,GAA+B,KAAG;;;;;;;;;;MA+CxC,MAAMC,GAAkC,KAAG;;;;;;;;;;MA8CD,KAAG;;;;;;;;MA2C7C,MAAMC,GAAiC,KAAG;;;;;;;;;;;MA8C1C,MAAMC,GAAiC,KAAG;;;;;;;;;;;;;;;;;;;;;MA4D1C,MAAMC,GAAmC,KAAG;;;;;;;;;;;;;;;;;;;;;MAkDX,KAAG;;;;;;;;MAyCJ,KAAG;;;;;;MAwCD,KAAG;;;;;;;;;;;;;;;;;;;;;;;MA0DH,KAAG;;;;;;;;;;;;;;;;;;;;6CC1wUrC,MAAMC,GAA4B,8BAE5BC,GACX,mCCeWC,GAA+B,CAC1ChN,EACAtF,KAEA,MAAM,KAAEL,EAAI,OAAE4S,GAAWvS,EAEnB0E,EAAO6N,EAAO,WAAW,GACzBC,EAAkBD,EAAO,kBAAkB,GAAGpM,MAAM,GACpD7B,GAAY,WACZiN,EAAcgB,EAAO,aAAa,IAElC,KAAE3N,EAAO,YAAI6N,GAhCQ,CAAC9S,IAC5B,MAAMK,EAAS,GAAA0S,GAAGC,QAAO,SAAWhT,IAC9BiF,EAAO5E,EAAO4S,MAAMhO,KACpB6N,EAAWzS,EAAO4S,MAAMH,SAC3BpQ,KAAKwE,IACJ,MAAMgM,EAAUhM,EAAQiM,QAAQ3M,MAAM,GACtC,OAAI0M,IAAYT,GACP,GAAAW,QAAQJ,OAAO9L,EAAQjH,OAG5BiT,IAAYR,GACP,GAAAW,aAAaL,OAAO9L,EAAQjH,YADrC,CAGO,IAERiG,QAAQgB,QAAwB,IAAZA,IAEvB,MAAO,CAAEjC,OAAM6N,WAAU,EAeOQ,CAActT,EAAKC,MAAMsT,SAASlC,IAE5DjM,EAAiC,GAevC,OAdA0N,EAAU3P,SAAQ,CAAC+D,EAASoK,KAC1BlM,EAAaoC,KAAK,CAChBzC,OACAuM,QACAtO,KAAM6P,EACNlO,YACA+M,SAAS,EACTzR,MAAOiH,EACPjC,OACAU,SACAiM,eACA,IAGGxM,CAAY,E,8DCtDrB,MAAMoO,GAAqB,IAAI,MAC7B,SAAa,CACXC,IAAK,MACLC,YAAcC,IAA6B,EAC3CC,cAAe,GACfC,UAAWtU,MAAOuU,IAChBC,YAAW,IAAMnU,QAAQC,WAAWmU,KAAKC,IAAI,IAAO,GAAKH,EAAS,KAAO,KAqBlEI,GAAuBC,GAClC,IAAI,MAAc,MAAa,CAC7BnC,OAAQmC,ICnBZ,MAAMC,GAAkB7U,OACtB8U,cACAC,gBACAhI,SAAS,EACT6H,wBAOkBD,GAAoBC,GAAaI,QAGjDnC,GAA8B,CAC9BoC,MChC2B,IDiC3BlI,SACAmI,QAAS,CAAC,CAAE9P,UAAWwN,GAASuC,MAChCC,MAAO,CACLC,IAAK,CACH,CAAEC,YAAa,CAAEC,IAAKT,IACtB,CAAEU,cAAe,CAAED,IAAKT,KAE1B1P,UAAW,CAAEqQ,KAAK,SAAgBV,QAI3BW,WAqBPC,GAA0B3V,OAC9BoG,SACAwP,gBACAb,gBACArI,YACAK,SAAS,EACT6H,kBASA,MAAMQ,EAAQ,CACZS,KAAM,CACJ,CACEzQ,UAAW,CACTqQ,KAAK,SAAgBV,KAGzB,CACE3O,OAAQ,CACNmP,IAAKnP,IAGT,CAAEoP,cAAe,CAAEM,IAAKF,MAkB5B,aAdkBjB,GAAoBC,GAAaI,QAGjDnC,GAA8B,CAC9BoC,MAAOvI,EACPK,SACAmI,QAAS,CACP,CACE9P,UAAWwN,GAASuC,MAGxBC,WAGSM,UAAU,EAGVK,GAAkC/V,MAC7CoG,EACAwP,EACAb,EACArI,EACAkI,IAEAhI,GAAsB+I,GAAyB,CAC7CvP,SACAwP,gBACAb,gBACArI,YACAkI,gBE/GEoB,GAA+BC,GACnC,IACK,IAAIC,IAAI,IACND,EAAM9S,KAAKgT,GAASA,EAAKlR,QACzBgR,EAAM9S,KAAKgT,GAASA,EAAKrQ,UAKrBsQ,GAAqCpW,MAChDuN,EACA8I,EACAC,EACAC,EACA3B,KAEA,MAAM4B,EFkGwB,EAC9B1B,EACAC,EACAH,IAEAhI,GAAsBiI,GAAiB,CACrCC,cACAC,gBACAH,gBE1GyB6B,CACzBlJ,EACA8I,EACAzB,GAEIqB,EAAQ,GAEd,gBAAiBtJ,KAAS6J,EAAoB,CAC5CP,EAAMhO,QAAQ0E,GACd,MAAMjI,EAAYsR,GAA4BrJ,GAC1CjI,EAAUmE,OAAS,SACf0D,EACJ7H,GACC4M,GACCgF,EAAmBI,aACjBpF,EACApN,EAAiBgB,SACjBqR,IXvC2B,GW4CrC,CAEA,OAAON,CAAK,EC5BP,MAAMU,GAAgC,EAC3CvQ,SACA2O,gBACAhI,SAAS,EACT6J,QAAQ,GACRC,iBAAiB,OACjB5B,YACF,CACE/O,QAAS,IAAIE,KACb6O,QACA6B,gBAAgB,SAAgB/B,GAChChI,SACA6J,MAAO,IAAIA,EAAMzT,KAAK4T,GAAM,IAAIA,OAAMC,KAAK,SAC3CC,gBAAiBJ,IAGbK,GAAoBlX,OACxBoG,SACA2O,gBACAhI,SAAS,EACT6J,QAAQ,GACRC,iBAAiB,OACjB5B,QACAL,kBAEA,MAAMuC,QAAYxC,GAAoBC,GAAaI,QAIjDhC,GACA2D,GAA8B,CAC5BvQ,SACA2O,gBACAhI,SACA6J,QACAC,iBACA5B,QACAL,iBAIJ,OAAOuC,GAAKC,mBAAmB,ECrB3BC,GAAkB,CACtB/S,EACAgT,EACAP,EACAQ,EACAC,KAEA,MAAMzS,EAAOT,EAAMxC,IAAIwV,GACjBzR,EAAed,GAAMc,cAAgB,GAS3C,OAPAA,EAAaoC,KAAK8O,GAClBzS,EAAM5C,IAAI4V,EAAM,CACdG,YAAaH,EACbI,kBAAmBF,EAAWT,EAAE3R,UAAYL,GAAM2S,mBAAqB,EACvEC,KAAM,CAAEJ,SAAQ7R,KAAMqR,EAAErR,KAAMkS,UAAWJ,EAAW,KAAO,QAC3D3R,iBAEKvB,CAAK,ECrDDuT,GAAc7X,MACzBiP,EACA6I,EACA/C,EACAtC,EACAsF,GAAwB,KAExB,MAAMC,QAAkB/I,EAAGgJ,eAAe,CACxClS,QAAS+R,EACThT,UAAWd,EAAUe,OAGjBmT,EAAe,IAAI3J,IAAIyJ,GAAW7U,KAAKgG,GAAM,CAACA,EAAEvD,GAAIuD,MAOpDgP,EDnByB,EAC/BL,EACAjS,KASA,GAAgC,KAN9BA,EAAcc,QACXoQ,GACCA,EAAEtT,OAASyP,IACX6D,EAAEtT,OAAS0P,MACV,IAEctK,OACnB,MAAO,GAET,MAAMvE,EAAQ,IAAIiK,IAmBlB,OAlBA1I,EAAajC,SAASmT,IACpB,IAAIU,EAAc,GAClB,GAAIV,EAAEtT,OAAS0P,GAAiC,CAC9C,MAAM,OAAElN,EAAM,QAAEmS,GAAYrB,EAAErW,MACxB8W,EAAWvR,EAAOwC,MAAMU,GAAMA,EAAEjD,UAAY4R,KAC7BN,EAAWY,EAAUnS,GAC7BrC,SAASuD,GACpBkQ,GAAgB/S,EAAO6C,EAAIjB,QAAS6Q,EAAG5P,EAAIkR,MAAOb,IAEtD,MAAO,GAAIT,EAAEtT,OAASyP,GAA2B,CAC/C,MAAM,YAAElN,EAAW,UAAEsS,EAAS,OAAEf,GAC9BR,EAAErW,MACE8W,EAAWxR,IAAgB8R,EACjCL,EAAcD,EAAWc,EAAYtS,EACrCqR,GAAgB/S,EAAOmT,EAAaV,EAAGQ,EAAQC,EACjD,KAGKlT,CAAK,ECdIiU,CAAkBT,QALL7I,EAAGuJ,gBAAgBV,EAAW,CACzDW,MAAO,MACP1D,mBAKI2D,EAA2B,GAGjC,UAAW3T,KAAQoT,EAAQrO,SAAU,CACnC,MAAM6O,EAAWT,EAAapW,IAAIiD,EAAK0S,aACjCmB,EAAkB7T,EAAKc,aAAagT,IAAI,IAEtCzT,UAAW0T,EAAoB,KAAEtT,EAAI,MAAEuM,GAAU6G,EACnDG,EAAiB,CACrBjU,UAAWd,EAAUe,KACrBgB,QAAS+R,EACT9S,KAAM,CACJO,gBAAiBC,EACjBuM,UAKJ,GAAK4G,EAmBE,CACL,MAAM,GACJ/S,EAAE,cACFoT,EAAa,gBACb3C,EAAe,KACfrR,EACAR,YAAayU,GACXN,EAEEO,EAAoBzE,KAAK0E,IAC7BH,EACAjU,EAAK2S,oBAED,uBAAE0B,EAAyB,EAAC,oBAAEC,EAAsB,GAAMrU,EAC1DsU,EAAsB7E,KAAK0E,IAC/BpU,EAAK2S,kBACL2B,GAEI7U,EACJyU,EACAlU,EAAKc,aAAac,QAAQoQ,GAAMA,EAAE3R,UAAYkU,IAC3CzQ,OAEL,GAAIwN,EAAkByC,EAAsB,CAE1C,MAAMS,EAAyBxB,EAC3Be,EACAO,EAEEG,EAAoB,IACrBT,EACHnT,KACApB,cACAwU,cAAeE,EAGf7C,gBAAiB5B,KAAK0E,IACpBL,EACAM,EACAG,GAGFvU,KAAM,IACD+T,EAAe/T,KAClBqU,oBAAqBE,EACrBH,iCAKE7G,GACJtD,EAAGwK,iBAAiBC,KAAKzK,GACzBwD,EAFIF,CAGJiH,GAEFd,EAAQzQ,KAAK,IACR0Q,KACAa,EACHxU,KAAM4T,GAEV,CACF,KAhFe,CACb,MAAMpU,EAAcO,EAAKc,aAAac,QACnCoQ,GAAMA,EAAE3R,UAAYL,EAAK2S,oBAC1B7O,OAEI8Q,EAAU,IACXZ,EACHnT,GAAIb,EAAK0S,YACTjT,cAEA6R,gBAAiB0B,EAAwBe,EAAuB,EAChEE,cAAejU,EAAK2S,kBACpBkC,UAAU,SAINrH,GAAetD,EAAG4K,cAAcH,KAAKzK,GAAKwD,EAA1CF,CAAkDoH,GAExDjB,EAAQzQ,KAAK,IAAK0R,EAAS3U,KAAM4T,GACnC,CA8DF,CACA,OAAOF,CAAO,E,wCCxHT,MAAMoB,GAqBXxO,YAAYyO,GApBZ,KAAQC,eAAkC,GAE1C,KAAQC,cAAgB,EAExB,KAAQC,kBAAoB,EAE5B,KAAQC,eAAiB,EAEzB,KAAQzN,UAAY,EAalBnB,KAAKwO,iBAAmBA,CAC1B,CAVW1N,eACT,MAAO,CACL+N,WAAY7O,KAAK0O,cACjBI,cAAe9O,KAAK2O,kBACpBC,cAAe5O,KAAK4O,cAExB,CAMOvJ,MAAMqJ,EAAuBvN,EAAY,GAO9C,OANAnB,KAAK0O,cAAgBA,EACrB1O,KAAKyO,eAAiB,GACtBzO,KAAK2O,kBAAoB,EACzB3O,KAAK4O,eAAiB,EACtB5O,KAAKmB,UAAYA,EAEVnB,KAAKc,QACd,CAEOjK,IAAIkY,GAGT,OAFA/O,KAAK0O,eAAiBK,EAEf/O,KAAKc,QACd,CAEOkO,cAAcC,GAOnB,GANAjP,KAAKkP,iBAAiBD,GAElBjP,KAAKyO,eAAenR,OAtDL,IAuDjB0C,KAAKyO,eAAeU,QAGlBnP,KAAKyO,eAAenR,OAAS,EAAG,CAClC,MAGM8R,EAHqBpP,KAAKqP,gCACNrP,KAAK0O,cAAgB1O,KAAK2O,mBACAM,GAIpDjP,KAAK2O,mBAAqBM,EAC1BjP,KAAK4O,cAAgB1F,KAAKoG,MAAMF,GAChCpP,KAAKwO,kBAAoBxO,KAAKwO,iBAAiBxO,KAAKc,SACtD,CAEA,OAAOd,KAAKc,QACd,CAEQoO,iBAAiBK,GACvBvP,KAAKyO,eAAe/R,KAAK,CAAE7C,UAAWC,KAAK0V,MAAOD,aACpD,CAEQF,8BACN,IAAII,EAAY,EACZC,EAAa,EAEjB,QAAS9R,EAAI,EAAGA,EAAIoC,KAAKyO,eAAenR,OAAQM,IAAK,CACnD,MAAM+R,EACJ3P,KAAKyO,eAAe7Q,GAAG/D,UAAYmG,KAAKyO,eAAe7Q,EAAI,GAAG/D,WAC1D,UAAE0V,GAAcvP,KAAKyO,eAAe7Q,GAE1C6R,GAAaE,EAAWJ,EACxBG,GAAcH,CAChB,CAEA,OAAsB,IAAfG,EAAmB,EAAID,EAAYC,CAC5C,ECyBF,OAjGA,MAuBE3P,YACElE,EACA+G,EACAmI,GASA,GA5BF,KAAU6E,gBAAkB,IAAIrB,GAEhC,KAAU3N,WAAa,IAAI,EAM3B,KAAUW,OAA4B,CACpCgL,UAAW,MAYXvM,KAAKnE,KAAOA,EAEZmE,KAAK6P,gBAAkB,IAAIC,gBAE3B9P,KAAK6C,UAAYlC,EAAgB9E,EAAMmE,KAAKY,YAC5CZ,KAAK+K,kBAAoBA,EACzB/K,KAAK+P,UAAW,QAAoB,CAAEC,OAAQ,OAAQC,OAAQpU,KACzD+G,EAAKsN,QACR,MAAM,IAAIhN,MAAM,0BAGlBN,EAAKW,YAAY/M,WAAWkN,IAC1B1D,KAAK0D,GAAKA,CAAE,IAGd1D,KAAK+K,kBAAoBA,EAEzB/K,KAAK4D,eAAiB5D,KAAKmQ,4BAA4BvN,GAEvD5C,KAAK4D,eAAepN,WAAWiP,IAC7BzF,KAAK+P,SAASK,KACZ,OAAOpQ,KAAKnE,UAAU4J,EAAgB,cAAgB,cAExDzF,KAAK6C,UAAUhC,WAAW4E,EAAgB,cAAgB,WAAW,IAGvEzF,KAAK4D,eACFJ,MAAK,EAAA6M,GAAA,IAAU,IAAMzN,EAAKsN,WAC1B1Z,WAAW+K,IACVvB,KAAKuB,OAASA,EACdvB,KAAK+P,SAASK,KAAK,OAAOpQ,KAAKnE,wBAAyB,CACtD3G,KAAMqM,GACN,IAINvB,KAAK4D,eACFJ,MACC,EAAApI,EAAA,IAAQqK,KAAoBA,KAC5B,EAAA4K,GAAA,IAAU,IAAMrQ,KAAKsQ,sBAAsB1N,EAAKsN,YAEjD1Z,WAAU,KACTwJ,KAAKuQ,SAAS,GAEpB,CAEUC,sBACRxQ,KAAK6P,gBAAkB,IAAIC,eAC7B,CAOUQ,sBAAsBJ,GAC9B,OAAOA,EAAQ1M,MACb,EAAA5L,EAAA,IAAK2J,GAAWA,EAAOgL,aACvB,EAAAkE,GAAA,IAAqB,CAACC,EAAYC,IAAcD,IAAeC,KAC/D,EAAA/Y,EAAA,IAAKgZ,KAAQA,KACb,EAAAxV,EAAA,IAAQwV,KAAQA,IAEpB,GCrGK,MAAMC,GAAwB,CACnCjN,EACAkN,EACAC,IAEAnN,EAAeJ,MACb,EAAAiN,GAAA,MACA,EAAAlL,EAAA,IAAKE,GAAkBsL,IAAWtL,MAClC,EAAArK,EAAA,IAAQ4V,GAAgBA,KACxB,EAAAX,GAAA,IAAU,IAAMS,KAChB,EAAAhL,EAAA,M,gBC+DJ,OAzEA,cAAsC,GAKpC/F,YACElE,EACA+G,EACAmI,GAEAkG,MAAMpV,EAAM+G,EAAMmI,GAPpB,KAAmBmG,eAAiB,IAAIC,EAAA,EAStC,MAAM7L,EAAUuL,GACd7Q,KAAK4D,eACL5D,KAAKkR,eAAe1N,MAClB,EAAA4N,GAAA,GAAU,OACV,EAAA7L,EAAA,IAAI,KAEFvF,KAAKwQ,qBAAqB,KAE5B,EAAAH,GAAA,IAAU,IACRrQ,KAAKqR,uBAAuB7N,MAC1B,EAAA6M,GAAA,IAAW7G,GACTxJ,KAAKsR,uBAAuB9H,GAAehG,MACzC,EAAA+B,EAAA,IAAI,IAAMvF,KAAK6C,UAAUhC,WAAW,aACpC,EAAAwP,GAAA,IAAWnb,IAAS,EAAAqF,GAAA,GAAKyF,KAAKuR,SAASrc,EAAM8K,KAAKuB,mBAM3DkE,IACC,GAAQtO,IAAI,OAAO0E,kBAAsB4J,GACzCzF,KAAK6C,UAAUhC,WAAW4E,EAAgB,cAAgB,WAAW,IAIzEH,EAAQ9O,UAAU,CAChBlE,KAAM,KACJ0N,KAAK6C,UAAUhC,WAAW,SAAS,EAErCpK,MAAQkO,IACN3E,KAAK6C,UAAUhC,WAAW,QAAS8D,EAAI,IAG3C3E,KAAKsF,QAAUA,CACjB,CAQOiL,UACLvQ,KAAK6P,iBAAiB2B,QACtBxR,KAAKkR,eAAe5e,OACpB,GAAQ6E,IAAI,OAAO6I,KAAKnE,sBAC1B,CAOOwJ,QAIL,OAHArF,KAAKsF,QAAQ9O,WAAU,SAGhBwJ,IACT,GCsRF,OA9SA,cAAmC,GACvBmQ,4BAA4BvN,GAepC,OAduB,EAAAiB,EAAA,GAAc,CACnCjB,EAAKW,YACLX,EAAKsN,QAAS1M,MACZ,EAAA5L,EAAA,IAAK2J,GAAWA,EAAOgL,aACvB,EAAAkE,GAAA,MAEFzQ,KAAK+K,kBAAmBnH,iBACvBJ,MACD,EAAA5L,EAAA,IACE,EAAEmM,EAAYwI,EAAWkF,OACrB1N,KAAgB0N,KAA0BlF,IAKpD,CAGU+E,uBACR9H,GAEA,MAAM,UAAE+C,GAAcvM,KAAKuB,OAC3BvB,KAAK+P,SAASK,KACZ,OAAOpQ,KAAKnE,kBAAkB0Q,WAAkB,SAC9C/C,MAIJ,MAAMkI,EAAYtG,GAA8B,CAC9CvQ,OAAQ0R,EACR/C,gBACA6B,MAAO,GACPC,eAAgB,OAChB5B,MAAO,MAGHiI,EXrDH,SACLC,EACAF,GAEA,MAKMG,EALS,IAAIC,GAAA,EAAa,CAC9BlH,KAAMlC,GACNqJ,MAAO,IAAI,OAGmBvb,UAAU,CAAEob,QAAOF,cACnD,OAAO,IAAItb,EAAA,GAAY4b,IACrB,MAAMpb,EAAeib,EAAiBrb,UAAU,CAC9ClE,KAAKiD,GACHyc,EAAW1f,KAAKiD,EAAOL,KACzB,EACAuB,MAAMkO,GACJqN,EAAWvb,MAAMkO,EACnB,EACAjO,WACEsb,EAAWtb,UACb,IAIF,MAAO,IAAME,EAAaE,aAAa,GAE3C,CW4BMmb,CACEvK,GACAgK,GACAlO,MACA,EAAA5L,EAAA,IAAKsa,IACI,CACLC,OAAQ,UACR7X,aAAc4X,EAASrG,oBAAoBjU,KAAKgG,GAC9C0I,GAA8BiG,EAAY3O,UAM9CwU,ECpGH,SACLzX,EACAiX,EACAza,GAEA,OAAO,IAAIf,EAAA,GAAY4b,IACrB,MAAMK,EAAK,IAAIC,UAAU,OA8BzB,OA5BAD,EAAGE,OAAS,KACVpb,EAAI,wBAAwB,cAAsBya,KAClDS,EAAGG,KACD1a,KAAKC,UAAU,CACb0a,QAAS,MACTjb,OAAQ,YACR6C,GAAI,IACJkH,OAAQ,CAAEqQ,WAEb,EAGHS,EAAGrd,UAAaC,IACd,MAAMmH,EAAUtE,KAAK6F,MAAM1I,EAAMC,MACjCiC,EAAI,WAAWwD,cAAqByB,GACpC4V,EAAW1f,KAAK8J,EAAQ7G,OAAO,EAGjC8c,EAAGK,QAAWzd,IACZkC,EAAI,WAAWwD,UAAiB,CAAElE,MAAOxB,IACzC+c,EAAWvb,MAAMxB,EAAM,EAGzBod,EAAGM,QAAU,KACXxb,EAAI,WAAWwD,YACfqX,EAAWtb,UAAU,EAGhB,KACL2b,EAAGvc,OAAO,CACX,GAEL,CD4D6B8c,CACvBrG,GCzGoC5R,ED0GV4R,ECzG9B,yCAAyC5R,OD0GrC,CAACyB,EAASyW,IAAQ7S,KAAK+P,SAASK,KAAKhU,EAAS,CAAE0W,KAAM,aAAcD,MACpErP,MACA,EAAApI,EAAA,IAAQlG,KAAU,KAAA6d,SAAQ7d,MAC1B,EAAA0C,EAAA,IAAK1C,IACI,CACLid,OAAQ,OACR7X,aAAcuN,GAA6B0E,EAAYrX,QCjHxB,IAACyF,EDsHtC,OAAO,EAAAqY,GAAA,GACLrB,EACAS,EAEJ,CAEUf,uBACR,OAAO,EAAA4B,GAAA,IAAM,KAAM,EAAA1Y,GAAA,GAAKyF,KAAKkT,aAE/B,CAEA,iBACE,MAAM,UAAE3G,GAAcvM,KAAKuB,QACrB,OAAE2F,GAAWlH,KAAK6P,gBAClBzC,QAAiBpN,KAAK0D,GAAIyP,cAAc5G,EAAYA,GAEpD6G,QAAiCpT,KAAKqT,iBAC1C9G,EACAA,EACAa,GAGFpN,KAAK6C,UAAUhC,WAAW,cAAe,iBACzC,MAAMyS,QAAwBhH,GAC5BtM,KAAK0D,GACL6I,EACAa,EAAStC,gBACT5D,GAMF,OAHAlH,KAAKY,WAAWL,gBAAgB+S,GAChCtT,KAAK6C,UAAUhC,WAAW,UAEnBuS,CACT,CAEA,gBACE,OAAEjB,EAAM,aAAE7X,GACViH,GAEA,MAAM,UAAEgL,GAAchL,GAChB,OAAE2F,GAAWlH,KAAK6P,gBACxB,GAA4B,IAAxBvV,EAAagD,OAEf,YADA0C,KAAK+P,SAASK,KAAK,OAAOpQ,KAAKnE,QAAQ0Q,wBAGzC,MAAMa,QAAiBpN,KAAK0D,GAAIyP,cAAc5G,EAAYA,SAEpDvM,KAAKuT,yBACThH,EACAA,EACAjS,EACA8S,EACA+E,GAGFnS,KAAK6C,UAAUhC,WAAW,cAAe,iBACzC,MAAMyS,QAAwBhH,GAC5BtM,KAAK0D,GACL6I,EACAa,EAAStC,gBACT5D,EACW,SAAXiL,GAGFnS,KAAKY,WAAWL,gBAAgB+S,GAChCtT,KAAK6C,UAAUhC,WAAW,SAC5B,CAEA,+BACE0L,EACA5R,EACAL,GACA,cAAEmT,EAAa,YAAExU,EAAW,gBAAE6R,GAC9BqH,GAEA,MAAM,OAAEjL,GAAWlH,KAAK6P,gBAIlBrD,EAAmC,SAAX2F,EAE9BnS,KAAK+P,SAASK,KACZ,iCAAiCzV,KAAWwX,eAC1C7X,EAAagD,iBACJhD,EAAagT,GAAG,IAAIzT,kBAC7BS,EAAagT,IAAI,IAAIzT,mBAKnBmN,GAAehH,KAAK0D,GAAI8P,gBAAiBtM,EAAzCF,CAAiD1M,GAGvD0F,KAAKyT,UAAUnZ,EAAc4M,GAE7B,MAAM,KACJjN,EAAI,MACJuM,EAAK,UAEL3M,GACES,EAAagT,IAAI,GAEfoG,EAAoB7Z,EAGpB8Z,EAAc,CAClBnZ,QAAS+R,EACThT,UAAWd,EAAU6B,aACrBD,GAAIM,EACJmQ,gBAAiB0B,EACbkH,EACA5I,EACJ7R,YAAaA,EAAeqB,EAAagD,OACzCmQ,cAAeA,GAAiB,EAChCY,UAAU,EACV5U,KAAM,CACJO,gBAAiBC,EACjBuM,UAMJ,aAFMQ,GAAehH,KAAK0D,GAAI4K,cAAepH,EAAvCF,CAA+C2M,GAE9CD,CACT,CAEA,uBACEnH,EACA5R,EACAyS,GAEA,MAAM,YAAEnU,EAAW,gBAAE6R,GAAoBsC,EACnC5D,EAAgBsB,EAAkB,EAExC9K,KAAK6C,UAAUhC,WAAW,cAE1B,MAAM+S,OP7LmCnf,OAC3CkG,EACA6O,EACAH,KAEA,MAAMuC,QAAYxC,GAAoBC,GAAaI,QAGjDjC,GAAgC,CAChC7M,QAAS,IAAIA,KACbd,WAAW,SAAgB2P,KAG7B,OAAOoC,GAAKiI,8BAA8BC,WAAWC,KAAK,EOgLxBC,CAC9BrZ,EACA6O,EACAxJ,KAAK6P,gBAAiB3I,QAOxB,GAJAlH,KAAK+P,SAASK,KACZ,gCAAgCzV,cAAoBiZ,YAA4BpK,KAGxD,IAAtBoK,EACF,OAAOpK,EAGTxJ,KAAK6C,UAAUhC,WACb,cACA,QAAQlG,OACRqF,KAAK4P,gBAAgBvK,MACnB6D,KAAK+K,KAAKL,ETtRe,OS0R7B,MAAMM,EPnM+B,GACvCrZ,SACA2O,gBACA6B,QACAC,iBACA5B,QACAL,iBAEAhI,GAAsBsK,GAAmB,CACvC9Q,SACA2O,gBACA6B,QACAC,iBACA5B,QACAL,gBOqLkC8K,CAA0B,CAC1DtZ,OAAQF,EACR6O,gBACA6B,MAAO,GACPC,eAAgB,MAChB5B,MT/R2B,ISgS3BL,YAAarJ,KAAK6P,iBAAiB3I,SAGrC,IAAIkN,EAAmB,EACnBV,EAAoBlK,EAGxB,gBAAiBpI,KAAS8S,EAA2B,CACnDlU,KAAK6C,UAAUhC,WACb,cACA,QAAQlG,OACRqF,KAAK4P,gBAAgBZ,cAAc,IAGrCoF,GAAoBhT,EAAM9D,OAE1B,MAAMhD,EAAe8G,EAAMxJ,KAAKgG,GAC9B0I,GAA8B3L,EAASiD,KAGzC8V,QAA0B1T,KAAKuT,yBAC7BhH,EACA5R,EACAL,EACA,IACK8S,EACHnU,YAAaA,EAAcmb,GAE7B,UAEJ,CAEA,OAAOV,CACT,CAEA,gBAAwBtS,EAAyB8F,GAC/C,MAAM,OAAEmN,EAAM,eAAEC,EAAc,MAAE5J,GR1Q7B,SAAyCtJ,GAC9C,MAAM+I,EAAa/I,EAAMhG,QACtBmZ,GLvCsC,qCKuChCA,EAAErc,OAELoc,EAAiB,IAAI3J,IACrBD,EAAmB,GAuBzB,MAAO,CACL2J,OAtB2ClK,EAAW/M,QAEtD,CAACC,GAAOlI,QAAO8E,OAAMJ,gBACpB1E,EAAyBuV,MAAMrS,SAASuS,IACvC0J,EAAezd,IAAI+T,EAAKlR,IACxB4a,EAAezd,IAAI+T,EAAKrQ,MACxB,MAAMia,EAAS,IACV5J,EACH/Q,YACAgB,OAAS1F,EAAyB0F,OAClCb,gBAAiBC,GAEnByQ,EAAMhO,KAAK8X,GAEP5J,EAAKrQ,OAAS8H,KAChBhF,EAAImX,EAAO9a,IAAM8a,EACnB,IAEKnX,IACN,CAAC,GAIFiX,eAAgB,IAAIA,GACpB5J,QAEJ,CQ0OM+J,CAAgCrT,GAC9BsJ,EAAMpN,OAAS,SACX0D,EACJ0J,GACCA,GAAU1D,GAAehH,KAAK0D,GAAIgR,cAAexN,EAAvCF,CAA+C0D,InBlU7B,KmBuUjC,MAAMiK,EAAiBxc,OAAOC,KAAKic,GAE7BO,EAAoBN,EAAelZ,QACtC4G,IAAS2S,EAAeE,SAAS7S,WAI9BhC,KAAK+K,kBAAmBI,aAC5BwJ,EACAhc,EAAiBgB,SACjBrF,EAAcwgB,MAIZF,EAAkBtX,OAAS,SACvB0C,KAAK+K,kBAAmBI,aAC5ByJ,EACAjc,EAAiBgB,SACjBrF,EAAcygB,IAGpB,GEhWK,MAAMC,GAAgBC,GAC3BA,EAAIC,QAAQ,gBAAiBC,GAC3BA,EAAMC,cAAcF,QAAQ,IAAK,IAAIA,QAAQ,IAAK,MAQ/C,SAASG,GACdC,GAEA,IAAKA,GAAgC,iBAAbA,EACtB,OAAOA,EAET,MAAMC,EAA2B,CAAC,EAelC,OAdApd,OAAOC,KAAKkd,GAAUjd,SAASmd,IAC7B,GAAIrd,OAAOsd,UAAUC,eAAeC,KAAKL,EAAUE,GAAM,CACvD,MAAMI,EAAeZ,GAAaQ,GAClC,IAAIrgB,EAAQmgB,EAASE,GACjBK,MAAMC,QAAQR,EAASE,IACzBrgB,EAAQmgB,EAASE,GAAK5d,KAAK0B,GAAS+b,GAAY/b,KACd,iBAAlBgc,EAASE,GACzBrgB,EAAQkgB,GAAYC,EAASE,IACK,iBAAlBF,EAASE,KACzBrgB,EAA0BA,EnBT7B+f,QAAQ,OAAQ,MAChBA,QAAQ,OAAQ,MAChBA,QAAQ,OAAQ,KAChBA,QAAQ,QAAS,KACjBA,QAAQ,QAAS,MACjBA,QAAQ,QAAS,MmBMhBK,EAAIK,GAAgBzgB,CACtB,KAEKogB,CACT,CCzBO,SAASQ,GACdrL,EACAlQ,EACAwb,EAAoB,EACpBtI,EAAkB,GAElB,MAAMuI,EAAkBvL,EAAMtP,QAC3BwP,GAASA,EAAK/Q,UAAYmc,IAEvBE,GAAkB,KAAAC,eACtBF,GACCrL,GAASA,EAAK/P,SAAWL,IAGtBvB,EACJid,EAAkB,EACdxI,EAAkBuI,EAAgB3Y,OAClC2Y,EAAgB3Y,OAAS4Y,EAAkB,EAKjD,MAAO,CACLzI,cAHAyI,EAAkB,EAAIF,EAAoBtL,EAAMwL,GAAiBrc,UAIjEZ,cAEJ,CAEO,SAASmd,GACdC,EACA3L,EACAlQ,EACAgS,GAAwB,GAExB,MAAM,cAAEiB,EAAa,YAAExU,GAAgB8c,GACrCrL,EACAlQ,EACA6b,EAAW5I,cACX4I,EAAWpd,aAGPqd,EAAWjB,GAAY3K,EAAMA,EAAMpN,OAAS,IAC5CwN,EAAkBwL,EAASzc,UACjC,MAAO,IACFwc,EACH7b,UACAjB,UAAWd,EAAUkB,SACrB0U,UAAU,EACVpV,cACAQ,KAAM,IACD6c,EACHzc,UAAWiR,GAEb2C,gBACA3C,gBAAiB0B,EACb1B,EACAuL,EAAWvL,gBAEnB,CAEA,MCtEayL,GAAoBC,GAC/BA,aAAapP,cAA2B,eAAXoP,EAAE3a,K,4ECsFjC,OA3EA,cAAoC,GAKlCkE,YACElE,EACA4a,EACA7T,EACAmI,GACA,SACE2L,GAGE,CAAEA,SAAU,IAEhBzF,MAAMpV,EAAM+G,EAAMmI,GAElB,MAAM,MAAE5G,EAAK,YAAEwS,GCPiB,EAClC/S,EACAkN,EACA8F,EAAiC,CAAC,KAElC,MAAM,WACJH,EAAU,SACVC,EAAW,EAAC,gBACZG,EAAe,QACfC,EAAO,aACPC,EAAe,EAAC,SAChBhG,GACE6F,EAEEI,EAAkB,IAAI7F,EAAA,EAEtB8F,EAAqBD,EAAgBxT,MACzC,EAAA4N,GAAA,GAAU,OACV,EAAAf,GAAA,IAAU,KAAM,EAAA6G,GAAA,GAAST,GAAYjT,MAAK,EAAA4N,GAAA,GAAU,IAAI,EAAA+F,GAAA,GAAMT,OAsBhE,MAAO,CACLvS,MApBc0M,GACdjN,EACAqT,EAAmBzT,MACjB,EAAA+B,EAAA,IAAI,IAAMsR,GAAmBA,OAC7B,EAAAO,GAAA,IAAW,IACTtG,EAAkBtN,MAChB,EAAA6T,GAAA,GAAM,CACJF,MAAQ1gB,IACN,GAAQU,IAAI,QAASV,GACrBqgB,GAAWA,EAAQrgB,IACZ,EAAAygB,GAAA,GAASH,WAMzBtR,GAAkBsL,IAAWtL,KAK9BkR,YAAa,KAGXK,EAAgB1kB,MAAM,EAEzB,EDxCgCglB,CAC7BtX,KAAK4D,gBAEL,EAAAqP,GAAA,IAAM,KAAM,EAAA1Y,GAAA,GAAKyF,KAAKuX,YACtB,CACEd,aACAC,WAEAI,QAAUrgB,IACRuJ,KAAK+P,SAASK,KAAK,OAAOvU,UAAcpF,EAAMmO,YAC9C5E,KAAK6C,UAAUhC,WAAW,QAASpK,EAAMmO,WAAW,EAEtDmM,SAAWtL,IACTzF,KAAK+P,SAASK,KAAK,OAAOvU,kBAAqB4J,KAC/CzF,KAAK6C,UAAUhC,WAAW4E,EAAgB,cAAgB,WAAW,IAK3EzF,KAAKmE,MAAQA,EACbnE,KAAK2W,YAAcA,CACrB,CAEOpG,UACLvQ,KAAK6P,iBAAiB2B,QACtBxR,KAAK2W,gBACL3W,KAAK+P,SAASK,KAAK,OAAOpQ,KAAKnE,oBACjC,CAEOwJ,QAEL,OADArF,KAAKmE,MAAM3N,WAAU,IAAMwJ,KAAK6C,UAAUhC,WAAW,YAC9Cb,IACT,CAEA,eACE,MAAMuB,GAAS,UAAMvB,KAAKuB,QAC1BvB,KAAKwQ,sBACL,UACQxQ,KAAKwX,KAAKjW,EAClB,CAAE,MAAOiV,GACP,MAAMiB,EAAYlB,GAAiBC,GAQnC,GAPAxW,KAAK+P,SAASK,KACZ,OAAOpQ,KAAKnE,QAAQ0F,EAAOgL,8BAA8BkL,MACzD,CACEhhB,MAAO+f,KAINiB,EACH,MAAMjB,CAEV,CACF,GE2HF,OAlLA,cAAgC,GACpBrG,4BAA4BvN,GAmBpC,OAlBuB,EAAAiB,EAAA,GAAc,CACnCjB,EAAKW,YACLX,EAAKkB,cACLlB,EAAKsN,QAAS1M,MACZ,EAAA5L,EAAA,IAAK2J,GAAWA,EAAOgL,aACvB,EAAAkE,GAAA,MAEFzQ,KAAK+K,kBAAmBnH,iBACvBJ,MACD,EAAA5L,EAAA,IACE,EAAEmM,EAAYC,EAAcuI,EAAWmL,QACnC1T,GACAD,GACA2T,GACAnL,KAKV,CAEA,WAAqBhL,GACnB,MAAM,UAAEgL,GAAchL,GAChB,OAAE2F,GAAWlH,KAAK6P,gBACxB7P,KAAK6C,UAAUhC,WAAW,cAE1B,MAAM8W,QAA0B3X,KAAK0D,GAAIgJ,eAAe,CACtDlS,QAAS+R,EACThT,UAAWd,EAAUkB,WAGjBmR,EAAkB6M,EAAkBrK,GAAG,IAAIxC,iBAAmB,EAG9D8M,OjBfmBnjB,OAC3BkG,EACA0P,EACAb,EACAH,KAEA,MAAMuC,QAAYxC,GAAoBC,GAAaI,QAGjDlC,GAAiC,CACjC5M,UACAkd,eAAgBxN,EAChBxQ,WAAW,SAAgB2P,KAG7B,OAAOoC,EAAIkM,qBAAqBhE,WAAWC,KAAK,EiBAnBgE,CACzBxL,EACA,CAAClK,IACDyI,EACA5D,GAaF,GAVAlH,KAAK+P,SAASK,KACZ,uBAAuB7D,WAAmBqL,KAE5C5X,KAAK4P,gBAAgBvK,MAAMuS,EAAeD,EAAkBra,QAC5D0C,KAAK6C,UAAUhC,WACb,cACA,eACAb,KAAK4P,gBAAgB9O,UAGnB8W,EAAe,EAAG,CAEpB,MAAMI,QAA6BhY,KAAKiY,eACtC1L,EACAzB,EACA5D,GAIFyQ,EAAkBjb,QAAQsb,EAC5B,OACMhY,KAAKkY,cAAc3L,EAAYoL,EAAmBzQ,EAC1D,CAEA,qBACEqF,EACAzB,EACA5D,GAEA,MAAMiR,QAA4B3N,GAChC+B,EACA,CAAClK,IACDyI,EhBrGyB,IgBuGzB9K,KAAK6P,iBAAiB3I,QAGlBkR,EAA6B,GAC7BC,QAA0BrY,KAAK0D,GAAIgJ,eAAe,CACtDlS,QAAS+R,EACThT,UAAWd,EAAUkB,WAEjB2e,EAAuB,IAAItV,IAC/BqV,EAAkBzgB,KAAKgG,GAAM,CAACA,EAAEvD,GAAIuD,MAGtC,gBAAiB2a,KAAeJ,EAAqB,CACnDnY,KAAK6C,UAAUhC,WACb,cACA,yBACAb,KAAK4P,gBAAgBZ,cAAc,IAErC,MAAMwJ,EAAqBD,EAAY3gB,IAAIyd,IAAazd,KAAK0B,IAC3D,MAAM,UAAEO,EAAS,GAAEH,GAAOJ,EACpBwR,GAAkB,EAAAjE,GAAA,IAAgBhN,GAGlC4e,EAAoBH,EAAqB/hB,IAAImD,IAC/C,EAAAmN,GAAA,IAAgBhN,GAChB,EAGJ,MAAO,CACLW,QAAS+R,EACTlS,GAAIX,EACJH,UAAWd,EAAUkB,SACrBmR,gBAAiB2N,EACjBhL,cAAe3C,EACf7R,YAAa,EACboV,UAAU,EACV5U,KAAM,IAAKH,EAAMO,UAAWiR,GAC7B,IAGC0N,EAAmBlb,OAAS,UACxB0J,GACJhH,KAAK0D,GAAI4K,cACTpH,EAFIF,CAGJwR,GACFJ,EAAU1b,QAAQ8b,GAEtB,CAEA,OAAOJ,CACT,CAEA,oBACE7L,EACAE,EACAvF,GAEA,MAAMwR,EAAoC,GAG1C,UAAWtL,KAAYX,EAAW,CAChC,MAAM,GAAEpS,EAAE,gBAAEyQ,GAAoBsC,EAEhCpN,KAAK6C,UAAUhC,WACb,cACA,4BACAb,KAAK4P,gBAAgBZ,cAAc,IAGrC,MAAM2J,QAAqB9N,GACzBxQ,EACAyQ,EACA9K,KAAK+K,kBACLzW,EAAc8N,OACdpC,KAAK6P,iBAAiB3I,QAGxB,GAAIyR,EAAarb,OAAS,EAAG,CAC3B,MAAMoN,EAAQiO,EAAa/gB,IAAImP,UAIzB/F,EACJ0J,GACCA,GAAU1D,GAAehH,KAAK0D,GAAIgR,cAAexN,EAAvCF,CAA+C0D,I1BrL/B,K0ByL7B,MAAM0D,EAAUgI,GAAyBhJ,EAAU1C,EAAO6B,GAE1DmM,EAAiBhc,KAAK0R,EACxB,CACF,CAEIsK,EAAiBpb,OAAS,SACtB0J,GAAehH,KAAK0D,GAAI4K,cAAepH,EAAvCF,CAA+C0R,GAEvD1Y,KAAKY,WAAWL,gBAAgBmY,EAClC,GC0BF,OAlMA,cAAgC,GAG9B3Y,YACElE,EACA4a,EACA7T,EACAmI,GACA,SAAE2L,GAAmC,CAAEA,SAAU,IAEjD,IAAK9T,EAAKgW,YACR,MAAM,IAAI1V,MAAM,2BAGlB+N,MAAMpV,EAAM4a,EAAY7T,EAAMmI,EAAmB,CAC/C2L,aAdJ,KAAUmC,WAA8B,EAgBxC,CAEU1I,4BAA4BvN,GACpC,MAAMkW,EAAyB,IAAI/V,EAAA,GAAyB,GAC5DH,EAAKsN,SACD1M,MACA,EAAA5L,EAAA,IAAK2J,GAAWA,EAAOgL,aACvB,EAAAkE,GAAA,MAEDja,WAAU,KACTsiB,EAAuBxmB,MAAK,EAAM,IAGtCsQ,EAAKgW,YAAapiB,WAAWqiB,IAC3B7Y,KAAK6Y,WAAaA,EAClBC,EAAuBxmB,MAAK,GAE5B0N,KAAKuQ,SAAS,IAkBhB,OAfuB,EAAA1M,EAAA,GAAc,CACnCjB,EAAKW,YACLX,EAAKsN,QACLlQ,KAAK+K,kBAAmBnH,eACxBkV,IACCtV,MACD,EAAA5L,EAAA,IACE,EAAEmM,EAAYxC,EAAQkQ,EAAsBsH,OACxChV,KACAxC,EAAOgL,aACPkF,GACFsH,IAKR,CAEA,WAAqBxX,GACnB,MAAM,OAAE2F,GAAWlH,KAAK6P,gBAExB7P,KAAK6C,UAAUhC,WAAW,cAAe,gBACzC,MAAM,UAAE0L,GAAchL,GAEhB,WAAEsX,GAAe7Y,KAEvBA,KAAK6C,UAAUhC,WAAW,cAE1Bb,KAAK+P,SAASK,KACZ,qBAAqB7D,WAAmBsM,EAAWvb,SACnD,CACEwV,KAAM,eACN5d,KAAM2jB,IAIV7Y,KAAK4P,gBAAgBvK,MAAMwT,EAAWvb,QACtC0C,KAAK6C,UAAUhC,WACb,cACA,UACAb,KAAK4P,gBAAgB9O,UAIvB,UAAWiL,KAAQ8M,QAEX7Y,KAAKyT,UAAUlH,EAAYR,EAAM7E,EAE3C,CAEA,gBACEqF,EACA5R,EACAuM,GAEA,IAAI8R,EAAc,GAClB,IACEhZ,KAAK6C,UAAUhC,WACb,cACA,iBAAiBlG,OACjBqF,KAAK4P,gBAAgB9O,UAEvB,MAAM,cAAE2M,EAAa,YAAExU,EAAW,KAAEQ,SAAeuG,KAAK0D,GAAIyP,cAC1D5G,EACA5R,IAGI,oBAAEmT,EAAsB,EAAC,uBAAED,EAAyB,GACxDpU,GAAQ,CAAC,EAEL+P,EAAgBqE,EAAyB,EAEzCoL,QAA2BzO,GAC/B7P,EACA4H,GACAiH,EjBjJuB,IiBmJvBtC,GAIF,gBAAiBgS,KAAcD,EAAoB,CACjDjZ,KAAK6C,UAAUhC,WACb,cACA,QAAQlG,OACRqF,KAAK4P,gBAAgBZ,cAAc,IAGrC,MAAMtE,EAAQwO,EAAWthB,IAAImP,KAErB0G,cAAe0L,EAAkBlgB,YAAamgB,GACpDrD,GAAgBrL,EAAO6B,EAAWkB,EAAexU,GAInD,GAAIyR,EAAMpN,OAAS,EAAG,CACpB,MAAMgZ,EAAWjB,GAAY3K,EAAM4C,IAAI,IACjC+L,EAA4B/C,EAAUzc,gBAEtCmN,GAAehH,KAAK0D,GAAIgR,cAAexN,EAAvCF,CAA+C0D,GAErD,MAAMvR,EAAYuR,EAAM9S,KAAK4T,GAAMA,EAAE9R,WAC/BsG,KAAK+K,kBAAmBI,aAC5BhS,EACAR,EAAiBgB,SACjBrF,EAAcwgB,MAGhB,MAAMnB,EAAc,CAClBnZ,QAAS+R,EACThT,UAAWd,EAAUe,KACrBa,GAAIM,EACJmQ,gBAAiB5B,KAAK0E,IACpByL,EACAvL,GAEF7U,YAAamgB,EACb3L,cAAe0L,EACf9K,UAAU,EACV5U,KAAM,IACD6c,EACHzI,uBAAwBwL,EACxBvL,8BAIE9G,GAAehH,KAAK0D,GAAI4K,cAAepH,EAAvCF,CAA+C2M,GAErDqF,EAAYtc,KAAKiX,EACnB,CACF,CACF,CAAE,MAAOhP,GAIP,GAHA3E,KAAK+P,SAAStZ,MAAM,qBAAqBkE,UAAiB,CACxDlE,MAAOkO,IAEJ4R,GAAiB5R,GAIpB,MADAqU,EAAc,GACRrU,EAHN3E,KAAK6C,UAAUhC,WAAW,QAAS8D,EAAIC,WAK3C,CAAE,QAEA5E,KAAKY,WAAWL,gBAAgByY,EAClC,CACF,G,iCCvMF,MAAMjJ,IAAW,QAAoB,CACnCC,OAAQ,OACR8C,KAAM,6BAIKwG,GAA4B,CACvCC,EACA5e,EACA6e,EACAtS,IAEO,IAAI9Q,EAAA,GAAiC4b,IAC1CA,EAAW1f,KAAK,CAAE4J,OAAQ,QAAS+E,MAAO,KAE1C,WACE,MAAMwY,QAAwBF,EAAMG,aAAa/e,GAEjDqX,EAAW1f,KAAK,CAAE4J,OAAQ,MAAO+E,MAAOwY,IAExC,MAAME,EAAsB,IAAI3W,IAC9ByW,EAAgB7hB,KAAKgiB,GAAM,CAACA,EAAEjgB,SAAUigB,MAGpCC,EAAwB7X,GAC5B2X,EAAoBpjB,IAAIyL,IAAQ,CAC9BxH,QAASG,EACTkB,KAAM,GACNie,WAAW,EACXC,UAAU,GAGRC,OCxCmBvlB,OAC7BkG,EACAuM,KAaA,MAAMgL,QAAiB,EAAA+H,GAAA,GAAM,CAC3BziB,OAAQ,MACRmR,IAAK,GAAG,8BAAgChO,4BAAkC2H,sBAC1E4E,WAGF,OAAKgL,EAAShd,KAAKglB,IAGZhI,EAAShd,KAAKglB,IAAItiB,KACtB0B,GAASA,EAAKiN,GAAGpR,MAAMyG,IAAI,GAAGzG,MAAMuV,MAAM,GAAGhR,KAHvC,EAIR,EDc6B,CAAgBiB,EAASuM,GAC7CiT,OCZgB1lB,OAC1BkG,EACAuM,KAEA,MAAMkT,QAAoB,QAAYzf,GAEhCuX,QAAiB,EAAA+H,GAAA,GAAM,CAC3BziB,OAAQ,MACRmR,IAAK,GAAG,oCAAsCrG,2BAAmC8X,qBACjFlT,WAGF,OAAKgL,EAAShd,KAAKglB,IAGZhI,EAAShd,KAAKglB,IAAItiB,KAAK0B,GAASA,EAAKiN,GAAGpR,MAAMyG,IAAI,GAAGzG,MAAM0F,SAFzD,EAEgE,EDH7C,CAAaF,EAASuM,GAExCmT,EAAkBL,EAAY5e,QACjC4G,IAASyX,EAAgB9d,MAAMiC,GAAMA,EAAEjE,WAAaqI,GAAOpE,EAAEkc,cAG1DQ,EAAsBH,EAAU/e,QACnC2Q,IAAU0N,EAAgB9d,MAAMiC,GAAMA,EAAE/C,SAAWkR,GAAQnO,EAAEmc,aAGhEhK,GAASK,KACP,uBAAuBzV,wBAA8B8e,EAAgBnc,yBAAyB+c,EAAgB/c,yBAAyBgd,EAAoBhd,UAG7J,MAAMid,QAA2BzlB,QAAQgQ,IACvCwV,EAAoB1iB,KAAInD,MAAOoG,IAC7B,MAAMmH,QAAY,QAAYnH,GAExB2f,EAAgB,IACjBX,EAAqB7X,GACxBrI,SAAUqI,EACVnH,SACAkf,UAAU,GAKZ,aAFMR,EAAMkB,aAAaD,GACzBb,EAAoBxjB,IAAI6L,EAAKwY,GACtBA,CAAa,KAIxBxI,EAAW1f,KAAK,CAAE4J,OAAQ,MAAO+E,MAAOsZ,UAElCzlB,QAAQgQ,IACZuV,EAAgBziB,KAAInD,MAAOuN,IACzB,MAAMnH,SAAgB2e,EAAoBxX,EAAK1N,EAAcomB,UACzDnlB,QAAQkN,YACZ,GAAI5H,GAAUA,EAAOrC,MAAM,OAAgB,CACzC,MAAMgiB,EAAgB,IACjBX,EAAqB7X,GACxBnH,SACAlB,SAAUqI,EACV8X,WAAW,SAGPP,EAAMkB,aAAaD,GACzBb,EAAoBxjB,IAAI6L,EAAKwY,GAC7BxI,EAAW1f,KAAK,CAAE4J,OAAQ,MAAO+E,MAAO,CAACuZ,IAC3C,MAIJzK,GAASK,KAAK,uBAAuBzV,WAMrCqX,EAAW1f,KAAK,CAAE4J,OAAQ,WAAY+E,MAAO,KAE7C+Q,EAAWtb,UACZ,EA/ED,GA+EK2N,OAAOM,IACVoL,GAAStZ,MAAM,uBAAuBkE,WAAkB,CAAElE,MAAOkO,IACjEqN,EAAWvb,MAAMkO,EAAI,GACrB,I,gBEhBN,OApFA,MAOE5E,YACEgL,EACAxH,GARF,KAAQtD,QAAU,IAAIC,iBAAiBJ,GAUrCE,KAAK+K,kBAAoBA,EACzBxH,EAAY/M,WAAWoa,IACrB5Q,KAAKuD,YAAYjR,KAAKse,EAAE,IAE1B5Q,KAAKuD,YAAc,IAAIR,EAAA,OAA+B,GAEtD/C,KAAKC,QAAQjL,UAAaC,GAAU+K,KAAK2a,UAAU1lB,GAEnD+K,KAAKC,QAAQ2a,eAAkB3lB,GAC7B,GAAQwB,MAAM,GAAGqJ,UAA2B7K,EAChD,CAEA,wBACE,OAAO,IAAIH,SAASC,IAClB,MAAMwkB,EAAQvZ,KAAKuD,YAAYW,WAC3BqV,GACFxkB,EAAQwkB,GAGVvZ,KAAKuD,YACFC,MACC,EAAAC,EAAA,IAAOtO,QAAoB,IAAVA,KAElBqB,WAAWrB,IACVJ,EAAQI,EAAe,GACvB,GAER,CAEA,gBAAwBuV,GACtB,MAAM6O,QAAcvZ,KAAK6a,wBACPtB,EAAM7E,cAAchK,EAExC,CAEA,oBAA4B7I,GAC1B,IACE,MAAM0X,QAAcvZ,KAAK6a,kBACnBC,E3BrDuB,CAACnhB,IAClC,MAAM,IAAEqI,EAAG,KAAEvI,EAAI,YAAEgJ,GAAgB9I,GAC7B,KAAE2J,EAAI,KAAEyX,EAAI,KAAE7iB,EAAI,OAAE8iB,EAAM,UAAEC,GAAcxhB,EDR3C,IAAuB2M,ECe5B,MAAO,CACLpE,MACAsB,KAAMA,GAAQ,EACdyX,KAAMA,GAAQ,UACd7iB,OACAoM,KATW7B,GDXe2D,EA8BvB,SAAkC8U,GAEvC,IAAI5W,EAAO4W,EAAShG,QAAQ,eAAgB,IAiC5C,OA9BA5Q,EAAOA,EAAK4Q,QAAQ,oBAAqB,MACzC5Q,EAAOA,EAAK4Q,QAAQ,iBAAkB,MACtC5Q,EAAOA,EAAK4Q,QAAQ,eAAgB,MAGpC5Q,EAAOA,EAAK4Q,QAAQ,yBAA0B,MAC9C5Q,EAAOA,EAAK4Q,QAAQ,kBAAmB,IAGvC5Q,EAAOA,EAAK4Q,QAAQ,iBAAkB,IAGtC5Q,EAAOA,EAAK4Q,QAAQ,oBAAqB,MAGzC5Q,EAAOA,EAAK4Q,QAAQ,qBAAsB,MAG1C5Q,EAAOA,EAAK4Q,QAAQ,YAAa,IAGjC5Q,EAAOA,EAAK4Q,QAAQ,iBAAkB,IAGtC5Q,EAAOA,EAAK4Q,QAAQ,iBAAkB,IAGtC5Q,EAAOA,EAAK4Q,QAAQ,UAAW,QAC/B5Q,EAAOA,EAAK4Q,QAAQ,aAAc,IAE3B5Q,CACT,CCtDoB6W,CAAyB1Y,GDXpC2D,EAAO8O,QAAQ,KAAM,MCYxB,GAQFkG,WAAYH,IAAc,EAC1BD,OAAQA,GAAU,EACnB,E2BoCkBK,CAAoBxZ,UACd0X,EAAM+B,aAAaR,IAC7BS,UACH3Z,GAA8BC,EAExC,CAAE,MAAO2U,GAOP,MANA,GAAQrf,IACN,oBACA0K,EACAA,EAAQY,YACR+T,EAAE5R,YAEE4R,CACR,CACF,CAEA,iBAAyBthB,SAEjB8K,KAAK6a,kBAEX7a,KAAK+K,kBAAkBrJ,QAAQmU,MAAMC,QAAQ5gB,GAAQA,EAAO,CAACA,GAC/D,CAEQylB,UAAU/e,GAChB,MAAM,KAAE1D,EAAI,KAAEhD,GAAS0G,EAAI1G,KACd,SAATgD,EACF8H,KAAKwb,UAAUtmB,GACG,aAATgD,EACT8H,KAAKyb,cAAcvmB,GACD,SAATgD,GACT8H,KAAK0b,WAAWxmB,EAEpB,GCvEF,MAAM,IAAW,QAAoB,CAAE8a,OAAQ,SAGxC,MAAM2L,GAOX5b,YAAY6C,GAJZ,KAAQhC,WAAa,IAAI,EAEzB,KAAQgb,MAAsD,CAAC,EAG7D,MAAM,YAAErY,EAAW,cAAEO,GAAkBlB,EAEjCmI,EAAoB,IAAI,GAAuBnI,GAAMyC,QAErC,IAAIwW,GACxB9Q,EACAxH,GAGFvD,KAAK4D,gBAAiB,EAAAC,EAAA,GAAc,CAACN,EAAaO,IAAgBN,MAChE,EAAA5L,EAAA,IAAI,EAAEmM,EAAYC,OAAoBD,KAAgBC,KAGxDhE,KAAK4D,eAAepN,UAAU,CAC5BlE,KAAOiD,GACEA,GAAUyK,KAAKY,WAAWT,kBAAkB,OAAQ,WAE7D1J,MAAQkO,GAAQ3E,KAAKY,WAAWT,kBAAkB,OAAQ,QAASwE,KAGrE,MAAMmX,ECpCK,SACblZ,GAEA,MAAM,YAAEW,EAAW,cAAEO,EAAa,QAAEoM,GAAYtN,EAC1C3C,EAAU,IAAI,EAEpB,OAAO,EAAA4D,EAAA,GAAc,CACnBN,EACA2M,EAAS1M,MACP,EAAA5L,EAAA,IAAK2J,GAAWA,EAAOgL,aACvB,EAAAkE,GAAA,MAEF3M,IACCN,MACD,EAAApI,EAAA,IACE,EAAE2I,EAAYwI,EAAWvI,OACrBD,KAAgBC,KAAkBuI,KAExC,EAAA8D,GAAA,IAAU,EAAEkJ,EAAOhN,EAAWvI,MAC5B,MAAM,uBAAEf,GAA2BL,EACnC,IAAImZ,EAA4B,GAChC,OAAO,IAAI3lB,EAAA,GAA4BC,IACrCA,EAAS/D,KAAK,IAEdgnB,GACEC,EACAhN,EACAtJ,GACAzM,WAAU,EAAG0F,SAAQ+E,YACrBhB,EAAQS,KAAK,CAAExI,KAAM,iBAAkB/C,MAAO,CAAE+G,SAAQ+E,WAEzC,UAAX/E,EACF6f,EAAY,GACH,CAAC,MAAO,YAAYpgB,MAAMoF,GAAMA,IAAM7E,KAC/C6f,EAAUrf,QAAQuE,GAGL,aAAX/E,IACF7F,EAAS/D,KAAKypB,GACd1lB,EAASK,WACX,GACA,GACF,IAGR,CDT2BslB,CAAqBpZ,GAC5CkZ,EAAetlB,WAAWulB,IACxB,GAAS3L,KAAK,2BAAyB,CACrC0C,KAAM,YACN5d,KAAM6mB,GACN,IAGJ,MAAMnD,EAAckD,EAAetY,MACjC,EAAA5L,EAAA,IAAKgiB,GAAMA,EAAExe,QAAQwC,GAAMA,EAAEkc,eAC7B,EAAAliB,EAAA,IAAKgiB,GAAMA,EAAEhiB,KAAKgG,GAAMA,EAAE/C,YAK5B,IAAI,GAAqB,eAAgB+H,EAAMmI,GAAmB1F,QAElE,IAAI,GACF,Y/BtEoC,I+BwEpCzC,EACAmI,GACA1F,QAEF,IAAI,GACF,a/B5EkC,I+B8ElC,IAAKzC,EAAMgW,eACX7N,GAEA1F,OACJ,CAEOkL,QAAQ1U,GACbmE,KAAK4b,MAAM/f,IAAO0U,SACpB,E,gHErEK,MAAM0L,GAAwBxnB,MACnCynB,IAEA,IAAKA,EACH,MAAO,UAGT,MAAMC,QAAiB,SAAmBD,GAE1C,OAAOC,GAAUpB,MAAQ,cC5B3B,MAAMrX,GAAK,I,SAAI,IAAM,mBACrBA,GAAG0Y,QAAQ,GAAGC,OAAO,CACnBra,IAAK,MACL8X,UAAW,QAGb,UCkBA,OAHS,CAAEjjB,IApBGpC,MAAOuN,EAAaka,KAG9B,UAFsB,GAAGI,MAAM,OAAO/lB,IAAI,CAAEyL,QAE9B,CACZ,MAAMua,EAA6B,CACjCva,MACA9M,KAAMgnB,GAER,GAAGI,MAAM,OAAOzlB,IAAI0lB,EACtB,GAWYhmB,IARF9B,MAAOuN,IAEjB,MAAMwa,QAAgB,GAAGF,MAAM,OAAO/lB,IAAI,CAAEyL,QAG5C,OAAOwa,GAAStnB,MAAQsnB,GAAS3a,OAAO,G,YClBrC,MAAM4a,GACX,iDAEWC,GAA2B,+CAA+CD,KAC1EE,GAA2B,mCAAmCF,KAI9DG,GAAoB,oCCajC,OAfqB,MACnB,MAAMC,EAAU,IAAI,MDDU,2BCY9B,MAAO,CAAEhmB,IATGpC,MACVqoB,IAEA,MAAMC,EACY,iBAATD,EAAoB,IAAIE,KAAK,CAACF,GAAO,YAAcA,EAC5D,OAAOD,EAAQhmB,IAAIkmB,EAAU,CAAEE,WAAY,EAAGC,WAAW,GAAQ,EAIrDvgB,OADClI,MAAOuN,GAAgB6a,EAAQlgB,OAAOqF,GAC/B,EAGxB,G,kDCTA,SAASmb,GAAgBC,EAAqBllB,GAC5C,MAAMmlB,EAAO,IAAIC,KAAK,CAACF,GAAU,CAAEllB,SACnC,OAAOqlB,IAAIJ,gBAAgBE,EAC7B,CAEA,SAASG,GAAcJ,EAAqBllB,GAG1C,MADa,QAAQA,aADH,QAAwBklB,EAAS,WAGrD,CAGO,MAAMK,GACX1C,IAEA,GAAIA,EAAM,CACR,GAAIA,EAAKlG,SAAS,SAChB,MAAO,QAGT,GAAIkG,EAAKlG,SAAS,SAChB,MAAO,QAGT,GAAIkG,EAAKlG,SAAS,QAChB,MAAO,MAEX,CACO,EAGH6I,GAAQ,+DAQP,MAMMC,GACX5C,IAEA,IAAKA,EACH,MAAO,QAGT,MAAM6C,EAAcH,GAAyB1C,GAC7C,OAAI6C,KAK8B,IAAhC7C,EAAK8C,QAAQ,gBACwB,IAArC9C,EAAK8C,QAAQ,mBAEN,QAEsB,IAA3B9C,EAAK8C,QAAQ,SACR,SAEgC,IAArC9C,EAAK8C,QAAQ,mBACR,MAEF,UAIIC,GAA0BrpB,MACrCoN,EACAG,EACA+b,KAGA,IAAKlc,IAAYA,GAAStM,OACxB,MAAO,CACLyoB,SAAS,EACT1Z,KAAMtC,EAAI4C,WACV5C,OAIJ,MAAM,OAAEzM,EAAM,KAAEkE,GAASoI,GAEnB,KAAEkZ,EAAI,YAAEvY,GAAgB/I,EAE9B,IAAKshB,EACH,MAAO,CACL/Y,MACAgc,SAAS,EACT1Z,KAAM,yBAAyBtC,EAAI4C,cAGvC,MAEMsN,EAA+B,CACnCtH,KAAM,SAAS5I,IACfgc,SAAS,EACThc,IALiBH,EAAQG,IAMzB9J,KAAMsK,GAGR,GAAIib,GAAyB1C,GAC3B,MAAO,IAAK7I,EAAU8L,SAAS,GAGjC,MAAMZ,EACc,iBAAX7nB,OL1BsBd,OAC/Byd,EACA6L,KAEA,IAAIE,EAAkB,EACtB,IACE,GAAI/L,aAAoBgM,WAEtB,OADAH,GAAcA,EAAW7L,EAASiM,YAC3BjM,EAET,MAAMkM,EAA4B,GAElC,GAAIlM,aAAoBmM,eAAgB,CACtC,MAAMC,EAASpM,EAASqM,YAElBC,EAAa/pB,OACjBG,OACAO,WAEIP,GACK,QAAiBwpB,IAG1BA,EAAO1hB,KAAKvH,GACZ8oB,GAAmB9oB,EAAOgpB,WAC1BJ,GAAcA,EAAWE,GAClBK,EAAOG,OAAO9nB,KAAK6nB,IAK5B,aAFoCF,EAAOG,OAAO9nB,KAAK6nB,EAGzD,CAEA,GAAI9pB,OAAOC,iBAAiBud,EAAU,CACpC,MAAMoM,EAASpM,EAASxd,OAAOC,iBAM/B,gBAAiB+pB,KAASJ,EACpBI,aAAiBR,aACnBE,EAAO1hB,KAAKgiB,GACZT,GAAmBS,EAAMP,WACzBJ,GAAcA,EAAWE,IAI7B,OADe,QAAiBG,EAElC,CACA,MACF,CAAE,MAAO3nB,GAQP,YAPA,GAAQA,MACN,gEACAA,EAMJ,GKlCYkoB,CAAkBppB,EAAQwoB,GAChCxoB,EAEAqpB,EAAkC,iBAAZxB,EAG5B,IAAKA,EACH,MAAO,IACFlL,EACH8L,SAAS,EACT1Z,KAAM,2BAA2BtC,EAAI4C,cAKzC,GAAsB,SAAlBsN,EAASha,KAAiB,CAE5B,IAAK0mB,GAAgB,KAAMC,GAAOtkB,KAAK6iB,IACrC,MAAO,IACFlL,EACHha,KAAM,QACN2J,QAAS2b,GAAcJ,EAAS,kBAIpC,MAAMnI,EAAM2J,EAAexB,GAAU,QAAwBA,GAE7D,OAAInI,EAAIzc,MAAM,OACL,IACF0Z,EACHha,KAAM,MACN2J,QAASoT,GAGTA,EAAIzc,MAAM,OACL,IACF0Z,EACHha,KAAM,OACN2J,QAASoT,GAtHjB,SAAgB7O,GACd,MAAM0Y,EAAY1Y,EAAO2Y,OAAOrjB,MAAM,EAAG,KACzC,OAAOgiB,GAAMsB,KAAKF,EACpB,CAsHQG,CAAOhK,GACF,IACF/C,EACHha,KAAM,OACN8lB,SAAS,EACTnc,QAASG,EAAI4C,YAMV,IACFsN,EACHtH,KAAMqK,EAAI3X,OAAS,GAAK,SAAS0E,IAAQ,WAAWiT,IACpD/c,KAAM,OACNoM,KAAM6B,GAAc8O,GACpBpT,QAASoT,EAEb,CAEA,IAAK2J,EAAc,CACjB,GAAsB,UAAlB1M,EAASha,KACX,MAAO,IAAKga,EAAUrQ,QAAS2b,GAAcJ,EAASrC,IAExD,GAAsB,QAAlB7I,EAASha,KACX,MAAO,IACFga,EACHrQ,QAASsb,GAAgBC,EAASrC,GAClCiD,SAAS,EAGf,CAEA,OAAO9L,CAAQ,EAiBJgN,GAAoB,CAC/BC,EACA3c,EACA4c,EAAgB,OAEhB,GAAKD,EAGL,MAAqB,iBAAVA,EACFA,EAAMzjB,MAAM,EAAG0jB,GAEjB5c,GAA+B,SAAhBA,GAClB,QAAwB2c,GAAOzjB,MAAM,EAAG0jB,QACxC,G,gBC3MN,MAAMC,GAAwB5qB,MAC5BuN,IAKA,MAAM9M,QAAa,GAAYqB,IAAIyL,GACnC,GAAI9M,GAAQA,EAAKoI,OAAQ,CAEvB,MAAMyd,QAAakB,GAAsB/mB,GACnCsN,EAAcmb,GAAsB5C,GAEpCtY,EAAcyc,GAAkBhqB,EAAMsN,GAS5C,MAAO,CAAEjN,OAAQL,EAAM8M,MAAKvI,KAPE,CAC5BvB,KAAM,OACNoL,KAAMpO,EAAKoI,OACX2d,UAAW/lB,EAAKoI,OAChByd,OACAvY,eAEgC2P,OAAQ,KAAM1P,cAClD,CAEO,EAGH6c,GAA4B,CAChCpnB,KAAM,OACNoL,UAAM,EACN2X,eAAW,EACXD,YAAQ,GAGJuE,GAAuB9qB,MAC3BuN,EACAwd,EACAtY,KAEA,GAAIsY,EAAM,CAER,aADoBA,EAAKC,KAAKzd,EAAK,CAAEkF,UAEvC,CACA,OAAOoY,EAAU,EAGbI,GAA2BjrB,MAC/BuN,EACAwd,EACAG,KAEA,MAAMC,EAAmBD,GAAc,IAAI7P,iBACrC,OAAE5I,GAAW0Y,EACnB,IAAIC,EAEJ,GAAKL,EAAL,CAKKG,IACHE,EAAQ5W,YAAW,KACjB2W,EAAiBpO,OAAO,GACvB,MAIL,IAEE,MAAMsO,EAAYhmB,KAAK0V,MACjBuQ,QAAcR,GAAqBvd,EAAKwd,EAAMtY,GAC9CzN,EAAOsmB,EACPC,EAAgBlmB,KAAK0V,MAC3B/V,EAAKwmB,UAAYD,EAAgBF,EACjC,MAAMI,IAAcH,EAAMzc,MAAOyc,EAAMzc,KH1FT,IG6F9B,GAFAuc,GAASM,aAAaN,GAGf,cADCE,EAAM7nB,KAGV,MAAO,CAAE8J,MAAKoe,mBAAmB,EAAMjO,OAAQ,OAAQ1Y,KAAMsmB,GAEtD,CAEP,MAAQ5qB,MAAOkrB,SAAqBb,EACjCc,IAAIte,EAAK,CAAEkF,SAAQ5J,OAAQ,KAAMkE,OAAQ,IACzC9M,OAAOC,iBACPrC,OAEHmH,EAAKshB,WAAakB,GAAsBoE,GACxC5mB,EAAK+I,YAAcmb,GAAsBlkB,EAAKshB,MAC9C,MAAMwF,EACJR,EAAMzc,MAAQyc,EAAMzc,MAAQ,GAAK+c,EAAW/iB,QAAUyiB,EAAMzc,KAExDb,EAAcyc,GAAkBmB,EAAY5mB,EAAK+I,aAEnD+d,SACI,GAAY1pB,IAAImL,GAAK,QAAiB,CAACqe,KAI/C,MAAMG,EAASD,EACXF,EACAH,EACAV,EAAKc,IAAIte,EAAK,CAAEkF,gBAChB,EAcJ,OAZAzN,EAAKgnB,QAAU3mB,KAAK0V,MAAQwQ,GAIvBvmB,EAAKinB,OAASR,GACjBV,EAAKmB,IAAI3e,GAETvI,EAAKmnB,QAAU9mB,KAAK0V,MAAQ/V,EAAKgnB,SAEjChnB,EAAKmnB,SAAW,EAGX,CACLrrB,OAAQirB,EACR/d,cACAT,MACAvI,OACA0Y,OAAQ,OAGZ,CAEJ,CAAE,MAAO1b,GAEP,OADA,GAAQoqB,MAAM,iCAAkCpqB,GACzC,CACLuL,MACAoe,mBAAmB,EACnBjO,OAAQ,OACR1Y,KAAM,IAAK6lB,IAEf,CA/EA,MAFE,GAAQnoB,IAAI,8DAiFd,EAGI2pB,GAA8BrsB,MAClCuN,EACAwd,EACAG,EACAoB,KAGA,MAAMC,EAAoC,aAAnBxB,GAAMyB,SAEvBlB,EAAQiB,QACJzB,GAAqBvd,EAAKwd,EAAMG,GAAYzY,QAClDoY,GAEE4B,EAAa,GAAGtE,WAA0B5a,IAC1CkQ,QAAiBiP,MAAMD,EAAY,CACvC1pB,OAAQ,MACR0P,OAAQyY,GAAYzY,OACpB6Z,YAEF,GAAI7O,GAAYA,EAAS/J,KAAM,CAe7B,MAAMiZ,EAAgBhD,GACnB4C,EAEGlsB,QAAQC,UADR,GAAY8B,IAAImL,GAAK,QAAiBoc,KAGtC,KAAErD,EAAI,OAAExlB,EAAM,WAAE8qB,SN9KnB5rB,eACL+rB,EACAa,GAEA,MAAOC,EAAkBC,GAAcf,EAAOgB,MACxCpD,EAA4B,GAG5BqD,EAAcH,EAAiB/C,aAC/B,MAAEppB,SAAgBssB,EAAYhD,OAC9B1D,EAAO5lB,QAAc8mB,GAAsB9mB,QAAS,EAEpDusB,EAAaH,EAAWhD,YAExBoD,EAA2C,CAC/CltB,OAAQC,OAAOC,iBACb,OAAa,CACX,MAAM,KAAEC,EAAMO,MAAAA,SAAgBusB,EAAWjD,OACzC,GAAI7pB,EAEF,YADAysB,GAASA,EAAMjD,EAAQrD,IAGzBsG,GAASjD,EAAO1hB,KAAKvH,SACfA,CACR,CACF,GAGF,MAAO,CAAE4lB,OAAMxlB,OAAQosB,EAAetB,WAAYlrB,EACpD,CMiJ+CysB,CACzC1P,EAAS/J,KACTiZ,GAGI5e,EAAcmb,GAAsB5C,GAEpCtY,EAAcyc,GAAkBmB,EAAY7d,GAClD,MAAO,CACLR,MACAS,cACAhJ,KAAM,IAAKsmB,EAAOhF,OAAMvY,eACxBjN,SACA4c,OAAQ,UACR+O,aAEJ,CAEO,EAiCT,MAAMW,GAAiBptB,MACrBuN,EACAwd,EACAG,EACAmC,KAEA,MAAMC,QAAsB1C,GAAsBrd,GAClD,QAAsB,IAAlB+f,EACF,OAAOA,EAGT,GAAIvC,EAAM,CACRsC,GAAsBA,EAAmB,6BAIzC,aAF0BpC,GAAyB1d,EAAKwd,EAAMG,EAGhE,CAEAmC,GAAsBA,EAAmB,+BAQzC,aAN6BhB,GAC3B9e,EACAwd,EACAG,EAGmB,EAqDjBqC,GAAkBvtB,MACtB+qB,EACA3d,KAEA,IAAIG,EAQJ,OAPIwd,IACFxd,QAAYwd,EAAK3oB,IAAIgL,IAGvB,GAAahL,IAAIgL,GAEjBG,SAAc,GAAYnL,IAAImL,OD7IGvN,OACjCoN,GAEO,IAAIqc,WACU,iBAAZrc,EACHgd,GAAOtkB,KAAKsH,SACNA,EAAQogB,eCuIqBC,CAAoBrgB,IACtDG,CAAG,E,gBC5VL,MAAMmgB,GAKXpiB,YAAYqiB,EAAyBlV,GACnClN,KAAKoiB,SAAWA,EAChBpiB,KAAKkN,MAAQA,CACf,CAEAmV,cAAclQ,GACZ,MAAM3L,EAAQxG,KAAKkN,MAAM2Q,QAAQ1L,GACjC,OAAO3L,EAAQxG,KAAKkN,MAAM5P,OAAS0C,KAAKkN,MAAM1G,EAAQ,QAAK,CAC7D,ECfK,MAAM8b,WAA8Bpf,MACzCnD,YAAYwiB,GACVtR,MAAM,iBAAiBsR,KACvBpqB,OAAOqqB,eAAexiB,KAAMsiB,GAAsB7M,UACpD,ECHK,MAAMgN,GACC,eAGP,IAAKC,GAAL,CAAKA,IACVA,EAAA,aAAe,gBADLA,GAAL,CAAKA,IAAA,I,YC8CZ,SAASC,GAA0BrpB,GACjC,OAAQA,EAAK6I,UAAY,IAAM7I,EAAKspB,kBAAoB,EAC1D,CAEA,MAIMC,GAAa,CACjBC,SAAU,IAAIX,GACZ,CACEze,GAAI,CAAEqf,QAAS,IAAMC,wBAAyB,KAC9CxD,KAAM,CAAEuD,QAAS,IAAWC,wBAAyB,IACrDhF,QAAS,CAAE+E,QAAS,IAAOC,wBAAyB,KAEtD,CAAC,KAAM,OAAQ,YAEjBC,SAAU,IAAId,GACZ,CACEze,GAAI,CAAEqf,QAAS,IAAMC,wBAAyB,KAC9CxD,KAAM,CAAEuD,QAAS,IAAWC,wBAAyB,IACrDhF,QAAS,CAAE+E,QAAS,KAAOC,wBAAyB,KAEtD,CAAC,KAAM,UAAW,SAEpBE,MAAO,IAAIf,GACT,CACEze,GAAI,CAAEqf,QAAS,IAAMC,wBAAyB,KAC9CxD,KAAM,CAAEuD,QAAS,IAAWC,wBAAyB,IACrDhF,QAAS,CAAE+E,QAAS,IAAOC,wBAAyB,KAEtD,CAAC,KAAM,OAAQ,aAganB,OA1ZA,MA+LEjjB,YACE+D,GACA,SACEqf,EAAQ,gBACRC,IAlMJ,KAAQC,OAAS,IAAItgB,EAAA,EAA0B,IAAIC,KAEnD,KAAQwc,UAAgC,EAMxC,KAAQ8D,iBAA2BxpB,KAAK0V,MAExC,KAAQvP,QAAU,IAAI,EAEtB,KAAQ0F,UAAmD,CACzDjC,GAAI,IAAIiH,IACR6U,KAAM,IAAI7U,IACVqT,QAAS,IAAIrT,KAyLb7G,EAActN,WAAWgpB,IACnBA,GACFxf,KAAKujB,QAAQ/D,EACf,IAGFxf,KAAKmjB,SAAWA,GAAYN,GAAWI,SACvCjjB,KAAKojB,gBAAkBA,GAxPD,IA4PtB,EAAAlM,GAAA,GA3P+B,MA4P5B1T,MACC,EAAApI,EAAA,IACE,MACI4E,KAAKwf,QACL,IAAIxf,KAAKqjB,OAAOluB,MAAMoJ,UAAUrB,MAAMU,GAAmB,SAAbA,EAAEuU,YAGrD3b,WAAU,KAMTwJ,KAAKwf,KAAMgE,kBAAiB,EAAK,IAGrC,MAAM5f,GAAiB,EAAAC,EAAA,GAAc,CAACC,IAAgBN,MACpD,EAAA5L,EAAA,IAAI,EAAEoM,OAAoBA,GAAgBA,GAAcyf,aACxD,EAAAhT,GAAA,MACA,EAAA3K,EAAA,MAGFlC,EAAepN,WAAWiP,IACxBA,GAAiB,GAAQtO,IAAI,6BAC7B6I,KAAKwf,MAAMgE,kBAAiB,EAAK,IAGnCxjB,KAAKqjB,OACF7f,MACC,EAAAkgB,GAAA,GAAe9f,IACf,EAAAxI,EAAA,IAAO,EAAE,CAAEqK,KAAmBA,KAC9B,EAAAke,GAAA,GAAa3jB,KAAKojB,kBAClB,EAAAxrB,EAAA,IAAI,EAAEyL,KAAWrD,KAAK4jB,yBAAyBvgB,MAC/C,EAAAqC,EAAA,IAAUrC,IACR,MAAMwgB,EAAY7jB,KAAK8jB,2BAA2BzgB,GAElD,OAAIwgB,EAAUvmB,OAAS,GAErB0C,KAAKwf,MAAMgE,kBAAiB,IAErB,EAAAxQ,GAAA,MAAS6Q,EAAUjsB,KAAK0B,GAAS0G,KAAK+jB,WAAWzqB,OAEnD,GAAK,KAGf9C,WAAU,EAAG8C,OAAMqD,SAAQwV,SAAQ5c,aAClC,MAAM,IAAEyM,GAAQ1I,EACV0qB,EAAYhkB,KAAKqjB,OAAOluB,MAAMoB,IAAIyL,IAAMgiB,WAAa,GAc3D,GAVAA,EAAUpsB,KAAKqsB,GAAaA,EAASjiB,EAAKrF,EAAQwV,EAAQ5c,KAG3C,SAAX4c,IACFnS,KAAKsjB,iBAAmBxpB,KAAK0V,OAG/BxP,KAAK2F,UAAUwM,GAAQ/M,OAAOpD,GAGf,cAAXrF,GAAqC,cAAXA,EAE5BqD,KAAKkkB,cAAcliB,OACd,CAIL,MAAMmiB,EAAankB,KAAKmjB,SAASd,cAAclQ,GAE3CgS,EACFnkB,KAAKokB,oBAAoB9qB,EAAM6qB,IAE/BnkB,KAAKkkB,cAAcliB,GAEnBgiB,EAAUpsB,KAAKqsB,GACbA,EAASjiB,EAAK,YAAamQ,EAAQ5c,KAGzC,CAEAyK,KAAKqkB,aAAa,GAExB,CArRQC,eAAenB,GACrBnjB,KAAKmjB,SAAWA,CAClB,CAEA,cAAqB3D,EAAmB+E,GACtC,GAAQptB,IACN,sBAAsB6I,KAAKwf,MAAMyB,UAAY,eAC3CzB,EAAKyB,YAGTjhB,KAAKwf,KAAOA,EACZxf,KAAKskB,eAAeC,GAAkB1B,GAAWrD,EAAKyB,UACxD,CAEQ6C,2BAA2BzgB,GACjC,MAAMwB,EAAe,IAAIxB,EAAM9E,UAAUnD,QACtCwC,GAAmB,YAAbA,EAAEjB,SAGL6nB,EAAkB,QAAW5mB,GAAMA,EAAEuU,QAAQtN,GAE7C4f,EAA8B,GAEpC,UAAYC,EAAazjB,KAAU9I,OAAO0H,QAAQ2kB,GAAkB,CAClE,MAEMG,EAFW3kB,KAAKmjB,SAASf,SAASsC,GAG7B1B,wBACThjB,KAAK2F,UAAU+e,GAAkCphB,KAC7CshB,EAAkB3jB,EACrBzD,MACC,CAACC,EAAGC,IAAMilB,GAA0BjlB,GAAKilB,GAA0BllB,KAEpE/B,MAAM,EAAGipB,GAEZF,EAAe/nB,QAAQkoB,EACzB,CAEA,OAAOH,CACT,CAEQJ,cACN,MAAMrrB,EAAU,WAAWgH,KAAKqjB,OAAOluB,MAAMmO,gBAAgBtD,KAAK2F,UAAUjC,GAAGJ,eAAetD,KAAK2F,UAAU6Z,KAAKlc,kBAAkBtD,KAAK2F,UAAUqY,QAAQ1a,QAC3JtD,KAAKC,QAAQE,kBAAkB,OAAQ,UAAWnH,EACpD,CAEQ+qB,WAAWzqB,GACjB,MAAM,IAAE0I,EAAG,OAAEmQ,EAAM,UAAE6R,EAAS,WAAErE,GAAermB,EAEzC8oB,EAAWpiB,KAAKmjB,SAASf,SAASjQ,GACxCnS,KAAK2F,UAAUwM,GAAQtb,IAAImL,GAC3BhC,KAAKqkB,cACL,MAAMQ,EAAY7kB,KAAKqjB,OAAOluB,MAAMoB,IAAIyL,GAWxC,OATAhC,KAAKqjB,OAAOluB,MAAMgB,IAAI6L,EAAK,IACtB6iB,EACHloB,OAAQ,YACRmoB,cAAehrB,KAAK0V,MACpBmQ,WAAY,IAAI7P,kBAGlBkU,EAAUpsB,KAAKqsB,GAAaA,EAASjiB,EAAK,YAAamQ,MCjKpB4S,EDmKRtwB,SJgE/BA,eACEuN,EACAmQ,EACAyE,GAEA,MAAM,KAAE4I,EAAI,WAAEG,EAAU,QAAEoB,GAAYnK,EAEtC,IACE,OAAQzE,GACN,IAAK,KACH,OAAOkN,GAAsBrd,GAC/B,IAAK,OACH,OAAO0d,GAAyB1d,EAAKwd,EAAMG,GAC7C,IAAK,UACH,OAAOmB,GAA4B9e,EAAKwd,EAAMG,EAAYoB,GAC5D,QACE,OAEN,CAAE,MAAOvK,GAEP,YADA,GAAQrf,IAAI,6BAA8Bqf,EAE5C,CACF,CIrFawO,CAAiBhjB,EAAKmQ,EAAQ,CACnCwN,aACAH,KAAMxf,KAAKwf,KACXuB,QAAS,CACP,CAAC0B,IAA2BC,GAAiBuC,gBAE9CtuB,MAAMkL,IAEHA,GAAsB,OAAXsQ,G7C5IY,CAACtQ,IAClCJ,GAAUC,QAAQ,CAChBxJ,KAAM,WAENhD,KAAM,IAAK2M,EAAStM,YAAQ,IAGnB,E6CsIH2vB,CAAoBrjB,GAGfA,KC/KN,IAAIzL,EAAA,GAAeC,IACxB0uB,IACGpuB,MAAMub,IACL7b,EAAS/D,KAAK4f,GACd7b,EAASK,UAAU,IAEpB2N,OAAO5N,IACN,GAAQoqB,MAAM,gCAAiCpqB,GAC/CJ,EAASI,MAAMA,EAAM,GACrB,KDwKD+M,MACD,EAAAuf,GAAA,GAAQ,CACNoC,KAAM/C,EAASW,QACfqC,KAAM,KACJ,EAAAC,GAAA,IAAW,KACT1F,GAAYnO,MAAM,WAEX,IAAI8Q,GAAsBF,EAASW,eAGhD,EAAAnrB,EAAA,IAAKrC,IACI,CACL+D,OACAqD,OAAQpH,EAAS,YAAc,QAC/B4c,SACA5c,cAGJ,EAAA+vB,GAAA,IAAY7uB,GAENA,aAAiB6rB,IACZ,EAAAiD,GAAAA,IAAG,CACRjsB,OACAqD,OAAQ,UACRwV,WAIgB,eAAhB1b,GAAOoF,MACF,EAAA0pB,GAAAA,IAAG,CAAEjsB,OAAMqD,OAAQ,YAAawV,YAElC,EAAAoT,GAAAA,IAAG,CAAEjsB,OAAMqD,OAAQ,QAASwV,cCjNpC,IAAgC4S,CDoNrC,CAQQS,gBAAgBxjB,EAAayjB,GACnC,MAAMpiB,EAAQrD,KAAKqjB,OAAOluB,MACpBmE,EAAO+J,EAAM9M,IAAIyL,GAKvB,OAJI1I,GACF+J,EAAMlN,IAAI6L,EAAK,IAAK1I,KAASmsB,IAGxBzlB,KAAKqjB,OAAO/wB,KAAK+Q,EAC1B,CAEQ6gB,cAAcliB,GACpB,MAAMqB,EAAQrD,KAAKqjB,OAAOluB,MAC1BkO,EAAM+B,OAAOpD,GACbhC,KAAKqjB,OAAO/wB,KAAK+Q,EACnB,CAGQ+gB,oBAAoB9qB,EAAiB6qB,GAC3C7qB,EAAK0qB,UAAUpsB,KAAKqsB,GAAaA,EAAS3qB,EAAK0I,IAAK,UAAWmiB,KAE/DnkB,KAAKwlB,gBAAgBlsB,EAAK0I,IAAK,CAAErF,OAAQ,UAAWwV,OAAQgS,GAC9D,CAEQP,yBAAyBvgB,GAmB/B,MAlBC,CAAC,OAAQ,WAAmChL,SAAS8Z,IACpD0D,MAAMtb,KAAKyF,KAAK2F,UAAUwM,IAAS9Z,SAAS2J,IAC1C,MAAM1I,EAAO+J,EAAM9M,IAAIyL,GACnB1I,GAAQqpB,GAA0BrpB,GAAQ,GAAKA,EAAKqmB,aAEtDrmB,EAAKqmB,WAAWnO,MAAM,aACtBlY,EAAK0qB,UAAUpsB,KAAKqsB,GAClBA,EAAS3qB,EAAK0I,IAAK,UAAW1I,EAAK6Y,UAGrC9O,EAAMlN,IAAI6L,EAAK,IAAK1I,EAAMqD,OAAQ,YAGlCqD,KAAK2F,UAAUwM,GAAQ/M,OAAOpD,GAChC,GACA,IAGGqB,CACT,CAEQqiB,iBAAiB1jB,GAEvB7J,OAAOC,KAAK4H,KAAK2F,WAAWtN,SAASmd,GACnCxV,KAAK2F,UAAU6P,GAA0BpQ,OAAOpD,IAEpD,CA6GON,QACLM,EACAiiB,EACArN,EAA4B,CAAC,GAE7B,MAAMvT,EAAQrD,KAAKqjB,OAAOluB,MACpBwwB,EAAetiB,EAAM9M,IAAIyL,GAK/B,GAAI2jB,EACF3lB,KAAKwlB,gBAAgBxjB,EAAK,CACxBgiB,UAAW,IAAI2B,EAAa3B,UAAWC,SAEpC,CACL,MAAM9R,EAASyE,EAAQgP,eAAiB5lB,KAAKmjB,SAASjW,MAAM,GACtD5T,EAAkB,CACtB0I,MACAgiB,UAAW,CAACC,GACZ9R,SACAxV,OAAQ,UACRkpB,gBAAgB,KACbjP,GAGLqN,EAASjiB,EAAK,UAAWmQ,GAEzB9O,EAAMlN,IAAI6L,EAAK1I,GACf0G,KAAKqjB,OAAO/wB,KAAK+Q,EACnB,CACF,CAEOyiB,eACL9jB,EACA4U,EAA4B,CAAC,GAE7B,OAAO,IAAI9hB,SAASC,IAOlBiL,KAAK0B,QAAQM,GANK,CAACA,EAAKrF,EAAQwV,EAAQ5c,KACvB,cAAXoH,GAAqC,cAAXA,GAC5B5H,EAAQ,CAAE4H,SAAQwV,SAAQ5c,UAC5B,GAG0BqhB,EAAQ,GAExC,CAEOmP,uBAAuB/jB,EAAa4gB,GACzC5iB,KAAKwlB,gBAAgBxjB,EAAK,CAAE4gB,oBAC9B,CAEOoD,OAAOhkB,GACZ,MACM1I,EADQ0G,KAAKqjB,OAAOluB,MACPoB,IAAIyL,GAEnB1I,IAGGA,EAAKqmB,WAGRrmB,EAAKqmB,WAAWnO,MAAM,aAFtBxR,KAAKkkB,cAAcliB,GAKzB,CAEOikB,eAAeC,GACpB,MAAM7iB,EAAQrD,KAAKqjB,OAAOluB,MAE1BkO,EAAMhL,SAAQ,CAACiB,EAAM0I,KACf1I,EAAK4sB,SAAWA,IAClBlmB,KAAK0lB,iBAAiB1jB,GACtB1I,EAAKqmB,YAAYnO,MAAM,aACvBnO,EAAM+B,OAAOpD,GACf,IAGFhC,KAAKqjB,OAAO/wB,KAAK+Q,EACnB,CAEO8iB,QACL,MAAM9iB,EAAQrD,KAAKqjB,OAAOluB,MAE1BkO,EAAMhL,SAAQ,CAACiB,EAAM0I,KACnBhC,KAAK0lB,iBAAiB1jB,GACtB1I,EAAKqmB,YAAYnO,MAAM,aACvBnO,EAAM+B,OAAOpD,EAAI,IAGnBhC,KAAKqjB,OAAO/wB,KAAK,IAAI0Q,IACvB,CAEOojB,cACL,OAAOpmB,KAAKqjB,OAAOluB,KACrB,CAEOkxB,eACL,OAAOxQ,MAAMtb,KAAKyF,KAAKqjB,OAAOluB,MAAMoJ,SACtC,CAEO+nB,WAOL,OANW,OACT,OAAqB,OAAO,WAC5B,OACA,OAAM,OAAS,CAAC,SAAU,WAGrBC,CAAGvmB,KAAKqmB,eACjB,G,kCE7eK,MAAMG,GAAezlB,GAAc,MAAIpD,MAAMoD,GACvC0lB,GAAoB1lB,GAAc,SAASA,I,gBC+GxD,OAnGA,oBACE,KAASkgB,SAAyB,WAIlC,KAAQyF,QAA6B,CAAC,EAMtC,KAAQC,YAAsB,EAJ1BC,aACF,OAAO5mB,KAAK0mB,OACd,CAIIjD,gBACF,OAAOzjB,KAAK2mB,UACd,CAEA,mBACE,MAAMzU,QAAiBlS,KAAKwf,KAAMoH,OAAOrwB,IAAI,qBAC7C,IAAK2b,EACH,MAAO,CAAE2U,WAAYjK,IAEvB,MAAMjiB,GAAU,SAAUuX,GAAoB4U,cAE9C,MAAO,CAAED,WAAY,UAAUlsB,EAAQA,WAAWA,EAAQnG,OAC5D,CAEAC,WAAWmiB,GACT5W,KAAKwf,MAAO,SAAiB5I,GAC7B5W,KAAK0mB,cAAgB1mB,KAAK+mB,aAEJ,oBAAXC,SACTA,OAAOxH,KAAOxf,KAAKwf,KACnBwH,OAAOC,MAAQT,IAEjB,GAAQrvB,IACN,2BACO6I,KAAKwf,KAAK0H,MAAMC,cAAcvvB,KAAK6F,GAAMA,EAAEmH,cAEpD5E,KAAK2mB,YAAa,CACpB,CAEAlyB,WAAWuN,EAAa4U,EAAwB,CAAC,GAC/C,OAAO5W,KAAKwf,KAAM4H,MAAM3H,KAAKgH,GAAiBzkB,GAAM,IAC/C4U,EACHyQ,WAAW,EACX/jB,MAAM,IACL3M,MAAMpB,IACP,MAAM,KAAE2C,EAAI,KAAEoL,EAAI,UAAE2X,EAAS,MAAEyF,EAAK,OAAE1F,GAAWzlB,EACjD,MAAO,CACL2C,OACAoL,KAAMA,IAAS,EACf2X,UAAWA,IAAc,EACzBD,SACD,GAEL,CAEAsF,IAAIte,EAAa4U,EAAsB,CAAC,GACtC,OAAO5W,KAAKwf,KAAMc,IAAIkG,GAAYxkB,GAAM4U,EAC1C,CAEAniB,UAAUoN,EAAwB+U,EAAwB,CAAC,GACzD,aAAc5W,KAAKwf,KAAM3oB,IAAIgL,EAAS+U,IAAU5U,IAAI4C,UACtD,CAEAnQ,UAAUuN,EAAa4U,EAAwB,CAAC,GAC9C,aAAc5W,KAAKwf,KAAMmB,IAAI9pB,IAAI2vB,GAAYxkB,GAAM4U,IAAUhS,UAC/D,CAEAnQ,iBACE,aAAcuL,KAAKwf,KAAM0H,MAAMI,SAAS1vB,KAAKgiB,GAAMA,EAAE2N,KAAK3iB,YAC5D,CAEAnQ,aAAc,CACdA,cAAe,CAEfA,kBAAkBkG,GAChB,MAAMoR,GAAO,SAAUpR,GAIvB,aAHMqF,KAAKwf,KAAMgI,UAAU3wB,IAAIkV,SAEzB/L,KAAKwf,KAAM0H,MAAMO,QAAQ1b,IACxB,CACT,CAEA2b,KACE,OAAO1nB,KAAKwf,KAAMmB,IAAI+G,IACxB,CAEAjzB,aACE,MAAM,SAAEkzB,SAAmB3nB,KAAKwf,KAAMO,MAAM6H,OAEtCC,QAAmB7nB,KAAKwf,KAAMnlB,MAC9B,aAAEytB,EAAY,GAAEztB,GAAOwtB,EAC7B,MAAO,CAAExtB,GAAIA,EAAGuK,WAAYkjB,eAAcH,WAC5C,G,wKCvEF,MA+DMI,GAAoC,CACxC9K,WAAY,EACZC,WAAW,GAiLb,OA9KA,oBACE,KAAS+D,SAAyB,QAMlC,KAAQ0F,YAAa,EAJjBC,aACF,MAAO,CAAEC,WAAYjK,GACvB,CAII6G,gBACF,OAAOzjB,KAAK2mB,UACd,CAMAlyB,aACE,MAAMuzB,EAAa,IAAI,KAAc,kBAC/BA,EAAWC,OAEjB,MAAMC,EAAY,IAAI,KAAa,kBAC7BA,EAAUD,OAEhB,MAOME,OAnGY1zB,OACpByzB,EACAE,EAA0B,WAEL,QAAa,CAChCF,YAOAG,WAAY,EACV,WACA,WACA,QAAO,CACLC,iBAAkB,CAChBC,WAAY,CACV,CACEC,KAAM,CACJ,+BACA,mCACA,yBACA,wCACA,6BAGJ,CACEC,WAAY,OACZC,SAAU,OACVF,KAAM,CAAC,yBAA0B,iCAKzC,WACA,QAAsB,CACpBG,eAAgB,KAGpBC,qBAAsB,EAAC,WACvBC,aAAc,EAAC,WACfC,gBAAiB,CACfC,kBAAmB,KACV,GAOXC,cAAe,EACb,QAAU,CACRnwB,KAAMuvB,KAGVa,SAAU,CACRC,UAAU,EAAAA,GAAA,SA0CSC,CAAcjB,EAPb,CACpB,kFACA,kFACA,kFACA,kFACA,+FAIFloB,KAAKwf,WAAa,QAAY,CAAEwI,aAAYE,YAAWC,WAEvDnoB,KAAKopB,IAAK,SAAOppB,KAAKwf,MAEA,oBAAXwH,SACTA,OAAOmB,OAASA,EAChBnB,OAAOxH,KAAOxf,KAAKwf,KACnBwH,OAAOoC,GAAKppB,KAAKopB,GACjBpC,OAAOC,MAAQT,IAIjB2B,EAAOkB,iBAAiB,gBAAiBC,IACvC,MAAMC,EAASD,EAAIE,OAAO5kB,WACpB6kB,EAAOtB,EAAOuB,eAAeH,IAAW,GACxCI,EAAmBxxB,OAAOyxB,YAC9BH,EAAK7xB,KAAKgiB,GAAM,CACdA,EAAEiQ,WAAWjlB,WACbgV,EAAEiQ,WAAWC,aAAalyB,KAAKgZ,IAAM,SAAUA,IAAI/U,WAGvD,GAAQglB,MAAM,gBAAgB0I,IAAUI,EAAiB,IAe3DxB,EAAOkB,iBAAiB,mBAAoBC,IAC1C,GAAQzI,MAAM,qBAAqByI,EAAIE,OAAO5kB,aAAa,IAE7D,GAAQzN,IACN,qBACAgxB,EAAO4B,gBAAgBnyB,KAAK6F,GAAMA,EAAEmH,cAStC5E,KAAK2mB,YAAa,CACpB,CAEAlyB,WAAWuN,EAAa4U,EAAwB,CAAC,GAC/C,OAAO5W,KAAKopB,GAAI3J,KAAK+G,GAAYxkB,GAAM4U,GAASjgB,MAAMpB,IACpD,MAAM,KAAE2C,EAAI,SAAE8xB,EAAQ,cAAEC,EAAa,OAAEjP,EAAM,QAAEkP,EAAO,MAAEC,GAAU50B,EAClE,MAAO,CACL2C,OACAoL,KAAM0mB,IAAa,EACnB/O,UAAWgP,IAAkB,EAC7BjP,SACD,GAEL,CAEAsF,IAAIte,EAAa4U,EAAsB,CAAC,GACtC,OAAO5W,KAAKopB,GAAI9I,IAAIkG,GAAYxkB,GAAM4U,EACxC,CAEAniB,UAAUoN,EAAwB+U,EAAwB,CAAC,GAEzD,MAAMwT,EAAY,IACbxT,KACAmR,IAGL,IAAI/lB,EAEJ,GAAIH,aAAmBmb,KAAM,CAC3B,MAAMqN,EAAWxoB,EAAQhG,KACnBomB,QAAoBpgB,EAAQogB,cAC5B/sB,EAAO,IAAIgpB,WAAW+D,GAC5BjgB,QAAYhC,KAAKopB,GAAIkB,QACnB,CAAEC,KAAMF,EAAUxoB,QAAS3M,GAC3Bk1B,EAEJ,KAAO,CACL,MAAMl1B,GAAO,IAAIs1B,aAAcC,OAAO5oB,GACtCG,QAAYhC,KAAKopB,GAAIsB,SAASx1B,EAAMk1B,EACtC,CAGA,OADApqB,KAAK2gB,IAAI3e,EAAI4C,WAAYgS,GAClB5U,EAAI4C,UACb,CAEAnQ,UAAUuN,EAAa4U,EAAwB,CAAC,GAC9C,MAAM+T,EAAOnE,GAAYxkB,GAEzB,UADuBhC,KAAKwf,MAAMoL,KAAKC,SAASF,EAAM/T,IACvC,QAEL5W,KAAKwf,MAAMoL,KAAK/zB,IAAI8zB,EAAM/T,MAC/B5U,IAAI4C,UAET,CAGF,CAEAnQ,iBACE,OAAOuL,KAAKwf,KAAM2I,OAAQuB,iBAAiB9xB,KAAKgiB,GAC9CA,EAAEkR,WAAWlmB,YAEjB,CAEAnQ,mBACQuL,KAAKwf,MAAMuL,OACnB,CAEAt2B,oBACQuL,KAAKwf,MAAMna,QACnB,CAEA5Q,kBAAkBkG,SACGqF,KAAKwf,KAAM2I,OAAQ6C,MAAK,SAAUrwB,IACrD,OAAO,CACT,CAEA+sB,KACE,MAAMnyB,EA/OVd,gBACEw2B,GAGA,gBAAiB3xB,KAAQ2xB,EAAU,CACjC,MAAM,IAAEjpB,EAAG,SAAEkpB,GAAa5xB,OACpB,CAAE0I,IAAKA,EAAImpB,OAAQD,WAAUhzB,KAAM,YAC3C,CACF,CAuOmBkzB,CAAcprB,KAAKwf,KAAMoL,KAAKlD,MAC7C,OAAOnyB,CACT,CAEAd,aAIE,MAAO,CAAE4F,GAHE2F,KAAKwf,KAAM2I,OAAOoB,OAAO3kB,WAGvBkjB,aAFQ9nB,KAAKwf,KAAM2I,OAAQc,SAAUC,SAAUmC,KACzDvD,aACwBH,UAAW,EACxC,G,wBC1LF,OAtFmB,MACjBtiB,OAAO,EACPuiB,KAAM,qBACN0D,MAAO,CACLC,SAAS,EACTC,IAAK,CACHD,SAAS,IAGbE,QAAS,CACPF,SAAS,GAEX3E,OAAQ,CACN8E,IAAK,CACHC,YAAa,CACX,+BAAgC,CAAC,MAAO,QACxC,8BAA+B,CAC7B,wBACA,wBACA,wBACA,2BAINC,UAAW,CACTC,QAAS,0BACTC,MAAO,GAKPC,UAAW,IAMbC,UAAW,CACTC,KAAM,CACJC,SAAS,EACTC,SAAU,IAEZC,WAAY,CACVF,SAAS,IAGbG,UAAW,GAQXC,OAAQ,CACNJ,SAAS,GAEXJ,MAAO,CACLS,QAAS,CACPC,UAAW,IACXC,SAAU,IAEZC,mBAAmB,GAErBC,QAAS,CACPC,KAAM,cAGVzE,OAAQ,CACNE,WAAY,EAIV,QAAW,CACTjtB,OAAQyxB,GAAA,MAGZC,IAAK,CACHvB,SAAS,IAGbwB,aAAc,CACZC,YAAY,KCWhB,OAnFA,oBACE,KAAS/L,SAAyB,WAMlC,KAAQ0F,YAAa,EAJjBC,aACF,MAAO,CAAEC,WAAYjK,GACvB,CAII6G,gBACF,OAAOzjB,KAAK2mB,UACd,CAIAlyB,aACEuL,KAAKwf,WAAa,SAAmB,MACf,oBAAXwH,SACTA,OAAOxH,KAAOxf,KAAKwf,KACnBwH,OAAOC,MAAQT,IAGjBxmB,KAAK2mB,YAAa,CACpB,CAEAlyB,WAAWuN,EAAa4U,EAAwB,CAAC,GAC/C,OAAO5W,KAAKwf,KAAM4H,MAAM3H,KAAKgH,GAAiBzkB,GAAM,IAC/C4U,EACHyQ,WAAW,EACX/jB,MAAM,IACL3M,MAAMpB,IACP,MAAM,KAAE2C,EAAI,KAAEoL,EAAI,UAAE2X,EAAS,MAAEyF,EAAK,OAAE1F,GAAWzlB,EACjD,MAAO,CACL2C,OACAoL,KAAMA,IAAS,EACf2X,UAAWA,IAAc,EACzBD,SACD,GAEL,CAEAsF,IAAIte,EAAa4U,EAAsB,CAAC,GACtC,OAAO5W,KAAKwf,KAAMc,IAAIkG,GAAYxkB,GAAM4U,EAC1C,CAEAniB,UAAUoN,EAAwB+U,EAAwB,CAAC,GACzD,aAAc5W,KAAKwf,KAAM3oB,IAAIgL,EAAS+U,IAAU5U,IAAI4C,UACtD,CAEAnQ,UAAUuN,EAAa4U,EAAwB,CAAC,GAC9C,aAAc5W,KAAKwf,KAAMmB,IAAI9pB,IAAI2vB,GAAYxkB,GAAM4U,IAAUhS,UAC/D,CAEAnQ,iBACE,aAAcuL,KAAKwf,KAAM0H,MAAMI,SAAS1vB,KAAKgiB,GAAMA,EAAE2N,KAAK3iB,YAC5D,CAEAnQ,aAAc,CACdA,cAAe,CAEfA,kBAAkBkG,GAChB,MAAMoR,GAAO,SAAUpR,GAIvB,aAHMqF,KAAKwf,KAAMgI,UAAU3wB,IAAIkV,SAEzB/L,KAAKwf,KAAM0H,MAAMO,QAAQ1b,IACxB,CACT,CAEA2b,KACE,OAAO1nB,KAAKwf,KAAMmB,IAAI+G,IACxB,CAEAjzB,aACE,MAAMyd,QAAiBlS,KAAKwf,KAAMO,MAAM6H,OAClCD,EAAWsF,OAAO/a,EAASyV,UAE3BE,QAAmB7nB,KAAKwf,KAAMnlB,MAC9B,aAAEytB,EAAY,GAAEztB,GAAOwtB,EAC7B,MAAO,CAAExtB,GAAIA,EAAGuK,WAAYkjB,eAAcH,WAC5C,G,YCjFF,MAAMuF,GAAyD,CAC7DhK,MAAO,GACPD,SAAU,GACVH,SAAU,IAILruB,eAAe04B,GACpBvW,GAEA,MAAM,aAAEwW,KAAiBC,GAAgBzW,EASnC0W,ECvBR,SACEC,EACA3W,GAEA,OAAO,cAA+B2W,EACpC94B,uBACEuN,EACAwrB,EACA3d,GAEA,MAAMhO,QAAgBggB,GAAe7f,EAAKhC,KAAM6P,GAE1C4d,QAAgB3P,GAAwBjc,EAASG,GACvD,OAAQwrB,EAEJC,GAASv1B,OAASs1B,EAClBC,OACA,EAHAA,CAIN,CAEAh5B,iBAAiBoN,GACf,OAAOmgB,GAAgBhiB,KAAM6B,EAC/B,CAEApN,2BAEE,eADoBwc,MAAMyc,YACXxwB,MAAMqsB,GAAWA,IAAW3S,EAAQ+W,aACrD,CAEAl5B,uBAAuBm5B,GAAS,SACG5tB,KAAK6tB,uBACXD,GAMzB3c,MACG6c,YAAYlX,EAAQmX,kBACpBp3B,MAAK,KACJ,GAAQQ,IAAI,2BAA2Byf,EAAQmX,qBACxC,KAER1pB,OAAOM,IACN,GAAQxN,IACN,0BAA0Byf,EAAQmX,qBAAqBppB,EAAIvI,YAEtD,IAGf,EAEJ,CD7BwB4xB,CAAgBd,GAAaE,GAAe,CAChEO,YARkBlR,GASlBsR,iBANiB,aAAjBX,EACIzQ,GACAD,KAOAuR,EAAW,IAAIX,EAQrB,aANMW,EAASC,KAAK,CAAEvlB,IAAK0kB,EAAYc,gBAKjCF,EAASzK,mBACRyK,CACT,C,wDE5BA,OAAIG,kBAAmB,EAMvB,MAAMC,GAA4C,CAChDC,iBAAkB,CAChBzyB,KAAM,qBACN0yB,MAAO,4BAiGEC,GAAc,CACzBjrB,EACAkrB,KAEA,MAAMC,EAAoB,IAAIvd,EAAA,EACxBhO,EA9CoB,EAC1BI,EACAmrB,KAEA,MAAMC,EAAgB,IAAIC,GAAA,EAAc,GAiCxC,OA/BA,EAAA/qB,EAAA,GAAc,CAACN,EAAamrB,IAAoBl4B,WAC9C,EAAEuN,EAAYuqB,MACZ,GAAIvqB,GAAcuqB,EAAkB,CAClC,MAAM7pB,EAAkBhQ,MAAO6P,UACRgqB,EAAiBhqB,EAAM,CAC1CuqB,QAAS,OACTC,WAAW,KAGC55B,KAGV65B,EAAoBt6B,MAAO6P,EAAcyP,KAC7C,MAAMvP,QAAYC,EAAgBH,GAMlC,aAHmBP,EAAWgrB,kBAAkBvqB,EAAKuP,EAG1C,EAGPib,EAAM,CACVvqB,kBACAsqB,qBAEFJ,EAAcr8B,MAAK,QAAM08B,GAC3B,KAIGL,CAAa,EASEM,CAAoB1rB,EAAamrB,GAEjDQ,EAAuBz6B,MAAO06B,IAClC,MAAM,KAAEtzB,EAAI,MAAE0yB,GAAUF,GAAWc,GAE7BC,OA/FW,EACnBvzB,EACA0yB,EACAxQ,KAEO,UAASliB,EAAM0yB,EAAO,CAC3Bc,kBAAoBC,IAClB,IACE,MAAM,OACJ3yB,EAAM,SACNmE,EAAQ,OAERyuB,EAAM,MACNr2B,GACEo2B,EAEElzB,EAAUmzB,EAAS,GAAGhB,OAAWgB,KAAUr2B,UAAgBq1B,EAE3D35B,EAAO,CAAC,QAAS,SAAS+G,MAAMoF,GAAMA,IAAMpE,IAE5C6yB,EAAmB,CACvB7yB,SACAP,UACAxH,OACAkM,SAAUA,EAAWoI,KAAKoG,MAAMxO,GAAY,GAI9Cid,EAAWyR,EACb,CAAE,MAAOhZ,GACP,GAAQrf,IAAI,yBAA0Bo3B,EAAO/X,EAAE5R,WACjD,KAgEqB6qB,CAAa5zB,EAAM0yB,GAAQr5B,GAChDu5B,EAAanuB,wBAAwB6uB,EAAOj6B,KAEjC,uBAAT2G,GACF6yB,EAAkBp8B,KAAK88B,GAEzB,GAAQj4B,IAAI,GAAGg4B,aAAiB,EAG5BjB,EAAOz5B,UACXg6B,EAAatuB,kBAAkB,KAAM,YACrC,GAAQuvB,KAAK,qBAEN56B,QAAQgQ,IAAI,CACjBoqB,EAAqB,sBAIpBv4B,MAAMpB,IACL0T,YAAW,IAAMwlB,EAAatuB,kBAAkB,KAAM,YAAY,GAClE,GAAQwvB,QAAQ,qBAETp6B,KAER8O,OAAOmS,GACNiY,EAAatuB,kBAAkB,KAAM,QAASqW,EAAE5R,eAMtD,OAFAspB,IAEO,CAAE/qB,gBAAe+qB,OAAM,E,8oBC/GhC,MAAM0B,GAAgB,CACpBC,OAAQ,IACRC,cAAc,EACdC,cAAc,EACdnZ,QAAS,IAGLoZ,GAAwC,CAC5CC,UAAU,EACVC,SAAS,EACTC,SAAU,OACVC,WAAY,CAAC,EACb7uB,OAAQ,CAAC,EACT8uB,M,mvIACAC,OAAQ,IA0QV,MAAMC,GA9PN,WACE,IAAIC,EAA0C,CAAC,EAC3CC,EAAyB,CAAElvB,OAAQ,CAAC,EAAGmvB,KAAM,CAAC,EAAGC,QAAS,CAAC,GAC/D,MAAM/sB,EAAiB,IAAIb,EAAA,GAAyB,GAC9C6tB,EAAe,IAAI7tB,EAAA,EAA4C,CAAC,GAEhE8tB,EAAkB,IAAI7tB,IAEtB8tB,EAAqB,IAAIlC,GAAA,EAAc,IAC7C,EAAA/qB,EAAA,GAAc,CAACD,EAAgBgtB,IAC5BptB,MACC,EAAA5L,EAAA,IACE,EAAE6N,EAAe+qB,QACZ/qB,IAAiB+qB,EAAY72B,aAEpC,EAAA8W,GAAA,MAEDja,WAAWoa,IACVkgB,EAAmBx+B,KAAKse,EAAE,IAG9BggB,EAAap6B,WAAWoa,IACtB4f,EAAc5f,CAAC,IAGjB,MAgCMmgB,EAAMt8B,MACV67B,EACAU,EACA/M,EACAgN,EAAkC,CAAC,KAEnC,MAAMC,GAAQ,UAAStsB,WAEvBqf,GAAY4M,EAAgB16B,IAAI+6B,EAAOjN,GACvC,MAIMkN,EAAiB,IAClBnB,MACAgB,EACHX,MAAOC,EACPW,QAAS,IAAKA,EAASG,QAAS,IAChC7vB,OATmB,CACnB8vB,IAAKZ,EACLS,UAUII,QAAmB,SAAQH,EAAgBvB,KAG3C,OAAEr6B,EAAM,MAAEkB,GAAU66B,EAE1B,IAGE,OAFAT,EAAgBzrB,OAAO8rB,GAEhB,IACF7b,GAAYic,GACf76B,QACAlB,OAAQA,EACO,OAAXA,EACE,GACAuC,KAAK6F,OnD9IiByI,EmD8IS7Q,EnD7IpC6Q,EAAO8O,QAAQ7O,GAAoB,MmD8IhC,CAAEnK,OAAQ,QAASE,QAAS,aAEpC,CAAE,MAAOoa,GASP,OARAqa,EAAgBzrB,OAAO8rB,GAEvB,GAAQ/5B,IACN,kBAAkBg6B,EAAehB,WACjC3Z,EACA8a,EACAH,GAEK,CACLI,kBAAmB,0BAA0B/a,OAC1C8a,EACH/7B,OAAQ,CAAE2G,OAAQ,QAASE,QAASoa,GAAG5R,YAAc,iBAEzD,CnD/JG,IAA6BwB,CmD+JhC,EAGIorB,EAA4B,KAEhC,IAAKhB,EAAY72B,SACf,MAAO,CAAC,QAAS,IAGnB,MAAM,OAAE22B,EAAM,QAAE/E,GAAYiF,EAAY72B,SAExC,OAAK4xB,EAIE,CAAC,SAAU+E,GAHT,CAAC,OAAQ,GAGO,EA+G3B,MAAO,CACLpC,KAnNWz5B,UACX,GAAQi7B,KAAK,6BACP,WAEN,GAAQC,QAAQ,uBAChB/rB,EAAetR,MAAK,EAAK,EA+MzBw+B,qBACAC,MAEAU,aAnDmBh9B,MACnBuN,EACAQ,EACAX,EACAoiB,KAEA,MAAOyN,EAAYpB,GAAUkB,IAC7B,GAAmB,UAAfE,EACF,MAAO,CACLx1B,OAAQ,QACRy1B,UAAW,CAAC,CAAC,CAAEz5B,KAAM,OAAQoM,KAAM,6BAIvC,GAAmB,SAAfotB,EACF,MAAO,CAAEx1B,OAAQ,OAAQy1B,UAAW,IAGtC,MAAMC,QAAeb,EACnBT,EACA,CACEH,SAAU,gBACVC,WAAY,CAACpuB,EAAKQ,EAAaX,IAEjCoiB,GAGF,MAA6B,UAAzB2N,EAAOr8B,OAAO2G,QAChB,GAAQzF,MAAM,wBAAyBm7B,GAChC,CACL11B,OAAQ,QACRy1B,UAAW,CAAC,CAAC,CAAEz5B,KAAM,OAAQoM,KAAMstB,EAAOn7B,WAIvC,CAAEyF,OAAQ,SAAUy1B,UAAWC,EAAOr8B,OAAOsM,QAAS,EAiB7DgwB,kBAlHwBp9B,MACxB8M,IAEA,MAAOmwB,EAAYpB,GAAUkB,IAE7B,GAAmB,UAAfE,EACF,MAAO,CAAEx1B,OAAQ,QAASE,QAAS,0BAGrC,GAAmB,WAAfs1B,EACF,MAAO,CAAEx1B,OAAQ,QAGnB,MAAM,IAAE8F,EAAG,YAAEQ,EAAW,QAAEX,GAAYN,EAChCqwB,QAAeb,EAAIT,EAAQ,CAC/BH,SAAU,qBACVC,WAAY,CAACpuB,EAAKQ,EAAaX,MAE3B,OAAE3F,EAAQ2F,QAASiwB,GAAkBF,EAAOr8B,OAUlD,MARe,UAAX2G,GACF,GAAQzF,MACN,kCAAkC8K,EAAOS,MACzCT,EACAqwB,GAIAE,EACK,IAAKF,EAAOr8B,OAAQsM,QAASiwB,GAG/BF,EAAOr8B,MAAM,EAmFpBw8B,eAjMsBC,IACtBxB,GAAc,WACX5f,IAAM,IAAMA,EAAG0f,QAAQ,SAAkB1f,EAAE0f,WAC5C0B,GAEFpB,EAAat+B,KAAKk+B,EAAY,EA6L9ByB,YAlNkB,CAClBp2B,EACA1G,KAGAs7B,EAAQ50B,GAAQ1G,CAAK,EA8MrB+8B,WA3MkBC,IAClB,MAAMC,EAAa3B,EACnB0B,EAAM95B,SAASwD,IACbu2B,EAAWv2B,GAAQ,CAAC,CAAC,IAEvB40B,EAAU2B,CAAU,EAuMpBC,gBAnFsB59B,MACtB67B,EACAH,EACAC,KAEA,GAAQj5B,IAAI,4BAA6Bg5B,EAAUC,GAOnD,aANqBW,EAAIT,EAAQ,CAC/BH,WACAC,aACAH,UAAU,KAGE16B,MAAM,EAwEpB+8B,gBAnBsB79B,MAAOy8B,EAAeh8B,KAC5C,MAAM+uB,EAAW4M,EAAgBt6B,IAAI26B,GAEjCjN,SACIA,EAAS/uB,EACjB,EAeAq9B,SAAU,KAAM,CACd9B,UACAD,gBAGN,CAEqBgC,GAIrB,U,YC1UO,MC2EDC,GA9D4B,MAChC,MAAMhE,EAAe,IAAI,EAEnBlrB,EAAc,IAAI4N,EAAA,EAIlBjB,EAAU,IAAInN,EAAA,EAAmC,CACrDwJ,UAAW,QAGP,cAAEpJ,GAAkBqrB,GAAYjrB,EAAakrB,IAE7C,aAAEiE,EAAY,KAAEC,GD1BK,EAC3BxvB,EACAI,EACAkrB,KAEA,MAAMiE,EAAgB9vB,GACpBgwB,GAAA,EAASF,aAAa9vB,GAyBxB,OAvBAO,EAAc3M,WAAW4M,IACvBsvB,EAAa,CAAEtvB,gBAAe,IAGhCG,EAAY/M,WAAW+iB,IACrBmZ,EAAa,CAAEnZ,SAAQ,IAGzB,GAAKuX,mBAAmBt6B,WAAWrB,IACjCA,EACI8T,YAAW,IAAMwlB,EAAatuB,kBAAkB,OAAQ,YAAY,GACpEsuB,EAAatuB,kBAAkB,OAAQ,WAAW,IAG3C1L,WACXg6B,EAAatuB,kBAAkB,OAAQ,kBAEjC,GAAK+tB,OACXwE,EAAa,CAAEC,KAAI,IAAG,EAGxBzE,GAEO,CAAEyE,KAAI,GAAED,eAAclhB,MAAOohB,GAAA,EAASphB,MAAO,ECLrBqhB,CAC7B1vB,EACAI,EACAkrB,IAGI,UACJqE,EAAS,cACThvB,EACAkrB,IAAK+D,GCzBoB,EAC3BJ,EACAlE,KAEA,MAAM3qB,EAAgB,IAAIf,EAAA,OAAyC,GAC7D+vB,EAAY,IAAI,GAAahvB,EAAe,CAChD6uB,SAsCI3D,EAAM,CACV3pB,MA3BgB5Q,MAAOu+B,IACvB,IAEE,GADiBlvB,EAAcI,WAI7B,OADA+E,YAAW,IAAMwlB,EAAatuB,kBAAkB,OAAQ,YAAY,GAC7DrL,QAAQC,UAGjB05B,EAAatuB,kBAAkB,OAAQ,YACvC,GAAQuvB,KAAK,uBAEb,MAAMuD,QAAoB9F,GAAa6F,GAKvC,OAJA,GAAQrD,QAAQ,uBAEhB7rB,EAAcxR,KAAK2gC,GACnBhqB,YAAW,IAAMwlB,EAAatuB,kBAAkB,OAAQ,YAAY,IAC7D,CACT,CAAE,MAAOwE,GACP,GAAQxN,IAAI,4BAA6BwN,GACzC,MAAM/I,EAAM+I,aAAezB,MAAQyB,EAAIvI,QAAWuI,EAElD,MADA8pB,EAAatuB,kBAAkB,OAAQ,QAASvE,GAC1CsH,MAAMtH,EACd,GAKAmvB,KAtCet2B,UACf,MAAMy+B,EAAWpvB,EAAcI,WAE3BgvB,SACIA,EAASnI,OAEjBjnB,EAAcxR,UAAK,GACnBm8B,EAAatuB,kBAAkB,OAAQ,WAAW,EAgClDymB,OAAQnyB,SAAYqP,EAAcI,YAAY0iB,OAC9CxW,KAAM3b,SAAYqP,EAAcI,YAAYkM,OAC5C+iB,iBAAkB1+B,MAChBuN,EACAwrB,EACA7N,KAEA,MAAMuT,EAAWpvB,EAAcI,WAC/B,IAAKgvB,EACH,MAAM,IAAIhwB,MAAM,6BAElB,OAAOgwB,EAASC,iBAAiBnxB,EAAKwrB,EAAS7N,EAAW,EAE5Dje,QAASjN,MACPuN,EACAiiB,EACArN,IACGkc,EAAUpxB,QAAQM,EAAKiiB,EAAUrN,GACtCkP,eAAgBrxB,MAAOuN,EAAa4U,IAClCkc,EAAWhN,eAAe9jB,EAAK4U,GACjCwc,QAAS3+B,MAAOuN,GAAgB8wB,EAAU9M,OAAOhkB,GACjDqxB,gBAAiB5+B,MAAOyxB,GAAmB4M,EAAU7M,eAAeC,GACpEoN,WAAY7+B,SAAYq+B,EAAU3M,QAClCoN,WAAY9+B,MAAOoN,GACjBiC,EAAcI,YAAYqvB,WAAW1xB,IAGzC,MAAO,CAAEiC,gBAAegvB,YAAW9D,MAAK,EDhDpCwE,CAAcb,EAAMlE,GAOlBgF,EAAc,CAClBxwB,uBAN6B,CAC7BjB,EACAG,EAA0B7N,EAAc8N,SACrC0wB,EAAUhN,eAAe9jB,EAAK,CAAE6jB,gBAAgB,EAAO1jB,aAI1DoB,cACAO,gBACAX,gBACA+M,WAIkB,IAAIyL,GAAY8X,GAKpC,OAFAf,EAAa,CAAEK,YAER,CACLW,SAxCgBhwB,GAAcH,EAAYjR,KAAKoR,GAyC/CiwB,kBAAmB,MAAQ7vB,EAAcI,WAEzC6uB,SAAS,QAAMA,GACfJ,MAAM,QAAMA,GACZxvB,gBAEA2vB,WAAW,QAAMA,GACjBc,YACEhxB,GACG8vB,EAAa9vB,GAElBixB,UAAYtyB,GACV2O,EAAQ5d,KAAK,IAAK4d,EAAQ/a,SAAUoM,IACvC,EAGsBuyB,GnE4DlB,IAA4B78B,GAAoB+3B,GAApB/3B,GmEvDnBxE,KnEuDuCu8B,GmEvDjCyD,GnEwDpBv8B,SACgC,IAArBe,GAAO88B,UAChB98B,GAAO88B,UAAavd,IAClB,MAAMhiB,EAAOgiB,EAAEwd,MAAM,GACrBh9B,EAAgBxC,IAEhB,OAAOw6B,GAAKx6B,EAAK,GAInB,OAAOw6B,G,qCqE1GJ,SAASiF,EAAkB/Y,GAChC,MAAM,OAAEoV,EAAQpV,SAAUgZ,EAAE,QAAEC,GA3BzB,SAA4BjZ,GAEjC,MAAMkZ,EAAY,2BAElB,IAAI57B,EACA67B,EAAa,GACbC,EAAmBpZ,EACnBiZ,GAAU,EAEd,KAA8C,QAAtC37B,EAAQ47B,EAAUG,KAAKrZ,KAC7BiZ,GAAU,EAEVE,GAAc77B,EAAM,GAAK,KAGzB87B,EAAmBA,EAAiBpf,QAAQ1c,EAAM,GAAI,IAIxD,MAAO,CACL83B,OAAQ+D,EAAWtV,OACnB7D,SAAUoZ,EACVH,UAEJ,CAG4CK,CAAmBtZ,GAE7D,OAAOiZ,EAAU7D,EAAS4D,CAC5B,C,6LC9C2B,qBAAyC,GA6BpE,I,gDCzBA,MAaM,EACJ,qEA6HIO,EAA6BhgC,MAAOigC,EAAQ9iB,KAChD,IACE,MAAMM,QC9IHzd,eAA2Bmd,GAQhC,aAPuB,IAAMrb,IAC3B,GACE/C,EAAA,EAAgBC,QAAQb,qCACI,YAAmC,SAC/D,QAAQkF,KAAKC,UAAU6Z,SAGX1c,KAAKA,IACvB,CDqI2By/B,CAAY/iB,GAKnC,OAAOM,CACT,CAAE,MAAOzb,GAEP,OADA,EAAQU,IAAI,QAASV,GACd,IACT,G,4BE9JK,MAAMm+B,UAAgC1xB,MAG3CnD,YAAYmS,GACV,IAAI9V,EAAU,GACVy4B,GAAQ,EACR3iB,aAAoB2D,MACtBzZ,EAAU8V,EAASzG,KAAK,QACfyG,EAAS4iB,QAClB14B,EAAU8V,EAAS4iB,OAAOlwB,WAC1BiwB,EAAO3iB,EAAS2iB,MAEhBz4B,EAAUA,GAAS3F,MAGrBwa,MAAM7U,GACN24B,EAAOt+B,MAAM2F,EAAS,CAAE3F,MAAOyb,IAE/BlS,KAAK60B,KAAOA,CACd,EAGK,MCXDG,EAAa,CACjBhpB,OAAQ,GACRipB,IAAK,KAAmBrwB,YAGbswB,EAAgBzgC,MAC3BoG,EACAN,EACAb,GAEEsB,WACAm6B,iBAKFC,EAAcJ,KAEd,MACMz/B,EDR4B,CAClC2c,IAGA,GADwBA,aAAoB2D,OAA2B,IAAlB3D,EAAS2iB,KAE5D,MAAM,IAAID,EAAwB1iB,GAEpC,OAAOA,CAAQ,ECCA,OADQijB,EAAeE,UAAUx6B,EAAQN,EAAMb,EAAI07B,KAG5D,gBAAEp7B,GAAoBzE,EACtBqV,EAAO,CACXrQ,OACAb,KACAM,kBACAH,WAAW,UACXgB,UAOF,aAHMG,GAAUs6B,aAAa1qB,UACvB5P,GAAUu6B,kBAAkB3qB,IAE3B5Q,CAAe,E,eCbxB,MAwJM44B,EAxJiB,MACrB,MAAM4C,EAA0C,CAE9CzC,QAAS,IAAIhwB,EAAA,OAA0C,GACvD4vB,KAAM,IAAI5vB,EAAA,OAAuC,GACjD0yB,YAAa,IAAI1yB,EAAA,OAA8C,GAC/DK,aAAc,IAAIL,EAAA,OAAsC,GACxDoyB,cAAe,IAAIpyB,EAAA,OACjB,GAEF/H,SAAU,IAAI+H,EAAA,OAA2C,GACzDpI,QAAS,IAAIoI,EAAA,OAA0C,GACvDwW,MAAO,IAAIxW,EAAA,OAAwC,IAGrD,IAAI8M,EAEJ,MAAM6lB,EACJ75B,GAEO,IAAI/G,SAASC,IAClB,MAAM4gC,EAAQH,EAAY35B,GAGtB85B,EAAMzxB,YACRnP,EAAQ4gC,EAAMzxB,YAGhByxB,EACGnyB,MACC,EAAAC,EAAA,IAAOtO,KAAYA,KAGpBqB,WAAWrB,IACVJ,EAAQI,EAAM,GACd,IAIR,EAAAygC,YAAYnO,QAAQ,MAAS9wB,MAAM+9B,IACjCc,EAAYC,aAAanjC,KAAKoiC,EAAO,IAGvC,MAmBMmB,EAAoBphC,MAAOuN,UACR0zB,EAAmB,YAC3BvC,iBAAiBnxB,EAAK,QA2CjC8zB,EAAS,CACbC,sBAV4B,KAC5BlmB,EAAkB,IAAIC,gBACfD,GASPmmB,YAzDkBvhC,MAAOmd,EAAeqkB,EAAO,KAC/C,MAAMR,QAAqBC,EACzB,eAGIQ,QAAoB,QAAetkB,GAEzC,OAAO,QAAa6jB,EAAaS,EAAaD,EAAK,EAmDnDZ,UAAW5gC,MAAO8F,EAAcb,KAC9B,MAAMiB,EAAU66B,EAAY76B,QAAQuJ,WACpC,IAAKvJ,EACH,MAAM,IAAIuI,MAAM,6BAElB,MAAMlI,QAAkB06B,EAAmB,YACrCP,QAAuBO,EAC3B,iBAGF,OAAOR,EAAcv6B,EAASJ,EAAMb,EAAI,CACtCsB,WACAm6B,iBACA,EAEJgB,sBAAuB1hC,MAAO2hC,UACFV,EAAmB,eAA7C,MACMW,OJsFkB5hC,OAAOigC,EAAQ0B,KAC3C,IACE,MAAMxkB,EAAQ,CACZ0kB,qBAAsB,CACpBF,aAIJ,aADuB3B,EAA2BC,EAAQ9iB,EAE5D,CAAE,MAAOnb,GAEP,OADA,EAAQU,IAAI,QAASV,GACd,IACT,GIlG2B0/B,CAAsBV,EAAaW,GAE1D,OAAOC,CAAQ,EAEjBE,iBAAkB9hC,MAAO6P,EAAcyP,EAAQ,MAC7C,MAAM3Q,QAAsBsyB,EAC1B,gBAIF,aAFMA,EAAmB,SAElBtyB,EAAa2rB,kBAAkBzqB,EAAMyP,EAAM,EAEpDyiB,mBAxEyB/hC,MACzBuN,EACAmuB,EACA5uB,EAAS,CAAC,KAEV,IACE,MAAMhM,QAAesgC,EAAkB7zB,GACvC,QAAwB,IAApBzM,GAAQsM,QACV,MAAO,CAAE3F,OAAQ,QAASE,QAAS,sBAIrC,MAAMq6B,GAAa,QAAkBlhC,EAAOsM,SAI5C,aAFoB6zB,EAAmB,SAE3BrD,gBAAgBoE,EAAYtG,EAAU5uB,EACpD,CAAE,MAAOiV,GACP,MAAO,CAAEta,OAAQ,QAASE,QAASoa,EAAE5R,WACvC,GAsDAixB,oBACA7T,gBAAiBvtB,MAAOoN,UACC6zB,EAAmB,YAE3BnC,WAAW1xB,GAE5B60B,sBAzD4BjiC,MAAOy8B,EAAeh8B,EAAO,CAAC,KAC1D,IAEE,aADoBwgC,EAAmB,SAC3BpD,gBAAgBpB,EAAOh8B,EACrC,CAAE,MAAOshB,GACP,MAAO,CAAEta,OAAQ,QAASE,QAASoa,EAAE5R,WACvC,IAsDF,MAAO,CAAE8tB,aA1GaiE,IACpBx+B,OAAOC,KAAKu+B,GACTv7B,QAAQS,QAAuD,IAA9C86B,EAAa96B,KAC9BxD,SAASwD,IACR,MAAMvC,EAAOq9B,EAAa96B,GAC1B25B,EAAY35B,GAA6BvJ,KAAKgH,EAAK,GACnD,EAoGiBw8B,SAAQtkB,MA9CjB,KACZ3B,GAAiB2B,OAAO,EA6CY,EAGvBolB,GAMjB,O,2VC9KA,MAAMC,EAA6C,CACjDtI,MAAO,iBAGIuI,EAAmBriC,MAC9BuT,EACA+uB,EACAx1B,EAAgC,CAAC,EACjCy1B,EACAnnB,KAEA,MAAM1H,EAAOrQ,KAAKC,UAAU,CAC1BiQ,cACG6uB,KACAt1B,IAGCwf,EAAU,CACd,eAAgB,mBAChBkW,cAAe,UAAUF,KAGrB7kB,QAAiBiP,MAAM,6CAA8C,CACzE3pB,OAAQ,OACR0P,OAAQ2I,GAAiB3I,OACzB6Z,UACA5Y,SAGF,IAAK5G,EAAOif,OAAQ,CAGlB,aADmBtO,EAASglB,QAChBC,QAAQ,GAAG/6B,QAAQyF,OACjC,CAEA,MAAMyc,EAASpM,EAAS/J,MAAMoW,YACxB6Y,EAAU,IAAIC,YACpB,IAAI9hC,EAAS,GACT+hC,EAAS,GAEb,GAAIhZ,EAEF,OAAa,CAEX,MAAM,KAAE1pB,EAAI,MAAEO,SAAgBmpB,EAAOG,OACrC,GAAI7pB,GAAQib,GAAiB3I,OAAOC,QAClC,MAGFmwB,GAAUF,EAAQlvB,OAAO/S,EAAO,CAAEqrB,QAAQ,IAC1C,MAAM+W,EAAQD,EAAOE,MAAM,MAG3BF,EAASC,EAAME,OAAS,GAGxB,UAAWC,KAAQH,EAAO,CACxB,MAAMn7B,EAAUs7B,EAAKxiB,QAAQ,UAAW,IACxC,GAAgB,WAAZ9Y,EACF,OAAO7G,EAET,IACE,MAAMoiC,EAAS7/B,KAAK6F,MAAMvB,GAC1B,GAAIu7B,EAAOR,SAAWQ,EAAOR,QAAQ75B,OAAS,EAAG,CAC/C,MAAM,QAAEuE,GAAY81B,EAAOR,QAAQ,GAAGS,MACtCriC,GAAUsM,EACNA,SAEIm1B,EAAGn1B,EAEb,CACF,CAAE,MAAOpL,GACPY,EAAQZ,MAAM,gCAAiC2F,EAAS3F,EAC1D,CACF,CACF,CAGF,OAAOlB,CAAM,E,eCpFRd,eAAeojC,EAAcjmB,GAClC,OAAOghB,EAAA,EAASkD,OAAOE,YAAYpkB,EACrC,CAEOnd,eAAeqjC,EAAYC,EAAS9Q,GACzC,OAAO2L,EAAA,EAASkD,OAAOT,UAAU0C,EAAS9Q,EAC5C,CAEOxyB,eAAeujC,EAAwB5B,GAC5C,OAAOxD,EAAA,EAASkD,OAAOK,sBAAsBC,EAC/C,CAEO3hC,eAAewjC,EAAqBj2B,EAAKmuB,EAAU5uB,EAAS,CAAC,GAClE,OAAOqxB,EAAA,EAASkD,OAAOU,mBAAmBx0B,EAAKmuB,EAAU5uB,EAC3D,CAEO9M,eAAeyjC,EAAqBl2B,GACzC,OAAO4wB,EAAA,EAASiD,kBAAkB7zB,EACpC,CAEOvN,eAAe0jC,EAAkBt2B,GACtC,OAAO+wB,EAAA,EAAS5Q,gBAAgBngB,EAClC,CAEOpN,eAAe2jC,EAAwBlH,EAAOh8B,GAEnD,OADA,EAAQiC,IAAI,qBAAsB+5B,GAC3B0B,EAAA,EAASkD,OAAOY,sBAAsBxF,EAAOh8B,EACtD,CAEOT,eAAe4jC,EAAoBrwB,EAAU+uB,EAAQx1B,EAAQ2vB,GASlE,aAPqB4F,EACnB9uB,EACA+uB,EACAx1B,GAJe9M,MAAOS,GAASkjC,EAAwBlH,EAAOh8B,IAM9D09B,EAAA,EAASkD,OAAOC,wBAGpB,CAEOthC,eAAe6jC,EAAoBh0B,EAAMyP,GAC9C,OAAO6e,EAAA,EAASkD,OAAOS,iBAAiBjyB,EAAMyP,EAChD,CAEOtf,eAAe8jC,EAAiBv2B,GAErC,aADqB,QAAYA,EAEnC,CAEOvN,eAAe+jC,EAAex2B,GAEnC,aADqB,QAAUA,EAEjC,C,qECjEO,IAAWy2B,EAAX,CAAWA,IAChBA,EAAA,QAAU,UACVA,EAAA,YAAc,cACdA,EAAA,IAAM,MACNA,EAAA,KAAO,OACPA,EAAA,MAAQ,QACRA,EAAA,OAAS,cANOA,GAAX,CAAWA,GAAA,G,sECAlB,MAiBMh6B,EAAS,CACbI,uBAAwB,CACtB65B,WAAY,YACZC,OAAQ,SACR75B,MAAO,S,8ICnBJ,MAAM85B,EAAmB/+B,GAC9B,IAAW,IAAIC,KAAKD,GAAY,2BAA2B,GAEhDg/B,EAAmBC,GAC9Bh/B,KAAK6D,MAAMm7B,EAAUC,SAAS,KAAOD,EAAY,GAAGA,MAEzCE,EAAkB,IAAMl/B,KAAK0V,K,yICCnC,MAKMypB,EAAe7yB,GAC1B,IAAItR,SAAQ,CAACC,EAASmkC,KACpB,MAEM5B,EAFa,IAAI,IAAJ,CAAW,OAAQzY,EAAOtkB,KAAK6L,IAExB+yB,UAC1B,EAAAC,QAAA,OAAe9B,GAAQ,CAAC3yB,EAAK00B,KACvB10B,GACFu0B,EAAO,IAAIh2B,MAAM,+BAGnB,WAAYm2B,GAAS,CAAC5iC,EAAOuL,KAC3BjN,EAAQiN,EAAIs3B,sBAAsB,GAClC,GACF,G,qEC3BC,MAEMC,EAAgC,yB,iHCE7C,MAAMC,EAAqB,GAoJpB,MAAMC,EAAsB,CACjCC,EAAyC,CAAC,KAE1C,MAAMz5B,EAAU,IAAIC,iBAAiB,MAErC,SAASy5B,EACPC,EACAx9B,EACAq0B,GAEA,MAAM5d,EAAM,IAAK6mB,KAAmBjJ,GAChCA,GAASh6B,QACXoc,EAAIpc,MAAQqB,KAAKC,UAAU04B,EAAQh6B,QAErCwJ,EAAQpK,YAAY,CAClBqC,KAAM,MACN/C,MAAO,CAAEykC,QAAOx9B,UAASq0B,QAAS5d,IAEtC,CAkBA,MAAO,CAAEzC,KAhBT,SAAiBhU,EAAYq0B,GAC3B,OAAOkJ,EAAiB,OAAQv9B,EAASq0B,EAC3C,EAceh6B,MAZf,SAAkB2F,EAAYq0B,GAC5B,OAAOkJ,EAAiB,QAASv9B,EAASq0B,EAC5C,EAUsBn5B,KARtB,SAAiB8E,EAAYq0B,GAC3B,OAAOkJ,EAAiB,OAAQv9B,EAASq0B,EAC3C,EAM4BoJ,MAJ5B,SAAkBz9B,EAAYq0B,GAC5B,OAAOkJ,EAAiB,OAAQv9B,EAASq0B,EAC3C,EAEmC,EAG/BsE,EAzLN,SAAyB2E,EAAyC,CAAC,GAQjE,IAAII,EAAmB,CAAC,EAyCxB,SAAS3iC,EACPyiC,EACAx9B,EACAq0B,EAA2BiJ,GAE3B,IACE,MAAMK,EAAmBtJ,GAASuJ,UAC9BvJ,GAASuJ,UAAU59B,GACnBA,GAxDR,SAAmB69B,EAAkBC,GAAW,GAG9C,IAFAV,EAAQ98B,KAAKu9B,GAENC,GAAYV,EAAQl8B,OAAS,KAClCk8B,EAAQrqB,OAEZ,CA4DIgrB,CARiB,CACftgC,UAAW,IAAIC,KACf8/B,QACAx9B,QAAS29B,EACTK,WAAY3J,GAAS2J,WACrB3J,QAAS,SAAOA,EAAS,CAAC,YAAa,iBAKlBt4B,OAAOC,KAAK0hC,GAAkB18B,QACnD,CAACC,EAAcmY,KACb,MAAMjU,EAASu4B,EAAiBtkB,GAC1B6kB,EAAc5J,EAAQjb,GAC5B,OAAIjU,GAAU84B,EAEVh9B,GACW,QAAXkE,GACkB,IAAlBA,EAAOjE,QACPiE,EAAO5F,MAAM2+B,GAAMA,IAAMD,IAGtBh9B,CAAG,IAEZ,IA/DN,SACEu8B,EACAx9B,EACAq0B,GAEA,MAAM5d,EAAM,SAAO4d,EAAS,CAC1B,YACA,SACA,SACA,OACA,UAEI,OAAEzgB,EAAS,UAAIC,EAAS,QAAI6C,EAAO,QAAI5d,EAAO,IAAOu7B,EACrD8J,GAAU,IAAAxnB,SAAQF,GAAO,GAAKA,EAEhCgD,MAAMC,QAAQ1Z,GAChB/E,EAAQuiC,MAAUx9B,EAASm+B,GAIzB9J,GAASuJ,UACX3iC,EAAQuiC,GAAOnJ,GAASuJ,UAAU59B,GAAUm+B,GAI9CljC,EAAQuiC,GAAO,IAAI5pB,KAAUC,KAAU6C,MAAS1W,IAAWlH,EAAMqlC,EACnE,CAyCMC,CAAWZ,EAAOx9B,EAASq0B,EAE/B,CAAE,MAAOh6B,GACPY,EAAQF,IAAI,eAAgBV,EAC9B,CACF,CAyCA,OA3HgB,IAAIyJ,iBAAiB,MAE7BlL,UAAaC,IACK,WAApBA,EAAMC,KAAKgD,OACb4hC,EAAmB,IAAKA,KAAqB7kC,EAAMC,KAAKC,OAC1D,EAsHK,CACLgC,MACAiZ,KAzCF,SAAiBhU,EAAYq0B,GAC3B,OAAOt5B,EAAI,OAAQiF,EAASq0B,EAC9B,EAwCEh6B,MAtCF,SAAkB2F,EAAYq0B,GAC5B,OAAOt5B,EAAI,QAASiF,EAASq0B,EAC/B,EAqCEn5B,KAnCF,SAAiB8E,EAAYq0B,GAC3B,OAAOt5B,EAAI,OAAQiF,EAASq0B,EAC9B,EAkCEoJ,MAhCF,SAAkBz9B,EAAYq0B,GAC5B,OAAOt5B,EAAI,OAAQiF,EAASq0B,EAC9B,EA+BE+I,UACAiB,QAAS,IA7BFjB,EAAQ5hC,KAAKqiC,IAClB,MAAM,QAAExJ,KAAYh7B,GAASwkC,GACvB,KACJnnB,EAAO,UACP7C,EAAS,UACTD,EAAS,QACT9a,EAAO,GACPuB,MAAAA,EAAQ,cACR2jC,EAAa,IACX3J,GAAW,CAAC,EAChB,MAAO,IACFh7B,EACHqd,OACA7C,SACAD,SACA9a,OACAuB,MAAAA,EACA2jC,aACD,IAYHjU,MAAO,IAAMqT,EAAQkB,OAAO,EAAGlB,EAAQl8B,QACvCq9B,oBA5H0B,IAAMb,EA8HpC,CAyCec,CAAa,CAAE5qB,OAAQ,SAMtC,K,8NCkPA,MAAM6qB,EAAUpmC,MACduN,EACA9J,EAAkB,IAAgBqC,MAChCiH,SAAQkI,QAAOwD,QAAQ,oBAEzB,IAaE,aAZuB,OAAM,CAC3B1V,OAAQ,MACRmR,IAAK,GAAG,6BACRpH,OAAQ,CACN,oBAAqBC,EACrB,mBAAoBkI,EACpBC,QAAS,gBACT7B,OAAQ,qBACN5P,IAAS,IAAgBwB,GAAK,KAAO,WAClCsI,SAGO9M,IAClB,CAAE,MAAOshB,GAEP,OADAnf,EAAQF,IAAIqf,GACL,IACT,GAGWskB,EAAcrmC,MAAOuN,EAAKR,EAAQkI,IACtCmxB,EAAQ74B,EAAK,IAAgBzH,KAAM,CAAEiH,SAAQkI,UAGzCqxB,EAAYtmC,MAAOuN,EAAKR,EAAQkI,IACpCmxB,EAAQ74B,EAAK,IAAgBtI,GAAI,CAAE8H,SAAQkI,UAkH7C,MAsHMsxB,EAAiBvmC,MAAOmd,GACnCA,EAAMpZ,MAAM,MAAqBoZ,GAAQ,QAAY,QAAYA,IAEtDqpB,EAAexmC,MAC1BigC,EACAz6B,EACAg8B,KAEA,IAGE,aAFsBvB,EAAOwG,OAAOjhC,EAAMg8B,EAG5C,CAAE,MAAOx/B,GAGP,YADAY,EAAQZ,MAAMA,EAEhB,E,2KCpaF,MAEM0kC,EAAe72B,GAASA,EAAK4Q,QAAQ,OAAQ,I,g+BC1S/CkmB,EAA2B,CAAC,EAGhC,SAASC,EAAoBC,GAE5B,IAAIC,EAAeH,EAAyBE,GAC5C,QAAqBE,IAAjBD,EACH,OAAOA,EAAaE,QAGrB,IAAIxrB,EAASmrB,EAAyBE,GAAY,CACjDjhC,GAAIihC,EACJ/L,QAAQ,EACRkM,QAAS,CAAC,GAUX,OANAC,EAAoBJ,GAAU3lB,KAAK1F,EAAOwrB,QAASxrB,EAAQA,EAAOwrB,QAASJ,GAG3EprB,EAAOsf,QAAS,EAGTtf,EAAOwrB,OACf,CAGAJ,EAAoBM,EAAID,EAGxBL,EAAoBO,EAAI,WAGvB,IAAIC,EAAsBR,EAAoBS,OAAEN,EAAW,CAAC,EAAE,IAAI,GAAG,GAAG,IAAI,IAAI,MAAM,WAAa,OAAOH,EAAoB,MAAQ,IAEtI,OADAQ,EAAsBR,EAAoBS,EAAED,EAE7C,ECrCAR,EAAoBU,KAAO,CAAC,EhGAxB5pC,EAAW,GACfkpC,EAAoBS,EAAI,SAASvmC,EAAQymC,EAAUzV,EAAIpkB,GACtD,IAAG65B,EAAH,CAMA,IAAIC,EAAeC,IACnB,IAASt+B,EAAI,EAAGA,EAAIzL,EAASmL,OAAQM,IAAK,CACrCo+B,EAAW7pC,EAASyL,GAAG,GACvB2oB,EAAKp0B,EAASyL,GAAG,GACjBuE,EAAWhQ,EAASyL,GAAG,GAE3B,IAJA,IAGIM,GAAY,EACPi+B,EAAI,EAAGA,EAAIH,EAAS1+B,OAAQ6+B,MACpB,EAAXh6B,GAAsB85B,GAAgB95B,IAAahK,OAAOC,KAAKijC,EAAoBS,GAAGM,OAAM,SAAS5mB,GAAO,OAAO6lB,EAAoBS,EAAEtmB,GAAKwmB,EAASG,GAAK,IAChKH,EAAStB,OAAOyB,IAAK,IAErBj+B,GAAY,EACTiE,EAAW85B,IAAcA,EAAe95B,IAG7C,GAAGjE,EAAW,CACb/L,EAASuoC,OAAO98B,IAAK,GACrB,IAAIy+B,EAAI9V,SACEiV,IAANa,IAAiB9mC,EAAS8mC,EAC/B,CACD,CACA,OAAO9mC,CArBP,CAJC4M,EAAWA,GAAY,EACvB,IAAI,IAAIvE,EAAIzL,EAASmL,OAAQM,EAAI,GAAKzL,EAASyL,EAAI,GAAG,GAAKuE,EAAUvE,IAAKzL,EAASyL,GAAKzL,EAASyL,EAAI,GACrGzL,EAASyL,GAAK,CAACo+B,EAAUzV,EAAIpkB,EAwB/B,EiG5BAk5B,EAAoBiB,EAAI,SAASrsB,GAChC,IAAIssB,EAAStsB,GAAUA,EAAOusB,WAC7B,WAAa,OAAOvsB,EAAgB,OAAG,EACvC,WAAa,OAAOA,CAAQ,EAE7B,OADAorB,EAAoBoB,EAAEF,EAAQ,CAAE9+B,EAAG8+B,IAC5BA,CACR,EhGPIlqC,EAAW8F,OAAOukC,eAAiB,SAASpnC,GAAO,OAAO6C,OAAOukC,eAAepnC,EAAM,EAAI,SAASA,GAAO,OAAOA,EAAIqnC,SAAW,EAQpItB,EAAoB7vB,EAAI,SAASrW,EAAOynC,GAEvC,GADU,EAAPA,IAAUznC,EAAQ6K,KAAK7K,IAChB,EAAPynC,EAAU,OAAOznC,EACpB,GAAoB,iBAAVA,GAAsBA,EAAO,CACtC,GAAW,EAAPynC,GAAaznC,EAAMqnC,WAAY,OAAOrnC,EAC1C,GAAW,GAAPynC,GAAoC,mBAAfznC,EAAMwB,KAAqB,OAAOxB,CAC5D,CACA,IAAI0nC,EAAK1kC,OAAO2kC,OAAO,MACvBzB,EAAoBgB,EAAEQ,GACtB,IAAIE,EAAM,CAAC,EACX3qC,EAAiBA,GAAkB,CAAC,KAAMC,EAAS,CAAC,GAAIA,EAAS,IAAKA,EAASA,IAC/E,IAAI,IAAI2qC,EAAiB,EAAPJ,GAAYznC,EAAyB,iBAAX6nC,KAAyB5qC,EAAeyrB,QAAQmf,GAAUA,EAAU3qC,EAAS2qC,GACxH7kC,OAAO8kC,oBAAoBD,GAAS3kC,SAAQ,SAASmd,GAAOunB,EAAIvnB,GAAO,WAAa,OAAOrgB,EAAMqgB,EAAM,CAAG,IAI3G,OAFAunB,EAAa,QAAI,WAAa,OAAO5nC,CAAO,EAC5CkmC,EAAoBoB,EAAEI,EAAIE,GACnBF,CACR,EiGxBAxB,EAAoBoB,EAAI,SAAShB,EAASyB,GACzC,IAAI,IAAI1nB,KAAO0nB,EACX7B,EAAoB8B,EAAED,EAAY1nB,KAAS6lB,EAAoB8B,EAAE1B,EAASjmB,IAC5Erd,OAAOilC,eAAe3B,EAASjmB,EAAK,CAAE6nB,YAAY,EAAM9mC,IAAK2mC,EAAW1nB,IAG3E,ECPA6lB,EAAoBiC,EAAI,CAAC,EAGzBjC,EAAoB7kB,EAAI,SAAS+mB,GAChC,OAAOzoC,QAAQgQ,IAAI3M,OAAOC,KAAKijC,EAAoBiC,GAAGlgC,QAAO,SAASogC,EAAUhoB,GAE/E,OADA6lB,EAAoBiC,EAAE9nB,GAAK+nB,EAASC,GAC7BA,CACR,GAAG,IACJ,ECPAnC,EAAoBoC,EAAI,SAASF,GAEhC,OAAgB,IAAZA,EAAsB,gBACV,MAAZA,EAAwB,kBACZ,KAAZA,EAAuB,iBAEfA,EAAU,IAAM,CAAC,GAAK,WAAW,IAAM,WAAW,IAAM,WAAW,IAAM,WAAW,IAAM,YAAYA,GAAW,WAC9H,ECPAlC,EAAoBqC,SAAW,SAASH,GAGxC,ECJAlC,EAAoBsC,EAAI,WACvB,GAA0B,iBAAfC,WAAyB,OAAOA,WAC3C,IACC,OAAO59B,MAAQ,IAAI69B,SAAS,cAAb,EAChB,CAAE,MAAOrnB,GACR,GAAsB,iBAAXwQ,OAAqB,OAAOA,MACxC,CACA,CAPuB,GCAxBqU,EAAoByC,IAAM,SAAS7tB,GASlC,OARAA,EAAS9X,OAAO2kC,OAAO7sB,IACX8tB,WAAU9tB,EAAO8tB,SAAW,IACxC5lC,OAAOilC,eAAentB,EAAQ,UAAW,CACxCotB,YAAY,EACZlnC,IAAK,WACJ,MAAM,IAAI+M,MAAM,0FAA4F+M,EAAO5V,GACpH,IAEM4V,CACR,ECVAorB,EAAoB8B,EAAI,SAAS7nC,EAAK0oC,GAAQ,OAAO7lC,OAAOsd,UAAUC,eAAeC,KAAKrgB,EAAK0oC,EAAO,ECCtG3C,EAAoBgB,EAAI,SAASZ,GACX,oBAAX/mC,QAA0BA,OAAOupC,aAC1C9lC,OAAOilC,eAAe3B,EAAS/mC,OAAOupC,YAAa,CAAE9oC,MAAO,WAE7DgD,OAAOilC,eAAe3B,EAAS,aAAc,CAAEtmC,OAAO,GACvD,ECNAkmC,EAAoB6C,IAAM,SAASjuB,GAGlC,OAFAA,EAAOkuB,MAAQ,GACVluB,EAAO8tB,WAAU9tB,EAAO8tB,SAAW,IACjC9tB,CACR,ECJAorB,EAAoBf,EAAI,I,WCAxBe,EAAoB39B,EAAIjL,KAAK2rC,SAAW,GAIxC,IAAIC,EAAkB,CACrB,IAAK,GAkBNhD,EAAoBiC,EAAE1/B,EAAI,SAAS2/B,EAASC,GAEvCa,EAAgBd,IAElBe,cAAcjD,EAAoBf,EAAIe,EAAoBoC,EAAEF,GAG/D,EAEA,IAAIgB,EAAqB9rC,KAAsB,gBAAIA,KAAsB,iBAAK,GAC1E+rC,EAA6BD,EAAmB7hC,KAAKyR,KAAKowB,GAC9DA,EAAmB7hC,KAzBA,SAASxH,GAC3B,IAAI8mC,EAAW9mC,EAAK,GAChBupC,EAAcvpC,EAAK,GACnBk8B,EAAUl8B,EAAK,GACnB,IAAI,IAAIomC,KAAYmD,EAChBpD,EAAoB8B,EAAEsB,EAAanD,KACrCD,EAAoBM,EAAEL,GAAYmD,EAAYnD,IAIhD,IADGlK,GAASA,EAAQiK,GACdW,EAAS1+B,QACd+gC,EAAgBrC,EAASvE,OAAS,EACnC+G,EAA2BtpC,EAC5B,C,I1GtBI5C,EAAO+oC,EAAoBO,EAC/BP,EAAoBO,EAAI,WACvB,OAAO9mC,QAAQgQ,IAAI,CAAC,EAAE,IAAI,GAAG,GAAG,IAAI,IAAI,KAAKlN,IAAIyjC,EAAoB7kB,EAAG6kB,IAAsB1kC,KAAKrE,EACpG,E2GF0B+oC,EAAoBO,G","sources":["webpack://cyb/webpack/runtime/chunk loaded","webpack://cyb/webpack/runtime/create fake namespace object","webpack://cyb/webpack/runtime/startup chunk dependencies","webpack://cyb/./src/constants/config.ts","webpack://cyb/./src/constants/defaultNetworks.ts","webpack://cyb/./src/constants/patterns.ts","webpack://cyb/./src/containers/Search/types.ts","webpack://cyb/./src/services/QueueManager/types.ts","webpack://cyb/./src/services/backend/workers/serializers.ts","webpack://cyb/./src/services/backend/workers/factoryMethods.ts","webpack://cyb/./src/features/particle/utils.tsx","webpack://cyb/./src/services/CozoDb/types/entities.ts","webpack://cyb/./src/features/sense/redux/sense.redux.ts","webpack://cyb/./src/constants/localStorageKeys.ts","webpack://cyb/./src/redux/features/pocket.ts","webpack://cyb/./src/services/backend/channels/consts.ts","webpack://cyb/./src/services/backend/channels/BroadcastChannelSender.ts","webpack://cyb/./src/services/backend/channels/broadcastStatus.ts","webpack://cyb/./src/utils/async/iterable.ts","webpack://cyb/./src/services/backend/channels/BackendQueueChannel/backendQueueSenders.ts","webpack://cyb/./src/constants/app.ts","webpack://cyb/./src/services/backend/services/sync/services/consts.ts","webpack://cyb/./src/services/backend/services/sync/services/ParticlesResolverQueue/ParticlesResolverQueue.ts","webpack://cyb/./src/utils/string.ts","webpack://cyb/./src/services/CozoDb/mapping.ts","webpack://cyb/./src/utils/async/promise.ts","webpack://cyb/./src/generated/graphql.ts","webpack://cyb/./src/services/backend/services/indexer/types.ts","webpack://cyb/./src/services/lcd/utils/mapping.ts","webpack://cyb/./src/services/backend/services/indexer/utils/graphqlClient.ts","webpack://cyb/./src/services/backend/services/indexer/cyberlinks.ts","webpack://cyb/./src/services/backend/services/indexer/consts.ts","webpack://cyb/./src/services/backend/services/sync/services/utils/links.ts","webpack://cyb/./src/services/backend/services/indexer/transactions.ts","webpack://cyb/./src/services/backend/services/sync/services/utils/sense.ts","webpack://cyb/./src/services/backend/services/sync/services/SyncTransactionsLoop/services/chat.ts","webpack://cyb/./src/services/backend/services/sync/services/ProgressTracker/ProgressTracker.ts","webpack://cyb/./src/services/backend/services/sync/services/BaseSyncLoop/BaseSync.ts","webpack://cyb/./src/services/backend/services/sync/services/utils/rxjs/withInitializer.ts","webpack://cyb/./src/services/backend/services/sync/services/BaseSyncLoop/BaseSyncClient.ts","webpack://cyb/./src/services/backend/services/sync/services/SyncTransactionsLoop/SyncTransactionsLoop.ts","webpack://cyb/./src/services/lcd/websocket.ts","webpack://cyb/./src/utils/dto.ts","webpack://cyb/./src/services/backend/services/sync/utils.ts","webpack://cyb/./src/utils/exceptions/helpers.ts","webpack://cyb/./src/services/backend/services/sync/services/BaseSyncLoop/BaseSyncLoop.ts","webpack://cyb/./src/services/backend/services/sync/services/utils/rxjs/loop.ts","webpack://cyb/./src/services/backend/services/sync/services/SyncParticlesLoop/SyncParticlesLoop.ts","webpack://cyb/./src/services/backend/services/sync/services/SyncMyFriendsLoop/SyncMyFriendsLoop.ts","webpack://cyb/./src/services/community/community.ts","webpack://cyb/./src/services/community/lcd.ts","webpack://cyb/./src/services/backend/channels/BackendQueueChannel/BackendQueueChannel.ts","webpack://cyb/./src/services/backend/services/sync/sync.ts","webpack://cyb/./src/services/backend/services/sync/services/CommunitySync/CommunitySync.ts","webpack://cyb/./src/services/ipfs/utils/stream.ts","webpack://cyb/./src/db.js","webpack://cyb/./src/services/ipfs/utils/ipfsCacheDb.ts","webpack://cyb/./src/services/ipfs/config.ts","webpack://cyb/./src/services/ipfs/utils/cluster.ts","webpack://cyb/./src/services/ipfs/utils/content.ts","webpack://cyb/./src/services/ipfs/utils/utils-ipfs.ts","webpack://cyb/./src/services/QueueManager/QueueStrategy.ts","webpack://cyb/./src/services/QueueManager/QueueItemTimeoutError.ts","webpack://cyb/./src/services/QueueManager/constants.ts","webpack://cyb/./src/services/QueueManager/QueueManager.ts","webpack://cyb/./src/utils/rxjs/helpers.ts","webpack://cyb/./src/services/ipfs/utils/cid.ts","webpack://cyb/./src/services/ipfs/node/impl/kubo.ts","webpack://cyb/./src/services/ipfs/node/impl/helia.ts","webpack://cyb/./src/services/ipfs/node/impl/configs/jsIpfsConfig.ts","webpack://cyb/./src/services/ipfs/node/impl/js-ipfs.ts","webpack://cyb/./src/services/ipfs/node/factory.ts","webpack://cyb/./src/services/ipfs/node/mixins/withCybFeatures.ts","webpack://cyb/./src/services/backend/workers/background/api/mlApi.ts","webpack://cyb/./src/services/scripting/engine.ts","webpack://cyb/./src/services/backend/workers/background/api/runeApi.ts","webpack://cyb/./src/services/backend/workers/background/worker.ts","webpack://cyb/./src/services/backend/workers/background/api/ipfsApi.ts","webpack://cyb/./src/services/scripting/helpers.ts","webpack://cyb/./src/contexts/queryClient.tsx","webpack://cyb/./src/containers/portal/utils.ts","webpack://cyb/./src/services/passports/lcd.ts","webpack://cyb/./src/services/neuron/errors.ts","webpack://cyb/./src/services/neuron/neuronApi.ts","webpack://cyb/./src/services/scripting/runeDeps.ts","webpack://cyb/./src/services/scripting/services/llmRequests/openai.ts","webpack://cyb/./src/services/scripting/wasmBindings.js","webpack://cyb/./src/types/networks.ts","webpack://cyb/./src/utils/config.ts","webpack://cyb/./src/utils/date.ts","webpack://cyb/./src/utils/ipfs/helpers.ts","webpack://cyb/./src/utils/logging/constants.ts","webpack://cyb/./src/utils/logging/cyblog.ts","webpack://cyb/./src/utils/search/utils.ts","webpack://cyb/./src/utils/utils.ts","webpack://cyb/webpack/bootstrap","webpack://cyb/webpack/runtime/amd options","webpack://cyb/webpack/runtime/compat get default export","webpack://cyb/webpack/runtime/define property getters","webpack://cyb/webpack/runtime/ensure chunk","webpack://cyb/webpack/runtime/get javascript chunk filename","webpack://cyb/webpack/runtime/get mini-css chunk filename","webpack://cyb/webpack/runtime/global","webpack://cyb/webpack/runtime/harmony module decorator","webpack://cyb/webpack/runtime/hasOwnProperty shorthand","webpack://cyb/webpack/runtime/make namespace object","webpack://cyb/webpack/runtime/node module decorator","webpack://cyb/webpack/runtime/publicPath","webpack://cyb/webpack/runtime/importScripts chunk loading","webpack://cyb/webpack/startup"],"sourcesContent":["var deferred = [];\n__webpack_require__.O = function(result, chunkIds, fn, priority) {\n\tif(chunkIds) {\n\t\tpriority = priority || 0;\n\t\tfor(var i = deferred.length; i > 0 && deferred[i - 1][2] > priority; i--) deferred[i] = deferred[i - 1];\n\t\tdeferred[i] = [chunkIds, fn, priority];\n\t\treturn;\n\t}\n\tvar notFulfilled = Infinity;\n\tfor (var i = 0; i < deferred.length; i++) {\n\t\tvar chunkIds = deferred[i][0];\n\t\tvar fn = deferred[i][1];\n\t\tvar priority = deferred[i][2];\n\t\tvar fulfilled = true;\n\t\tfor (var j = 0; j < chunkIds.length; j++) {\n\t\t\tif ((priority & 1 === 0 || notFulfilled >= priority) && Object.keys(__webpack_require__.O).every(function(key) { return __webpack_require__.O[key](chunkIds[j]); })) {\n\t\t\t\tchunkIds.splice(j--, 1);\n\t\t\t} else {\n\t\t\t\tfulfilled = false;\n\t\t\t\tif(priority < notFulfilled) notFulfilled = priority;\n\t\t\t}\n\t\t}\n\t\tif(fulfilled) {\n\t\t\tdeferred.splice(i--, 1)\n\t\t\tvar r = fn();\n\t\t\tif (r !== undefined) result = r;\n\t\t}\n\t}\n\treturn result;\n};","var getProto = Object.getPrototypeOf ? function(obj) { return Object.getPrototypeOf(obj); } : function(obj) { return obj.__proto__; };\nvar leafPrototypes;\n// create a fake namespace object\n// mode & 1: value is a module id, require it\n// mode & 2: merge all properties of value into the ns\n// mode & 4: return value when already ns object\n// mode & 16: return value when it's Promise-like\n// mode & 8|1: behave like require\n__webpack_require__.t = function(value, mode) {\n\tif(mode & 1) value = this(value);\n\tif(mode & 8) return value;\n\tif(typeof value === 'object' && value) {\n\t\tif((mode & 4) && value.__esModule) return value;\n\t\tif((mode & 16) && typeof value.then === 'function') return value;\n\t}\n\tvar ns = Object.create(null);\n\t__webpack_require__.r(ns);\n\tvar def = {};\n\tleafPrototypes = leafPrototypes || [null, getProto({}), getProto([]), getProto(getProto)];\n\tfor(var current = mode & 2 && value; typeof current == 'object' && !~leafPrototypes.indexOf(current); current = getProto(current)) {\n\t\tObject.getOwnPropertyNames(current).forEach(function(key) { def[key] = function() { return value[key]; }; });\n\t}\n\tdef['default'] = function() { return value; };\n\t__webpack_require__.d(ns, def);\n\treturn ns;\n};","var next = __webpack_require__.x;\n__webpack_require__.x = function() {\n\treturn Promise.all([1,373,60,48,112,411,565].map(__webpack_require__.e, __webpack_require__)).then(next);\n};","import { Networks } from 'src/types/networks';\nimport defaultNetworks from './defaultNetworks';\n\n// FIXME: seems temp\nfunction isWorker() {\n  return (\n    typeof WorkerGlobalScope !== 'undefined' &&\n    self instanceof WorkerGlobalScope\n  );\n}\n\nconst LOCALSTORAGE_CHAIN_ID = !isWorker() && localStorage.getItem('chainId');\n\nconst DEFAULT_CHAIN_ID: Networks.BOSTROM | Networks.SPACE_PUSSY =\n  LOCALSTORAGE_CHAIN_ID || process.env.CHAIN_ID || Networks.BOSTROM;\n\nexport const CHAIN_ID = DEFAULT_CHAIN_ID;\n\nexport const LCD_URL =\n  process.env.LCD_URL || defaultNetworks[DEFAULT_CHAIN_ID].LCD_URL;\n\nexport const RPC_URL =\n  process.env.RPC_URL || defaultNetworks[DEFAULT_CHAIN_ID].RPC_URL;\n\nexport const WEBSOCKET_URL =\n  process.env.WEBSOCKET_URL || defaultNetworks[DEFAULT_CHAIN_ID].WEBSOCKET_URL;\n\nexport const INDEX_HTTPS =\n  process.env.INDEX_HTTPS || defaultNetworks[DEFAULT_CHAIN_ID].INDEX_HTTPS;\n\nexport const INDEX_WEBSOCKET =\n  process.env.INDEX_WEBSOCKET ||\n  defaultNetworks[DEFAULT_CHAIN_ID].INDEX_WEBSOCKET;\n\nexport const BECH32_PREFIX =\n  process.env.BECH32_PREFIX || defaultNetworks[DEFAULT_CHAIN_ID].BECH32_PREFIX;\n\nconst BECH32_PREFIX_VAL = `${BECH32_PREFIX}val`;\n\nexport const BECH32_PREFIX_VALOPER = `${BECH32_PREFIX_VAL}oper`;\n\nexport const BECH32_PREFIX_VAL_CONS = `${BECH32_PREFIX_VAL}cons`;\n\nexport const BASE_DENOM =\n  process.env.BASE_DENOM || defaultNetworks[DEFAULT_CHAIN_ID].BASE_DENOM;\n\nexport const DENOM_LIQUID =\n  process.env.DENOM_LIQUID || defaultNetworks[DEFAULT_CHAIN_ID].DENOM_LIQUID;\n\nexport const CYBER_GATEWAY =\n  process.env.CYBER_GATEWAY || 'https://gateway.ipfs.cybernode.ai';\n\nexport const DIVISOR_CYBER_G = 10 ** 9;\n\nexport const DEFAULT_GAS_LIMITS = 200000;\n\nexport const COIN_DECIMALS_RESOURCE = 3;\n\nexport const { MEMO_KEPLR } = defaultNetworks[DEFAULT_CHAIN_ID];\n","import { NetworkConfig, Networks } from 'src/types/networks';\n\ntype NetworksList = {\n  [key in Networks.BOSTROM | Networks.SPACE_PUSSY]: NetworkConfig;\n};\n\nconst defaultNetworks: NetworksList = {\n  bostrom: {\n    CHAIN_ID: Networks.BOSTROM,\n    BASE_DENOM: 'boot',\n    DENOM_LIQUID: 'hydrogen',\n    RPC_URL: process.env.IS_DEV\n      ? 'https://rpc.arch.bostrom.cybernode.ai:443'\n      : 'https://rpc.bostrom.cybernode.ai',\n    LCD_URL: process.env.IS_DEV\n      ? 'https://lcd.arch.bostrom.cybernode.ai:443'\n      : 'https://lcd.bostrom.cybernode.ai',\n    WEBSOCKET_URL: process.env.IS_DEV\n      ? 'wss://rpc.arch.bostrom.cybernode.ai:443/websocket'\n      : 'wss://rpc.bostrom.cybernode.ai/websocket',\n    INDEX_HTTPS: 'https://index.bostrom.cybernode.ai/v1/graphql',\n    INDEX_WEBSOCKET: 'wss://index.bostrom.cybernode.ai/v1/graphql',\n    BECH32_PREFIX: 'bostrom',\n    MEMO_KEPLR: '[bostrom] cyb.ai, using keplr',\n  },\n  'space-pussy': {\n    CHAIN_ID: Networks.SPACE_PUSSY,\n    BASE_DENOM: 'pussy',\n    DENOM_LIQUID: 'liquidpussy',\n    RPC_URL: 'https://rpc.space-pussy.cybernode.ai/',\n    LCD_URL: 'https://lcd.space-pussy.cybernode.ai',\n    WEBSOCKET_URL: 'wss://rpc.space-pussy.cybernode.ai/websocket',\n    INDEX_HTTPS: 'https://index.space-pussy.cybernode.ai/v1/graphql',\n    INDEX_WEBSOCKET: 'wss://index.space-pussy.cybernode.ai/v1/graphql',\n    BECH32_PREFIX: 'pussy',\n    MEMO_KEPLR: '[space-pussy] cyb.ai, using keplr',\n  },\n};\n\nexport default defaultNetworks;\n","import { BECH32_PREFIX, BECH32_PREFIX_VALOPER } from './config';\n\nexport const PATTERN_CYBER = new RegExp(\n  `^${BECH32_PREFIX}[a-zA-Z0-9]{39}$`,\n  'g'\n);\n\nexport const PATTERN_SPACE_PUSSY = /^pussy[a-zA-Z0-9]{39}$/g;\n\nexport const PATTERN_IPFS_HASH = /^Qm[a-zA-Z0-9]{44}$/g;\n\nexport const PATTERN_CYBER_CONTRACT = new RegExp(\n  `^${BECH32_PREFIX}[a-zA-Z0-9]{59}$`,\n  'g'\n);\n\nexport const PATTERN_CYBER_VALOPER = new RegExp(\n  `^${BECH32_PREFIX_VALOPER}valoper[a-zA-Z0-9]{39}$`,\n  'g'\n);\n\nexport const PATTERN_COSMOS = /^cosmos[a-zA-Z0-9]{39}$/g;\n\nexport const PATTERN_OSMOS = /^osmo[a-zA-Z0-9]{39}$/g;\n\nexport const PATTERN_TERRA = /^terra[a-zA-Z0-9]{39}$/g;\n\nexport const PATTERN_ETH = /^0x[a-fA-F0-9]{40}$/g;\n\nexport const PATTERN_TX = /[0-9a-fA-F]{64}$/g;\n\nexport const PATTERN_BLOCK = /^[0-9]+$/g;\n\nexport const PATTERN_HTTP = /^https:\\/\\/|^http:\\/\\//g;\n\nexport const PATTERN_HTML = /<\\/?[\\w\\d]+>/gi;\n","export enum LinksTypeFilter {\n  to = 'to',\n  from = 'from',\n  all = 'all',\n}\n\nexport type LinksType = Exclude<LinksTypeFilter, LinksTypeFilter.all>;\n\nexport type SearchItem = {\n  cid: string;\n  rank?: string;\n  grade?: string;\n  timestamp?: string;\n  type?: LinksTypeFilter;\n};\n\nexport enum SortBy {\n  rank = 'rank',\n  date = 'date',\n  // not ready\n  // popular = 'popular',\n  // mine = 'mine',\n}\n","import { Option } from 'src/types';\nimport { IPFSContent, IpfsContentSource } from '../ipfs/types';\n\n/* eslint-disable import/no-unused-modules */\nexport type QueueItemStatus =\n  | 'pending'\n  | 'executing'\n  | 'timeout'\n  | 'completed'\n  | 'cancelled'\n  | 'error'\n  | 'not_found';\n\nexport type QueueSourceSettings = {\n  timeout: number;\n  maxConcurrentExecutions: number;\n};\n\nexport type QueueSource = IpfsContentSource;\n\nexport type QueueSettings = Record<QueueSource, QueueSourceSettings>;\n\nexport interface IQueueStrategy {\n  settings: QueueSettings;\n  order: QueueSource[];\n  getNextSource(source: QueueSource): QueueSource | undefined;\n}\n\nexport type QueueStats = {\n  status: QueueItemStatus;\n  count: number;\n};\n\nexport enum QueuePriority {\n  ZERO = 0,\n  LOW = 0.1,\n  MEDIUM = 0.5,\n  HIGH = 0.9,\n  URGENT = 1,\n}\nexport type QueueItemOptions = {\n  parent?: string;\n  priority?: QueuePriority | number;\n  viewPortPriority?: number;\n  initialSource?: QueueSource;\n  postProcessing?: boolean;\n};\n\nexport type QueueItemCallback = (\n  cid: string,\n  status: QueueItemStatus,\n  source: QueueSource,\n  result?: Option<IPFSContent>\n) => void;\n\nexport type QueueItem = {\n  cid: string;\n  source: QueueSource;\n  status: QueueItemStatus;\n  callbacks: QueueItemCallback[];\n  controller?: AbortController;\n  executionTime?: number;\n} & Omit<QueueItemOptions, 'initialSource'>;\n\nexport type QueueItemResult = {\n  item: QueueItem;\n  status: QueueItemStatus;\n  source: QueueSource;\n  result?: Option<IPFSContent>;\n};\n\nexport type QueueItemAsyncResult = Omit<QueueItemResult, 'item'>;\n\nexport type QueueItemPostProcessor = (\n  content: Option<IPFSContent>\n) => Promise<Option<IPFSContent>>;\n\nexport type FetchParticleAsync = (\n  cid: string,\n  options?: QueueItemOptions\n) => Promise<QueueItemAsyncResult>;\n","import type { TransferHandler } from 'comlink';\nimport { IPFSContent } from 'src/services/ipfs/types';\n\nexport type IPFSContentTransferable = Omit<IPFSContent, 'result'> & {\n  port: MessagePort;\n};\n\nfunction createAsyncIterable(port: MessagePort): AsyncIterable<Uint8Array> {\n  return {\n    async *[Symbol.asyncIterator](): AsyncGenerator<\n      Uint8Array,\n      void,\n      undefined\n    > {\n      let done = false;\n      while (!done) {\n        // eslint-disable-next-line no-loop-func\n        const promise = new Promise<Uint8Array | null>((resolve) => {\n          // resolve = res;\n          port.onmessage = (event: MessageEvent) => {\n            if (event.data === null) {\n              done = true;\n              resolve(null);\n            } else {\n              resolve(event.data);\n            }\n          };\n        });\n        // eslint-disable-next-line no-await-in-loop\n        const value = await promise;\n        // eslint-disable-next-line no-await-in-loop\n        if (value !== null) {\n          yield value;\n        }\n      }\n    },\n  };\n}\n\nconst IPFSContentTransferHandler: TransferHandler<\n  IPFSContent | undefined,\n  IPFSContentTransferable | null\n> = {\n  canHandle: (obj: IPFSContent | undefined) =>\n    obj && obj.result && typeof obj.result[Symbol.asyncIterator] === 'function',\n  serialize(obj: IPFSContent) {\n    if (obj === undefined) {\n      return [null, []];\n    }\n    const { result, ...rest } = obj;\n    const { port1, port2 } = new MessageChannel();\n    if (result) {\n      (async () => {\n        // eslint-disable-next-line no-restricted-syntax\n        for await (const value of result) {\n          port1.postMessage(value);\n        }\n        port1.postMessage(null); // Send  \"end\" message\n\n        port1.close();\n      })();\n    }\n    return [{ ...rest, port: port2 }, [port2]];\n  },\n  deserialize(serializedObj: IPFSContentTransferable | null) {\n    if (!serializedObj) {\n      return undefined;\n    }\n    const { port, ...rest } = serializedObj;\n\n    return {\n      ...rest,\n      result: createAsyncIterable(port),\n    };\n  },\n};\n\nexport {\n  IPFSContentTransferHandler,\n  // serializeIPFSContent,\n  // deserializeIPFSContent,\n};\n","import {\n  wrap,\n  Remote,\n  proxy,\n  releaseProxy,\n  expose,\n  transferHandlers,\n} from 'comlink';\nimport { IPFSContentTransferHandler } from './serializers';\nimport { Observable, Observer, Subscribable, Subscription } from 'rxjs'; // v7.8.0\ntype WorkerType = SharedWorker | Worker;\n\nconst isSharedWorkersSupported = typeof SharedWorker !== 'undefined';\n\nconst isSharedWorkerUsed = isSharedWorkersSupported && !process.env.IS_DEV;\n\n// apply serializers for custom types\nfunction installTransferHandlers() {\n  transferHandlers.set('IPFSContent', IPFSContentTransferHandler);\n  transferHandlers.set('observable', {\n    canHandle: (value: unknown): value is Observable<unknown> => {\n      return value instanceof Observable;\n    },\n    deserialize: (value: MessagePort) => {\n      return new Observable<unknown>((observer) => {\n        const remote = transferHandlers\n          .get('proxy')!\n          .deserialize(value) as Remote<Subscribable<unknown>>;\n\n        remote\n          .subscribe(\n            proxy({\n              next: (next: unknown) => observer.next(next),\n              error: (error: unknown) => observer.error(error),\n              complete: () => observer.complete(),\n            })\n          )\n          .then((subscription) =>\n            observer.add(() => {\n              subscription.unsubscribe();\n              remote[releaseProxy]();\n            })\n          );\n      });\n    },\n    serialize: (value: Observable<unknown>) => {\n      return transferHandlers.get('proxy')!.serialize({\n        subscribe: (observer: Remote<Observer<unknown>>) =>\n          value.subscribe({\n            next: (next: unknown) => observer.next(next).then(),\n            error: (error: unknown) => observer.error(error).then(),\n            complete: () => observer.complete().then(),\n          }),\n      });\n    },\n  });\n\n  transferHandlers.set('subscription', {\n    canHandle: (value: unknown): value is Subscription => {\n      return value instanceof Subscription;\n    },\n    deserialize: (value: MessagePort) => {\n      return new Subscription(() => {\n        const remote = transferHandlers\n          .get('proxy')!\n          .deserialize(value) as Remote<Subscription>;\n\n        remote.unsubscribe().then(() => {\n          remote[releaseProxy]();\n        });\n      });\n    },\n    serialize: (value: Subscription) => {\n      return transferHandlers.get('proxy')!.serialize({\n        unsubscribe: () => value.unsubscribe(),\n      });\n    },\n  });\n}\n\nfunction safeStringify(obj: any): string {\n  try {\n    return JSON.stringify(obj);\n  } catch (error) {\n    return String(obj);\n  }\n}\n\n// Override console.log to send logs to main thread\nfunction overrideLogging(worker: Worker | MessagePort) {\n  const consoleLogMap = {\n    log: { original: console.log },\n    error: { original: console.error },\n    warn: { original: console.warn },\n  };\n  const replaceConsoleLog = (method: keyof typeof consoleLogMap) => {\n    const { original } = consoleLogMap[method];\n\n    consoleLogMap[method].original = console[method];\n\n    console[method] = (...args) => {\n      original.apply(console, args);\n      const serializableArgs = args.map((arg) => safeStringify(arg));\n\n      worker.postMessage({ type: 'console', method, args: serializableArgs });\n    };\n  };\n\n  Object.keys(consoleLogMap).forEach((method) =>\n    replaceConsoleLog(method as keyof typeof consoleLogMap)\n  );\n}\n\n// Install handlers for logging from worker\nfunction installLoggingHandler(worker: Worker | MessagePort, name: string) {\n  // Add event listener\n  worker.addEventListener('message', (event) => {\n    if (event.data.type === 'console') {\n      const { method, args } = event.data;\n\n      console[method](name, ...args);\n    }\n  });\n}\n\n// Create Shared Worker with fallback to usual Worker(in case of DEV too)\nexport function createWorkerApi<T>(\n  workerUrl: URL,\n  workerName: string\n): { worker: WorkerType; workerApiProxy: Remote<T> } {\n  installTransferHandlers();\n  //&& !process.env.IS_DEV\n  if (isSharedWorkerUsed) {\n    const worker = new SharedWorker(workerUrl, { name: workerName });\n    installLoggingHandler(worker.port, workerName);\n    return { worker, workerApiProxy: wrap<T>(worker.port) };\n  }\n\n  const worker = new Worker(workerUrl);\n  // installLoggingHandler(worker, workerName);\n  return { worker, workerApiProxy: wrap<T>(worker) };\n}\n\nexport function exposeWorkerApi<T>(worker: WorkerType, api: T) {\n  installTransferHandlers();\n  if (typeof worker.onconnect !== 'undefined') {\n    worker.onconnect = (e) => {\n      const port = e.ports[0];\n      overrideLogging(port);\n\n      expose(api, port);\n    };\n  } else {\n    // overrideLogging(worker);\n    expose(api);\n  }\n}\n","export function isParticle(value: string) {\n  // copied from src/utils/config.ts , to prevent crash in worker, need refactor\n  // import { PATTERN_IPFS_HASH } from 'src/utils/config';\n  return Boolean(value.match(/^Qm[a-zA-Z0-9]{44}$/g));\n}\n","import { PinType } from 'ipfs-core-types/src/pin';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { Transaction } from 'src/services/backend/services/indexer/types';\nimport {\n  SenseChatExtension,\n  SenseLinkMeta,\n  SenseListItemtMeta,\n  SenseTransactionMeta,\n} from 'src/services/backend/types/sense';\nimport { IpfsContentType } from 'src/services/ipfs/types';\nimport { NeuronAddress, ParticleCid, TransactionHash } from 'src/types/base';\nimport { DtoToEntity } from 'src/types/dto';\n\ntype PinEntryType = Exclude<PinType, 'all'>;\n// example of db optimization for classifiers\n\nexport const PinTypeMap: Record<PinEntryType, number> = {\n  indirect: -1,\n  direct: 0,\n  recursive: 1,\n};\n\nexport enum EntryType {\n  transactions = 1,\n  particle = 2,\n  chat = 3,\n}\n\n// Transaction if formed by frontend\n// Should be replaced after sync\n\nexport type PinDbEntity = {\n  cid: string;\n  type: keyof typeof PinTypeMap;\n};\n\nexport type TransactionDbEntity = {\n  hash: string;\n  index: number;\n  type: string;\n  timestamp: number;\n  block_height: number;\n  value: Transaction['value'];\n  success: boolean;\n  memo: string;\n  neuron: NeuronAddress;\n};\n\ntype SyncItemMeta = DtoToEntity<\n  (SenseLinkMeta | SenseTransactionMeta) & SenseChatExtension\n>;\n\nexport type SyncStatusDbEntity = {\n  entry_type: EntryType;\n  id: NeuronAddress | ParticleCid;\n  owner_id: NeuronAddress;\n  timestamp_update: number;\n  timestamp_read: number;\n  disabled: boolean;\n  unread_count: number;\n  meta: SyncItemMeta;\n};\n\nexport type ParticleDbEntity = {\n  id: ParticleCid;\n  size: number;\n  size_local: number;\n  blocks: number;\n  mime: string;\n  type: IpfsContentType;\n  text: string;\n};\n\nexport type LinkDbEntity = {\n  from: ParticleCid;\n  to: ParticleCid;\n  neuron: NeuronAddress;\n  timestamp: number;\n  transaction_hash: string;\n};\n\nexport type ConfigDbEntity = {\n  key: string;\n  group_key: string;\n  value: NonNullable<unknown>;\n};\n\nexport enum SyncQueueStatus {\n  pending = 0,\n  executing = 1,\n  done = 2,\n  error = -1,\n}\n\nexport enum SyncQueueJobType {\n  particle = 0,\n  embedding = 1,\n}\n\nexport type SyncQueueKey = {\n  id: string;\n  job_type: SyncQueueJobType;\n};\n\nexport type SyncQueueDbEntity = SyncQueueKey & {\n  data: string;\n  status: SyncQueueStatus;\n  priority: QueuePriority | number;\n};\n\nexport type CommunityDbEntity = {\n  ownerId: NeuronAddress;\n  particle: ParticleCid;\n  neuron: NeuronAddress;\n  name: string;\n  following: boolean;\n  follower: boolean;\n};\n\nexport type EmbeddinsDbEntity = {\n  cid: ParticleCid;\n  vec: number[];\n};\n\nexport type DbEntity =\n  | TransactionDbEntity\n  | ParticleDbEntity\n  | SyncStatusDbEntity\n  | ConfigDbEntity\n  | SyncQueueDbEntity\n  | EmbeddinsDbEntity;\n","import {\n  createAsyncThunk,\n  createSelector,\n  createSlice,\n  PayloadAction,\n} from '@reduxjs/toolkit';\nimport { SenseApi } from 'src/contexts/backend/services/senseApi';\nimport {\n  SenseItemLinkMeta,\n  SenseListItem,\n  SenseListItemTransactionMeta,\n  SenseUnread,\n} from 'src/services/backend/types/sense';\nimport { isParticle } from '../../particle/utils';\nimport { SenseItemId } from '../types/sense';\nimport { EntryType } from 'src/services/CozoDb/types/entities';\nimport {\n  MsgMultiSendValue,\n  MsgSendValue,\n} from 'src/services/backend/services/indexer/types';\nimport { RootState } from 'src/redux/store';\n\n// similar to blockchain/tx/message type\nexport type SenseItem = {\n  id: SenseItemId;\n  transactionHash: string;\n\n  // add normal type\n  type: string;\n\n  meta: SenseListItem['meta'];\n  timestamp: string;\n  memo: string | undefined;\n  from: string;\n\n  // for optimistic update\n  status?: 'pending' | 'error';\n  fromLog?: boolean;\n};\n\ntype Chat = {\n  id: SenseItemId;\n  isLoading: boolean;\n  error: string | undefined;\n  data: SenseItem[];\n  unreadCount: number;\n};\n\ntype SliceState = {\n  list: {\n    isLoading: boolean;\n    data: string[];\n    error: string | undefined;\n  };\n  chats: {\n    [key in SenseItemId]?: Chat;\n  };\n  summary: {\n    unreadCount: {\n      total: number;\n      particles: number;\n      neurons: number;\n    };\n  };\n};\n\nconst initialState: SliceState = {\n  list: {\n    isLoading: false,\n    data: [],\n    error: undefined,\n  },\n  chats: {},\n  summary: {\n    unreadCount: {\n      total: 0,\n      particles: 0,\n      neurons: 0,\n    },\n  },\n};\n\nfunction formatApiData(item: SenseListItem): SenseItem {\n  if (item.entryType === EntryType.chat && item.meta.to) {\n    item.entryType = EntryType.particle;\n  }\n\n  const { meta } = item;\n\n  const formatted: SenseItem = {\n    timestamp: new Date(meta.timestamp).toISOString(),\n\n    // lol\n    transactionHash:\n      item.transactionHash ||\n      item.hash ||\n      item.meta.transaction_hash ||\n      item.meta.hash ||\n      item.meta.transactionHash,\n\n    memo: item.memo || meta.memo,\n\n    senseChatId: item.id,\n    // not good\n    unreadCount: item.unreadCount || 0,\n  };\n\n  switch (item.entryType) {\n    case EntryType.chat:\n    case EntryType.transactions: {\n      const meta = item.meta as SenseListItemTransactionMeta;\n      const { type } = meta;\n\n      let from = item.ownerId;\n\n      if (type === 'cosmos.bank.v1beta1.MsgSend') {\n        const value = meta.value as MsgSendValue;\n        from = value.fromAddress;\n      } else if (type === 'cosmos.bank.v1beta1.MsgMultiSend') {\n        const value = meta.value as MsgMultiSendValue;\n\n        from = value.inputs[0].address;\n      }\n\n      Object.assign(formatted, {\n        type,\n        from,\n        meta: item.meta.value,\n      });\n\n      break;\n    }\n\n    case EntryType.particle: {\n      const meta = item.meta as SenseItemLinkMeta;\n\n      Object.assign(formatted, {\n        type: 'cyber.graph.v1beta1.MsgCyberlink',\n        from: meta.neuron,\n        meta: meta,\n        fromLog: true,\n      });\n\n      break;\n    }\n\n    default:\n      // sholdn't be\n      debugger;\n      return {};\n  }\n\n  return formatted;\n}\n\nconst getSenseList = createAsyncThunk(\n  'sense/getSenseList',\n  async (senseApi: SenseApi) => {\n    const data = await senseApi!.getList();\n    return data.map(formatApiData);\n  }\n);\n\nconst getSenseChat = createAsyncThunk(\n  'sense/getSenseChat',\n  async ({ id, senseApi }: { id: SenseItemId; senseApi: SenseApi }) => {\n    const particle = isParticle(id);\n\n    if (particle) {\n      const links = await senseApi!.getLinks(id);\n      const formattedLinks = links.map((item) => {\n        if (item.timestamp === 0) {\n          // FIXME:\n          return;\n        }\n        return formatApiData({\n          ...item,\n          id,\n          entryType: EntryType.particle,\n          meta: item,\n        });\n      });\n\n      return formattedLinks.filter(Boolean);\n    }\n\n    const data = await senseApi!.getFriendItems(id);\n    const formattedData = data.map((item) => {\n      const entryType = item.to ? EntryType.particle : EntryType.chat;\n      return formatApiData({\n        ...item,\n        entryType,\n        id,\n        meta: item,\n      });\n    });\n\n    return formattedData;\n  }\n);\n\nconst markAsRead = createAsyncThunk(\n  'sense/markAsRead',\n  async ({ id, senseApi }: { id: SenseItemId; senseApi: SenseApi }) => {\n    return senseApi!.markAsRead(id);\n  }\n);\n\nconst newChatStructure: Chat = {\n  id: '',\n  isLoading: false,\n  data: [],\n  error: undefined,\n  unreadCount: 0,\n};\n\nfunction checkIfMessageExists(chat: Chat, newMessage: SenseItem) {\n  const lastMessages = chat.data.slice(-5);\n\n  const isMessageExists = lastMessages.some((msg) => {\n    return msg.transactionHash === newMessage.transactionHash;\n  });\n\n  return isMessageExists;\n}\n\nconst slice = createSlice({\n  name: 'sense',\n  initialState,\n  reducers: {\n    // backend may push this action\n    updateSenseList: {\n      reducer: (state, action: PayloadAction<SenseItem[]>) => {\n        const data = action.payload;\n\n        data.forEach((message) => {\n          const { senseChatId: id } = message;\n\n          if (!state.chats[id]) {\n            state.chats[id] = { ...newChatStructure };\n          }\n\n          const chat = state.chats[id]!;\n\n          Object.assign(chat, {\n            id,\n            // fix ts\n            unreadCount: message.unreadCount || 0,\n          });\n\n          if (!checkIfMessageExists(chat, message)) {\n            chat.data = chat.data.concat(message);\n          }\n        });\n\n        slice.caseReducers.orderSenseList(state);\n      },\n      prepare: (data: SenseListItem[]) => {\n        return {\n          payload: data.map(formatApiData),\n        };\n      },\n    },\n    // optimistic update\n    addSenseItem(\n      state,\n      action: PayloadAction<{ id: SenseItemId; item: SenseItem }>\n    ) {\n      const { id, item } = action.payload;\n      const chat = state.chats[id]!;\n\n      chat.data.push({\n        ...item,\n        meta: item.meta,\n        status: 'pending',\n      });\n\n      const newList = state.list.data.filter((item) => item !== id);\n      newList.unshift(id);\n      state.list.data = newList;\n    },\n    // optimistic confirm/error\n    updateSenseItem(\n      state,\n      action: PayloadAction<{\n        chatId: SenseItemId;\n        txHash: string;\n        isSuccess: boolean;\n      }>\n    ) {\n      const { chatId, txHash, isSuccess } = action.payload;\n      const chat = state.chats[chatId]!;\n\n      const item = chat.data.find((item) => item.transactionHash === txHash);\n\n      if (item) {\n        if (isSuccess) {\n          delete item.status;\n        } else {\n          item.status = 'error';\n        }\n      }\n    },\n    orderSenseList(state) {\n      const chatsLastMessage = Object.keys(state.chats).reduce<\n        {\n          id: string;\n          lastMsg: SenseItem;\n        }[]\n      >((acc, id) => {\n        const chat = state.chats[id]!;\n\n        // may be loading this moment, no data\n        if (!chat.data.length) {\n          return acc;\n        }\n\n        const lastMsg = chat.data[chat.data.length - 1];\n        acc.push({ id, lastMsg });\n\n        return acc;\n      }, []);\n\n      const sorted = chatsLastMessage.sort((a, b) => {\n        return (\n          Date.parse(b.lastMsg.timestamp) - Date.parse(a.lastMsg.timestamp)\n        );\n      });\n\n      state.list.data = sorted.map((i) => i.id);\n    },\n    reset() {\n      return initialState;\n    },\n  },\n\n  extraReducers: (builder) => {\n    builder.addCase(getSenseList.pending, (state) => {\n      state.list.isLoading = true;\n    });\n\n    builder.addCase(getSenseList.fulfilled, (state, action) => {\n      state.list.isLoading = false;\n\n      const newList: SliceState['list']['data'] = [];\n\n      action.payload.forEach((message) => {\n        const { senseChatId: id } = message;\n\n        if (!state.chats[id]) {\n          state.chats[id] = { ...newChatStructure };\n        }\n\n        const chat = state.chats[id]!;\n\n        Object.assign(chat, {\n          id,\n          // fix\n          unreadCount: message.unreadCount || 0,\n        });\n\n        if (!checkIfMessageExists(chat, message)) {\n          chat.data = chat.data.concat(message);\n        }\n\n        newList.push(id);\n      });\n\n      state.list.data = newList;\n    });\n    builder.addCase(getSenseList.rejected, (state, action) => {\n      console.error(action);\n\n      state.list.isLoading = false;\n      state.list.error = action.error.message;\n    });\n\n    builder.addCase(getSenseChat.pending, (state, action) => {\n      const { id } = action.meta.arg;\n\n      if (!state.chats[id]) {\n        state.chats[id] = { ...newChatStructure };\n      }\n\n      // don't understand why ts warning\n      state.chats[id].isLoading = true;\n    });\n\n    builder.addCase(getSenseChat.fulfilled, (state, action) => {\n      const { id } = action.meta.arg;\n      const chat = state.chats[id]!;\n      chat.isLoading = false;\n\n      chat.id = id;\n\n      chat.data = action.payload;\n    });\n    builder.addCase(getSenseChat.rejected, (state, action) => {\n      console.error(action);\n\n      const chat = state.chats[action.meta.arg.id]!;\n      chat.isLoading = false;\n      chat.error = action.error.message;\n    });\n\n    // maybe add .pending, .rejected\n    // can be optimistic\n    builder.addCase(markAsRead.fulfilled, (state, action) => {\n      const { id } = action.meta.arg;\n      const chat = state.chats[id]!;\n\n      const particle = isParticle(id);\n\n      const { unreadCount } = chat;\n\n      state.summary.unreadCount.total -= unreadCount;\n      if (particle) {\n        state.summary.unreadCount.particles -= unreadCount;\n      } else {\n        state.summary.unreadCount.neurons -= unreadCount;\n      }\n\n      chat.unreadCount = 0;\n    });\n  },\n});\n\nconst selectUnreadCounts = createSelector(\n  (state: RootState) => state.sense.chats,\n  (chats) => {\n    let unreadCountParticle = 0;\n    let unreadCountNeuron = 0;\n\n    Object.values(chats).forEach(({ id, unreadCount }) => {\n      const particle = isParticle(id);\n\n      if (particle) {\n        unreadCountParticle += unreadCount;\n      } else {\n        unreadCountNeuron += unreadCount;\n      }\n    });\n\n    const total = unreadCountParticle + unreadCountNeuron;\n\n    return {\n      total,\n      particles: unreadCountParticle,\n      neurons: unreadCountNeuron,\n    };\n  }\n);\n\nexport const { addSenseItem, updateSenseItem, updateSenseList, reset } =\n  slice.actions;\n\nexport { getSenseList, getSenseChat, markAsRead };\n\n// selectors\nexport { selectUnreadCounts };\n\nexport default slice.reducer;\n","export const localStorageKeys = {\n  pocket: {\n    POCKET: 'pocket',\n    POCKET_ACCOUNT: 'pocketAccount',\n  },\n  MENU_SHOW: 'menuShow',\n  settings: {\n    adviserAudio: 'adviserAudio',\n    adviserVoice: 'adviserVoice',\n  },\n};\n","import { Dispatch } from 'redux';\nimport { localStorageKeys } from 'src/constants/localStorageKeys';\n\nimport {\n  Account,\n  AccountValue,\n  Accounts,\n  DefaultAccount,\n} from 'src/types/defaultAccount';\nimport { PayloadAction, createSlice } from '@reduxjs/toolkit';\nimport { POCKET } from '../../utils/config';\nimport { RootState } from '../store';\n\ntype SliceState = {\n  actionBar: {\n    tweet: string;\n  };\n  defaultAccount: DefaultAccount;\n  accounts: null | Accounts;\n};\n\nconst initialState: SliceState = {\n  actionBar: {\n    tweet: POCKET.STAGE_TWEET_ACTION_BAR.TWEET, // stage for tweet ActionBar: 'addAvatar' 'follow' 'tweet'\n  },\n  defaultAccount: {\n    name: null,\n    account: null,\n  },\n  accounts: null,\n};\n\nconst checkAddress = (obj, network, address) =>\n  Object.keys(obj).some((k) => {\n    if (obj[k][network]) {\n      return obj[k][network].bech32 === address;\n    }\n  });\n\nfunction saveToLocalStorage(state: SliceState) {\n  const { defaultAccount, accounts } = state;\n\n  defaultAccount &&\n    localStorage.setItem(\n      localStorageKeys.pocket.POCKET,\n      JSON.stringify({\n        [defaultAccount.name]: defaultAccount.account,\n      })\n    );\n  accounts &&\n    localStorage.setItem(\n      localStorageKeys.pocket.POCKET_ACCOUNT,\n      JSON.stringify(accounts)\n    );\n}\n\nconst slice = createSlice({\n  name: 'pocket',\n  initialState,\n  reducers: {\n    setDefaultAccount: (\n      state,\n      {\n        payload: { name, account },\n      }: PayloadAction<{ name: string; account?: Account }>\n    ) => {\n      state.defaultAccount = {\n        name,\n        account: account || state.accounts?.[name] || null,\n      };\n\n      saveToLocalStorage(state);\n    },\n    setAccounts: (state, { payload }: PayloadAction<Accounts>) => {\n      state.accounts = payload;\n\n      saveToLocalStorage(state);\n    },\n    setStageTweetActionBar: (state, { payload }: PayloadAction<string>) => {\n      state.actionBar.tweet = payload;\n    },\n\n    // bullshit\n    deleteAddress: (state, { payload }: PayloadAction<string>) => {\n      if (state.accounts) {\n        Object.keys(state.accounts).forEach((accountKey) => {\n          Object.keys(state.accounts[accountKey]).forEach((networkKey) => {\n            if (state.accounts[accountKey][networkKey].bech32 === payload) {\n              delete state.accounts[accountKey][networkKey];\n\n              if (Object.keys(state.accounts[accountKey]).length === 0) {\n                delete state.accounts[accountKey];\n              }\n\n              if (state.defaultAccount?.account?.cyber?.bech32 === payload) {\n                const entries = Object.entries(state.accounts);\n\n                const entryCyber = entries.find(\n                  ([, value]) => value.cyber?.bech32\n                );\n\n                if (entryCyber) {\n                  state.defaultAccount = {\n                    name: entryCyber[0],\n                    account: entryCyber[1],\n                  };\n                } else {\n                  state.defaultAccount = {\n                    name: null,\n                    account: null,\n                  };\n                }\n              }\n\n              saveToLocalStorage(state);\n            }\n          });\n        });\n      }\n    },\n  },\n});\n\nexport const selectCurrentAddress = (store: RootState) =>\n  store.pocket.defaultAccount.account?.cyber?.bech32;\n\nexport const {\n  setDefaultAccount,\n  setAccounts,\n  setStageTweetActionBar,\n  deleteAddress,\n} = slice.actions;\n\nexport default slice.reducer;\n\n// refactor this\nexport const initPocket = () => (dispatch: Dispatch) => {\n  let defaultAccounts = null;\n  let defaultAccountsKeys = null;\n  let accountsTemp: Accounts | null = null;\n\n  const localStoragePocketAccount = localStorage.getItem(\n    localStorageKeys.pocket.POCKET_ACCOUNT\n  );\n  const localStoragePocket = localStorage.getItem(\n    localStorageKeys.pocket.POCKET\n  );\n  if (localStoragePocket !== null) {\n    const localStoragePocketData = JSON.parse(localStoragePocket);\n    const keyPocket = Object.keys(localStoragePocketData)[0];\n    const accountPocket = Object.values(localStoragePocketData)[0];\n    defaultAccounts = accountPocket;\n    defaultAccountsKeys = keyPocket;\n  }\n  if (localStoragePocketAccount !== null) {\n    const localStoragePocketAccountData = JSON.parse(localStoragePocketAccount);\n    if (localStoragePocket === null) {\n      const keys0 = Object.keys(localStoragePocketAccountData)[0];\n      localStorage.setItem(\n        localStorageKeys.pocket.POCKET,\n        JSON.stringify({ [keys0]: localStoragePocketAccountData[keys0] })\n      );\n      defaultAccounts = localStoragePocketAccountData[keys0];\n      defaultAccountsKeys = keys0;\n    } else if (defaultAccountsKeys !== null) {\n      accountsTemp = {\n        [defaultAccountsKeys]:\n          localStoragePocketAccountData[defaultAccountsKeys] || undefined,\n        ...localStoragePocketAccountData,\n      };\n    }\n  } else {\n    localStorage.removeItem(localStorageKeys.pocket.POCKET);\n    localStorage.removeItem(localStorageKeys.pocket.POCKET_ACCOUNT);\n  }\n\n  defaultAccountsKeys &&\n    defaultAccounts &&\n    dispatch(\n      setDefaultAccount({\n        name: defaultAccountsKeys,\n        account: defaultAccounts,\n      })\n    );\n\n  accountsTemp &&\n    Object.keys(accountsTemp).forEach((key) => {\n      if (!accountsTemp[key] || Object.keys(accountsTemp[key]).length === 0) {\n        delete accountsTemp[key];\n      }\n    });\n\n  accountsTemp && dispatch(setAccounts(accountsTemp));\n};\n\nconst defaultNameAccount = () => {\n  let key = 'Account 1';\n  let count = 1;\n\n  const localStorageCount = localStorage.getItem('count');\n\n  if (localStorageCount !== null) {\n    const dataCount = JSON.parse(localStorageCount);\n    count = parseFloat(dataCount);\n    key = `Account ${count}`;\n  }\n\n  localStorage.setItem('count', JSON.stringify(count + 1));\n\n  return key;\n};\n\nexport const addAddressPocket =\n  (accounts: AccountValue) => (dispatch: Dispatch) => {\n    const key = accounts.name || defaultNameAccount();\n\n    let dataPocketAccount = null;\n    let valueObj = {};\n    let pocketAccount: Accounts = {};\n\n    const localStorageStory = localStorage.getItem(\n      localStorageKeys.pocket.POCKET_ACCOUNT\n    );\n\n    if (localStorageStory !== null) {\n      dataPocketAccount = JSON.parse(localStorageStory);\n      valueObj = Object.values(dataPocketAccount);\n    }\n\n    const isAdded = !checkAddress(valueObj, 'cyber', accounts.bech32);\n\n    if (!isAdded) {\n      return;\n    }\n\n    const cyberAccounts: Account = {\n      cyber: accounts,\n    };\n\n    if (localStorageStory !== null) {\n      pocketAccount = { [key]: cyberAccounts, ...dataPocketAccount };\n    } else {\n      pocketAccount = { [key]: cyberAccounts };\n    }\n\n    if (Object.keys(pocketAccount).length > 0) {\n      dispatch(setAccounts(pocketAccount));\n      if (accounts.keys !== 'read-only') {\n        dispatch(setDefaultAccount({ name: key, account: cyberAccounts }));\n      }\n    }\n  };\n","export const CYB_BROADCAST_CHANNEL = 'cyb-broadcast-channel';\nexport const CYB_QUEUE_CHANNEL = 'cyb-queue-channel';\n","import { updateSenseList } from 'src/features/sense/redux/sense.redux';\nimport { setDefaultAccount } from 'src/redux/features/pocket';\nimport { Account } from 'src/types/defaultAccount';\nimport { SenseListItem } from '../types/sense';\nimport {\n  BroadcastChannelMessage,\n  ServiceName,\n  ServiceStatus,\n  SyncEntryName,\n  SyncProgress,\n} from '../types/services';\nimport { CYB_BROADCAST_CHANNEL } from './consts';\n\nclass BroadcastChannelSender {\n  private channel: BroadcastChannel;\n\n  constructor() {\n    this.channel = new BroadcastChannel(CYB_BROADCAST_CHANNEL);\n  }\n\n  public postServiceStatus(\n    name: ServiceName,\n    status: ServiceStatus,\n    message?: string\n  ) {\n    this.channel.postMessage({\n      type: 'service_status',\n      value: { name, status, message },\n    });\n  }\n\n  public postSyncEntryProgress(entry: SyncEntryName, state: SyncProgress) {\n    // console.log('postSyncEntryProgress', entry, state);\n    this.channel.postMessage({ type: 'sync_entry', value: { entry, state } });\n  }\n\n  public postMlSyncEntryProgress(entry: string, state: SyncProgress) {\n    // console.log('postMlSyncEntryProgress', entry, state);\n    this.channel.postMessage({\n      type: 'sync_ml_entry',\n      value: { entry, state },\n    });\n  }\n\n  public postSenseUpdate(senseList: SenseListItem[]) {\n    // console.log('postSenseUpdate', senseList);\n    if (senseList.length > 0) {\n      this.channel.postMessage(updateSenseList(senseList));\n    }\n  }\n\n  public postSetDefaultAccount(name: string, account?: Account) {\n    this.channel.postMessage(\n      setDefaultAccount({\n        name,\n        account,\n      })\n    );\n  }\n\n  post(msg: BroadcastChannelMessage) {\n    this.channel.postMessage(msg);\n  }\n}\n\nexport default BroadcastChannelSender;\n","import { createCyblogChannel } from 'src/utils/logging/cyblog';\nimport {\n  ProgressTracking,\n  SyncEntryName,\n  SyncProgress,\n} from '../types/services';\nimport BroadcastChannelSender from './BroadcastChannelSender';\n\nexport const broadcastStatus = (\n  name: SyncEntryName,\n  channelApi: BroadcastChannelSender\n) => {\n  // const cyblogCh = createCyblogChannel({ thread: 'bckd', module: name });\n  return {\n    sendStatus: (\n      status: SyncProgress['status'],\n      message?: string,\n      progress?: ProgressTracking\n    ) => {\n      // cyblogCh.info(`>>>$ sync ${name} status: ${status} message: ${message}`);\n      channelApi.postSyncEntryProgress(name, {\n        status,\n        message,\n        progress,\n        done: ['active', 'error', 'listen'].some((s) => s === status),\n      });\n    },\n  };\n};\n","async function* arrayToAsyncIterable<T>(array: T[]): AsyncIterable<T> {\n  // eslint-disable-next-line no-restricted-syntax\n  for (const item of array) {\n    yield item;\n  }\n}\n\nasync function asyncIterableBatchProcessor<T, K>(\n  items: AsyncIterable<T> | Iterable<T>,\n  batchProcess: (arg: T[]) => Promise<K>,\n  batchSize = 10\n): Promise<void> {\n  let batch = [];\n  // eslint-disable-next-line no-restricted-syntax\n  for await (const item of items) {\n    batch.push(item);\n    if (batch.length === batchSize) {\n      await batchProcess(batch);\n      batch = [];\n    }\n  }\n  // process the rest\n  if (batch.length > 0) {\n    await batchProcess(batch);\n  }\n}\n\nasync function asyncIterableToArray<T>(asyncIterable: AsyncIterable<T>) {\n  const resultArray = [];\n  // eslint-disable-next-line no-restricted-syntax\n  for await (const item of asyncIterable) {\n    resultArray.push(item);\n  }\n  return resultArray;\n}\n// Create a helper function to create AsyncIterable from a list and iterate one by one\nfunction createAsyncIterable<T>(data: T[]): AsyncIterable<T> {\n  let index = 0;\n  return {\n    [Symbol.asyncIterator]() {\n      return {\n        next(): Promise<IteratorResult<T>> {\n          if (index < data.length) {\n            return Promise.resolve({ done: false, value: data[index++] });\n          }\n          return Promise.resolve({ done: true, value: undefined as any });\n        },\n      };\n    },\n  };\n}\n\n// eslint-disable-next-line import/prefer-default-export\nexport async function* fetchIterableByOffset<T, P>(\n  fetchFunction: (params: P & { offset: number }) => Promise<T[]>,\n  params: P\n): AsyncGenerator<T[], void, undefined> {\n  let offset = 0;\n  while (true) {\n    // eslint-disable-next-line no-await-in-loop\n    const items = await fetchFunction({ ...params, offset });\n\n    if (items.length === 0) {\n      break;\n    }\n\n    yield items;\n\n    offset += items.length;\n  }\n}\n\nexport {\n  arrayToAsyncIterable,\n  asyncIterableBatchProcessor,\n  asyncIterableToArray,\n  createAsyncIterable,\n};\n","import { SyncQueueJobType } from 'src/services/CozoDb/types/entities';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { IPFSContent } from 'src/services/ipfs/types';\nimport { LinkDto } from 'src/services/CozoDb/types/dto';\n\nimport { getTextContentIfShouldEmbed } from '../../services/sync/services/ParticlesResolverQueue/ParticlesResolverQueue';\nimport { CYB_QUEUE_CHANNEL } from '../consts';\nimport { QueueChannelMessage } from './types';\n\nexport const createBackendQueueSender = () => {\n  const channel = new BroadcastChannel(CYB_QUEUE_CHANNEL);\n\n  return {\n    enqueue: (msg: QueueChannelMessage) => {\n      channel.postMessage(msg);\n    },\n  };\n};\n\nconst busSender = createBackendQueueSender();\n\nexport const enqueueParticleEmbeddingMaybe = async (content: IPFSContent) => {\n  const contentToEmbed = await getTextContentIfShouldEmbed(content);\n\n  if (contentToEmbed) {\n    busSender.enqueue({\n      type: 'sync',\n      data: {\n        id: content.cid,\n        data: contentToEmbed,\n        jobType: SyncQueueJobType.embedding,\n        priority: QueuePriority.MEDIUM,\n      },\n    });\n  }\n\n  return !!contentToEmbed;\n};\n\nexport const enqueueParticleSave = (content: IPFSContent) => {\n  busSender.enqueue({\n    type: 'particle',\n    // TODO: add AsyncIterator serializer\n    data: { ...content, result: undefined } as IPFSContent,\n  });\n\n  return true;\n};\n\nexport const enqueueLinksSave = (links: LinkDto[]) => {\n  busSender.enqueue({\n    type: 'link',\n    data: links,\n  });\n};\n","// export const CID_AVATAR = 'Qmf89bXkJH9jw4uaLkHmZkxQ51qGKfUPtAMxA8rTwBrmTs';\nexport const CID_TWEET = 'QmbdH2WBamyKLPE5zu4mJ9v49qvY8BFfoumoVPMR5V4Rvx';\n\nexport const CID_FOLLOW = 'QmPLSA5oPqYxgc8F7EwrM8WS9vKrr1zPoDniSRFh8HSrxx';\n\nexport const INFINITY = '';\n\nexport const WP =\n  'https://ipfs.io/ipfs/QmQ1Vong13MDNxixDyUdjniqqEj8sjuNEBYMyhQU4gQgq3';\n\nexport const CYBER_CONGRESS_ADDRESS =\n  'bostrom1xszmhkfjs3s00z2nvtn7evqxw3dtus6yr8e4pw';\n","import { CID_FOLLOW, CID_TWEET } from 'src/constants/app';\nimport { SyncEntryName } from 'src/services/backend/types/services';\n\nexport const MY_PARTICLES_SYNC_INTERVAL = 5 * 60 * 1000; // 60 sec\nexport const MY_FRIENDS_SYNC_INTERVAL = 5 * 60 * 1000; // 60 sec\nexport const IPFS_SYNC_INTERVAL = 15 * 60 * 1000; // 15 minutes\n\nexport const MAX_DATABASE_PUT_SIZE = 500;\n\nexport const MAX_LINKS_RESOLVE_BATCH = 20;\n\nexport const DAY_IN_MS = 24 * 60 * 60 * 1000;\n\nexport const SENSE_FRIEND_PARTICLES = [CID_TWEET, CID_FOLLOW];\n\nexport const SYNC_ENTRIES_TO_TRACK_PROGRESS = [\n  'my-friends',\n  'particles',\n  'transactions',\n] as SyncEntryName[];\n","import {\n  BehaviorSubject,\n  Observable,\n  filter,\n  mergeMap,\n  tap,\n  map,\n  combineLatest,\n  share,\n  EMPTY,\n  Subject,\n  first,\n} from 'rxjs';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport { broadcastStatus } from 'src/services/backend/channels/broadcastStatus';\nimport { ParticleCid } from 'src/types/base';\nimport {\n  SyncQueueJobType,\n  SyncQueueStatus,\n} from 'src/services/CozoDb/types/entities';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { asyncIterableBatchProcessor } from 'src/utils/async/iterable';\n\nimport { enqueueParticleEmbeddingMaybe } from 'src/services/backend/channels/BackendQueueChannel/backendQueueSenders';\n\nimport { PATTERN_COSMOS, PATTERN_CYBER } from 'src/constants/patterns';\nimport { EmbeddingApi } from 'src/services/backend/workers/background/api/mlApi';\nimport { Option } from 'src/types';\n\nimport { IPFSContent } from 'src/services/ipfs/types';\nimport { FetchIpfsFunc } from '../../types';\nimport { ServiceDeps } from '../types';\nimport { SyncQueueItem } from './types';\nimport { MAX_DATABASE_PUT_SIZE } from '../consts';\n\nimport DbApi from '../../../DbApi/DbApi';\n\nconst QUEUE_BATCH_SIZE = 100;\n\nconst getContentToEmbed = async (content: IPFSContent) => {\n  const contentType = content?.meta?.contentType || '';\n\n  // create embedding for allowed content\n  if (contentType === 'text') {\n    return [contentType, content.textPreview];\n  }\n\n  return [contentType, undefined];\n};\n\nexport const getTextContentIfShouldEmbed = async (content: IPFSContent) => {\n  const [contentType, data] = await getContentToEmbed(content);\n\n  let shouldEmbed = contentType === 'text' && !!data;\n\n  shouldEmbed =\n    shouldEmbed &&\n    (!data!.match(PATTERN_COSMOS) || !data!.match(PATTERN_CYBER));\n\n  return shouldEmbed ? data : undefined;\n};\n\nclass ParticlesResolverQueue {\n  public isInitialized$: Observable<boolean>;\n\n  private db: Option<DbApi>;\n\n  private embeddingApi: Option<EmbeddingApi>;\n\n  private get canEmbed() {\n    return !!this.embeddingApi;\n  }\n\n  private waitForParticleResolve: FetchIpfsFunc;\n\n  private statusApi = broadcastStatus('resolver', new BroadcastChannelSender());\n\n  private _syncQueue$ = new BehaviorSubject<Map<ParticleCid, SyncQueueItem>>(\n    new Map()\n  );\n\n  public get queue(): Map<ParticleCid, SyncQueueItem> {\n    return this._syncQueue$.getValue();\n  }\n\n  private _loop$: Observable<any> | undefined;\n\n  public get loop$(): Observable<any> | undefined {\n    return this._loop$;\n  }\n\n  constructor(deps: ServiceDeps) {\n    if (!deps.waitForParticleResolve) {\n      throw new Error('waitForParticleResolve is not defined');\n    }\n\n    this.waitForParticleResolve = deps.waitForParticleResolve;\n\n    deps.embeddingApi$.subscribe((embeddingApi) => {\n      this.embeddingApi = embeddingApi;\n      // if embedding function is provided, retriger the queue\n      if (this.queue.size > 0) {\n        this._syncQueue$.next(this.queue);\n      }\n    });\n\n    deps.dbInstance$\n      .pipe(\n        first((value) => value !== undefined) // Automatically unsubscribes after the first valid value\n      )\n      .subscribe(async (db) => {\n        this.db = db;\n        await this.loadSyncQueue();\n      });\n\n    this.isInitialized$ = combineLatest([\n      deps.dbInstance$,\n      deps.ipfsInstance$,\n    ]).pipe(\n      map(([dbInstance, ipfsInstance]) => !!ipfsInstance && !!dbInstance)\n    );\n  }\n\n  private async resolveIpfsParticle(id: ParticleCid, priority: QueuePriority) {\n    return this.waitForParticleResolve(id, priority)\n      .then(async ({ status, result }) => {\n        const isResolved = status !== 'not_found';\n        if (!isResolved || !result) {\n          return false;\n        }\n\n        await enqueueParticleEmbeddingMaybe(result);\n        return true;\n      })\n      .catch(() => false);\n  }\n\n  private async saveEmbedding(cid: ParticleCid, text: string) {\n    try {\n      const hasItem = await this.db!.existEmbedding(cid);\n\n      if (!hasItem) {\n        const vec = await this.embeddingApi!.createEmbedding(text);\n\n        const result = await this.db!.putEmbedding(cid, vec);\n      }\n\n      return true;\n    } catch (err) {\n      console.error(`saveEmbedding error: ${cid} - ${text} `, err.toString());\n      return false;\n    }\n  }\n\n  private async processSyncQueue(pendingItems: SyncQueueItem[]) {\n    // console.log('------processSyncQueue ', pendingItems);\n\n    const batchSize = pendingItems.length;\n\n    this.statusApi.sendStatus(\n      'in-progress',\n      `processing batch ${batchSize}/${batchSize} batch. ${this.queue.size} pending...`\n    );\n\n    let i = batchSize;\n    await Promise.all(\n      pendingItems.map(async (item) => {\n        const { id, jobType, data } = item;\n\n        let jobPromise = Promise.resolve(false);\n\n        if (jobType === SyncQueueJobType.embedding && data) {\n          jobPromise = this.saveEmbedding(id, data as string);\n        } else if (jobType === SyncQueueJobType.particle) {\n          jobPromise = this.resolveIpfsParticle(id, QueuePriority.MEDIUM);\n        }\n\n        // eslint-disable-next-line no-await-in-loop\n        return jobPromise.then(async (result) => {\n          if (result) {\n            await this.db!.removeSyncQueue({ id, jobType });\n          } else {\n            await this.db!.updateSyncQueue({\n              id,\n              jobType,\n              status: SyncQueueStatus.error,\n            });\n          }\n\n          const queue = this._syncQueue$.value;\n          queue.delete(id);\n          i--;\n          this._syncQueue$.next(queue);\n\n          this.statusApi.sendStatus(\n            'in-progress',\n            `processing batch ${batchSize - i}/${batchSize} batch. ${\n              this.queue.size\n            } pending...`\n          );\n        });\n      })\n    );\n  }\n\n  start() {\n    const source$ = this.isInitialized$.pipe(\n      tap((q) => console.log(`sync queue isInitialized - ${q}`)),\n      filter((isInitialized) => isInitialized === true),\n      mergeMap(() => this._syncQueue$), // Merge the queue$ stream here.\n      // tap((q) => console.log(`sync queue - ${q.size}`)),\n      filter((q) => q.size > 0),\n      mergeMap((queue) => {\n        const list = [...queue.values()];\n\n        const executingCount = list.filter(\n          (i) => i.status === SyncQueueStatus.executing\n        ).length;\n\n        const batchSize = QUEUE_BATCH_SIZE - executingCount;\n\n        const jobTypeFilter = (i: SyncQueueItem) =>\n          i.jobType === SyncQueueJobType.particle ||\n          (i.jobType === SyncQueueJobType.embedding && this.canEmbed);\n\n        if (batchSize > 0) {\n          const pendingItems = list\n            .filter(\n              (i) => i.status === SyncQueueStatus.pending && jobTypeFilter(i)\n            )\n            .sort((a, b) => {\n              return a.priority - b.priority;\n            })\n            .slice(0, batchSize);\n\n          if (pendingItems.length > 0) {\n            pendingItems.forEach((i) => {\n              queue.set(i.id, {\n                ...i,\n                status: SyncQueueStatus.executing,\n              });\n            });\n\n            this._syncQueue$.next(queue);\n\n            this.statusApi.sendStatus('in-progress', `starting...`);\n            return this.processSyncQueue(pendingItems);\n          }\n        }\n\n        return EMPTY;\n      })\n    );\n\n    this._loop$ = source$.pipe(share());\n\n    this._loop$.subscribe({\n      next: (result) => {\n        this.statusApi.sendStatus('active');\n      },\n      error: (err) => this.statusApi.sendStatus('error', err.toString()),\n    });\n\n    return this;\n  }\n\n  public async enqueueBatch(\n    cids: ParticleCid[],\n    jobType: SyncQueueJobType,\n    priority: QueuePriority\n  ) {\n    return asyncIterableBatchProcessor(\n      cids,\n      (cids) =>\n        this.enqueue(\n          cids.map((cid) => ({\n            id: cid /* from is tweet */,\n            priority,\n            jobType,\n          }))\n        ),\n      MAX_DATABASE_PUT_SIZE\n    );\n  }\n\n  public async enqueue(items: SyncQueueItem[]) {\n    if (items.length === 0) {\n      return;\n    }\n\n    const result = await this.db!.putSyncQueue(items);\n\n    const queue = this._syncQueue$.value;\n\n    items.forEach((item) =>\n      queue.set(item.id, { ...item, status: SyncQueueStatus.pending })\n    );\n    this._syncQueue$.next(queue);\n  }\n\n  private async loadSyncQueue() {\n    const queue = await this.db!.getSyncQueue({\n      statuses: [SyncQueueStatus.pending],\n    }).then((items) => new Map(items.map((item) => [item.id, item])));\n\n    this._syncQueue$.next(new Map([...queue, ...this.queue]));\n  }\n}\n\nexport default ParticlesResolverQueue;\n","export function shortenString(string: string, length = 300) {\n  return string.length > length ? `${string.slice(0, length)}...` : string;\n}\n\nexport function replaceQuotes(string: string) {\n  return string.replace(/\"/g, \"'\");\n}\n\nexport function serializeString(input: string): string {\n  return input\n    .replace(/\\\\/g, '\\\\\\\\') // Escape backslashes\n    .replace(/\"/g, \"\\\\''\") // Escape double quotes\n    .replace(/'/g, \"\\\\'\") // Escape single quotes\n    .replace(/\\n/g, '\\\\n') // Escape newlines\n    .replace(/\\r/g, '\\\\r') // Escape carriage returns\n    .replace(/#/g, '\\\\!!'); // Escape  - that's comment in cozo\n}\n\nexport function deserializeString(serialized: string): string {\n  return serialized\n    .replace(/\\\\r/g, '\\r') // Unescape carriage returns\n    .replace(/\\\\n/g, '\\n') // Unescape newlines\n    .replace(/\\\\'/g, \"'\") // Unescape single quotes\n    .replace(/\\\\''/g, '\"') // Unescape double quotes\n    .replace(/\\\\\\\\/g, '\\\\') // Unescape backslashes\n    .replace(/\\\\!!/g, '#'); // Unescape # cozo comment\n}\n\nconst specialCharsRegexe = /\\\\u\\{[a-fA-F0-9]+\\}/g;\n\nexport function removeBrokenUnicode(string: string): string {\n  return string.replace(specialCharsRegexe, '');\n}\n\nexport function removeMarkdownFormatting(markdown: string): string {\n  // Remove headers\n  let text = markdown.replace(/^#{1,6}\\s+/gm, '');\n\n  // Remove emphasis (bold, italic, strikethrough)\n  text = text.replace(/(\\*\\*|__)(.*?)\\1/g, '$2');\n  text = text.replace(/(\\*|_)(.*?)\\1/g, '$2');\n  text = text.replace(/(~~)(.*?)\\1/g, '$2');\n\n  // Remove inline code and code blocks\n  text = text.replace(/`{1,3}[^`](.*?)`{1,3}/g, '$1');\n  text = text.replace(/```[\\s\\S]*?```/g, '');\n\n  // Remove blockquotes\n  text = text.replace(/^\\s{0,3}>\\s?/gm, '');\n\n  // Remove links\n  text = text.replace(/\\[(.*?)\\]\\(.*?\\)/g, '$1');\n\n  // Remove images\n  text = text.replace(/!\\[(.*?)\\]\\(.*?\\)/g, '$1');\n\n  // Remove horizontal rules\n  text = text.replace(/^-{3,}$/gm, '');\n\n  // Remove unordered lists\n  text = text.replace(/^\\s*[-+*]\\s+/gm, '');\n\n  // Remove ordered lists\n  text = text.replace(/^\\s*\\d+\\.\\s+/gm, '');\n\n  // Remove extra spaces and new lines\n  text = text.replace(/\\n{2,}/g, '\\n\\n');\n  text = text.replace(/^\\s+|\\s+$/g, '');\n\n  return text;\n}\n","import { LsResult } from 'ipfs-core-types/src/pin';\nimport { dateToUtcNumber } from 'src/utils/date';\nimport { NeuronAddress, ParticleCid, TransactionHash } from 'src/types/base';\nimport { IPFSContent } from '../ipfs/types';\nimport { LinkDbEntity, PinTypeMap } from './types/entities';\nimport { Transaction } from '../backend/services/indexer/types';\nimport { LinkDto, ParticleDto, PinDto, TransactionDto } from './types/dto';\nimport { CyberlinksByParticleQuery } from 'src/generated/graphql';\nimport { removeMarkdownFormatting, replaceQuotes } from 'src/utils/string';\n\nexport const mapParticleToEntity = (particle: IPFSContent): ParticleDto => {\n  const { cid, meta, textPreview } = particle;\n  const { size, mime, type, blocks, sizeLocal } = meta;\n\n  // hack to fix string command\n  const text = textPreview\n    ? replaceQuotes(removeMarkdownFormatting(textPreview))\n    : '';\n\n  return {\n    cid,\n    size: size || 0,\n    mime: mime || 'unknown',\n    type,\n    text,\n    size_local: sizeLocal || -1,\n    blocks: blocks || 0,\n  };\n};\n\n//TODO: REFACTOR\nexport const mapPinToEntity = (pin: LsResult): PinDto => ({\n  cid: pin.cid.toString(),\n  type: PinTypeMap[pin.type],\n});\n\nexport const mapIndexerTransactionToEntity = (\n  neuron: string,\n  tx: Transaction\n): TransactionDto => {\n  const {\n    transaction_hash,\n    index,\n    transaction: {\n      memo,\n      block: { timestamp, height },\n      success,\n    },\n    type,\n    value,\n  } = tx;\n  return {\n    hash: transaction_hash,\n    index,\n    type,\n    timestamp: dateToUtcNumber(timestamp),\n    // value: JSON.stringify(value),\n    memo,\n    value,\n    success,\n    neuron,\n    blockHeight: height,\n  };\n};\n\n// export const mapSyncStatusToEntity = (\n//   id: NeuronAddress | ParticleCid,\n//   entryType: EntryType,\n//   unreadCount: number,\n//   timestampUpdate: number,\n//   lastId: TransactionHash | ParticleCid = '',\n//   timestampRead: number = timestampUpdate,\n//   meta: Object = {}\n// ): SyncStatusDbEntity => {\n//   return {\n//     entry_type: entryType,\n//     id,\n//     timestamp_update: timestampUpdate,\n//     timestamp_read: timestampRead,\n//     unread_count: unreadCount,\n//     disabled: false,\n//     last_id: lastId,\n//     meta,\n//   };\n// };\n\nexport const mapLinkToLinkDto = (\n  from: ParticleCid,\n  to: ParticleCid,\n  neuron: NeuronAddress = '',\n  timestamp: number = 0\n): LinkDto => ({\n  from,\n  to,\n  neuron,\n  timestamp,\n});\n\nexport const mapLinkFromIndexerToDto = ({\n  from,\n  to,\n  neuron,\n  timestamp,\n  transaction_hash,\n}: CyberlinksByParticleQuery['cyberlinks'][0]): LinkDto => ({\n  from,\n  to,\n  neuron,\n  timestamp: dateToUtcNumber(timestamp),\n  transactionHash: transaction_hash,\n});\n","export async function waitUntil(cond: () => boolean, timeoutDuration = 60000) {\n  if (cond()) {\n    return true;\n  }\n\n  const waitPromise = new Promise((resolve) => {\n    const interval = setInterval(() => {\n      if (cond()) {\n        clearInterval(interval);\n        resolve(true);\n      }\n    }, 10);\n  });\n\n  const timeoutPromise = new Promise((_, reject) => {\n    setTimeout(() => {\n      reject(new Error('waitUntil timed out!'));\n    }, timeoutDuration);\n  });\n\n  return Promise.race([waitPromise, timeoutPromise]);\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport function makeCancellable<T extends (...args: any[]) => Promise<any>>(\n  func: T,\n  signal: AbortSignal\n): (...funcArgs: Parameters<T>) => Promise<ReturnType<T>> {\n  return async (...args: Parameters<T>): Promise<ReturnType<T>> => {\n    // Promise that listens for the abort signal\n    const abortPromise = new Promise<ReturnType<T>>((_, reject) => {\n      const abortHandler = () => {\n        signal.removeEventListener('abort', abortHandler); // Clean up the event listener\n        reject(new DOMException('The operation was aborted.', 'AbortError'));\n      };\n      signal.addEventListener('abort', abortHandler, { once: true });\n    });\n\n    // Wrapping the original function in a promise\n    const taskPromise = new Promise<ReturnType<T>>(async (resolve, reject) => {\n      try {\n        const result = await func(...args);\n        resolve(result);\n      } catch (error) {\n        reject(error);\n      }\n    });\n\n    // Using Promise.race to handle cancellation\n    return Promise.race([taskPromise, abortPromise]);\n  };\n}\n\nexport function throwIfAborted<T extends (...args: any[]) => Promise<any>>(\n  func: T,\n  signal: AbortSignal\n): (...funcArgs: Parameters<T>) => Promise<ReturnType<T>> {\n  return async (...args: Parameters<T>): Promise<ReturnType<T>> => {\n    if (signal.aborted) {\n      throw new DOMException('The operation was aborted.', 'AbortError');\n    }\n    return func(...args);\n  };\n}\n\n/**\n * Promise will be rejected after timeout.\n *\n * @param promise\n * @param timeout ms\n * @param abortController trigger abort\n * @returns\n */\n// eslint-disable-next-line import/no-unused-modules\nexport async function withTimeout<T>(\n  promise: Promise<T>,\n  timeout: number,\n  abortController?: AbortController\n): Promise<T> {\n  return Promise.race([\n    promise,\n    new Promise<T>((_, reject) => {\n      const timer = setTimeout(() => {\n        abortController?.abort('timeout');\n        clearTimeout(timer);\n        reject(new DOMException('timeout', 'AbortError'));\n      }, timeout);\n    }),\n  ]);\n}\n","import { gql } from '@apollo/client';\nimport * as Apollo from '@apollo/client';\nexport type Maybe<T> = T | null;\nexport type InputMaybe<T> = Maybe<T>;\nexport type Exact<T extends { [key: string]: unknown }> = { [K in keyof T]: T[K] };\nexport type MakeOptional<T, K extends keyof T> = Omit<T, K> & { [SubKey in K]?: Maybe<T[SubKey]> };\nexport type MakeMaybe<T, K extends keyof T> = Omit<T, K> & { [SubKey in K]: Maybe<T[SubKey]> };\nexport type MakeEmpty<T extends { [key: string]: unknown }, K extends keyof T> = { [_ in K]?: never };\nexport type Incremental<T> = T | { [P in keyof T]?: P extends ' $fragmentName' | '__typename' ? T[P] : never };\nconst defaultOptions = {} as const;\n/** All built-in and custom scalars, mapped to their actual values */\nexport type Scalars = {\n  ID: { input: string; output: string; }\n  String: { input: string; output: string; }\n  Boolean: { input: boolean; output: boolean; }\n  Int: { input: number; output: number; }\n  Float: { input: number; output: number; }\n  _coin: { input: any; output: any; }\n  _text: { input: any; output: any; }\n  bigint: { input: any; output: any; }\n  coin: { input: any; output: any; }\n  date: { input: any; output: any; }\n  float8: { input: any; output: any; }\n  json: { input: any; output: any; }\n  jsonb: { input: any; output: any; }\n  numeric: { input: any; output: any; }\n  timestamp: { input: any; output: any; }\n};\n\n/** Boolean expression to compare columns of type \"Boolean\". All fields are combined with logical 'AND'. */\nexport type Boolean_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['Boolean']['input']>;\n  _gt?: InputMaybe<Scalars['Boolean']['input']>;\n  _gte?: InputMaybe<Scalars['Boolean']['input']>;\n  _in?: InputMaybe<Array<Scalars['Boolean']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['Boolean']['input']>;\n  _lte?: InputMaybe<Scalars['Boolean']['input']>;\n  _neq?: InputMaybe<Scalars['Boolean']['input']>;\n  _nin?: InputMaybe<Array<Scalars['Boolean']['input']>>;\n};\n\n/** Boolean expression to compare columns of type \"Int\". All fields are combined with logical 'AND'. */\nexport type Int_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['Int']['input']>;\n  _gt?: InputMaybe<Scalars['Int']['input']>;\n  _gte?: InputMaybe<Scalars['Int']['input']>;\n  _in?: InputMaybe<Array<Scalars['Int']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['Int']['input']>;\n  _lte?: InputMaybe<Scalars['Int']['input']>;\n  _neq?: InputMaybe<Scalars['Int']['input']>;\n  _nin?: InputMaybe<Array<Scalars['Int']['input']>>;\n};\n\n/** Boolean expression to compare columns of type \"String\". All fields are combined with logical 'AND'. */\nexport type String_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['String']['input']>;\n  _gt?: InputMaybe<Scalars['String']['input']>;\n  _gte?: InputMaybe<Scalars['String']['input']>;\n  /** does the column match the given case-insensitive pattern */\n  _ilike?: InputMaybe<Scalars['String']['input']>;\n  _in?: InputMaybe<Array<Scalars['String']['input']>>;\n  /** does the column match the given POSIX regular expression, case insensitive */\n  _iregex?: InputMaybe<Scalars['String']['input']>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  /** does the column match the given pattern */\n  _like?: InputMaybe<Scalars['String']['input']>;\n  _lt?: InputMaybe<Scalars['String']['input']>;\n  _lte?: InputMaybe<Scalars['String']['input']>;\n  _neq?: InputMaybe<Scalars['String']['input']>;\n  /** does the column NOT match the given case-insensitive pattern */\n  _nilike?: InputMaybe<Scalars['String']['input']>;\n  _nin?: InputMaybe<Array<Scalars['String']['input']>>;\n  /** does the column NOT match the given POSIX regular expression, case insensitive */\n  _niregex?: InputMaybe<Scalars['String']['input']>;\n  /** does the column NOT match the given pattern */\n  _nlike?: InputMaybe<Scalars['String']['input']>;\n  /** does the column NOT match the given POSIX regular expression, case sensitive */\n  _nregex?: InputMaybe<Scalars['String']['input']>;\n  /** does the column NOT match the given SQL regular expression */\n  _nsimilar?: InputMaybe<Scalars['String']['input']>;\n  /** does the column match the given POSIX regular expression, case sensitive */\n  _regex?: InputMaybe<Scalars['String']['input']>;\n  /** does the column match the given SQL regular expression */\n  _similar?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** Boolean expression to compare columns of type \"_coin\". All fields are combined with logical 'AND'. */\nexport type _Coin_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['_coin']['input']>;\n  _gt?: InputMaybe<Scalars['_coin']['input']>;\n  _gte?: InputMaybe<Scalars['_coin']['input']>;\n  _in?: InputMaybe<Array<Scalars['_coin']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['_coin']['input']>;\n  _lte?: InputMaybe<Scalars['_coin']['input']>;\n  _neq?: InputMaybe<Scalars['_coin']['input']>;\n  _nin?: InputMaybe<Array<Scalars['_coin']['input']>>;\n};\n\n/** Boolean expression to compare columns of type \"_text\". All fields are combined with logical 'AND'. */\nexport type _Text_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['_text']['input']>;\n  _gt?: InputMaybe<Scalars['_text']['input']>;\n  _gte?: InputMaybe<Scalars['_text']['input']>;\n  _in?: InputMaybe<Array<Scalars['_text']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['_text']['input']>;\n  _lte?: InputMaybe<Scalars['_text']['input']>;\n  _neq?: InputMaybe<Scalars['_text']['input']>;\n  _nin?: InputMaybe<Array<Scalars['_text']['input']>>;\n};\n\n/** columns and relationships of \"_transaction\" */\nexport type _Transaction = {\n  fee?: Maybe<Scalars['jsonb']['output']>;\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Scalars['_text']['output']>;\n  logs?: Maybe<Scalars['jsonb']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  messages?: Maybe<Scalars['jsonb']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Scalars['_text']['output']>;\n  signer_infos?: Maybe<Scalars['jsonb']['output']>;\n  subject1?: Maybe<Scalars['String']['output']>;\n  subject2?: Maybe<Scalars['String']['output']>;\n  success?: Maybe<Scalars['Boolean']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n  value?: Maybe<Scalars['jsonb']['output']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionFeeArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionLogsArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionMessagesArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionSigner_InfosArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionValueArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregated selection of \"_transaction\" */\nexport type _Transaction_Aggregate = {\n  aggregate?: Maybe<_Transaction_Aggregate_Fields>;\n  nodes: Array<_Transaction>;\n};\n\n/** aggregate fields of \"_transaction\" */\nexport type _Transaction_Aggregate_Fields = {\n  avg?: Maybe<_Transaction_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<_Transaction_Max_Fields>;\n  min?: Maybe<_Transaction_Min_Fields>;\n  stddev?: Maybe<_Transaction_Stddev_Fields>;\n  stddev_pop?: Maybe<_Transaction_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<_Transaction_Stddev_Samp_Fields>;\n  sum?: Maybe<_Transaction_Sum_Fields>;\n  var_pop?: Maybe<_Transaction_Var_Pop_Fields>;\n  var_samp?: Maybe<_Transaction_Var_Samp_Fields>;\n  variance?: Maybe<_Transaction_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"_transaction\" */\nexport type _Transaction_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<_Transaction_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type _Transaction_Avg_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"_transaction\". All fields are combined with a logical 'AND'. */\nexport type _Transaction_Bool_Exp = {\n  _and?: InputMaybe<Array<_Transaction_Bool_Exp>>;\n  _not?: InputMaybe<_Transaction_Bool_Exp>;\n  _or?: InputMaybe<Array<_Transaction_Bool_Exp>>;\n  fee?: InputMaybe<Jsonb_Comparison_Exp>;\n  gas_used?: InputMaybe<Bigint_Comparison_Exp>;\n  gas_wanted?: InputMaybe<Bigint_Comparison_Exp>;\n  hash?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  index?: InputMaybe<Bigint_Comparison_Exp>;\n  involved_accounts_addresses?: InputMaybe<_Text_Comparison_Exp>;\n  logs?: InputMaybe<Jsonb_Comparison_Exp>;\n  memo?: InputMaybe<String_Comparison_Exp>;\n  messages?: InputMaybe<Jsonb_Comparison_Exp>;\n  raw_log?: InputMaybe<String_Comparison_Exp>;\n  signatures?: InputMaybe<_Text_Comparison_Exp>;\n  signer_infos?: InputMaybe<Jsonb_Comparison_Exp>;\n  subject1?: InputMaybe<String_Comparison_Exp>;\n  subject2?: InputMaybe<String_Comparison_Exp>;\n  success?: InputMaybe<Boolean_Comparison_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  value?: InputMaybe<Jsonb_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type _Transaction_Max_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  subject1?: Maybe<Scalars['String']['output']>;\n  subject2?: Maybe<Scalars['String']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type _Transaction_Min_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  subject1?: Maybe<Scalars['String']['output']>;\n  subject2?: Maybe<Scalars['String']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"_transaction\". */\nexport type _Transaction_Order_By = {\n  fee?: InputMaybe<Order_By>;\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  logs?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  messages?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n  signer_infos?: InputMaybe<Order_By>;\n  subject1?: InputMaybe<Order_By>;\n  subject2?: InputMaybe<Order_By>;\n  success?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"_transaction\" */\nexport enum _Transaction_Select_Column {\n  /** column name */\n  Fee = 'fee',\n  /** column name */\n  GasUsed = 'gas_used',\n  /** column name */\n  GasWanted = 'gas_wanted',\n  /** column name */\n  Hash = 'hash',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Index = 'index',\n  /** column name */\n  InvolvedAccountsAddresses = 'involved_accounts_addresses',\n  /** column name */\n  Logs = 'logs',\n  /** column name */\n  Memo = 'memo',\n  /** column name */\n  Messages = 'messages',\n  /** column name */\n  RawLog = 'raw_log',\n  /** column name */\n  Signatures = 'signatures',\n  /** column name */\n  SignerInfos = 'signer_infos',\n  /** column name */\n  Subject1 = 'subject1',\n  /** column name */\n  Subject2 = 'subject2',\n  /** column name */\n  Success = 'success',\n  /** column name */\n  TransactionHash = 'transaction_hash',\n  /** column name */\n  Type = 'type',\n  /** column name */\n  Value = 'value'\n}\n\n/** aggregate stddev on columns */\nexport type _Transaction_Stddev_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type _Transaction_Stddev_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type _Transaction_Stddev_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type _Transaction_Sum_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type _Transaction_Var_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type _Transaction_Var_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type _Transaction_Variance_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"_uptime_temp\" */\nexport type _Uptime_Temp = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"_uptime_temp\" */\nexport type _Uptime_Temp_Aggregate = {\n  aggregate?: Maybe<_Uptime_Temp_Aggregate_Fields>;\n  nodes: Array<_Uptime_Temp>;\n};\n\n/** aggregate fields of \"_uptime_temp\" */\nexport type _Uptime_Temp_Aggregate_Fields = {\n  avg?: Maybe<_Uptime_Temp_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<_Uptime_Temp_Max_Fields>;\n  min?: Maybe<_Uptime_Temp_Min_Fields>;\n  stddev?: Maybe<_Uptime_Temp_Stddev_Fields>;\n  stddev_pop?: Maybe<_Uptime_Temp_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<_Uptime_Temp_Stddev_Samp_Fields>;\n  sum?: Maybe<_Uptime_Temp_Sum_Fields>;\n  var_pop?: Maybe<_Uptime_Temp_Var_Pop_Fields>;\n  var_samp?: Maybe<_Uptime_Temp_Var_Samp_Fields>;\n  variance?: Maybe<_Uptime_Temp_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"_uptime_temp\" */\nexport type _Uptime_Temp_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type _Uptime_Temp_Avg_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"_uptime_temp\". All fields are combined with a logical 'AND'. */\nexport type _Uptime_Temp_Bool_Exp = {\n  _and?: InputMaybe<Array<_Uptime_Temp_Bool_Exp>>;\n  _not?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n  _or?: InputMaybe<Array<_Uptime_Temp_Bool_Exp>>;\n  pre_commits?: InputMaybe<Bigint_Comparison_Exp>;\n  validator_address?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type _Uptime_Temp_Max_Fields = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type _Uptime_Temp_Min_Fields = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"_uptime_temp\". */\nexport type _Uptime_Temp_Order_By = {\n  pre_commits?: InputMaybe<Order_By>;\n  validator_address?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"_uptime_temp\" */\nexport enum _Uptime_Temp_Select_Column {\n  /** column name */\n  PreCommits = 'pre_commits',\n  /** column name */\n  ValidatorAddress = 'validator_address'\n}\n\n/** aggregate stddev on columns */\nexport type _Uptime_Temp_Stddev_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type _Uptime_Temp_Stddev_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type _Uptime_Temp_Stddev_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type _Uptime_Temp_Sum_Fields = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type _Uptime_Temp_Var_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type _Uptime_Temp_Var_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type _Uptime_Temp_Variance_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"account\" */\nexport type Account = {\n  /** An object relationship */\n  account_balance?: Maybe<Account_Balance>;\n  address: Scalars['String']['output'];\n  /** fetch data from the table: \"cyberlinks\" */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An array relationship */\n  routesBySource: Array<Routes>;\n  /** An aggregate relationship */\n  routesBySource_aggregate: Routes_Aggregate;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountRoutesBySourceArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountRoutesBySource_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n/** aggregated selection of \"account\" */\nexport type Account_Aggregate = {\n  aggregate?: Maybe<Account_Aggregate_Fields>;\n  nodes: Array<Account>;\n};\n\n/** aggregate fields of \"account\" */\nexport type Account_Aggregate_Fields = {\n  count: Scalars['Int']['output'];\n  max?: Maybe<Account_Max_Fields>;\n  min?: Maybe<Account_Min_Fields>;\n};\n\n\n/** aggregate fields of \"account\" */\nexport type Account_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Account_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** columns and relationships of \"account_balance\" */\nexport type Account_Balance = {\n  /** An object relationship */\n  account: Account;\n  address: Scalars['String']['output'];\n  coins: Scalars['_coin']['output'];\n  height: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"account_balance\" */\nexport type Account_Balance_Aggregate = {\n  aggregate?: Maybe<Account_Balance_Aggregate_Fields>;\n  nodes: Array<Account_Balance>;\n};\n\n/** aggregate fields of \"account_balance\" */\nexport type Account_Balance_Aggregate_Fields = {\n  avg?: Maybe<Account_Balance_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Account_Balance_Max_Fields>;\n  min?: Maybe<Account_Balance_Min_Fields>;\n  stddev?: Maybe<Account_Balance_Stddev_Fields>;\n  stddev_pop?: Maybe<Account_Balance_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Account_Balance_Stddev_Samp_Fields>;\n  sum?: Maybe<Account_Balance_Sum_Fields>;\n  var_pop?: Maybe<Account_Balance_Var_Pop_Fields>;\n  var_samp?: Maybe<Account_Balance_Var_Samp_Fields>;\n  variance?: Maybe<Account_Balance_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"account_balance\" */\nexport type Account_Balance_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Account_Balance_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"account_balance\". All fields are combined with a logical 'AND'. */\nexport type Account_Balance_Bool_Exp = {\n  _and?: InputMaybe<Array<Account_Balance_Bool_Exp>>;\n  _not?: InputMaybe<Account_Balance_Bool_Exp>;\n  _or?: InputMaybe<Array<Account_Balance_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  coins?: InputMaybe<_Coin_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Account_Balance_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Account_Balance_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"account_balance\". */\nexport type Account_Balance_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  address?: InputMaybe<Order_By>;\n  coins?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"account_balance\" */\nexport enum Account_Balance_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Coins = 'coins',\n  /** column name */\n  Height = 'height'\n}\n\n/** aggregate stddev on columns */\nexport type Account_Balance_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Account_Balance_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Account_Balance_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Account_Balance_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Account_Balance_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Account_Balance_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Account_Balance_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"account\". All fields are combined with a logical 'AND'. */\nexport type Account_Bool_Exp = {\n  _and?: InputMaybe<Array<Account_Bool_Exp>>;\n  _not?: InputMaybe<Account_Bool_Exp>;\n  _or?: InputMaybe<Array<Account_Bool_Exp>>;\n  account_balance?: InputMaybe<Account_Balance_Bool_Exp>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  cyberlinks?: InputMaybe<Cyberlinks_Bool_Exp>;\n  investmints?: InputMaybe<Investmints_Bool_Exp>;\n  particles?: InputMaybe<Particles_Bool_Exp>;\n  routes?: InputMaybe<Routes_Bool_Exp>;\n  routesBySource?: InputMaybe<Routes_Bool_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Account_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Account_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"account\". */\nexport type Account_Order_By = {\n  account_balance?: InputMaybe<Account_Balance_Order_By>;\n  address?: InputMaybe<Order_By>;\n  cyberlinks_aggregate?: InputMaybe<Cyberlinks_Aggregate_Order_By>;\n  investmints_aggregate?: InputMaybe<Investmints_Aggregate_Order_By>;\n  particles_aggregate?: InputMaybe<Particles_Aggregate_Order_By>;\n  routesBySource_aggregate?: InputMaybe<Routes_Aggregate_Order_By>;\n  routes_aggregate?: InputMaybe<Routes_Aggregate_Order_By>;\n};\n\n/** select columns of table \"account\" */\nexport enum Account_Select_Column {\n  /** column name */\n  Address = 'address'\n}\n\n/** Boolean expression to compare columns of type \"bigint\". All fields are combined with logical 'AND'. */\nexport type Bigint_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['bigint']['input']>;\n  _gt?: InputMaybe<Scalars['bigint']['input']>;\n  _gte?: InputMaybe<Scalars['bigint']['input']>;\n  _in?: InputMaybe<Array<Scalars['bigint']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['bigint']['input']>;\n  _lte?: InputMaybe<Scalars['bigint']['input']>;\n  _neq?: InputMaybe<Scalars['bigint']['input']>;\n  _nin?: InputMaybe<Array<Scalars['bigint']['input']>>;\n};\n\n/** columns and relationships of \"block\" */\nexport type Block = {\n  /** fetch data from the table: \"cyberlinks\" */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  hash: Scalars['String']['output'];\n  height: Scalars['bigint']['output'];\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  num_txs?: Maybe<Scalars['Int']['output']>;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  proposer_address?: Maybe<Scalars['String']['output']>;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n  timestamp: Scalars['timestamp']['output'];\n  total_gas?: Maybe<Scalars['bigint']['output']>;\n  /** An array relationship */\n  transactions: Array<Transaction>;\n  /** An aggregate relationship */\n  transactions_aggregate: Transaction_Aggregate;\n  /** An object relationship */\n  validator?: Maybe<Validator>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockTransactionsArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockTransactions_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n/** aggregated selection of \"block\" */\nexport type Block_Aggregate = {\n  aggregate?: Maybe<Block_Aggregate_Fields>;\n  nodes: Array<Block>;\n};\n\n/** aggregate fields of \"block\" */\nexport type Block_Aggregate_Fields = {\n  avg?: Maybe<Block_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Block_Max_Fields>;\n  min?: Maybe<Block_Min_Fields>;\n  stddev?: Maybe<Block_Stddev_Fields>;\n  stddev_pop?: Maybe<Block_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Block_Stddev_Samp_Fields>;\n  sum?: Maybe<Block_Sum_Fields>;\n  var_pop?: Maybe<Block_Var_Pop_Fields>;\n  var_samp?: Maybe<Block_Var_Samp_Fields>;\n  variance?: Maybe<Block_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"block\" */\nexport type Block_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Block_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"block\" */\nexport type Block_Aggregate_Order_By = {\n  avg?: InputMaybe<Block_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Block_Max_Order_By>;\n  min?: InputMaybe<Block_Min_Order_By>;\n  stddev?: InputMaybe<Block_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Block_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Block_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Block_Sum_Order_By>;\n  var_pop?: InputMaybe<Block_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Block_Var_Samp_Order_By>;\n  variance?: InputMaybe<Block_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Block_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"block\" */\nexport type Block_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"block\". All fields are combined with a logical 'AND'. */\nexport type Block_Bool_Exp = {\n  _and?: InputMaybe<Array<Block_Bool_Exp>>;\n  _not?: InputMaybe<Block_Bool_Exp>;\n  _or?: InputMaybe<Array<Block_Bool_Exp>>;\n  cyberlinks?: InputMaybe<Cyberlinks_Bool_Exp>;\n  hash?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  investmints?: InputMaybe<Investmints_Bool_Exp>;\n  num_txs?: InputMaybe<Int_Comparison_Exp>;\n  particles?: InputMaybe<Particles_Bool_Exp>;\n  proposer_address?: InputMaybe<String_Comparison_Exp>;\n  routes?: InputMaybe<Routes_Bool_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  total_gas?: InputMaybe<Bigint_Comparison_Exp>;\n  transactions?: InputMaybe<Transaction_Bool_Exp>;\n  validator?: InputMaybe<Validator_Bool_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Block_Max_Fields = {\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  num_txs?: Maybe<Scalars['Int']['output']>;\n  proposer_address?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  total_gas?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by max() on columns of table \"block\" */\nexport type Block_Max_Order_By = {\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  proposer_address?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Block_Min_Fields = {\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  num_txs?: Maybe<Scalars['Int']['output']>;\n  proposer_address?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  total_gas?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by min() on columns of table \"block\" */\nexport type Block_Min_Order_By = {\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  proposer_address?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"block\". */\nexport type Block_Order_By = {\n  cyberlinks_aggregate?: InputMaybe<Cyberlinks_Aggregate_Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  investmints_aggregate?: InputMaybe<Investmints_Aggregate_Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  particles_aggregate?: InputMaybe<Particles_Aggregate_Order_By>;\n  proposer_address?: InputMaybe<Order_By>;\n  routes_aggregate?: InputMaybe<Routes_Aggregate_Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n  transactions_aggregate?: InputMaybe<Transaction_Aggregate_Order_By>;\n  validator?: InputMaybe<Validator_Order_By>;\n};\n\n/** select columns of table \"block\" */\nexport enum Block_Select_Column {\n  /** column name */\n  Hash = 'hash',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  NumTxs = 'num_txs',\n  /** column name */\n  ProposerAddress = 'proposer_address',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TotalGas = 'total_gas'\n}\n\n/** aggregate stddev on columns */\nexport type Block_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"block\" */\nexport type Block_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Block_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"block\" */\nexport type Block_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Block_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"block\" */\nexport type Block_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate sum on columns */\nexport type Block_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  num_txs?: Maybe<Scalars['Int']['output']>;\n  total_gas?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"block\" */\nexport type Block_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Block_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"block\" */\nexport type Block_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Block_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"block\" */\nexport type Block_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Block_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"block\" */\nexport type Block_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to compare columns of type \"coin\". All fields are combined with logical 'AND'. */\nexport type Coin_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['coin']['input']>;\n  _gt?: InputMaybe<Scalars['coin']['input']>;\n  _gte?: InputMaybe<Scalars['coin']['input']>;\n  _in?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['coin']['input']>;\n  _lte?: InputMaybe<Scalars['coin']['input']>;\n  _neq?: InputMaybe<Scalars['coin']['input']>;\n  _nin?: InputMaybe<Array<Scalars['coin']['input']>>;\n};\n\n/** columns and relationships of \"contracts\" */\nexport type Contracts = {\n  address: Scalars['String']['output'];\n  admin: Scalars['String']['output'];\n  code_id: Scalars['bigint']['output'];\n  creation_time: Scalars['String']['output'];\n  creator: Scalars['String']['output'];\n  fees: Scalars['bigint']['output'];\n  gas: Scalars['bigint']['output'];\n  height: Scalars['bigint']['output'];\n  label: Scalars['String']['output'];\n  tx: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"contracts\" */\nexport type Contracts_Aggregate = {\n  aggregate?: Maybe<Contracts_Aggregate_Fields>;\n  nodes: Array<Contracts>;\n};\n\n/** aggregate fields of \"contracts\" */\nexport type Contracts_Aggregate_Fields = {\n  avg?: Maybe<Contracts_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Contracts_Max_Fields>;\n  min?: Maybe<Contracts_Min_Fields>;\n  stddev?: Maybe<Contracts_Stddev_Fields>;\n  stddev_pop?: Maybe<Contracts_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Contracts_Stddev_Samp_Fields>;\n  sum?: Maybe<Contracts_Sum_Fields>;\n  var_pop?: Maybe<Contracts_Var_Pop_Fields>;\n  var_samp?: Maybe<Contracts_Var_Samp_Fields>;\n  variance?: Maybe<Contracts_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"contracts\" */\nexport type Contracts_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Contracts_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Contracts_Avg_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"contracts\". All fields are combined with a logical 'AND'. */\nexport type Contracts_Bool_Exp = {\n  _and?: InputMaybe<Array<Contracts_Bool_Exp>>;\n  _not?: InputMaybe<Contracts_Bool_Exp>;\n  _or?: InputMaybe<Array<Contracts_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  admin?: InputMaybe<String_Comparison_Exp>;\n  code_id?: InputMaybe<Bigint_Comparison_Exp>;\n  creation_time?: InputMaybe<String_Comparison_Exp>;\n  creator?: InputMaybe<String_Comparison_Exp>;\n  fees?: InputMaybe<Bigint_Comparison_Exp>;\n  gas?: InputMaybe<Bigint_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  label?: InputMaybe<String_Comparison_Exp>;\n  tx?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Contracts_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  admin?: Maybe<Scalars['String']['output']>;\n  code_id?: Maybe<Scalars['bigint']['output']>;\n  creation_time?: Maybe<Scalars['String']['output']>;\n  creator?: Maybe<Scalars['String']['output']>;\n  fees?: Maybe<Scalars['bigint']['output']>;\n  gas?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  label?: Maybe<Scalars['String']['output']>;\n  tx?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Contracts_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  admin?: Maybe<Scalars['String']['output']>;\n  code_id?: Maybe<Scalars['bigint']['output']>;\n  creation_time?: Maybe<Scalars['String']['output']>;\n  creator?: Maybe<Scalars['String']['output']>;\n  fees?: Maybe<Scalars['bigint']['output']>;\n  gas?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  label?: Maybe<Scalars['String']['output']>;\n  tx?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"contracts\". */\nexport type Contracts_Order_By = {\n  address?: InputMaybe<Order_By>;\n  admin?: InputMaybe<Order_By>;\n  code_id?: InputMaybe<Order_By>;\n  creation_time?: InputMaybe<Order_By>;\n  creator?: InputMaybe<Order_By>;\n  fees?: InputMaybe<Order_By>;\n  gas?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  label?: InputMaybe<Order_By>;\n  tx?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"contracts\" */\nexport enum Contracts_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Admin = 'admin',\n  /** column name */\n  CodeId = 'code_id',\n  /** column name */\n  CreationTime = 'creation_time',\n  /** column name */\n  Creator = 'creator',\n  /** column name */\n  Fees = 'fees',\n  /** column name */\n  Gas = 'gas',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Label = 'label',\n  /** column name */\n  Tx = 'tx'\n}\n\n/** aggregate stddev on columns */\nexport type Contracts_Stddev_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Contracts_Stddev_Pop_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Contracts_Stddev_Samp_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Contracts_Sum_Fields = {\n  code_id?: Maybe<Scalars['bigint']['output']>;\n  fees?: Maybe<Scalars['bigint']['output']>;\n  gas?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  tx?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Contracts_Var_Pop_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Contracts_Var_Samp_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Contracts_Variance_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"cyb_cohort\" */\nexport type Cyb_Cohort = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neurons_activated?: Maybe<Scalars['bigint']['output']>;\n  redelegation_percent?: Maybe<Scalars['float8']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  undelegation_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"cyb_cohort\" */\nexport type Cyb_Cohort_Aggregate = {\n  aggregate?: Maybe<Cyb_Cohort_Aggregate_Fields>;\n  nodes: Array<Cyb_Cohort>;\n};\n\n/** aggregate fields of \"cyb_cohort\" */\nexport type Cyb_Cohort_Aggregate_Fields = {\n  avg?: Maybe<Cyb_Cohort_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyb_Cohort_Max_Fields>;\n  min?: Maybe<Cyb_Cohort_Min_Fields>;\n  stddev?: Maybe<Cyb_Cohort_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyb_Cohort_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyb_Cohort_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyb_Cohort_Sum_Fields>;\n  var_pop?: Maybe<Cyb_Cohort_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyb_Cohort_Var_Samp_Fields>;\n  variance?: Maybe<Cyb_Cohort_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyb_cohort\" */\nexport type Cyb_Cohort_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Cyb_Cohort_Avg_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"cyb_cohort\". All fields are combined with a logical 'AND'. */\nexport type Cyb_Cohort_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyb_Cohort_Bool_Exp>>;\n  _not?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyb_Cohort_Bool_Exp>>;\n  cyberlink_10_percent?: InputMaybe<Float8_Comparison_Exp>;\n  cyberlink_100_percent?: InputMaybe<Float8_Comparison_Exp>;\n  cyberlink_percent?: InputMaybe<Float8_Comparison_Exp>;\n  hero_hired_percent?: InputMaybe<Float8_Comparison_Exp>;\n  investmint_percent?: InputMaybe<Float8_Comparison_Exp>;\n  neurons_activated?: InputMaybe<Bigint_Comparison_Exp>;\n  redelegation_percent?: InputMaybe<Float8_Comparison_Exp>;\n  swap_percent?: InputMaybe<Float8_Comparison_Exp>;\n  undelegation_percent?: InputMaybe<Float8_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyb_Cohort_Max_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neurons_activated?: Maybe<Scalars['bigint']['output']>;\n  redelegation_percent?: Maybe<Scalars['float8']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  undelegation_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Cyb_Cohort_Min_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neurons_activated?: Maybe<Scalars['bigint']['output']>;\n  redelegation_percent?: Maybe<Scalars['float8']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  undelegation_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"cyb_cohort\". */\nexport type Cyb_Cohort_Order_By = {\n  cyberlink_10_percent?: InputMaybe<Order_By>;\n  cyberlink_100_percent?: InputMaybe<Order_By>;\n  cyberlink_percent?: InputMaybe<Order_By>;\n  hero_hired_percent?: InputMaybe<Order_By>;\n  investmint_percent?: InputMaybe<Order_By>;\n  neurons_activated?: InputMaybe<Order_By>;\n  redelegation_percent?: InputMaybe<Order_By>;\n  swap_percent?: InputMaybe<Order_By>;\n  undelegation_percent?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyb_cohort\" */\nexport enum Cyb_Cohort_Select_Column {\n  /** column name */\n  Cyberlink_10Percent = 'cyberlink_10_percent',\n  /** column name */\n  Cyberlink_100Percent = 'cyberlink_100_percent',\n  /** column name */\n  CyberlinkPercent = 'cyberlink_percent',\n  /** column name */\n  HeroHiredPercent = 'hero_hired_percent',\n  /** column name */\n  InvestmintPercent = 'investmint_percent',\n  /** column name */\n  NeuronsActivated = 'neurons_activated',\n  /** column name */\n  RedelegationPercent = 'redelegation_percent',\n  /** column name */\n  SwapPercent = 'swap_percent',\n  /** column name */\n  UndelegationPercent = 'undelegation_percent',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Cyb_Cohort_Stddev_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyb_Cohort_Stddev_Pop_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyb_Cohort_Stddev_Samp_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Cyb_Cohort_Sum_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neurons_activated?: Maybe<Scalars['bigint']['output']>;\n  redelegation_percent?: Maybe<Scalars['float8']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  undelegation_percent?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyb_Cohort_Var_Pop_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyb_Cohort_Var_Samp_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Cyb_Cohort_Variance_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"cyb_new_cohort\" */\nexport type Cyb_New_Cohort = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activation?: Maybe<Scalars['numeric']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"cyb_new_cohort\" */\nexport type Cyb_New_Cohort_Aggregate = {\n  aggregate?: Maybe<Cyb_New_Cohort_Aggregate_Fields>;\n  nodes: Array<Cyb_New_Cohort>;\n};\n\n/** aggregate fields of \"cyb_new_cohort\" */\nexport type Cyb_New_Cohort_Aggregate_Fields = {\n  avg?: Maybe<Cyb_New_Cohort_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyb_New_Cohort_Max_Fields>;\n  min?: Maybe<Cyb_New_Cohort_Min_Fields>;\n  stddev?: Maybe<Cyb_New_Cohort_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyb_New_Cohort_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyb_New_Cohort_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyb_New_Cohort_Sum_Fields>;\n  var_pop?: Maybe<Cyb_New_Cohort_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyb_New_Cohort_Var_Samp_Fields>;\n  variance?: Maybe<Cyb_New_Cohort_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyb_new_cohort\" */\nexport type Cyb_New_Cohort_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyb_New_Cohort_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Cyb_New_Cohort_Avg_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"cyb_new_cohort\". All fields are combined with a logical 'AND'. */\nexport type Cyb_New_Cohort_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyb_New_Cohort_Bool_Exp>>;\n  _not?: InputMaybe<Cyb_New_Cohort_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyb_New_Cohort_Bool_Exp>>;\n  cyberlink_10_percent?: InputMaybe<Float8_Comparison_Exp>;\n  cyberlink_100_percent?: InputMaybe<Float8_Comparison_Exp>;\n  cyberlink_percent?: InputMaybe<Float8_Comparison_Exp>;\n  hero_hired_percent?: InputMaybe<Float8_Comparison_Exp>;\n  investmint_percent?: InputMaybe<Float8_Comparison_Exp>;\n  neuron_activation?: InputMaybe<Numeric_Comparison_Exp>;\n  swap_percent?: InputMaybe<Float8_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyb_New_Cohort_Max_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activation?: Maybe<Scalars['numeric']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Cyb_New_Cohort_Min_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activation?: Maybe<Scalars['numeric']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"cyb_new_cohort\". */\nexport type Cyb_New_Cohort_Order_By = {\n  cyberlink_10_percent?: InputMaybe<Order_By>;\n  cyberlink_100_percent?: InputMaybe<Order_By>;\n  cyberlink_percent?: InputMaybe<Order_By>;\n  hero_hired_percent?: InputMaybe<Order_By>;\n  investmint_percent?: InputMaybe<Order_By>;\n  neuron_activation?: InputMaybe<Order_By>;\n  swap_percent?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyb_new_cohort\" */\nexport enum Cyb_New_Cohort_Select_Column {\n  /** column name */\n  Cyberlink_10Percent = 'cyberlink_10_percent',\n  /** column name */\n  Cyberlink_100Percent = 'cyberlink_100_percent',\n  /** column name */\n  CyberlinkPercent = 'cyberlink_percent',\n  /** column name */\n  HeroHiredPercent = 'hero_hired_percent',\n  /** column name */\n  InvestmintPercent = 'investmint_percent',\n  /** column name */\n  NeuronActivation = 'neuron_activation',\n  /** column name */\n  SwapPercent = 'swap_percent',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Cyb_New_Cohort_Stddev_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyb_New_Cohort_Stddev_Pop_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyb_New_Cohort_Stddev_Samp_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Cyb_New_Cohort_Sum_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activation?: Maybe<Scalars['numeric']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyb_New_Cohort_Var_Pop_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyb_New_Cohort_Var_Samp_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Cyb_New_Cohort_Variance_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"cyber_gift\" */\nexport type Cyber_Gift = {\n  address: Scalars['String']['output'];\n  audience: Scalars['String']['output'];\n  gift: Scalars['numeric']['output'];\n  grade: Scalars['Int']['output'];\n  segment: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"cyber_gift\" */\nexport type Cyber_Gift_Aggregate = {\n  aggregate?: Maybe<Cyber_Gift_Aggregate_Fields>;\n  nodes: Array<Cyber_Gift>;\n};\n\n/** aggregate fields of \"cyber_gift\" */\nexport type Cyber_Gift_Aggregate_Fields = {\n  avg?: Maybe<Cyber_Gift_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyber_Gift_Max_Fields>;\n  min?: Maybe<Cyber_Gift_Min_Fields>;\n  stddev?: Maybe<Cyber_Gift_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyber_Gift_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyber_Gift_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyber_Gift_Sum_Fields>;\n  var_pop?: Maybe<Cyber_Gift_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyber_Gift_Var_Samp_Fields>;\n  variance?: Maybe<Cyber_Gift_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyber_gift\" */\nexport type Cyber_Gift_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyber_Gift_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Cyber_Gift_Avg_Fields = {\n  gift?: Maybe<Scalars['Float']['output']>;\n  grade?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"cyber_gift\". All fields are combined with a logical 'AND'. */\nexport type Cyber_Gift_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyber_Gift_Bool_Exp>>;\n  _not?: InputMaybe<Cyber_Gift_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyber_Gift_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  audience?: InputMaybe<String_Comparison_Exp>;\n  gift?: InputMaybe<Numeric_Comparison_Exp>;\n  grade?: InputMaybe<Int_Comparison_Exp>;\n  segment?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyber_Gift_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  audience?: Maybe<Scalars['String']['output']>;\n  gift?: Maybe<Scalars['numeric']['output']>;\n  grade?: Maybe<Scalars['Int']['output']>;\n  segment?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Cyber_Gift_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  audience?: Maybe<Scalars['String']['output']>;\n  gift?: Maybe<Scalars['numeric']['output']>;\n  grade?: Maybe<Scalars['Int']['output']>;\n  segment?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"cyber_gift\". */\nexport type Cyber_Gift_Order_By = {\n  address?: InputMaybe<Order_By>;\n  audience?: InputMaybe<Order_By>;\n  gift?: InputMaybe<Order_By>;\n  grade?: InputMaybe<Order_By>;\n  segment?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  details?: Maybe<Scalars['json']['output']>;\n  proof?: Maybe<Scalars['String']['output']>;\n};\n\n\n/** columns and relationships of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_ProofsDetailsArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregated selection of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs_Aggregate = {\n  aggregate?: Maybe<Cyber_Gift_Proofs_Aggregate_Fields>;\n  nodes: Array<Cyber_Gift_Proofs>;\n};\n\n/** aggregate fields of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs_Aggregate_Fields = {\n  avg?: Maybe<Cyber_Gift_Proofs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyber_Gift_Proofs_Max_Fields>;\n  min?: Maybe<Cyber_Gift_Proofs_Min_Fields>;\n  stddev?: Maybe<Cyber_Gift_Proofs_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyber_Gift_Proofs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyber_Gift_Proofs_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyber_Gift_Proofs_Sum_Fields>;\n  var_pop?: Maybe<Cyber_Gift_Proofs_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyber_Gift_Proofs_Var_Samp_Fields>;\n  variance?: Maybe<Cyber_Gift_Proofs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Cyber_Gift_Proofs_Avg_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"cyber_gift_proofs\". All fields are combined with a logical 'AND'. */\nexport type Cyber_Gift_Proofs_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyber_Gift_Proofs_Bool_Exp>>;\n  _not?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyber_Gift_Proofs_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  amount?: InputMaybe<Bigint_Comparison_Exp>;\n  details?: InputMaybe<Json_Comparison_Exp>;\n  proof?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyber_Gift_Proofs_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  proof?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Cyber_Gift_Proofs_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  proof?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"cyber_gift_proofs\". */\nexport type Cyber_Gift_Proofs_Order_By = {\n  address?: InputMaybe<Order_By>;\n  amount?: InputMaybe<Order_By>;\n  details?: InputMaybe<Order_By>;\n  proof?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyber_gift_proofs\" */\nexport enum Cyber_Gift_Proofs_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Amount = 'amount',\n  /** column name */\n  Details = 'details',\n  /** column name */\n  Proof = 'proof'\n}\n\n/** aggregate stddev on columns */\nexport type Cyber_Gift_Proofs_Stddev_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyber_Gift_Proofs_Stddev_Pop_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyber_Gift_Proofs_Stddev_Samp_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Cyber_Gift_Proofs_Sum_Fields = {\n  amount?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyber_Gift_Proofs_Var_Pop_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyber_Gift_Proofs_Var_Samp_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Cyber_Gift_Proofs_Variance_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** select columns of table \"cyber_gift\" */\nexport enum Cyber_Gift_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Audience = 'audience',\n  /** column name */\n  Gift = 'gift',\n  /** column name */\n  Grade = 'grade',\n  /** column name */\n  Segment = 'segment'\n}\n\n/** aggregate stddev on columns */\nexport type Cyber_Gift_Stddev_Fields = {\n  gift?: Maybe<Scalars['Float']['output']>;\n  grade?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyber_Gift_Stddev_Pop_Fields = {\n  gift?: Maybe<Scalars['Float']['output']>;\n  grade?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyber_Gift_Stddev_Samp_Fields = {\n  gift?: Maybe<Scalars['Float']['output']>;\n  grade?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Cyber_Gift_Sum_Fields = {\n  gift?: Maybe<Scalars['numeric']['output']>;\n  grade?: Maybe<Scalars['Int']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyber_Gift_Var_Pop_Fields = {\n  gift?: Maybe<Scalars['Float']['output']>;\n  grade?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyber_Gift_Var_Samp_Fields = {\n  gift?: Maybe<Scalars['Float']['output']>;\n  grade?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Cyber_Gift_Variance_Fields = {\n  gift?: Maybe<Scalars['Float']['output']>;\n  grade?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"cyberlinks\" */\nexport type Cyberlinks = {\n  /** An object relationship */\n  account: Account;\n  /** An object relationship */\n  block: Block;\n  /** An object relationship */\n  from?: Maybe<Particles>;\n  height: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n  neuron: Scalars['String']['output'];\n  particle_from: Scalars['String']['output'];\n  particle_to: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  /** An object relationship */\n  to?: Maybe<Particles>;\n  /** An object relationship */\n  transaction: Transaction;\n  transaction_hash: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"cyberlinks\" */\nexport type Cyberlinks_Aggregate = {\n  aggregate?: Maybe<Cyberlinks_Aggregate_Fields>;\n  nodes: Array<Cyberlinks>;\n};\n\n/** aggregate fields of \"cyberlinks\" */\nexport type Cyberlinks_Aggregate_Fields = {\n  avg?: Maybe<Cyberlinks_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyberlinks_Max_Fields>;\n  min?: Maybe<Cyberlinks_Min_Fields>;\n  stddev?: Maybe<Cyberlinks_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyberlinks_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyberlinks_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyberlinks_Sum_Fields>;\n  var_pop?: Maybe<Cyberlinks_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyberlinks_Var_Samp_Fields>;\n  variance?: Maybe<Cyberlinks_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyberlinks\" */\nexport type Cyberlinks_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"cyberlinks\" */\nexport type Cyberlinks_Aggregate_Order_By = {\n  avg?: InputMaybe<Cyberlinks_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Cyberlinks_Max_Order_By>;\n  min?: InputMaybe<Cyberlinks_Min_Order_By>;\n  stddev?: InputMaybe<Cyberlinks_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Cyberlinks_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Cyberlinks_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Cyberlinks_Sum_Order_By>;\n  var_pop?: InputMaybe<Cyberlinks_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Cyberlinks_Var_Samp_Order_By>;\n  variance?: InputMaybe<Cyberlinks_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Cyberlinks_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"cyberlinks\". All fields are combined with a logical 'AND'. */\nexport type Cyberlinks_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyberlinks_Bool_Exp>>;\n  _not?: InputMaybe<Cyberlinks_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyberlinks_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  from?: InputMaybe<Particles_Bool_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n  particle_from?: InputMaybe<String_Comparison_Exp>;\n  particle_to?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  to?: InputMaybe<Particles_Bool_Exp>;\n  transaction?: InputMaybe<Transaction_Bool_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyberlinks_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  particle_from?: Maybe<Scalars['String']['output']>;\n  particle_to?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle_from?: InputMaybe<Order_By>;\n  particle_to?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Cyberlinks_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  particle_from?: Maybe<Scalars['String']['output']>;\n  particle_to?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle_from?: InputMaybe<Order_By>;\n  particle_to?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"cyberlinks\". */\nexport type Cyberlinks_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  from?: InputMaybe<Particles_Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle_from?: InputMaybe<Order_By>;\n  particle_to?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  to?: InputMaybe<Particles_Order_By>;\n  transaction?: InputMaybe<Transaction_Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyberlinks\" */\nexport enum Cyberlinks_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  Neuron = 'neuron',\n  /** column name */\n  ParticleFrom = 'particle_from',\n  /** column name */\n  ParticleTo = 'particle_to',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TransactionHash = 'transaction_hash'\n}\n\n/** columns and relationships of \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats = {\n  cyberlinks?: Maybe<Scalars['numeric']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats_Aggregate = {\n  aggregate?: Maybe<Cyberlinks_Stats_Aggregate_Fields>;\n  nodes: Array<Cyberlinks_Stats>;\n};\n\n/** aggregate fields of \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats_Aggregate_Fields = {\n  avg?: Maybe<Cyberlinks_Stats_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyberlinks_Stats_Max_Fields>;\n  min?: Maybe<Cyberlinks_Stats_Min_Fields>;\n  stddev?: Maybe<Cyberlinks_Stats_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyberlinks_Stats_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyberlinks_Stats_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyberlinks_Stats_Sum_Fields>;\n  var_pop?: Maybe<Cyberlinks_Stats_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyberlinks_Stats_Var_Samp_Fields>;\n  variance?: Maybe<Cyberlinks_Stats_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Cyberlinks_Stats_Avg_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"cyberlinks_stats\". All fields are combined with a logical 'AND'. */\nexport type Cyberlinks_Stats_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyberlinks_Stats_Bool_Exp>>;\n  _not?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyberlinks_Stats_Bool_Exp>>;\n  cyberlinks?: InputMaybe<Numeric_Comparison_Exp>;\n  cyberlinks_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyberlinks_Stats_Max_Fields = {\n  cyberlinks?: Maybe<Scalars['numeric']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Cyberlinks_Stats_Min_Fields = {\n  cyberlinks?: Maybe<Scalars['numeric']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"cyberlinks_stats\". */\nexport type Cyberlinks_Stats_Order_By = {\n  cyberlinks?: InputMaybe<Order_By>;\n  cyberlinks_per_day?: InputMaybe<Order_By>;\n  date?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyberlinks_stats\" */\nexport enum Cyberlinks_Stats_Select_Column {\n  /** column name */\n  Cyberlinks = 'cyberlinks',\n  /** column name */\n  CyberlinksPerDay = 'cyberlinks_per_day',\n  /** column name */\n  Date = 'date'\n}\n\n/** aggregate stddev on columns */\nexport type Cyberlinks_Stats_Stddev_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyberlinks_Stats_Stddev_Pop_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyberlinks_Stats_Stddev_Samp_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Cyberlinks_Stats_Sum_Fields = {\n  cyberlinks?: Maybe<Scalars['numeric']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyberlinks_Stats_Var_Pop_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyberlinks_Stats_Var_Samp_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Cyberlinks_Stats_Variance_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev on columns */\nexport type Cyberlinks_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyberlinks_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyberlinks_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate sum on columns */\nexport type Cyberlinks_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** order by sum() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyberlinks_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyberlinks_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Cyberlinks_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons_Aggregate = {\n  aggregate?: Maybe<Daily_Amount_Of_Active_Neurons_Aggregate_Fields>;\n  nodes: Array<Daily_Amount_Of_Active_Neurons>;\n};\n\n/** aggregate fields of \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons_Aggregate_Fields = {\n  avg?: Maybe<Daily_Amount_Of_Active_Neurons_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Daily_Amount_Of_Active_Neurons_Max_Fields>;\n  min?: Maybe<Daily_Amount_Of_Active_Neurons_Min_Fields>;\n  stddev?: Maybe<Daily_Amount_Of_Active_Neurons_Stddev_Fields>;\n  stddev_pop?: Maybe<Daily_Amount_Of_Active_Neurons_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Daily_Amount_Of_Active_Neurons_Stddev_Samp_Fields>;\n  sum?: Maybe<Daily_Amount_Of_Active_Neurons_Sum_Fields>;\n  var_pop?: Maybe<Daily_Amount_Of_Active_Neurons_Var_Pop_Fields>;\n  var_samp?: Maybe<Daily_Amount_Of_Active_Neurons_Var_Samp_Fields>;\n  variance?: Maybe<Daily_Amount_Of_Active_Neurons_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Daily_Amount_Of_Active_Neurons_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"daily_amount_of_active_neurons\". All fields are combined with a logical 'AND'. */\nexport type Daily_Amount_Of_Active_Neurons_Bool_Exp = {\n  _and?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Bool_Exp>>;\n  _not?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n  _or?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Daily_Amount_Of_Active_Neurons_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Daily_Amount_Of_Active_Neurons_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"daily_amount_of_active_neurons\". */\nexport type Daily_Amount_Of_Active_Neurons_Order_By = {\n  count?: InputMaybe<Order_By>;\n  date?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"daily_amount_of_active_neurons\" */\nexport enum Daily_Amount_Of_Active_Neurons_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Date = 'date'\n}\n\n/** aggregate stddev on columns */\nexport type Daily_Amount_Of_Active_Neurons_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Daily_Amount_Of_Active_Neurons_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Daily_Amount_Of_Active_Neurons_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Daily_Amount_Of_Active_Neurons_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Daily_Amount_Of_Active_Neurons_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Daily_Amount_Of_Active_Neurons_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Daily_Amount_Of_Active_Neurons_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas = {\n  daily_gas?: Maybe<Scalars['numeric']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n  gas_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas_Aggregate = {\n  aggregate?: Maybe<Daily_Amount_Of_Used_Gas_Aggregate_Fields>;\n  nodes: Array<Daily_Amount_Of_Used_Gas>;\n};\n\n/** aggregate fields of \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas_Aggregate_Fields = {\n  avg?: Maybe<Daily_Amount_Of_Used_Gas_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Daily_Amount_Of_Used_Gas_Max_Fields>;\n  min?: Maybe<Daily_Amount_Of_Used_Gas_Min_Fields>;\n  stddev?: Maybe<Daily_Amount_Of_Used_Gas_Stddev_Fields>;\n  stddev_pop?: Maybe<Daily_Amount_Of_Used_Gas_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Daily_Amount_Of_Used_Gas_Stddev_Samp_Fields>;\n  sum?: Maybe<Daily_Amount_Of_Used_Gas_Sum_Fields>;\n  var_pop?: Maybe<Daily_Amount_Of_Used_Gas_Var_Pop_Fields>;\n  var_samp?: Maybe<Daily_Amount_Of_Used_Gas_Var_Samp_Fields>;\n  variance?: Maybe<Daily_Amount_Of_Used_Gas_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Daily_Amount_Of_Used_Gas_Avg_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"daily_amount_of_used_gas\". All fields are combined with a logical 'AND'. */\nexport type Daily_Amount_Of_Used_Gas_Bool_Exp = {\n  _and?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Bool_Exp>>;\n  _not?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n  _or?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Bool_Exp>>;\n  daily_gas?: InputMaybe<Numeric_Comparison_Exp>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  gas_total?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Daily_Amount_Of_Used_Gas_Max_Fields = {\n  daily_gas?: Maybe<Scalars['numeric']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n  gas_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Daily_Amount_Of_Used_Gas_Min_Fields = {\n  daily_gas?: Maybe<Scalars['numeric']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n  gas_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"daily_amount_of_used_gas\". */\nexport type Daily_Amount_Of_Used_Gas_Order_By = {\n  daily_gas?: InputMaybe<Order_By>;\n  date?: InputMaybe<Order_By>;\n  gas_total?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"daily_amount_of_used_gas\" */\nexport enum Daily_Amount_Of_Used_Gas_Select_Column {\n  /** column name */\n  DailyGas = 'daily_gas',\n  /** column name */\n  Date = 'date',\n  /** column name */\n  GasTotal = 'gas_total'\n}\n\n/** aggregate stddev on columns */\nexport type Daily_Amount_Of_Used_Gas_Stddev_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Daily_Amount_Of_Used_Gas_Stddev_Pop_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Daily_Amount_Of_Used_Gas_Stddev_Samp_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Daily_Amount_Of_Used_Gas_Sum_Fields = {\n  daily_gas?: Maybe<Scalars['numeric']['output']>;\n  gas_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Daily_Amount_Of_Used_Gas_Var_Pop_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Daily_Amount_Of_Used_Gas_Var_Samp_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Daily_Amount_Of_Used_Gas_Variance_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions = {\n  date?: Maybe<Scalars['date']['output']>;\n  txs_per_day?: Maybe<Scalars['bigint']['output']>;\n  txs_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions_Aggregate = {\n  aggregate?: Maybe<Daily_Number_Of_Transactions_Aggregate_Fields>;\n  nodes: Array<Daily_Number_Of_Transactions>;\n};\n\n/** aggregate fields of \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions_Aggregate_Fields = {\n  avg?: Maybe<Daily_Number_Of_Transactions_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Daily_Number_Of_Transactions_Max_Fields>;\n  min?: Maybe<Daily_Number_Of_Transactions_Min_Fields>;\n  stddev?: Maybe<Daily_Number_Of_Transactions_Stddev_Fields>;\n  stddev_pop?: Maybe<Daily_Number_Of_Transactions_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Daily_Number_Of_Transactions_Stddev_Samp_Fields>;\n  sum?: Maybe<Daily_Number_Of_Transactions_Sum_Fields>;\n  var_pop?: Maybe<Daily_Number_Of_Transactions_Var_Pop_Fields>;\n  var_samp?: Maybe<Daily_Number_Of_Transactions_Var_Samp_Fields>;\n  variance?: Maybe<Daily_Number_Of_Transactions_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Daily_Number_Of_Transactions_Avg_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"daily_number_of_transactions\". All fields are combined with a logical 'AND'. */\nexport type Daily_Number_Of_Transactions_Bool_Exp = {\n  _and?: InputMaybe<Array<Daily_Number_Of_Transactions_Bool_Exp>>;\n  _not?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n  _or?: InputMaybe<Array<Daily_Number_Of_Transactions_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  txs_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n  txs_total?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Daily_Number_Of_Transactions_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  txs_per_day?: Maybe<Scalars['bigint']['output']>;\n  txs_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Daily_Number_Of_Transactions_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  txs_per_day?: Maybe<Scalars['bigint']['output']>;\n  txs_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"daily_number_of_transactions\". */\nexport type Daily_Number_Of_Transactions_Order_By = {\n  date?: InputMaybe<Order_By>;\n  txs_per_day?: InputMaybe<Order_By>;\n  txs_total?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"daily_number_of_transactions\" */\nexport enum Daily_Number_Of_Transactions_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  TxsPerDay = 'txs_per_day',\n  /** column name */\n  TxsTotal = 'txs_total'\n}\n\n/** aggregate stddev on columns */\nexport type Daily_Number_Of_Transactions_Stddev_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Daily_Number_Of_Transactions_Stddev_Pop_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Daily_Number_Of_Transactions_Stddev_Samp_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Daily_Number_Of_Transactions_Sum_Fields = {\n  txs_per_day?: Maybe<Scalars['bigint']['output']>;\n  txs_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Daily_Number_Of_Transactions_Var_Pop_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Daily_Number_Of_Transactions_Var_Samp_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Daily_Number_Of_Transactions_Variance_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to compare columns of type \"date\". All fields are combined with logical 'AND'. */\nexport type Date_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['date']['input']>;\n  _gt?: InputMaybe<Scalars['date']['input']>;\n  _gte?: InputMaybe<Scalars['date']['input']>;\n  _in?: InputMaybe<Array<Scalars['date']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['date']['input']>;\n  _lte?: InputMaybe<Scalars['date']['input']>;\n  _neq?: InputMaybe<Scalars['date']['input']>;\n  _nin?: InputMaybe<Array<Scalars['date']['input']>>;\n};\n\n/** Boolean expression to compare columns of type \"float8\". All fields are combined with logical 'AND'. */\nexport type Float8_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['float8']['input']>;\n  _gt?: InputMaybe<Scalars['float8']['input']>;\n  _gte?: InputMaybe<Scalars['float8']['input']>;\n  _in?: InputMaybe<Array<Scalars['float8']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['float8']['input']>;\n  _lte?: InputMaybe<Scalars['float8']['input']>;\n  _neq?: InputMaybe<Scalars['float8']['input']>;\n  _nin?: InputMaybe<Array<Scalars['float8']['input']>>;\n};\n\n/** columns and relationships of \"follow_stats\" */\nexport type Follow_Stats = {\n  date?: Maybe<Scalars['date']['output']>;\n  follow_total?: Maybe<Scalars['numeric']['output']>;\n  follows_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregated selection of \"follow_stats\" */\nexport type Follow_Stats_Aggregate = {\n  aggregate?: Maybe<Follow_Stats_Aggregate_Fields>;\n  nodes: Array<Follow_Stats>;\n};\n\n/** aggregate fields of \"follow_stats\" */\nexport type Follow_Stats_Aggregate_Fields = {\n  avg?: Maybe<Follow_Stats_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Follow_Stats_Max_Fields>;\n  min?: Maybe<Follow_Stats_Min_Fields>;\n  stddev?: Maybe<Follow_Stats_Stddev_Fields>;\n  stddev_pop?: Maybe<Follow_Stats_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Follow_Stats_Stddev_Samp_Fields>;\n  sum?: Maybe<Follow_Stats_Sum_Fields>;\n  var_pop?: Maybe<Follow_Stats_Var_Pop_Fields>;\n  var_samp?: Maybe<Follow_Stats_Var_Samp_Fields>;\n  variance?: Maybe<Follow_Stats_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"follow_stats\" */\nexport type Follow_Stats_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Follow_Stats_Avg_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"follow_stats\". All fields are combined with a logical 'AND'. */\nexport type Follow_Stats_Bool_Exp = {\n  _and?: InputMaybe<Array<Follow_Stats_Bool_Exp>>;\n  _not?: InputMaybe<Follow_Stats_Bool_Exp>;\n  _or?: InputMaybe<Array<Follow_Stats_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  follow_total?: InputMaybe<Numeric_Comparison_Exp>;\n  follows_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Follow_Stats_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  follow_total?: Maybe<Scalars['numeric']['output']>;\n  follows_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Follow_Stats_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  follow_total?: Maybe<Scalars['numeric']['output']>;\n  follows_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"follow_stats\". */\nexport type Follow_Stats_Order_By = {\n  date?: InputMaybe<Order_By>;\n  follow_total?: InputMaybe<Order_By>;\n  follows_per_day?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"follow_stats\" */\nexport enum Follow_Stats_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  FollowTotal = 'follow_total',\n  /** column name */\n  FollowsPerDay = 'follows_per_day'\n}\n\n/** aggregate stddev on columns */\nexport type Follow_Stats_Stddev_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Follow_Stats_Stddev_Pop_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Follow_Stats_Stddev_Samp_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Follow_Stats_Sum_Fields = {\n  follow_total?: Maybe<Scalars['numeric']['output']>;\n  follows_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Follow_Stats_Var_Pop_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Follow_Stats_Var_Samp_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Follow_Stats_Variance_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation = {\n  count?: Maybe<Scalars['float8']['output']>;\n  neurons?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation_Aggregate = {\n  aggregate?: Maybe<Genesis_Neurons_Activation_Aggregate_Fields>;\n  nodes: Array<Genesis_Neurons_Activation>;\n};\n\n/** aggregate fields of \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation_Aggregate_Fields = {\n  avg?: Maybe<Genesis_Neurons_Activation_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Genesis_Neurons_Activation_Max_Fields>;\n  min?: Maybe<Genesis_Neurons_Activation_Min_Fields>;\n  stddev?: Maybe<Genesis_Neurons_Activation_Stddev_Fields>;\n  stddev_pop?: Maybe<Genesis_Neurons_Activation_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Genesis_Neurons_Activation_Stddev_Samp_Fields>;\n  sum?: Maybe<Genesis_Neurons_Activation_Sum_Fields>;\n  var_pop?: Maybe<Genesis_Neurons_Activation_Var_Pop_Fields>;\n  var_samp?: Maybe<Genesis_Neurons_Activation_Var_Samp_Fields>;\n  variance?: Maybe<Genesis_Neurons_Activation_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Genesis_Neurons_Activation_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"genesis_neurons_activation\". All fields are combined with a logical 'AND'. */\nexport type Genesis_Neurons_Activation_Bool_Exp = {\n  _and?: InputMaybe<Array<Genesis_Neurons_Activation_Bool_Exp>>;\n  _not?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n  _or?: InputMaybe<Array<Genesis_Neurons_Activation_Bool_Exp>>;\n  count?: InputMaybe<Float8_Comparison_Exp>;\n  neurons?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Genesis_Neurons_Activation_Max_Fields = {\n  count?: Maybe<Scalars['float8']['output']>;\n  neurons?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Genesis_Neurons_Activation_Min_Fields = {\n  count?: Maybe<Scalars['float8']['output']>;\n  neurons?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"genesis_neurons_activation\". */\nexport type Genesis_Neurons_Activation_Order_By = {\n  count?: InputMaybe<Order_By>;\n  neurons?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"genesis_neurons_activation\" */\nexport enum Genesis_Neurons_Activation_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Neurons = 'neurons'\n}\n\n/** aggregate stddev on columns */\nexport type Genesis_Neurons_Activation_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Genesis_Neurons_Activation_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Genesis_Neurons_Activation_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Genesis_Neurons_Activation_Sum_Fields = {\n  count?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Genesis_Neurons_Activation_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Genesis_Neurons_Activation_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Genesis_Neurons_Activation_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"investmints\" */\nexport type Investmints = {\n  /** An object relationship */\n  account: Account;\n  amount: Scalars['coin']['output'];\n  /** An object relationship */\n  block: Block;\n  height: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n  length: Scalars['bigint']['output'];\n  neuron: Scalars['String']['output'];\n  resource: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  /** An object relationship */\n  transaction: Transaction;\n  transaction_hash: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"investmints\" */\nexport type Investmints_Aggregate = {\n  aggregate?: Maybe<Investmints_Aggregate_Fields>;\n  nodes: Array<Investmints>;\n};\n\n/** aggregate fields of \"investmints\" */\nexport type Investmints_Aggregate_Fields = {\n  avg?: Maybe<Investmints_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Investmints_Max_Fields>;\n  min?: Maybe<Investmints_Min_Fields>;\n  stddev?: Maybe<Investmints_Stddev_Fields>;\n  stddev_pop?: Maybe<Investmints_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Investmints_Stddev_Samp_Fields>;\n  sum?: Maybe<Investmints_Sum_Fields>;\n  var_pop?: Maybe<Investmints_Var_Pop_Fields>;\n  var_samp?: Maybe<Investmints_Var_Samp_Fields>;\n  variance?: Maybe<Investmints_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"investmints\" */\nexport type Investmints_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Investmints_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"investmints\" */\nexport type Investmints_Aggregate_Order_By = {\n  avg?: InputMaybe<Investmints_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Investmints_Max_Order_By>;\n  min?: InputMaybe<Investmints_Min_Order_By>;\n  stddev?: InputMaybe<Investmints_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Investmints_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Investmints_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Investmints_Sum_Order_By>;\n  var_pop?: InputMaybe<Investmints_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Investmints_Var_Samp_Order_By>;\n  variance?: InputMaybe<Investmints_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Investmints_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"investmints\" */\nexport type Investmints_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"investmints\". All fields are combined with a logical 'AND'. */\nexport type Investmints_Bool_Exp = {\n  _and?: InputMaybe<Array<Investmints_Bool_Exp>>;\n  _not?: InputMaybe<Investmints_Bool_Exp>;\n  _or?: InputMaybe<Array<Investmints_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  amount?: InputMaybe<Coin_Comparison_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  length?: InputMaybe<Bigint_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n  resource?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  transaction?: InputMaybe<Transaction_Bool_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Investmints_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  length?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  resource?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"investmints\" */\nexport type Investmints_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  resource?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Investmints_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  length?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  resource?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"investmints\" */\nexport type Investmints_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  resource?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"investmints\". */\nexport type Investmints_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  amount?: InputMaybe<Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  resource?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction?: InputMaybe<Transaction_Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"investmints\" */\nexport enum Investmints_Select_Column {\n  /** column name */\n  Amount = 'amount',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  Length = 'length',\n  /** column name */\n  Neuron = 'neuron',\n  /** column name */\n  Resource = 'resource',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TransactionHash = 'transaction_hash'\n}\n\n/** aggregate stddev on columns */\nexport type Investmints_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"investmints\" */\nexport type Investmints_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Investmints_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"investmints\" */\nexport type Investmints_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Investmints_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"investmints\" */\nexport type Investmints_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate sum on columns */\nexport type Investmints_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  length?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"investmints\" */\nexport type Investmints_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Investmints_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"investmints\" */\nexport type Investmints_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Investmints_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"investmints\" */\nexport type Investmints_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Investmints_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"investmints\" */\nexport type Investmints_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to compare columns of type \"json\". All fields are combined with logical 'AND'. */\nexport type Json_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['json']['input']>;\n  _gt?: InputMaybe<Scalars['json']['input']>;\n  _gte?: InputMaybe<Scalars['json']['input']>;\n  _in?: InputMaybe<Array<Scalars['json']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['json']['input']>;\n  _lte?: InputMaybe<Scalars['json']['input']>;\n  _neq?: InputMaybe<Scalars['json']['input']>;\n  _nin?: InputMaybe<Array<Scalars['json']['input']>>;\n};\n\n/** Boolean expression to compare columns of type \"jsonb\". All fields are combined with logical 'AND'. */\nexport type Jsonb_Comparison_Exp = {\n  /** is the column contained in the given json value */\n  _contained_in?: InputMaybe<Scalars['jsonb']['input']>;\n  /** does the column contain the given json value at the top level */\n  _contains?: InputMaybe<Scalars['jsonb']['input']>;\n  _eq?: InputMaybe<Scalars['jsonb']['input']>;\n  _gt?: InputMaybe<Scalars['jsonb']['input']>;\n  _gte?: InputMaybe<Scalars['jsonb']['input']>;\n  /** does the string exist as a top-level key in the column */\n  _has_key?: InputMaybe<Scalars['String']['input']>;\n  /** do all of these strings exist as top-level keys in the column */\n  _has_keys_all?: InputMaybe<Array<Scalars['String']['input']>>;\n  /** do any of these strings exist as top-level keys in the column */\n  _has_keys_any?: InputMaybe<Array<Scalars['String']['input']>>;\n  _in?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['jsonb']['input']>;\n  _lte?: InputMaybe<Scalars['jsonb']['input']>;\n  _neq?: InputMaybe<Scalars['jsonb']['input']>;\n  _nin?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n};\n\n/** columns and relationships of \"message\" */\nexport type Message = {\n  index: Scalars['bigint']['output'];\n  involved_accounts_addresses?: Maybe<Scalars['_text']['output']>;\n  /** An object relationship */\n  transaction: Transaction;\n  transaction_hash: Scalars['String']['output'];\n  type: Scalars['String']['output'];\n  value: Scalars['jsonb']['output'];\n};\n\n\n/** columns and relationships of \"message\" */\nexport type MessageValueArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregated selection of \"message\" */\nexport type Message_Aggregate = {\n  aggregate?: Maybe<Message_Aggregate_Fields>;\n  nodes: Array<Message>;\n};\n\n/** aggregate fields of \"message\" */\nexport type Message_Aggregate_Fields = {\n  avg?: Maybe<Message_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Message_Max_Fields>;\n  min?: Maybe<Message_Min_Fields>;\n  stddev?: Maybe<Message_Stddev_Fields>;\n  stddev_pop?: Maybe<Message_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Message_Stddev_Samp_Fields>;\n  sum?: Maybe<Message_Sum_Fields>;\n  var_pop?: Maybe<Message_Var_Pop_Fields>;\n  var_samp?: Maybe<Message_Var_Samp_Fields>;\n  variance?: Maybe<Message_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"message\" */\nexport type Message_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Message_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"message\" */\nexport type Message_Aggregate_Order_By = {\n  avg?: InputMaybe<Message_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Message_Max_Order_By>;\n  min?: InputMaybe<Message_Min_Order_By>;\n  stddev?: InputMaybe<Message_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Message_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Message_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Message_Sum_Order_By>;\n  var_pop?: InputMaybe<Message_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Message_Var_Samp_Order_By>;\n  variance?: InputMaybe<Message_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Message_Avg_Fields = {\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"message\" */\nexport type Message_Avg_Order_By = {\n  index?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"message\". All fields are combined with a logical 'AND'. */\nexport type Message_Bool_Exp = {\n  _and?: InputMaybe<Array<Message_Bool_Exp>>;\n  _not?: InputMaybe<Message_Bool_Exp>;\n  _or?: InputMaybe<Array<Message_Bool_Exp>>;\n  index?: InputMaybe<Bigint_Comparison_Exp>;\n  involved_accounts_addresses?: InputMaybe<_Text_Comparison_Exp>;\n  transaction?: InputMaybe<Transaction_Bool_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  value?: InputMaybe<Jsonb_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Message_Max_Fields = {\n  index?: Maybe<Scalars['bigint']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"message\" */\nexport type Message_Max_Order_By = {\n  index?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Message_Min_Fields = {\n  index?: Maybe<Scalars['bigint']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"message\" */\nexport type Message_Min_Order_By = {\n  index?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"message\". */\nexport type Message_Order_By = {\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  transaction?: InputMaybe<Transaction_Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"message\" */\nexport enum Message_Select_Column {\n  /** column name */\n  Index = 'index',\n  /** column name */\n  InvolvedAccountsAddresses = 'involved_accounts_addresses',\n  /** column name */\n  TransactionHash = 'transaction_hash',\n  /** column name */\n  Type = 'type',\n  /** column name */\n  Value = 'value'\n}\n\n/** aggregate stddev on columns */\nexport type Message_Stddev_Fields = {\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"message\" */\nexport type Message_Stddev_Order_By = {\n  index?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Message_Stddev_Pop_Fields = {\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"message\" */\nexport type Message_Stddev_Pop_Order_By = {\n  index?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Message_Stddev_Samp_Fields = {\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"message\" */\nexport type Message_Stddev_Samp_Order_By = {\n  index?: InputMaybe<Order_By>;\n};\n\n/** aggregate sum on columns */\nexport type Message_Sum_Fields = {\n  index?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"message\" */\nexport type Message_Sum_Order_By = {\n  index?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Message_Var_Pop_Fields = {\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"message\" */\nexport type Message_Var_Pop_Order_By = {\n  index?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Message_Var_Samp_Fields = {\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"message\" */\nexport type Message_Var_Samp_Order_By = {\n  index?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Message_Variance_Fields = {\n  index?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"message\" */\nexport type Message_Variance_Order_By = {\n  index?: InputMaybe<Order_By>;\n};\n\nexport type Messages_By_Address_Args = {\n  addresses?: InputMaybe<Scalars['_text']['input']>;\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n};\n\n/** columns and relationships of \"modules\" */\nexport type Modules = {\n  module_name: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"modules\" */\nexport type Modules_Aggregate = {\n  aggregate?: Maybe<Modules_Aggregate_Fields>;\n  nodes: Array<Modules>;\n};\n\n/** aggregate fields of \"modules\" */\nexport type Modules_Aggregate_Fields = {\n  count: Scalars['Int']['output'];\n  max?: Maybe<Modules_Max_Fields>;\n  min?: Maybe<Modules_Min_Fields>;\n};\n\n\n/** aggregate fields of \"modules\" */\nexport type Modules_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Modules_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** Boolean expression to filter rows from the table \"modules\". All fields are combined with a logical 'AND'. */\nexport type Modules_Bool_Exp = {\n  _and?: InputMaybe<Array<Modules_Bool_Exp>>;\n  _not?: InputMaybe<Modules_Bool_Exp>;\n  _or?: InputMaybe<Array<Modules_Bool_Exp>>;\n  module_name?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Modules_Max_Fields = {\n  module_name?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Modules_Min_Fields = {\n  module_name?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"modules\". */\nexport type Modules_Order_By = {\n  module_name?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"modules\" */\nexport enum Modules_Select_Column {\n  /** column name */\n  ModuleName = 'module_name'\n}\n\n/** columns and relationships of \"neuron_activation_source\" */\nexport type Neuron_Activation_Source = {\n  genesis_percent?: Maybe<Scalars['float8']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activated?: Maybe<Scalars['bigint']['output']>;\n  recieve_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"neuron_activation_source\" */\nexport type Neuron_Activation_Source_Aggregate = {\n  aggregate?: Maybe<Neuron_Activation_Source_Aggregate_Fields>;\n  nodes: Array<Neuron_Activation_Source>;\n};\n\n/** aggregate fields of \"neuron_activation_source\" */\nexport type Neuron_Activation_Source_Aggregate_Fields = {\n  avg?: Maybe<Neuron_Activation_Source_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Neuron_Activation_Source_Max_Fields>;\n  min?: Maybe<Neuron_Activation_Source_Min_Fields>;\n  stddev?: Maybe<Neuron_Activation_Source_Stddev_Fields>;\n  stddev_pop?: Maybe<Neuron_Activation_Source_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Neuron_Activation_Source_Stddev_Samp_Fields>;\n  sum?: Maybe<Neuron_Activation_Source_Sum_Fields>;\n  var_pop?: Maybe<Neuron_Activation_Source_Var_Pop_Fields>;\n  var_samp?: Maybe<Neuron_Activation_Source_Var_Samp_Fields>;\n  variance?: Maybe<Neuron_Activation_Source_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"neuron_activation_source\" */\nexport type Neuron_Activation_Source_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Neuron_Activation_Source_Avg_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"neuron_activation_source\". All fields are combined with a logical 'AND'. */\nexport type Neuron_Activation_Source_Bool_Exp = {\n  _and?: InputMaybe<Array<Neuron_Activation_Source_Bool_Exp>>;\n  _not?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n  _or?: InputMaybe<Array<Neuron_Activation_Source_Bool_Exp>>;\n  genesis_percent?: InputMaybe<Float8_Comparison_Exp>;\n  ibc_receive_percent?: InputMaybe<Float8_Comparison_Exp>;\n  neuron_activated?: InputMaybe<Bigint_Comparison_Exp>;\n  recieve_percent?: InputMaybe<Float8_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Neuron_Activation_Source_Max_Fields = {\n  genesis_percent?: Maybe<Scalars['float8']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activated?: Maybe<Scalars['bigint']['output']>;\n  recieve_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Neuron_Activation_Source_Min_Fields = {\n  genesis_percent?: Maybe<Scalars['float8']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activated?: Maybe<Scalars['bigint']['output']>;\n  recieve_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"neuron_activation_source\". */\nexport type Neuron_Activation_Source_Order_By = {\n  genesis_percent?: InputMaybe<Order_By>;\n  ibc_receive_percent?: InputMaybe<Order_By>;\n  neuron_activated?: InputMaybe<Order_By>;\n  recieve_percent?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"neuron_activation_source\" */\nexport enum Neuron_Activation_Source_Select_Column {\n  /** column name */\n  GenesisPercent = 'genesis_percent',\n  /** column name */\n  IbcReceivePercent = 'ibc_receive_percent',\n  /** column name */\n  NeuronActivated = 'neuron_activated',\n  /** column name */\n  RecievePercent = 'recieve_percent',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Neuron_Activation_Source_Stddev_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Neuron_Activation_Source_Stddev_Pop_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Neuron_Activation_Source_Stddev_Samp_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Neuron_Activation_Source_Sum_Fields = {\n  genesis_percent?: Maybe<Scalars['float8']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activated?: Maybe<Scalars['bigint']['output']>;\n  recieve_percent?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Neuron_Activation_Source_Var_Pop_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Neuron_Activation_Source_Var_Samp_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Neuron_Activation_Source_Variance_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons = {\n  date?: Maybe<Scalars['date']['output']>;\n  new_neurons_daily?: Maybe<Scalars['bigint']['output']>;\n  new_neurons_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons_Aggregate = {\n  aggregate?: Maybe<Number_Of_New_Neurons_Aggregate_Fields>;\n  nodes: Array<Number_Of_New_Neurons>;\n};\n\n/** aggregate fields of \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons_Aggregate_Fields = {\n  avg?: Maybe<Number_Of_New_Neurons_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Number_Of_New_Neurons_Max_Fields>;\n  min?: Maybe<Number_Of_New_Neurons_Min_Fields>;\n  stddev?: Maybe<Number_Of_New_Neurons_Stddev_Fields>;\n  stddev_pop?: Maybe<Number_Of_New_Neurons_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Number_Of_New_Neurons_Stddev_Samp_Fields>;\n  sum?: Maybe<Number_Of_New_Neurons_Sum_Fields>;\n  var_pop?: Maybe<Number_Of_New_Neurons_Var_Pop_Fields>;\n  var_samp?: Maybe<Number_Of_New_Neurons_Var_Samp_Fields>;\n  variance?: Maybe<Number_Of_New_Neurons_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Number_Of_New_Neurons_Avg_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"number_of_new_neurons\". All fields are combined with a logical 'AND'. */\nexport type Number_Of_New_Neurons_Bool_Exp = {\n  _and?: InputMaybe<Array<Number_Of_New_Neurons_Bool_Exp>>;\n  _not?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n  _or?: InputMaybe<Array<Number_Of_New_Neurons_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  new_neurons_daily?: InputMaybe<Bigint_Comparison_Exp>;\n  new_neurons_total?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Number_Of_New_Neurons_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  new_neurons_daily?: Maybe<Scalars['bigint']['output']>;\n  new_neurons_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Number_Of_New_Neurons_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  new_neurons_daily?: Maybe<Scalars['bigint']['output']>;\n  new_neurons_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"number_of_new_neurons\". */\nexport type Number_Of_New_Neurons_Order_By = {\n  date?: InputMaybe<Order_By>;\n  new_neurons_daily?: InputMaybe<Order_By>;\n  new_neurons_total?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"number_of_new_neurons\" */\nexport enum Number_Of_New_Neurons_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  NewNeuronsDaily = 'new_neurons_daily',\n  /** column name */\n  NewNeuronsTotal = 'new_neurons_total'\n}\n\n/** aggregate stddev on columns */\nexport type Number_Of_New_Neurons_Stddev_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Number_Of_New_Neurons_Stddev_Pop_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Number_Of_New_Neurons_Stddev_Samp_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Number_Of_New_Neurons_Sum_Fields = {\n  new_neurons_daily?: Maybe<Scalars['bigint']['output']>;\n  new_neurons_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Number_Of_New_Neurons_Var_Pop_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Number_Of_New_Neurons_Var_Samp_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Number_Of_New_Neurons_Variance_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to compare columns of type \"numeric\". All fields are combined with logical 'AND'. */\nexport type Numeric_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['numeric']['input']>;\n  _gt?: InputMaybe<Scalars['numeric']['input']>;\n  _gte?: InputMaybe<Scalars['numeric']['input']>;\n  _in?: InputMaybe<Array<Scalars['numeric']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['numeric']['input']>;\n  _lte?: InputMaybe<Scalars['numeric']['input']>;\n  _neq?: InputMaybe<Scalars['numeric']['input']>;\n  _nin?: InputMaybe<Array<Scalars['numeric']['input']>>;\n};\n\n/** columns and relationships of \"old_precommits\" */\nexport type Old_Precommits = {\n  consensus_address: Scalars['String']['output'];\n  consensus_pubkey: Scalars['String']['output'];\n  precommits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"old_precommits\" */\nexport type Old_Precommits_Aggregate = {\n  aggregate?: Maybe<Old_Precommits_Aggregate_Fields>;\n  nodes: Array<Old_Precommits>;\n};\n\n/** aggregate fields of \"old_precommits\" */\nexport type Old_Precommits_Aggregate_Fields = {\n  avg?: Maybe<Old_Precommits_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Old_Precommits_Max_Fields>;\n  min?: Maybe<Old_Precommits_Min_Fields>;\n  stddev?: Maybe<Old_Precommits_Stddev_Fields>;\n  stddev_pop?: Maybe<Old_Precommits_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Old_Precommits_Stddev_Samp_Fields>;\n  sum?: Maybe<Old_Precommits_Sum_Fields>;\n  var_pop?: Maybe<Old_Precommits_Var_Pop_Fields>;\n  var_samp?: Maybe<Old_Precommits_Var_Samp_Fields>;\n  variance?: Maybe<Old_Precommits_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"old_precommits\" */\nexport type Old_Precommits_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Old_Precommits_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Old_Precommits_Avg_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"old_precommits\". All fields are combined with a logical 'AND'. */\nexport type Old_Precommits_Bool_Exp = {\n  _and?: InputMaybe<Array<Old_Precommits_Bool_Exp>>;\n  _not?: InputMaybe<Old_Precommits_Bool_Exp>;\n  _or?: InputMaybe<Array<Old_Precommits_Bool_Exp>>;\n  consensus_address?: InputMaybe<String_Comparison_Exp>;\n  consensus_pubkey?: InputMaybe<String_Comparison_Exp>;\n  precommits?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Old_Precommits_Max_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  precommits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Old_Precommits_Min_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  precommits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"old_precommits\". */\nexport type Old_Precommits_Order_By = {\n  consensus_address?: InputMaybe<Order_By>;\n  consensus_pubkey?: InputMaybe<Order_By>;\n  precommits?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"old_precommits\" */\nexport enum Old_Precommits_Select_Column {\n  /** column name */\n  ConsensusAddress = 'consensus_address',\n  /** column name */\n  ConsensusPubkey = 'consensus_pubkey',\n  /** column name */\n  Precommits = 'precommits'\n}\n\n/** aggregate stddev on columns */\nexport type Old_Precommits_Stddev_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Old_Precommits_Stddev_Pop_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Old_Precommits_Stddev_Samp_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Old_Precommits_Sum_Fields = {\n  precommits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Old_Precommits_Var_Pop_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Old_Precommits_Var_Samp_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Old_Precommits_Variance_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** column ordering options */\nexport enum Order_By {\n  /** in ascending order, nulls last */\n  Asc = 'asc',\n  /** in ascending order, nulls first */\n  AscNullsFirst = 'asc_nulls_first',\n  /** in ascending order, nulls last */\n  AscNullsLast = 'asc_nulls_last',\n  /** in descending order, nulls first */\n  Desc = 'desc',\n  /** in descending order, nulls first */\n  DescNullsFirst = 'desc_nulls_first',\n  /** in descending order, nulls last */\n  DescNullsLast = 'desc_nulls_last'\n}\n\n/** columns and relationships of \"particles\" */\nexport type Particles = {\n  /** An object relationship */\n  account: Account;\n  /** An object relationship */\n  block: Block;\n  height: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n  /** An array relationship */\n  in: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  in_aggregate: Cyberlinks_Aggregate;\n  neuron: Scalars['String']['output'];\n  /** An array relationship */\n  out: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  out_aggregate: Cyberlinks_Aggregate;\n  particle: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  /** An object relationship */\n  transaction: Transaction;\n  transaction_hash: Scalars['String']['output'];\n};\n\n\n/** columns and relationships of \"particles\" */\nexport type ParticlesInArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"particles\" */\nexport type ParticlesIn_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"particles\" */\nexport type ParticlesOutArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"particles\" */\nexport type ParticlesOut_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n/** aggregated selection of \"particles\" */\nexport type Particles_Aggregate = {\n  aggregate?: Maybe<Particles_Aggregate_Fields>;\n  nodes: Array<Particles>;\n};\n\n/** aggregate fields of \"particles\" */\nexport type Particles_Aggregate_Fields = {\n  avg?: Maybe<Particles_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Particles_Max_Fields>;\n  min?: Maybe<Particles_Min_Fields>;\n  stddev?: Maybe<Particles_Stddev_Fields>;\n  stddev_pop?: Maybe<Particles_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Particles_Stddev_Samp_Fields>;\n  sum?: Maybe<Particles_Sum_Fields>;\n  var_pop?: Maybe<Particles_Var_Pop_Fields>;\n  var_samp?: Maybe<Particles_Var_Samp_Fields>;\n  variance?: Maybe<Particles_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"particles\" */\nexport type Particles_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Particles_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"particles\" */\nexport type Particles_Aggregate_Order_By = {\n  avg?: InputMaybe<Particles_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Particles_Max_Order_By>;\n  min?: InputMaybe<Particles_Min_Order_By>;\n  stddev?: InputMaybe<Particles_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Particles_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Particles_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Particles_Sum_Order_By>;\n  var_pop?: InputMaybe<Particles_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Particles_Var_Samp_Order_By>;\n  variance?: InputMaybe<Particles_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Particles_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"particles\" */\nexport type Particles_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"particles\". All fields are combined with a logical 'AND'. */\nexport type Particles_Bool_Exp = {\n  _and?: InputMaybe<Array<Particles_Bool_Exp>>;\n  _not?: InputMaybe<Particles_Bool_Exp>;\n  _or?: InputMaybe<Array<Particles_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  in?: InputMaybe<Cyberlinks_Bool_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n  out?: InputMaybe<Cyberlinks_Bool_Exp>;\n  particle?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  transaction?: InputMaybe<Transaction_Bool_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Particles_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  particle?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"particles\" */\nexport type Particles_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Particles_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  particle?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"particles\" */\nexport type Particles_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"particles\". */\nexport type Particles_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  in_aggregate?: InputMaybe<Cyberlinks_Aggregate_Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  out_aggregate?: InputMaybe<Cyberlinks_Aggregate_Order_By>;\n  particle?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction?: InputMaybe<Transaction_Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"particles\" */\nexport enum Particles_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  Neuron = 'neuron',\n  /** column name */\n  Particle = 'particle',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TransactionHash = 'transaction_hash'\n}\n\n/** aggregate stddev on columns */\nexport type Particles_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"particles\" */\nexport type Particles_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Particles_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"particles\" */\nexport type Particles_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Particles_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"particles\" */\nexport type Particles_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate sum on columns */\nexport type Particles_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** order by sum() on columns of table \"particles\" */\nexport type Particles_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Particles_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"particles\" */\nexport type Particles_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Particles_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"particles\" */\nexport type Particles_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Particles_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"particles\" */\nexport type Particles_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"pre_commit\" */\nexport type Pre_Commit = {\n  height: Scalars['bigint']['output'];\n  proposer_priority: Scalars['bigint']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  /** An object relationship */\n  validator: Validator;\n  validator_address: Scalars['String']['output'];\n  voting_power: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"pre_commit\" */\nexport type Pre_Commit_Aggregate = {\n  aggregate?: Maybe<Pre_Commit_Aggregate_Fields>;\n  nodes: Array<Pre_Commit>;\n};\n\n/** aggregate fields of \"pre_commit\" */\nexport type Pre_Commit_Aggregate_Fields = {\n  avg?: Maybe<Pre_Commit_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pre_Commit_Max_Fields>;\n  min?: Maybe<Pre_Commit_Min_Fields>;\n  stddev?: Maybe<Pre_Commit_Stddev_Fields>;\n  stddev_pop?: Maybe<Pre_Commit_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pre_Commit_Stddev_Samp_Fields>;\n  sum?: Maybe<Pre_Commit_Sum_Fields>;\n  var_pop?: Maybe<Pre_Commit_Var_Pop_Fields>;\n  var_samp?: Maybe<Pre_Commit_Var_Samp_Fields>;\n  variance?: Maybe<Pre_Commit_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pre_commit\" */\nexport type Pre_Commit_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"pre_commit\" */\nexport type Pre_Commit_Aggregate_Order_By = {\n  avg?: InputMaybe<Pre_Commit_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Pre_Commit_Max_Order_By>;\n  min?: InputMaybe<Pre_Commit_Min_Order_By>;\n  stddev?: InputMaybe<Pre_Commit_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Pre_Commit_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Pre_Commit_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Pre_Commit_Sum_Order_By>;\n  var_pop?: InputMaybe<Pre_Commit_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Pre_Commit_Var_Samp_Order_By>;\n  variance?: InputMaybe<Pre_Commit_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Pre_Commit_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"pre_commit\". All fields are combined with a logical 'AND'. */\nexport type Pre_Commit_Bool_Exp = {\n  _and?: InputMaybe<Array<Pre_Commit_Bool_Exp>>;\n  _not?: InputMaybe<Pre_Commit_Bool_Exp>;\n  _or?: InputMaybe<Array<Pre_Commit_Bool_Exp>>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  proposer_priority?: InputMaybe<Bigint_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  validator?: InputMaybe<Validator_Bool_Exp>;\n  validator_address?: InputMaybe<String_Comparison_Exp>;\n  voting_power?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pre_Commit_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  proposer_priority?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n  voting_power?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by max() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  validator_address?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Pre_Commit_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  proposer_priority?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n  voting_power?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by min() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  validator_address?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"pre_commit\". */\nexport type Pre_Commit_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  validator?: InputMaybe<Validator_Order_By>;\n  validator_address?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pre_commit\" */\nexport enum Pre_Commit_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  ProposerPriority = 'proposer_priority',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  ValidatorAddress = 'validator_address',\n  /** column name */\n  VotingPower = 'voting_power'\n}\n\n/** aggregate stddev on columns */\nexport type Pre_Commit_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pre_Commit_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pre_Commit_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate sum on columns */\nexport type Pre_Commit_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  proposer_priority?: Maybe<Scalars['bigint']['output']>;\n  voting_power?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pre_Commit_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pre_Commit_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Pre_Commit_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"pre_commits_rewards_view\" */\nexport type Pre_Commits_Rewards_View = {\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  max_block?: Maybe<Scalars['bigint']['output']>;\n  pre_commit_rewards?: Maybe<Scalars['numeric']['output']>;\n  precommits?: Maybe<Scalars['numeric']['output']>;\n  sum_precommits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"pre_commits_rewards_view\" */\nexport type Pre_Commits_Rewards_View_Aggregate = {\n  aggregate?: Maybe<Pre_Commits_Rewards_View_Aggregate_Fields>;\n  nodes: Array<Pre_Commits_Rewards_View>;\n};\n\n/** aggregate fields of \"pre_commits_rewards_view\" */\nexport type Pre_Commits_Rewards_View_Aggregate_Fields = {\n  avg?: Maybe<Pre_Commits_Rewards_View_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pre_Commits_Rewards_View_Max_Fields>;\n  min?: Maybe<Pre_Commits_Rewards_View_Min_Fields>;\n  stddev?: Maybe<Pre_Commits_Rewards_View_Stddev_Fields>;\n  stddev_pop?: Maybe<Pre_Commits_Rewards_View_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pre_Commits_Rewards_View_Stddev_Samp_Fields>;\n  sum?: Maybe<Pre_Commits_Rewards_View_Sum_Fields>;\n  var_pop?: Maybe<Pre_Commits_Rewards_View_Var_Pop_Fields>;\n  var_samp?: Maybe<Pre_Commits_Rewards_View_Var_Samp_Fields>;\n  variance?: Maybe<Pre_Commits_Rewards_View_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pre_commits_rewards_view\" */\nexport type Pre_Commits_Rewards_View_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pre_Commits_Rewards_View_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pre_Commits_Rewards_View_Avg_Fields = {\n  max_block?: Maybe<Scalars['Float']['output']>;\n  pre_commit_rewards?: Maybe<Scalars['Float']['output']>;\n  precommits?: Maybe<Scalars['Float']['output']>;\n  sum_precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pre_commits_rewards_view\". All fields are combined with a logical 'AND'. */\nexport type Pre_Commits_Rewards_View_Bool_Exp = {\n  _and?: InputMaybe<Array<Pre_Commits_Rewards_View_Bool_Exp>>;\n  _not?: InputMaybe<Pre_Commits_Rewards_View_Bool_Exp>;\n  _or?: InputMaybe<Array<Pre_Commits_Rewards_View_Bool_Exp>>;\n  consensus_pubkey?: InputMaybe<String_Comparison_Exp>;\n  max_block?: InputMaybe<Bigint_Comparison_Exp>;\n  pre_commit_rewards?: InputMaybe<Numeric_Comparison_Exp>;\n  precommits?: InputMaybe<Numeric_Comparison_Exp>;\n  sum_precommits?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pre_Commits_Rewards_View_Max_Fields = {\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  max_block?: Maybe<Scalars['bigint']['output']>;\n  pre_commit_rewards?: Maybe<Scalars['numeric']['output']>;\n  precommits?: Maybe<Scalars['numeric']['output']>;\n  sum_precommits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pre_Commits_Rewards_View_Min_Fields = {\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  max_block?: Maybe<Scalars['bigint']['output']>;\n  pre_commit_rewards?: Maybe<Scalars['numeric']['output']>;\n  precommits?: Maybe<Scalars['numeric']['output']>;\n  sum_precommits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"pre_commits_rewards_view\". */\nexport type Pre_Commits_Rewards_View_Order_By = {\n  consensus_pubkey?: InputMaybe<Order_By>;\n  max_block?: InputMaybe<Order_By>;\n  pre_commit_rewards?: InputMaybe<Order_By>;\n  precommits?: InputMaybe<Order_By>;\n  sum_precommits?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pre_commits_rewards_view\" */\nexport enum Pre_Commits_Rewards_View_Select_Column {\n  /** column name */\n  ConsensusPubkey = 'consensus_pubkey',\n  /** column name */\n  MaxBlock = 'max_block',\n  /** column name */\n  PreCommitRewards = 'pre_commit_rewards',\n  /** column name */\n  Precommits = 'precommits',\n  /** column name */\n  SumPrecommits = 'sum_precommits'\n}\n\n/** aggregate stddev on columns */\nexport type Pre_Commits_Rewards_View_Stddev_Fields = {\n  max_block?: Maybe<Scalars['Float']['output']>;\n  pre_commit_rewards?: Maybe<Scalars['Float']['output']>;\n  precommits?: Maybe<Scalars['Float']['output']>;\n  sum_precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pre_Commits_Rewards_View_Stddev_Pop_Fields = {\n  max_block?: Maybe<Scalars['Float']['output']>;\n  pre_commit_rewards?: Maybe<Scalars['Float']['output']>;\n  precommits?: Maybe<Scalars['Float']['output']>;\n  sum_precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pre_Commits_Rewards_View_Stddev_Samp_Fields = {\n  max_block?: Maybe<Scalars['Float']['output']>;\n  pre_commit_rewards?: Maybe<Scalars['Float']['output']>;\n  precommits?: Maybe<Scalars['Float']['output']>;\n  sum_precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Pre_Commits_Rewards_View_Sum_Fields = {\n  max_block?: Maybe<Scalars['bigint']['output']>;\n  pre_commit_rewards?: Maybe<Scalars['numeric']['output']>;\n  precommits?: Maybe<Scalars['numeric']['output']>;\n  sum_precommits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pre_Commits_Rewards_View_Var_Pop_Fields = {\n  max_block?: Maybe<Scalars['Float']['output']>;\n  pre_commit_rewards?: Maybe<Scalars['Float']['output']>;\n  precommits?: Maybe<Scalars['Float']['output']>;\n  sum_precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pre_Commits_Rewards_View_Var_Samp_Fields = {\n  max_block?: Maybe<Scalars['Float']['output']>;\n  pre_commit_rewards?: Maybe<Scalars['Float']['output']>;\n  precommits?: Maybe<Scalars['Float']['output']>;\n  sum_precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pre_Commits_Rewards_View_Variance_Fields = {\n  max_block?: Maybe<Scalars['Float']['output']>;\n  pre_commit_rewards?: Maybe<Scalars['Float']['output']>;\n  precommits?: Maybe<Scalars['Float']['output']>;\n  sum_precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"pre_commits_total\" */\nexport type Pre_Commits_Total = {\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  pre_commits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"pre_commits_total\" */\nexport type Pre_Commits_Total_Aggregate = {\n  aggregate?: Maybe<Pre_Commits_Total_Aggregate_Fields>;\n  nodes: Array<Pre_Commits_Total>;\n};\n\n/** aggregate fields of \"pre_commits_total\" */\nexport type Pre_Commits_Total_Aggregate_Fields = {\n  avg?: Maybe<Pre_Commits_Total_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pre_Commits_Total_Max_Fields>;\n  min?: Maybe<Pre_Commits_Total_Min_Fields>;\n  stddev?: Maybe<Pre_Commits_Total_Stddev_Fields>;\n  stddev_pop?: Maybe<Pre_Commits_Total_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pre_Commits_Total_Stddev_Samp_Fields>;\n  sum?: Maybe<Pre_Commits_Total_Sum_Fields>;\n  var_pop?: Maybe<Pre_Commits_Total_Var_Pop_Fields>;\n  var_samp?: Maybe<Pre_Commits_Total_Var_Samp_Fields>;\n  variance?: Maybe<Pre_Commits_Total_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pre_commits_total\" */\nexport type Pre_Commits_Total_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pre_Commits_Total_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pre_Commits_Total_Avg_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pre_commits_total\". All fields are combined with a logical 'AND'. */\nexport type Pre_Commits_Total_Bool_Exp = {\n  _and?: InputMaybe<Array<Pre_Commits_Total_Bool_Exp>>;\n  _not?: InputMaybe<Pre_Commits_Total_Bool_Exp>;\n  _or?: InputMaybe<Array<Pre_Commits_Total_Bool_Exp>>;\n  consensus_pubkey?: InputMaybe<String_Comparison_Exp>;\n  pre_commits?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pre_Commits_Total_Max_Fields = {\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  pre_commits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pre_Commits_Total_Min_Fields = {\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  pre_commits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"pre_commits_total\". */\nexport type Pre_Commits_Total_Order_By = {\n  consensus_pubkey?: InputMaybe<Order_By>;\n  pre_commits?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pre_commits_total\" */\nexport enum Pre_Commits_Total_Select_Column {\n  /** column name */\n  ConsensusPubkey = 'consensus_pubkey',\n  /** column name */\n  PreCommits = 'pre_commits'\n}\n\n/** aggregate stddev on columns */\nexport type Pre_Commits_Total_Stddev_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pre_Commits_Total_Stddev_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pre_Commits_Total_Stddev_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Pre_Commits_Total_Sum_Fields = {\n  pre_commits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pre_Commits_Total_Var_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pre_Commits_Total_Var_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pre_Commits_Total_Variance_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"pre_commits_view\" */\nexport type Pre_Commits_View = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  precommits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"pre_commits_view\" */\nexport type Pre_Commits_View_Aggregate = {\n  aggregate?: Maybe<Pre_Commits_View_Aggregate_Fields>;\n  nodes: Array<Pre_Commits_View>;\n};\n\n/** aggregate fields of \"pre_commits_view\" */\nexport type Pre_Commits_View_Aggregate_Fields = {\n  avg?: Maybe<Pre_Commits_View_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pre_Commits_View_Max_Fields>;\n  min?: Maybe<Pre_Commits_View_Min_Fields>;\n  stddev?: Maybe<Pre_Commits_View_Stddev_Fields>;\n  stddev_pop?: Maybe<Pre_Commits_View_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pre_Commits_View_Stddev_Samp_Fields>;\n  sum?: Maybe<Pre_Commits_View_Sum_Fields>;\n  var_pop?: Maybe<Pre_Commits_View_Var_Pop_Fields>;\n  var_samp?: Maybe<Pre_Commits_View_Var_Samp_Fields>;\n  variance?: Maybe<Pre_Commits_View_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pre_commits_view\" */\nexport type Pre_Commits_View_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pre_Commits_View_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pre_Commits_View_Avg_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pre_commits_view\". All fields are combined with a logical 'AND'. */\nexport type Pre_Commits_View_Bool_Exp = {\n  _and?: InputMaybe<Array<Pre_Commits_View_Bool_Exp>>;\n  _not?: InputMaybe<Pre_Commits_View_Bool_Exp>;\n  _or?: InputMaybe<Array<Pre_Commits_View_Bool_Exp>>;\n  consensus_address?: InputMaybe<String_Comparison_Exp>;\n  consensus_pubkey?: InputMaybe<String_Comparison_Exp>;\n  precommits?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pre_Commits_View_Max_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  precommits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pre_Commits_View_Min_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  precommits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"pre_commits_view\". */\nexport type Pre_Commits_View_Order_By = {\n  consensus_address?: InputMaybe<Order_By>;\n  consensus_pubkey?: InputMaybe<Order_By>;\n  precommits?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pre_commits_view\" */\nexport enum Pre_Commits_View_Select_Column {\n  /** column name */\n  ConsensusAddress = 'consensus_address',\n  /** column name */\n  ConsensusPubkey = 'consensus_pubkey',\n  /** column name */\n  Precommits = 'precommits'\n}\n\n/** aggregate stddev on columns */\nexport type Pre_Commits_View_Stddev_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pre_Commits_View_Stddev_Pop_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pre_Commits_View_Stddev_Samp_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Pre_Commits_View_Sum_Fields = {\n  precommits?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pre_Commits_View_Var_Pop_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pre_Commits_View_Var_Samp_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pre_Commits_View_Variance_Fields = {\n  precommits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"pruning\" */\nexport type Pruning = {\n  last_pruned_height: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"pruning\" */\nexport type Pruning_Aggregate = {\n  aggregate?: Maybe<Pruning_Aggregate_Fields>;\n  nodes: Array<Pruning>;\n};\n\n/** aggregate fields of \"pruning\" */\nexport type Pruning_Aggregate_Fields = {\n  avg?: Maybe<Pruning_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pruning_Max_Fields>;\n  min?: Maybe<Pruning_Min_Fields>;\n  stddev?: Maybe<Pruning_Stddev_Fields>;\n  stddev_pop?: Maybe<Pruning_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pruning_Stddev_Samp_Fields>;\n  sum?: Maybe<Pruning_Sum_Fields>;\n  var_pop?: Maybe<Pruning_Var_Pop_Fields>;\n  var_samp?: Maybe<Pruning_Var_Samp_Fields>;\n  variance?: Maybe<Pruning_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pruning\" */\nexport type Pruning_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pruning_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pruning_Avg_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pruning\". All fields are combined with a logical 'AND'. */\nexport type Pruning_Bool_Exp = {\n  _and?: InputMaybe<Array<Pruning_Bool_Exp>>;\n  _not?: InputMaybe<Pruning_Bool_Exp>;\n  _or?: InputMaybe<Array<Pruning_Bool_Exp>>;\n  last_pruned_height?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pruning_Max_Fields = {\n  last_pruned_height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pruning_Min_Fields = {\n  last_pruned_height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"pruning\". */\nexport type Pruning_Order_By = {\n  last_pruned_height?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pruning\" */\nexport enum Pruning_Select_Column {\n  /** column name */\n  LastPrunedHeight = 'last_pruned_height'\n}\n\n/** aggregate stddev on columns */\nexport type Pruning_Stddev_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pruning_Stddev_Pop_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pruning_Stddev_Samp_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Pruning_Sum_Fields = {\n  last_pruned_height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pruning_Var_Pop_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pruning_Var_Samp_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pruning_Variance_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"pussy_gift_proofs\" */\nexport type Pussy_Gift_Proofs = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  details?: Maybe<Scalars['String']['output']>;\n  proof?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"pussy_gift_proofs\" */\nexport type Pussy_Gift_Proofs_Aggregate = {\n  aggregate?: Maybe<Pussy_Gift_Proofs_Aggregate_Fields>;\n  nodes: Array<Pussy_Gift_Proofs>;\n};\n\n/** aggregate fields of \"pussy_gift_proofs\" */\nexport type Pussy_Gift_Proofs_Aggregate_Fields = {\n  avg?: Maybe<Pussy_Gift_Proofs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pussy_Gift_Proofs_Max_Fields>;\n  min?: Maybe<Pussy_Gift_Proofs_Min_Fields>;\n  stddev?: Maybe<Pussy_Gift_Proofs_Stddev_Fields>;\n  stddev_pop?: Maybe<Pussy_Gift_Proofs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pussy_Gift_Proofs_Stddev_Samp_Fields>;\n  sum?: Maybe<Pussy_Gift_Proofs_Sum_Fields>;\n  var_pop?: Maybe<Pussy_Gift_Proofs_Var_Pop_Fields>;\n  var_samp?: Maybe<Pussy_Gift_Proofs_Var_Samp_Fields>;\n  variance?: Maybe<Pussy_Gift_Proofs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pussy_gift_proofs\" */\nexport type Pussy_Gift_Proofs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pussy_Gift_Proofs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pussy_Gift_Proofs_Avg_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pussy_gift_proofs\". All fields are combined with a logical 'AND'. */\nexport type Pussy_Gift_Proofs_Bool_Exp = {\n  _and?: InputMaybe<Array<Pussy_Gift_Proofs_Bool_Exp>>;\n  _not?: InputMaybe<Pussy_Gift_Proofs_Bool_Exp>;\n  _or?: InputMaybe<Array<Pussy_Gift_Proofs_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  amount?: InputMaybe<Bigint_Comparison_Exp>;\n  details?: InputMaybe<String_Comparison_Exp>;\n  proof?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pussy_Gift_Proofs_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  details?: Maybe<Scalars['String']['output']>;\n  proof?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pussy_Gift_Proofs_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  details?: Maybe<Scalars['String']['output']>;\n  proof?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"pussy_gift_proofs\". */\nexport type Pussy_Gift_Proofs_Order_By = {\n  address?: InputMaybe<Order_By>;\n  amount?: InputMaybe<Order_By>;\n  details?: InputMaybe<Order_By>;\n  proof?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pussy_gift_proofs\" */\nexport enum Pussy_Gift_Proofs_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Amount = 'amount',\n  /** column name */\n  Details = 'details',\n  /** column name */\n  Proof = 'proof'\n}\n\n/** aggregate stddev on columns */\nexport type Pussy_Gift_Proofs_Stddev_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pussy_Gift_Proofs_Stddev_Pop_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pussy_Gift_Proofs_Stddev_Samp_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Pussy_Gift_Proofs_Sum_Fields = {\n  amount?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pussy_Gift_Proofs_Var_Pop_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pussy_Gift_Proofs_Var_Samp_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pussy_Gift_Proofs_Variance_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\nexport type Query_Root = {\n  /** fetch data from the table: \"_transaction\" */\n  _transaction: Array<_Transaction>;\n  /** fetch aggregated fields from the table: \"_transaction\" */\n  _transaction_aggregate: _Transaction_Aggregate;\n  /** fetch data from the table: \"_uptime_temp\" */\n  _uptime_temp: Array<_Uptime_Temp>;\n  /** fetch aggregated fields from the table: \"_uptime_temp\" */\n  _uptime_temp_aggregate: _Uptime_Temp_Aggregate;\n  /** fetch data from the table: \"account\" */\n  account: Array<Account>;\n  /** fetch aggregated fields from the table: \"account\" */\n  account_aggregate: Account_Aggregate;\n  /** fetch data from the table: \"account_balance\" */\n  account_balance: Array<Account_Balance>;\n  /** fetch aggregated fields from the table: \"account_balance\" */\n  account_balance_aggregate: Account_Balance_Aggregate;\n  /** fetch data from the table: \"account_balance\" using primary key columns */\n  account_balance_by_pk?: Maybe<Account_Balance>;\n  /** fetch data from the table: \"account\" using primary key columns */\n  account_by_pk?: Maybe<Account>;\n  /** fetch data from the table: \"block\" */\n  block: Array<Block>;\n  /** fetch aggregated fields from the table: \"block\" */\n  block_aggregate: Block_Aggregate;\n  /** fetch data from the table: \"block\" using primary key columns */\n  block_by_pk?: Maybe<Block>;\n  /** fetch data from the table: \"contracts\" */\n  contracts: Array<Contracts>;\n  /** fetch aggregated fields from the table: \"contracts\" */\n  contracts_aggregate: Contracts_Aggregate;\n  /** fetch data from the table: \"contracts\" using primary key columns */\n  contracts_by_pk?: Maybe<Contracts>;\n  /** fetch data from the table: \"cyb_cohort\" */\n  cyb_cohort: Array<Cyb_Cohort>;\n  /** fetch aggregated fields from the table: \"cyb_cohort\" */\n  cyb_cohort_aggregate: Cyb_Cohort_Aggregate;\n  /** fetch data from the table: \"cyb_new_cohort\" */\n  cyb_new_cohort: Array<Cyb_New_Cohort>;\n  /** fetch aggregated fields from the table: \"cyb_new_cohort\" */\n  cyb_new_cohort_aggregate: Cyb_New_Cohort_Aggregate;\n  /** fetch data from the table: \"cyber_gift\" */\n  cyber_gift: Array<Cyber_Gift>;\n  /** fetch aggregated fields from the table: \"cyber_gift\" */\n  cyber_gift_aggregate: Cyber_Gift_Aggregate;\n  /** fetch data from the table: \"cyber_gift_proofs\" */\n  cyber_gift_proofs: Array<Cyber_Gift_Proofs>;\n  /** fetch aggregated fields from the table: \"cyber_gift_proofs\" */\n  cyber_gift_proofs_aggregate: Cyber_Gift_Proofs_Aggregate;\n  /** fetch data from the table: \"cyberlinks\" */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  /** fetch data from the table: \"cyberlinks\" using primary key columns */\n  cyberlinks_by_pk?: Maybe<Cyberlinks>;\n  /** fetch data from the table: \"cyberlinks_stats\" */\n  cyberlinks_stats: Array<Cyberlinks_Stats>;\n  /** fetch aggregated fields from the table: \"cyberlinks_stats\" */\n  cyberlinks_stats_aggregate: Cyberlinks_Stats_Aggregate;\n  /** fetch data from the table: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons: Array<Daily_Amount_Of_Active_Neurons>;\n  /** fetch aggregated fields from the table: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons_aggregate: Daily_Amount_Of_Active_Neurons_Aggregate;\n  /** fetch data from the table: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas: Array<Daily_Amount_Of_Used_Gas>;\n  /** fetch aggregated fields from the table: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas_aggregate: Daily_Amount_Of_Used_Gas_Aggregate;\n  /** fetch data from the table: \"daily_number_of_transactions\" */\n  daily_number_of_transactions: Array<Daily_Number_Of_Transactions>;\n  /** fetch aggregated fields from the table: \"daily_number_of_transactions\" */\n  daily_number_of_transactions_aggregate: Daily_Number_Of_Transactions_Aggregate;\n  /** fetch data from the table: \"follow_stats\" */\n  follow_stats: Array<Follow_Stats>;\n  /** fetch aggregated fields from the table: \"follow_stats\" */\n  follow_stats_aggregate: Follow_Stats_Aggregate;\n  /** fetch data from the table: \"genesis_neurons_activation\" */\n  genesis_neurons_activation: Array<Genesis_Neurons_Activation>;\n  /** fetch aggregated fields from the table: \"genesis_neurons_activation\" */\n  genesis_neurons_activation_aggregate: Genesis_Neurons_Activation_Aggregate;\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  /** fetch data from the table: \"investmints\" using primary key columns */\n  investmints_by_pk?: Maybe<Investmints>;\n  /** fetch data from the table: \"message\" */\n  message: Array<Message>;\n  /** fetch aggregated fields from the table: \"message\" */\n  message_aggregate: Message_Aggregate;\n  /** execute function \"messages_by_address\" which returns \"message\" */\n  messages_by_address: Array<Message>;\n  /** execute function \"messages_by_address\" and query aggregates on result of table type \"message\" */\n  messages_by_address_aggregate: Message_Aggregate;\n  /** fetch data from the table: \"modules\" */\n  modules: Array<Modules>;\n  /** fetch aggregated fields from the table: \"modules\" */\n  modules_aggregate: Modules_Aggregate;\n  /** fetch data from the table: \"modules\" using primary key columns */\n  modules_by_pk?: Maybe<Modules>;\n  /** fetch data from the table: \"neuron_activation_source\" */\n  neuron_activation_source: Array<Neuron_Activation_Source>;\n  /** fetch aggregated fields from the table: \"neuron_activation_source\" */\n  neuron_activation_source_aggregate: Neuron_Activation_Source_Aggregate;\n  /** fetch data from the table: \"number_of_new_neurons\" */\n  number_of_new_neurons: Array<Number_Of_New_Neurons>;\n  /** fetch aggregated fields from the table: \"number_of_new_neurons\" */\n  number_of_new_neurons_aggregate: Number_Of_New_Neurons_Aggregate;\n  /** fetch data from the table: \"old_precommits\" */\n  old_precommits: Array<Old_Precommits>;\n  /** fetch aggregated fields from the table: \"old_precommits\" */\n  old_precommits_aggregate: Old_Precommits_Aggregate;\n  /** fetch data from the table: \"old_precommits\" using primary key columns */\n  old_precommits_by_pk?: Maybe<Old_Precommits>;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  /** fetch data from the table: \"particles\" using primary key columns */\n  particles_by_pk?: Maybe<Particles>;\n  /** fetch data from the table: \"pre_commit\" */\n  pre_commit: Array<Pre_Commit>;\n  /** fetch aggregated fields from the table: \"pre_commit\" */\n  pre_commit_aggregate: Pre_Commit_Aggregate;\n  /** fetch data from the table: \"pre_commits_rewards_view\" */\n  pre_commits_rewards_view: Array<Pre_Commits_Rewards_View>;\n  /** fetch aggregated fields from the table: \"pre_commits_rewards_view\" */\n  pre_commits_rewards_view_aggregate: Pre_Commits_Rewards_View_Aggregate;\n  /** fetch data from the table: \"pre_commits_total\" */\n  pre_commits_total: Array<Pre_Commits_Total>;\n  /** fetch aggregated fields from the table: \"pre_commits_total\" */\n  pre_commits_total_aggregate: Pre_Commits_Total_Aggregate;\n  /** fetch data from the table: \"pre_commits_view\" */\n  pre_commits_view: Array<Pre_Commits_View>;\n  /** fetch aggregated fields from the table: \"pre_commits_view\" */\n  pre_commits_view_aggregate: Pre_Commits_View_Aggregate;\n  /** fetch data from the table: \"pruning\" */\n  pruning: Array<Pruning>;\n  /** fetch aggregated fields from the table: \"pruning\" */\n  pruning_aggregate: Pruning_Aggregate;\n  /** fetch data from the table: \"pussy_gift_proofs\" */\n  pussy_gift_proofs: Array<Pussy_Gift_Proofs>;\n  /** fetch aggregated fields from the table: \"pussy_gift_proofs\" */\n  pussy_gift_proofs_aggregate: Pussy_Gift_Proofs_Aggregate;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n  /** fetch data from the table: \"routes\" using primary key columns */\n  routes_by_pk?: Maybe<Routes>;\n  /** fetch data from the table: \"supply\" */\n  supply: Array<Supply>;\n  /** fetch aggregated fields from the table: \"supply\" */\n  supply_aggregate: Supply_Aggregate;\n  /** fetch data from the table: \"supply\" using primary key columns */\n  supply_by_pk?: Maybe<Supply>;\n  /** fetch data from the table: \"test_gift\" */\n  test_gift: Array<Test_Gift>;\n  /** fetch aggregated fields from the table: \"test_gift\" */\n  test_gift_aggregate: Test_Gift_Aggregate;\n  /** fetch data from the table: \"today_top_txs\" */\n  today_top_txs: Array<Today_Top_Txs>;\n  /** fetch aggregated fields from the table: \"today_top_txs\" */\n  today_top_txs_aggregate: Today_Top_Txs_Aggregate;\n  /** fetch data from the table: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week: Array<Top_10_Of_Active_Neurons_Week>;\n  /** fetch aggregated fields from the table: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week_aggregate: Top_10_Of_Active_Neurons_Week_Aggregate;\n  /** fetch data from the table: \"top_first_txs\" */\n  top_first_txs: Array<Top_First_Txs>;\n  /** fetch aggregated fields from the table: \"top_first_txs\" */\n  top_first_txs_aggregate: Top_First_Txs_Aggregate;\n  /** fetch data from the table: \"top_leaders\" */\n  top_leaders: Array<Top_Leaders>;\n  /** fetch data from the table: \"top_txs\" */\n  top_txs: Array<Top_Txs>;\n  /** fetch aggregated fields from the table: \"top_txs\" */\n  top_txs_aggregate: Top_Txs_Aggregate;\n  /** fetch data from the table: \"transaction\" */\n  transaction: Array<Transaction>;\n  /** fetch aggregated fields from the table: \"transaction\" */\n  transaction_aggregate: Transaction_Aggregate;\n  /** fetch data from the table: \"transaction\" using primary key columns */\n  transaction_by_pk?: Maybe<Transaction>;\n  /** fetch data from the table: \"tweets_stats\" */\n  tweets_stats: Array<Tweets_Stats>;\n  /** fetch aggregated fields from the table: \"tweets_stats\" */\n  tweets_stats_aggregate: Tweets_Stats_Aggregate;\n  /** fetch data from the table: \"txs_ranked\" */\n  txs_ranked: Array<Txs_Ranked>;\n  /** fetch aggregated fields from the table: \"txs_ranked\" */\n  txs_ranked_aggregate: Txs_Ranked_Aggregate;\n  /** fetch data from the table: \"txs_stats\" */\n  txs_stats: Array<Txs_Stats>;\n  /** fetch aggregated fields from the table: \"txs_stats\" */\n  txs_stats_aggregate: Txs_Stats_Aggregate;\n  /** fetch data from the table: \"uptime\" */\n  uptime: Array<Uptime>;\n  /** fetch aggregated fields from the table: \"uptime\" */\n  uptime_aggregate: Uptime_Aggregate;\n  /** fetch data from the table: \"validator\" */\n  validator: Array<Validator>;\n  /** fetch aggregated fields from the table: \"validator\" */\n  validator_aggregate: Validator_Aggregate;\n  /** fetch data from the table: \"validator\" using primary key columns */\n  validator_by_pk?: Maybe<Validator>;\n  /** fetch data from the table: \"volts_demand\" */\n  volts_demand: Array<Volts_Demand>;\n  /** fetch aggregated fields from the table: \"volts_demand\" */\n  volts_demand_aggregate: Volts_Demand_Aggregate;\n  /** fetch data from the table: \"volts_stats\" */\n  volts_stats: Array<Volts_Stats>;\n  /** fetch aggregated fields from the table: \"volts_stats\" */\n  volts_stats_aggregate: Volts_Stats_Aggregate;\n};\n\n\nexport type Query_Root_TransactionArgs = {\n  distinct_on?: InputMaybe<Array<_Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Transaction_Order_By>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Query_Root_Transaction_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<_Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Transaction_Order_By>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Query_Root_Uptime_TempArgs = {\n  distinct_on?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Uptime_Temp_Order_By>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Query_Root_Uptime_Temp_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Uptime_Temp_Order_By>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Query_RootAccountArgs = {\n  distinct_on?: InputMaybe<Array<Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Order_By>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Query_RootAccount_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Order_By>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Query_RootAccount_BalanceArgs = {\n  distinct_on?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Balance_Order_By>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Query_RootAccount_Balance_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Balance_Order_By>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Query_RootAccount_Balance_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootAccount_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootBlockArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Query_RootBlock_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Query_RootBlock_By_PkArgs = {\n  height: Scalars['bigint']['input'];\n};\n\n\nexport type Query_RootContractsArgs = {\n  distinct_on?: InputMaybe<Array<Contracts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Contracts_Order_By>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Query_RootContracts_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Contracts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Contracts_Order_By>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Query_RootContracts_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootCyb_CohortArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Query_RootCyb_Cohort_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Query_RootCyb_New_CohortArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_New_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_New_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_New_Cohort_Bool_Exp>;\n};\n\n\nexport type Query_RootCyb_New_Cohort_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_New_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_New_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_New_Cohort_Bool_Exp>;\n};\n\n\nexport type Query_RootCyber_GiftArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Bool_Exp>;\n};\n\n\nexport type Query_RootCyber_Gift_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Bool_Exp>;\n};\n\n\nexport type Query_RootCyber_Gift_ProofsArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Query_RootCyber_Gift_Proofs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Query_RootCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Query_RootCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Query_RootCyberlinks_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootCyberlinks_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Stats_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootCyberlinks_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Stats_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Amount_Of_Active_NeuronsArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Amount_Of_Active_Neurons_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Amount_Of_Used_GasArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Amount_Of_Used_Gas_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Number_Of_TransactionsArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Number_Of_Transactions_Order_By>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Number_Of_Transactions_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Number_Of_Transactions_Order_By>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Query_RootFollow_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Follow_Stats_Order_By>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootFollow_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Follow_Stats_Order_By>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesis_Neurons_ActivationArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Neurons_Activation_Order_By>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesis_Neurons_Activation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Neurons_Activation_Order_By>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Query_RootInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Query_RootInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Query_RootInvestmints_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootMessageArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessage_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessages_By_AddressArgs = {\n  args: Messages_By_Address_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessages_By_Address_AggregateArgs = {\n  args: Messages_By_Address_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootModulesArgs = {\n  distinct_on?: InputMaybe<Array<Modules_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Modules_Order_By>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Query_RootModules_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Modules_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Modules_Order_By>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Query_RootModules_By_PkArgs = {\n  module_name: Scalars['String']['input'];\n};\n\n\nexport type Query_RootNeuron_Activation_SourceArgs = {\n  distinct_on?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Neuron_Activation_Source_Order_By>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Query_RootNeuron_Activation_Source_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Neuron_Activation_Source_Order_By>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Query_RootNumber_Of_New_NeuronsArgs = {\n  distinct_on?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Number_Of_New_Neurons_Order_By>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Query_RootNumber_Of_New_Neurons_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Number_Of_New_Neurons_Order_By>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Query_RootOld_PrecommitsArgs = {\n  distinct_on?: InputMaybe<Array<Old_Precommits_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Old_Precommits_Order_By>>;\n  where?: InputMaybe<Old_Precommits_Bool_Exp>;\n};\n\n\nexport type Query_RootOld_Precommits_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Old_Precommits_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Old_Precommits_Order_By>>;\n  where?: InputMaybe<Old_Precommits_Bool_Exp>;\n};\n\n\nexport type Query_RootOld_Precommits_By_PkArgs = {\n  consensus_address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Query_RootParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Query_RootParticles_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootPre_CommitArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Query_RootPre_Commit_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Query_RootPre_Commits_Rewards_ViewArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commits_Rewards_View_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commits_Rewards_View_Order_By>>;\n  where?: InputMaybe<Pre_Commits_Rewards_View_Bool_Exp>;\n};\n\n\nexport type Query_RootPre_Commits_Rewards_View_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commits_Rewards_View_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commits_Rewards_View_Order_By>>;\n  where?: InputMaybe<Pre_Commits_Rewards_View_Bool_Exp>;\n};\n\n\nexport type Query_RootPre_Commits_TotalArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commits_Total_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commits_Total_Order_By>>;\n  where?: InputMaybe<Pre_Commits_Total_Bool_Exp>;\n};\n\n\nexport type Query_RootPre_Commits_Total_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commits_Total_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commits_Total_Order_By>>;\n  where?: InputMaybe<Pre_Commits_Total_Bool_Exp>;\n};\n\n\nexport type Query_RootPre_Commits_ViewArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commits_View_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commits_View_Order_By>>;\n  where?: InputMaybe<Pre_Commits_View_Bool_Exp>;\n};\n\n\nexport type Query_RootPre_Commits_View_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commits_View_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commits_View_Order_By>>;\n  where?: InputMaybe<Pre_Commits_View_Bool_Exp>;\n};\n\n\nexport type Query_RootPruningArgs = {\n  distinct_on?: InputMaybe<Array<Pruning_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pruning_Order_By>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Query_RootPruning_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pruning_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pruning_Order_By>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Query_RootPussy_Gift_ProofsArgs = {\n  distinct_on?: InputMaybe<Array<Pussy_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pussy_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Pussy_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Query_RootPussy_Gift_Proofs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pussy_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pussy_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Pussy_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Query_RootRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Query_RootRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Query_RootRoutes_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootSupplyArgs = {\n  distinct_on?: InputMaybe<Array<Supply_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Supply_Order_By>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Query_RootSupply_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Supply_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Supply_Order_By>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Query_RootSupply_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootTest_GiftArgs = {\n  distinct_on?: InputMaybe<Array<Test_Gift_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Test_Gift_Order_By>>;\n  where?: InputMaybe<Test_Gift_Bool_Exp>;\n};\n\n\nexport type Query_RootTest_Gift_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Test_Gift_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Test_Gift_Order_By>>;\n  where?: InputMaybe<Test_Gift_Bool_Exp>;\n};\n\n\nexport type Query_RootToday_Top_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Today_Top_Txs_Order_By>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootToday_Top_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Today_Top_Txs_Order_By>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_10_Of_Active_Neurons_WeekArgs = {\n  distinct_on?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Order_By>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_10_Of_Active_Neurons_Week_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Order_By>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_First_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_First_Txs_Order_By>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_First_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_First_Txs_Order_By>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_LeadersArgs = {\n  distinct_on?: InputMaybe<Array<Top_Leaders_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Leaders_Order_By>>;\n  where?: InputMaybe<Top_Leaders_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Txs_Order_By>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Txs_Order_By>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTransactionArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Query_RootTransaction_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Query_RootTransaction_By_PkArgs = {\n  hash: Scalars['String']['input'];\n};\n\n\nexport type Query_RootTweets_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Tweets_Stats_Order_By>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootTweets_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Tweets_Stats_Order_By>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootTxs_RankedArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Ranked_Order_By>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Query_RootTxs_Ranked_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Ranked_Order_By>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Query_RootTxs_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Stats_Order_By>>;\n  where?: InputMaybe<Txs_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootTxs_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Stats_Order_By>>;\n  where?: InputMaybe<Txs_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootUptimeArgs = {\n  distinct_on?: InputMaybe<Array<Uptime_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Uptime_Order_By>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Query_RootUptime_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Uptime_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Uptime_Order_By>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Query_RootValidatorArgs = {\n  distinct_on?: InputMaybe<Array<Validator_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Validator_Order_By>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Query_RootValidator_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Validator_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Validator_Order_By>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Query_RootValidator_By_PkArgs = {\n  consensus_address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootVolts_DemandArgs = {\n  distinct_on?: InputMaybe<Array<Volts_Demand_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Volts_Demand_Order_By>>;\n  where?: InputMaybe<Volts_Demand_Bool_Exp>;\n};\n\n\nexport type Query_RootVolts_Demand_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Volts_Demand_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Volts_Demand_Order_By>>;\n  where?: InputMaybe<Volts_Demand_Bool_Exp>;\n};\n\n\nexport type Query_RootVolts_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Volts_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Volts_Stats_Order_By>>;\n  where?: InputMaybe<Volts_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootVolts_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Volts_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Volts_Stats_Order_By>>;\n  where?: InputMaybe<Volts_Stats_Bool_Exp>;\n};\n\n/** columns and relationships of \"routes\" */\nexport type Routes = {\n  /** An object relationship */\n  account: Account;\n  /** An object relationship */\n  accountBySource: Account;\n  alias: Scalars['String']['output'];\n  /** An object relationship */\n  block: Block;\n  destination: Scalars['String']['output'];\n  height: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n  source: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  /** An object relationship */\n  transaction: Transaction;\n  transaction_hash: Scalars['String']['output'];\n  value: Scalars['_coin']['output'];\n};\n\n/** aggregated selection of \"routes\" */\nexport type Routes_Aggregate = {\n  aggregate?: Maybe<Routes_Aggregate_Fields>;\n  nodes: Array<Routes>;\n};\n\n/** aggregate fields of \"routes\" */\nexport type Routes_Aggregate_Fields = {\n  avg?: Maybe<Routes_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Routes_Max_Fields>;\n  min?: Maybe<Routes_Min_Fields>;\n  stddev?: Maybe<Routes_Stddev_Fields>;\n  stddev_pop?: Maybe<Routes_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Routes_Stddev_Samp_Fields>;\n  sum?: Maybe<Routes_Sum_Fields>;\n  var_pop?: Maybe<Routes_Var_Pop_Fields>;\n  var_samp?: Maybe<Routes_Var_Samp_Fields>;\n  variance?: Maybe<Routes_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"routes\" */\nexport type Routes_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Routes_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"routes\" */\nexport type Routes_Aggregate_Order_By = {\n  avg?: InputMaybe<Routes_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Routes_Max_Order_By>;\n  min?: InputMaybe<Routes_Min_Order_By>;\n  stddev?: InputMaybe<Routes_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Routes_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Routes_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Routes_Sum_Order_By>;\n  var_pop?: InputMaybe<Routes_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Routes_Var_Samp_Order_By>;\n  variance?: InputMaybe<Routes_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Routes_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"routes\" */\nexport type Routes_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"routes\". All fields are combined with a logical 'AND'. */\nexport type Routes_Bool_Exp = {\n  _and?: InputMaybe<Array<Routes_Bool_Exp>>;\n  _not?: InputMaybe<Routes_Bool_Exp>;\n  _or?: InputMaybe<Array<Routes_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  accountBySource?: InputMaybe<Account_Bool_Exp>;\n  alias?: InputMaybe<String_Comparison_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  destination?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  source?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  transaction?: InputMaybe<Transaction_Bool_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n  value?: InputMaybe<_Coin_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Routes_Max_Fields = {\n  alias?: Maybe<Scalars['String']['output']>;\n  destination?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  source?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"routes\" */\nexport type Routes_Max_Order_By = {\n  alias?: InputMaybe<Order_By>;\n  destination?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  source?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Routes_Min_Fields = {\n  alias?: Maybe<Scalars['String']['output']>;\n  destination?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  source?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"routes\" */\nexport type Routes_Min_Order_By = {\n  alias?: InputMaybe<Order_By>;\n  destination?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  source?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"routes\". */\nexport type Routes_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  accountBySource?: InputMaybe<Account_Order_By>;\n  alias?: InputMaybe<Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  destination?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  source?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction?: InputMaybe<Transaction_Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"routes\" */\nexport enum Routes_Select_Column {\n  /** column name */\n  Alias = 'alias',\n  /** column name */\n  Destination = 'destination',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  Source = 'source',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TransactionHash = 'transaction_hash',\n  /** column name */\n  Value = 'value'\n}\n\n/** aggregate stddev on columns */\nexport type Routes_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"routes\" */\nexport type Routes_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Routes_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"routes\" */\nexport type Routes_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Routes_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"routes\" */\nexport type Routes_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate sum on columns */\nexport type Routes_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** order by sum() on columns of table \"routes\" */\nexport type Routes_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Routes_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"routes\" */\nexport type Routes_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Routes_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"routes\" */\nexport type Routes_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Routes_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"routes\" */\nexport type Routes_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\nexport type Subscription_Root = {\n  /** fetch data from the table: \"_transaction\" */\n  _transaction: Array<_Transaction>;\n  /** fetch aggregated fields from the table: \"_transaction\" */\n  _transaction_aggregate: _Transaction_Aggregate;\n  /** fetch data from the table: \"_uptime_temp\" */\n  _uptime_temp: Array<_Uptime_Temp>;\n  /** fetch aggregated fields from the table: \"_uptime_temp\" */\n  _uptime_temp_aggregate: _Uptime_Temp_Aggregate;\n  /** fetch data from the table: \"account\" */\n  account: Array<Account>;\n  /** fetch aggregated fields from the table: \"account\" */\n  account_aggregate: Account_Aggregate;\n  /** fetch data from the table: \"account_balance\" */\n  account_balance: Array<Account_Balance>;\n  /** fetch aggregated fields from the table: \"account_balance\" */\n  account_balance_aggregate: Account_Balance_Aggregate;\n  /** fetch data from the table: \"account_balance\" using primary key columns */\n  account_balance_by_pk?: Maybe<Account_Balance>;\n  /** fetch data from the table: \"account\" using primary key columns */\n  account_by_pk?: Maybe<Account>;\n  /** fetch data from the table: \"block\" */\n  block: Array<Block>;\n  /** fetch aggregated fields from the table: \"block\" */\n  block_aggregate: Block_Aggregate;\n  /** fetch data from the table: \"block\" using primary key columns */\n  block_by_pk?: Maybe<Block>;\n  /** fetch data from the table: \"contracts\" */\n  contracts: Array<Contracts>;\n  /** fetch aggregated fields from the table: \"contracts\" */\n  contracts_aggregate: Contracts_Aggregate;\n  /** fetch data from the table: \"contracts\" using primary key columns */\n  contracts_by_pk?: Maybe<Contracts>;\n  /** fetch data from the table: \"cyb_cohort\" */\n  cyb_cohort: Array<Cyb_Cohort>;\n  /** fetch aggregated fields from the table: \"cyb_cohort\" */\n  cyb_cohort_aggregate: Cyb_Cohort_Aggregate;\n  /** fetch data from the table: \"cyb_new_cohort\" */\n  cyb_new_cohort: Array<Cyb_New_Cohort>;\n  /** fetch aggregated fields from the table: \"cyb_new_cohort\" */\n  cyb_new_cohort_aggregate: Cyb_New_Cohort_Aggregate;\n  /** fetch data from the table: \"cyber_gift\" */\n  cyber_gift: Array<Cyber_Gift>;\n  /** fetch aggregated fields from the table: \"cyber_gift\" */\n  cyber_gift_aggregate: Cyber_Gift_Aggregate;\n  /** fetch data from the table: \"cyber_gift_proofs\" */\n  cyber_gift_proofs: Array<Cyber_Gift_Proofs>;\n  /** fetch aggregated fields from the table: \"cyber_gift_proofs\" */\n  cyber_gift_proofs_aggregate: Cyber_Gift_Proofs_Aggregate;\n  /** fetch data from the table: \"cyberlinks\" */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  /** fetch data from the table: \"cyberlinks\" using primary key columns */\n  cyberlinks_by_pk?: Maybe<Cyberlinks>;\n  /** fetch data from the table: \"cyberlinks_stats\" */\n  cyberlinks_stats: Array<Cyberlinks_Stats>;\n  /** fetch aggregated fields from the table: \"cyberlinks_stats\" */\n  cyberlinks_stats_aggregate: Cyberlinks_Stats_Aggregate;\n  /** fetch data from the table: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons: Array<Daily_Amount_Of_Active_Neurons>;\n  /** fetch aggregated fields from the table: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons_aggregate: Daily_Amount_Of_Active_Neurons_Aggregate;\n  /** fetch data from the table: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas: Array<Daily_Amount_Of_Used_Gas>;\n  /** fetch aggregated fields from the table: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas_aggregate: Daily_Amount_Of_Used_Gas_Aggregate;\n  /** fetch data from the table: \"daily_number_of_transactions\" */\n  daily_number_of_transactions: Array<Daily_Number_Of_Transactions>;\n  /** fetch aggregated fields from the table: \"daily_number_of_transactions\" */\n  daily_number_of_transactions_aggregate: Daily_Number_Of_Transactions_Aggregate;\n  /** fetch data from the table: \"follow_stats\" */\n  follow_stats: Array<Follow_Stats>;\n  /** fetch aggregated fields from the table: \"follow_stats\" */\n  follow_stats_aggregate: Follow_Stats_Aggregate;\n  /** fetch data from the table: \"genesis_neurons_activation\" */\n  genesis_neurons_activation: Array<Genesis_Neurons_Activation>;\n  /** fetch aggregated fields from the table: \"genesis_neurons_activation\" */\n  genesis_neurons_activation_aggregate: Genesis_Neurons_Activation_Aggregate;\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  /** fetch data from the table: \"investmints\" using primary key columns */\n  investmints_by_pk?: Maybe<Investmints>;\n  /** fetch data from the table: \"message\" */\n  message: Array<Message>;\n  /** fetch aggregated fields from the table: \"message\" */\n  message_aggregate: Message_Aggregate;\n  /** execute function \"messages_by_address\" which returns \"message\" */\n  messages_by_address: Array<Message>;\n  /** execute function \"messages_by_address\" and query aggregates on result of table type \"message\" */\n  messages_by_address_aggregate: Message_Aggregate;\n  /** fetch data from the table: \"modules\" */\n  modules: Array<Modules>;\n  /** fetch aggregated fields from the table: \"modules\" */\n  modules_aggregate: Modules_Aggregate;\n  /** fetch data from the table: \"modules\" using primary key columns */\n  modules_by_pk?: Maybe<Modules>;\n  /** fetch data from the table: \"neuron_activation_source\" */\n  neuron_activation_source: Array<Neuron_Activation_Source>;\n  /** fetch aggregated fields from the table: \"neuron_activation_source\" */\n  neuron_activation_source_aggregate: Neuron_Activation_Source_Aggregate;\n  /** fetch data from the table: \"number_of_new_neurons\" */\n  number_of_new_neurons: Array<Number_Of_New_Neurons>;\n  /** fetch aggregated fields from the table: \"number_of_new_neurons\" */\n  number_of_new_neurons_aggregate: Number_Of_New_Neurons_Aggregate;\n  /** fetch data from the table: \"old_precommits\" */\n  old_precommits: Array<Old_Precommits>;\n  /** fetch aggregated fields from the table: \"old_precommits\" */\n  old_precommits_aggregate: Old_Precommits_Aggregate;\n  /** fetch data from the table: \"old_precommits\" using primary key columns */\n  old_precommits_by_pk?: Maybe<Old_Precommits>;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  /** fetch data from the table: \"particles\" using primary key columns */\n  particles_by_pk?: Maybe<Particles>;\n  /** fetch data from the table: \"pre_commit\" */\n  pre_commit: Array<Pre_Commit>;\n  /** fetch aggregated fields from the table: \"pre_commit\" */\n  pre_commit_aggregate: Pre_Commit_Aggregate;\n  /** fetch data from the table: \"pre_commits_rewards_view\" */\n  pre_commits_rewards_view: Array<Pre_Commits_Rewards_View>;\n  /** fetch aggregated fields from the table: \"pre_commits_rewards_view\" */\n  pre_commits_rewards_view_aggregate: Pre_Commits_Rewards_View_Aggregate;\n  /** fetch data from the table: \"pre_commits_total\" */\n  pre_commits_total: Array<Pre_Commits_Total>;\n  /** fetch aggregated fields from the table: \"pre_commits_total\" */\n  pre_commits_total_aggregate: Pre_Commits_Total_Aggregate;\n  /** fetch data from the table: \"pre_commits_view\" */\n  pre_commits_view: Array<Pre_Commits_View>;\n  /** fetch aggregated fields from the table: \"pre_commits_view\" */\n  pre_commits_view_aggregate: Pre_Commits_View_Aggregate;\n  /** fetch data from the table: \"pruning\" */\n  pruning: Array<Pruning>;\n  /** fetch aggregated fields from the table: \"pruning\" */\n  pruning_aggregate: Pruning_Aggregate;\n  /** fetch data from the table: \"pussy_gift_proofs\" */\n  pussy_gift_proofs: Array<Pussy_Gift_Proofs>;\n  /** fetch aggregated fields from the table: \"pussy_gift_proofs\" */\n  pussy_gift_proofs_aggregate: Pussy_Gift_Proofs_Aggregate;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n  /** fetch data from the table: \"routes\" using primary key columns */\n  routes_by_pk?: Maybe<Routes>;\n  /** fetch data from the table: \"supply\" */\n  supply: Array<Supply>;\n  /** fetch aggregated fields from the table: \"supply\" */\n  supply_aggregate: Supply_Aggregate;\n  /** fetch data from the table: \"supply\" using primary key columns */\n  supply_by_pk?: Maybe<Supply>;\n  /** fetch data from the table: \"test_gift\" */\n  test_gift: Array<Test_Gift>;\n  /** fetch aggregated fields from the table: \"test_gift\" */\n  test_gift_aggregate: Test_Gift_Aggregate;\n  /** fetch data from the table: \"today_top_txs\" */\n  today_top_txs: Array<Today_Top_Txs>;\n  /** fetch aggregated fields from the table: \"today_top_txs\" */\n  today_top_txs_aggregate: Today_Top_Txs_Aggregate;\n  /** fetch data from the table: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week: Array<Top_10_Of_Active_Neurons_Week>;\n  /** fetch aggregated fields from the table: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week_aggregate: Top_10_Of_Active_Neurons_Week_Aggregate;\n  /** fetch data from the table: \"top_first_txs\" */\n  top_first_txs: Array<Top_First_Txs>;\n  /** fetch aggregated fields from the table: \"top_first_txs\" */\n  top_first_txs_aggregate: Top_First_Txs_Aggregate;\n  /** fetch data from the table: \"top_leaders\" */\n  top_leaders: Array<Top_Leaders>;\n  /** fetch data from the table: \"top_txs\" */\n  top_txs: Array<Top_Txs>;\n  /** fetch aggregated fields from the table: \"top_txs\" */\n  top_txs_aggregate: Top_Txs_Aggregate;\n  /** fetch data from the table: \"transaction\" */\n  transaction: Array<Transaction>;\n  /** fetch aggregated fields from the table: \"transaction\" */\n  transaction_aggregate: Transaction_Aggregate;\n  /** fetch data from the table: \"transaction\" using primary key columns */\n  transaction_by_pk?: Maybe<Transaction>;\n  /** fetch data from the table: \"tweets_stats\" */\n  tweets_stats: Array<Tweets_Stats>;\n  /** fetch aggregated fields from the table: \"tweets_stats\" */\n  tweets_stats_aggregate: Tweets_Stats_Aggregate;\n  /** fetch data from the table: \"txs_ranked\" */\n  txs_ranked: Array<Txs_Ranked>;\n  /** fetch aggregated fields from the table: \"txs_ranked\" */\n  txs_ranked_aggregate: Txs_Ranked_Aggregate;\n  /** fetch data from the table: \"txs_stats\" */\n  txs_stats: Array<Txs_Stats>;\n  /** fetch aggregated fields from the table: \"txs_stats\" */\n  txs_stats_aggregate: Txs_Stats_Aggregate;\n  /** fetch data from the table: \"uptime\" */\n  uptime: Array<Uptime>;\n  /** fetch aggregated fields from the table: \"uptime\" */\n  uptime_aggregate: Uptime_Aggregate;\n  /** fetch data from the table: \"validator\" */\n  validator: Array<Validator>;\n  /** fetch aggregated fields from the table: \"validator\" */\n  validator_aggregate: Validator_Aggregate;\n  /** fetch data from the table: \"validator\" using primary key columns */\n  validator_by_pk?: Maybe<Validator>;\n  /** fetch data from the table: \"volts_demand\" */\n  volts_demand: Array<Volts_Demand>;\n  /** fetch aggregated fields from the table: \"volts_demand\" */\n  volts_demand_aggregate: Volts_Demand_Aggregate;\n  /** fetch data from the table: \"volts_stats\" */\n  volts_stats: Array<Volts_Stats>;\n  /** fetch aggregated fields from the table: \"volts_stats\" */\n  volts_stats_aggregate: Volts_Stats_Aggregate;\n};\n\n\nexport type Subscription_Root_TransactionArgs = {\n  distinct_on?: InputMaybe<Array<_Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Transaction_Order_By>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Transaction_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<_Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Transaction_Order_By>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Uptime_TempArgs = {\n  distinct_on?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Uptime_Temp_Order_By>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Uptime_Temp_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Uptime_Temp_Order_By>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccountArgs = {\n  distinct_on?: InputMaybe<Array<Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Order_By>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Order_By>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_BalanceArgs = {\n  distinct_on?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Balance_Order_By>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_Balance_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Balance_Order_By>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_Balance_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootAccount_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootBlockArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Subscription_RootBlock_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Subscription_RootBlock_By_PkArgs = {\n  height: Scalars['bigint']['input'];\n};\n\n\nexport type Subscription_RootContractsArgs = {\n  distinct_on?: InputMaybe<Array<Contracts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Contracts_Order_By>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootContracts_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Contracts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Contracts_Order_By>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootContracts_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootCyb_CohortArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyb_Cohort_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyb_New_CohortArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_New_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_New_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_New_Cohort_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyb_New_Cohort_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_New_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_New_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_New_Cohort_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyber_GiftArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyber_Gift_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyber_Gift_ProofsArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyber_Gift_Proofs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootCyberlinks_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Stats_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Stats_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Active_NeuronsArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Active_Neurons_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Used_GasArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Used_Gas_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Number_Of_TransactionsArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Number_Of_Transactions_Order_By>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Number_Of_Transactions_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Number_Of_Transactions_Order_By>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFollow_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Follow_Stats_Order_By>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFollow_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Follow_Stats_Order_By>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_Neurons_ActivationArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Neurons_Activation_Order_By>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_Neurons_Activation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Neurons_Activation_Order_By>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Subscription_RootInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Subscription_RootInvestmints_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootMessageArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessages_By_AddressArgs = {\n  args: Messages_By_Address_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessages_By_Address_AggregateArgs = {\n  args: Messages_By_Address_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootModulesArgs = {\n  distinct_on?: InputMaybe<Array<Modules_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Modules_Order_By>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Subscription_RootModules_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Modules_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Modules_Order_By>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Subscription_RootModules_By_PkArgs = {\n  module_name: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootNeuron_Activation_SourceArgs = {\n  distinct_on?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Neuron_Activation_Source_Order_By>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNeuron_Activation_Source_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Neuron_Activation_Source_Order_By>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNumber_Of_New_NeuronsArgs = {\n  distinct_on?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Number_Of_New_Neurons_Order_By>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNumber_Of_New_Neurons_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Number_Of_New_Neurons_Order_By>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootOld_PrecommitsArgs = {\n  distinct_on?: InputMaybe<Array<Old_Precommits_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Old_Precommits_Order_By>>;\n  where?: InputMaybe<Old_Precommits_Bool_Exp>;\n};\n\n\nexport type Subscription_RootOld_Precommits_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Old_Precommits_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Old_Precommits_Order_By>>;\n  where?: InputMaybe<Old_Precommits_Bool_Exp>;\n};\n\n\nexport type Subscription_RootOld_Precommits_By_PkArgs = {\n  consensus_address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Subscription_RootParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Subscription_RootParticles_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootPre_CommitArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_Commit_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_Commits_Rewards_ViewArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commits_Rewards_View_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commits_Rewards_View_Order_By>>;\n  where?: InputMaybe<Pre_Commits_Rewards_View_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_Commits_Rewards_View_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commits_Rewards_View_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commits_Rewards_View_Order_By>>;\n  where?: InputMaybe<Pre_Commits_Rewards_View_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_Commits_TotalArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commits_Total_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commits_Total_Order_By>>;\n  where?: InputMaybe<Pre_Commits_Total_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_Commits_Total_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commits_Total_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commits_Total_Order_By>>;\n  where?: InputMaybe<Pre_Commits_Total_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_Commits_ViewArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commits_View_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commits_View_Order_By>>;\n  where?: InputMaybe<Pre_Commits_View_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_Commits_View_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commits_View_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commits_View_Order_By>>;\n  where?: InputMaybe<Pre_Commits_View_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPruningArgs = {\n  distinct_on?: InputMaybe<Array<Pruning_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pruning_Order_By>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPruning_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pruning_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pruning_Order_By>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPussy_Gift_ProofsArgs = {\n  distinct_on?: InputMaybe<Array<Pussy_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pussy_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Pussy_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPussy_Gift_Proofs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pussy_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pussy_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Pussy_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootRoutes_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootSupplyArgs = {\n  distinct_on?: InputMaybe<Array<Supply_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Supply_Order_By>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSupply_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Supply_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Supply_Order_By>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSupply_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootTest_GiftArgs = {\n  distinct_on?: InputMaybe<Array<Test_Gift_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Test_Gift_Order_By>>;\n  where?: InputMaybe<Test_Gift_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTest_Gift_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Test_Gift_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Test_Gift_Order_By>>;\n  where?: InputMaybe<Test_Gift_Bool_Exp>;\n};\n\n\nexport type Subscription_RootToday_Top_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Today_Top_Txs_Order_By>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootToday_Top_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Today_Top_Txs_Order_By>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_10_Of_Active_Neurons_WeekArgs = {\n  distinct_on?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Order_By>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_10_Of_Active_Neurons_Week_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Order_By>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_First_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_First_Txs_Order_By>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_First_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_First_Txs_Order_By>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_LeadersArgs = {\n  distinct_on?: InputMaybe<Array<Top_Leaders_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Leaders_Order_By>>;\n  where?: InputMaybe<Top_Leaders_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Txs_Order_By>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Txs_Order_By>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransactionArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransaction_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransaction_By_PkArgs = {\n  hash: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootTweets_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Tweets_Stats_Order_By>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTweets_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Tweets_Stats_Order_By>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTxs_RankedArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Ranked_Order_By>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTxs_Ranked_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Ranked_Order_By>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTxs_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Stats_Order_By>>;\n  where?: InputMaybe<Txs_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTxs_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Stats_Order_By>>;\n  where?: InputMaybe<Txs_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootUptimeArgs = {\n  distinct_on?: InputMaybe<Array<Uptime_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Uptime_Order_By>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Subscription_RootUptime_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Uptime_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Uptime_Order_By>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Subscription_RootValidatorArgs = {\n  distinct_on?: InputMaybe<Array<Validator_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Validator_Order_By>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Subscription_RootValidator_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Validator_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Validator_Order_By>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Subscription_RootValidator_By_PkArgs = {\n  consensus_address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootVolts_DemandArgs = {\n  distinct_on?: InputMaybe<Array<Volts_Demand_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Volts_Demand_Order_By>>;\n  where?: InputMaybe<Volts_Demand_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVolts_Demand_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Volts_Demand_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Volts_Demand_Order_By>>;\n  where?: InputMaybe<Volts_Demand_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVolts_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Volts_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Volts_Stats_Order_By>>;\n  where?: InputMaybe<Volts_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVolts_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Volts_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Volts_Stats_Order_By>>;\n  where?: InputMaybe<Volts_Stats_Bool_Exp>;\n};\n\n/** columns and relationships of \"supply\" */\nexport type Supply = {\n  coins: Scalars['_coin']['output'];\n  height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n};\n\n/** aggregated selection of \"supply\" */\nexport type Supply_Aggregate = {\n  aggregate?: Maybe<Supply_Aggregate_Fields>;\n  nodes: Array<Supply>;\n};\n\n/** aggregate fields of \"supply\" */\nexport type Supply_Aggregate_Fields = {\n  avg?: Maybe<Supply_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Supply_Max_Fields>;\n  min?: Maybe<Supply_Min_Fields>;\n  stddev?: Maybe<Supply_Stddev_Fields>;\n  stddev_pop?: Maybe<Supply_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Supply_Stddev_Samp_Fields>;\n  sum?: Maybe<Supply_Sum_Fields>;\n  var_pop?: Maybe<Supply_Var_Pop_Fields>;\n  var_samp?: Maybe<Supply_Var_Samp_Fields>;\n  variance?: Maybe<Supply_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"supply\" */\nexport type Supply_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Supply_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Supply_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"supply\". All fields are combined with a logical 'AND'. */\nexport type Supply_Bool_Exp = {\n  _and?: InputMaybe<Array<Supply_Bool_Exp>>;\n  _not?: InputMaybe<Supply_Bool_Exp>;\n  _or?: InputMaybe<Array<Supply_Bool_Exp>>;\n  coins?: InputMaybe<_Coin_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Supply_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Supply_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"supply\". */\nexport type Supply_Order_By = {\n  coins?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"supply\" */\nexport enum Supply_Select_Column {\n  /** column name */\n  Coins = 'coins',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  OneRowId = 'one_row_id'\n}\n\n/** aggregate stddev on columns */\nexport type Supply_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Supply_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Supply_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Supply_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Supply_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Supply_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Supply_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"test_gift\" */\nexport type Test_Gift = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  details?: Maybe<Scalars['json']['output']>;\n  proof?: Maybe<Scalars['String']['output']>;\n};\n\n\n/** columns and relationships of \"test_gift\" */\nexport type Test_GiftDetailsArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregated selection of \"test_gift\" */\nexport type Test_Gift_Aggregate = {\n  aggregate?: Maybe<Test_Gift_Aggregate_Fields>;\n  nodes: Array<Test_Gift>;\n};\n\n/** aggregate fields of \"test_gift\" */\nexport type Test_Gift_Aggregate_Fields = {\n  avg?: Maybe<Test_Gift_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Test_Gift_Max_Fields>;\n  min?: Maybe<Test_Gift_Min_Fields>;\n  stddev?: Maybe<Test_Gift_Stddev_Fields>;\n  stddev_pop?: Maybe<Test_Gift_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Test_Gift_Stddev_Samp_Fields>;\n  sum?: Maybe<Test_Gift_Sum_Fields>;\n  var_pop?: Maybe<Test_Gift_Var_Pop_Fields>;\n  var_samp?: Maybe<Test_Gift_Var_Samp_Fields>;\n  variance?: Maybe<Test_Gift_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"test_gift\" */\nexport type Test_Gift_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Test_Gift_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Test_Gift_Avg_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"test_gift\". All fields are combined with a logical 'AND'. */\nexport type Test_Gift_Bool_Exp = {\n  _and?: InputMaybe<Array<Test_Gift_Bool_Exp>>;\n  _not?: InputMaybe<Test_Gift_Bool_Exp>;\n  _or?: InputMaybe<Array<Test_Gift_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  amount?: InputMaybe<Bigint_Comparison_Exp>;\n  details?: InputMaybe<Json_Comparison_Exp>;\n  proof?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Test_Gift_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  proof?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Test_Gift_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  proof?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"test_gift\". */\nexport type Test_Gift_Order_By = {\n  address?: InputMaybe<Order_By>;\n  amount?: InputMaybe<Order_By>;\n  details?: InputMaybe<Order_By>;\n  proof?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"test_gift\" */\nexport enum Test_Gift_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Amount = 'amount',\n  /** column name */\n  Details = 'details',\n  /** column name */\n  Proof = 'proof'\n}\n\n/** aggregate stddev on columns */\nexport type Test_Gift_Stddev_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Test_Gift_Stddev_Pop_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Test_Gift_Stddev_Samp_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Test_Gift_Sum_Fields = {\n  amount?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Test_Gift_Var_Pop_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Test_Gift_Var_Samp_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Test_Gift_Variance_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to compare columns of type \"timestamp\". All fields are combined with logical 'AND'. */\nexport type Timestamp_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['timestamp']['input']>;\n  _gt?: InputMaybe<Scalars['timestamp']['input']>;\n  _gte?: InputMaybe<Scalars['timestamp']['input']>;\n  _in?: InputMaybe<Array<Scalars['timestamp']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['timestamp']['input']>;\n  _lte?: InputMaybe<Scalars['timestamp']['input']>;\n  _neq?: InputMaybe<Scalars['timestamp']['input']>;\n  _nin?: InputMaybe<Array<Scalars['timestamp']['input']>>;\n};\n\n/** columns and relationships of \"today_top_txs\" */\nexport type Today_Top_Txs = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"today_top_txs\" */\nexport type Today_Top_Txs_Aggregate = {\n  aggregate?: Maybe<Today_Top_Txs_Aggregate_Fields>;\n  nodes: Array<Today_Top_Txs>;\n};\n\n/** aggregate fields of \"today_top_txs\" */\nexport type Today_Top_Txs_Aggregate_Fields = {\n  avg?: Maybe<Today_Top_Txs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Today_Top_Txs_Max_Fields>;\n  min?: Maybe<Today_Top_Txs_Min_Fields>;\n  stddev?: Maybe<Today_Top_Txs_Stddev_Fields>;\n  stddev_pop?: Maybe<Today_Top_Txs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Today_Top_Txs_Stddev_Samp_Fields>;\n  sum?: Maybe<Today_Top_Txs_Sum_Fields>;\n  var_pop?: Maybe<Today_Top_Txs_Var_Pop_Fields>;\n  var_samp?: Maybe<Today_Top_Txs_Var_Samp_Fields>;\n  variance?: Maybe<Today_Top_Txs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"today_top_txs\" */\nexport type Today_Top_Txs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Today_Top_Txs_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"today_top_txs\". All fields are combined with a logical 'AND'. */\nexport type Today_Top_Txs_Bool_Exp = {\n  _and?: InputMaybe<Array<Today_Top_Txs_Bool_Exp>>;\n  _not?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n  _or?: InputMaybe<Array<Today_Top_Txs_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Today_Top_Txs_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Today_Top_Txs_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"today_top_txs\". */\nexport type Today_Top_Txs_Order_By = {\n  count?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"today_top_txs\" */\nexport enum Today_Top_Txs_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Today_Top_Txs_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Today_Top_Txs_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Today_Top_Txs_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Today_Top_Txs_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Today_Top_Txs_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Today_Top_Txs_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Today_Top_Txs_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week_Aggregate = {\n  aggregate?: Maybe<Top_10_Of_Active_Neurons_Week_Aggregate_Fields>;\n  nodes: Array<Top_10_Of_Active_Neurons_Week>;\n};\n\n/** aggregate fields of \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week_Aggregate_Fields = {\n  avg?: Maybe<Top_10_Of_Active_Neurons_Week_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Top_10_Of_Active_Neurons_Week_Max_Fields>;\n  min?: Maybe<Top_10_Of_Active_Neurons_Week_Min_Fields>;\n  stddev?: Maybe<Top_10_Of_Active_Neurons_Week_Stddev_Fields>;\n  stddev_pop?: Maybe<Top_10_Of_Active_Neurons_Week_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Top_10_Of_Active_Neurons_Week_Stddev_Samp_Fields>;\n  sum?: Maybe<Top_10_Of_Active_Neurons_Week_Sum_Fields>;\n  var_pop?: Maybe<Top_10_Of_Active_Neurons_Week_Var_Pop_Fields>;\n  var_samp?: Maybe<Top_10_Of_Active_Neurons_Week_Var_Samp_Fields>;\n  variance?: Maybe<Top_10_Of_Active_Neurons_Week_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Top_10_Of_Active_Neurons_Week_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"top_10_of_active_neurons_week\". All fields are combined with a logical 'AND'. */\nexport type Top_10_Of_Active_Neurons_Week_Bool_Exp = {\n  _and?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Bool_Exp>>;\n  _not?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n  _or?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  pubkey?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Top_10_Of_Active_Neurons_Week_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Top_10_Of_Active_Neurons_Week_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"top_10_of_active_neurons_week\". */\nexport type Top_10_Of_Active_Neurons_Week_Order_By = {\n  count?: InputMaybe<Order_By>;\n  pubkey?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"top_10_of_active_neurons_week\" */\nexport enum Top_10_Of_Active_Neurons_Week_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Pubkey = 'pubkey'\n}\n\n/** aggregate stddev on columns */\nexport type Top_10_Of_Active_Neurons_Week_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Top_10_Of_Active_Neurons_Week_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Top_10_Of_Active_Neurons_Week_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Top_10_Of_Active_Neurons_Week_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Top_10_Of_Active_Neurons_Week_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Top_10_Of_Active_Neurons_Week_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Top_10_Of_Active_Neurons_Week_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"top_first_txs\" */\nexport type Top_First_Txs = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"top_first_txs\" */\nexport type Top_First_Txs_Aggregate = {\n  aggregate?: Maybe<Top_First_Txs_Aggregate_Fields>;\n  nodes: Array<Top_First_Txs>;\n};\n\n/** aggregate fields of \"top_first_txs\" */\nexport type Top_First_Txs_Aggregate_Fields = {\n  avg?: Maybe<Top_First_Txs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Top_First_Txs_Max_Fields>;\n  min?: Maybe<Top_First_Txs_Min_Fields>;\n  stddev?: Maybe<Top_First_Txs_Stddev_Fields>;\n  stddev_pop?: Maybe<Top_First_Txs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Top_First_Txs_Stddev_Samp_Fields>;\n  sum?: Maybe<Top_First_Txs_Sum_Fields>;\n  var_pop?: Maybe<Top_First_Txs_Var_Pop_Fields>;\n  var_samp?: Maybe<Top_First_Txs_Var_Samp_Fields>;\n  variance?: Maybe<Top_First_Txs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"top_first_txs\" */\nexport type Top_First_Txs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Top_First_Txs_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"top_first_txs\". All fields are combined with a logical 'AND'. */\nexport type Top_First_Txs_Bool_Exp = {\n  _and?: InputMaybe<Array<Top_First_Txs_Bool_Exp>>;\n  _not?: InputMaybe<Top_First_Txs_Bool_Exp>;\n  _or?: InputMaybe<Array<Top_First_Txs_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Top_First_Txs_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Top_First_Txs_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"top_first_txs\". */\nexport type Top_First_Txs_Order_By = {\n  count?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"top_first_txs\" */\nexport enum Top_First_Txs_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Top_First_Txs_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Top_First_Txs_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Top_First_Txs_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Top_First_Txs_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Top_First_Txs_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Top_First_Txs_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Top_First_Txs_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"top_leaders\" */\nexport type Top_Leaders = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"top_leaders\". All fields are combined with a logical 'AND'. */\nexport type Top_Leaders_Bool_Exp = {\n  _and?: InputMaybe<Array<Top_Leaders_Bool_Exp>>;\n  _not?: InputMaybe<Top_Leaders_Bool_Exp>;\n  _or?: InputMaybe<Array<Top_Leaders_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** Ordering options when selecting data from \"top_leaders\". */\nexport type Top_Leaders_Order_By = {\n  count?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"top_leaders\" */\nexport enum Top_Leaders_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Neuron = 'neuron'\n}\n\n/** columns and relationships of \"top_txs\" */\nexport type Top_Txs = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"top_txs\" */\nexport type Top_Txs_Aggregate = {\n  aggregate?: Maybe<Top_Txs_Aggregate_Fields>;\n  nodes: Array<Top_Txs>;\n};\n\n/** aggregate fields of \"top_txs\" */\nexport type Top_Txs_Aggregate_Fields = {\n  avg?: Maybe<Top_Txs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Top_Txs_Max_Fields>;\n  min?: Maybe<Top_Txs_Min_Fields>;\n  stddev?: Maybe<Top_Txs_Stddev_Fields>;\n  stddev_pop?: Maybe<Top_Txs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Top_Txs_Stddev_Samp_Fields>;\n  sum?: Maybe<Top_Txs_Sum_Fields>;\n  var_pop?: Maybe<Top_Txs_Var_Pop_Fields>;\n  var_samp?: Maybe<Top_Txs_Var_Samp_Fields>;\n  variance?: Maybe<Top_Txs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"top_txs\" */\nexport type Top_Txs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Top_Txs_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"top_txs\". All fields are combined with a logical 'AND'. */\nexport type Top_Txs_Bool_Exp = {\n  _and?: InputMaybe<Array<Top_Txs_Bool_Exp>>;\n  _not?: InputMaybe<Top_Txs_Bool_Exp>;\n  _or?: InputMaybe<Array<Top_Txs_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Top_Txs_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Top_Txs_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"top_txs\". */\nexport type Top_Txs_Order_By = {\n  count?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"top_txs\" */\nexport enum Top_Txs_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Top_Txs_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Top_Txs_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Top_Txs_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Top_Txs_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Top_Txs_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Top_Txs_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Top_Txs_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"transaction\" */\nexport type Transaction = {\n  /** An object relationship */\n  block: Block;\n  /** fetch data from the table: \"cyberlinks\" */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  fee: Scalars['jsonb']['output'];\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash: Scalars['String']['output'];\n  height: Scalars['bigint']['output'];\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  logs?: Maybe<Scalars['jsonb']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  messages: Scalars['jsonb']['output'];\n  /** An array relationship */\n  messagesByTransactionHash: Array<Message>;\n  /** An aggregate relationship */\n  messagesByTransactionHash_aggregate: Message_Aggregate;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n  signatures: Scalars['_text']['output'];\n  signer_infos: Scalars['jsonb']['output'];\n  success: Scalars['Boolean']['output'];\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionFeeArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionLogsArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessagesArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessagesByTransactionHashArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessagesByTransactionHash_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionSigner_InfosArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregated selection of \"transaction\" */\nexport type Transaction_Aggregate = {\n  aggregate?: Maybe<Transaction_Aggregate_Fields>;\n  nodes: Array<Transaction>;\n};\n\n/** aggregate fields of \"transaction\" */\nexport type Transaction_Aggregate_Fields = {\n  avg?: Maybe<Transaction_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Transaction_Max_Fields>;\n  min?: Maybe<Transaction_Min_Fields>;\n  stddev?: Maybe<Transaction_Stddev_Fields>;\n  stddev_pop?: Maybe<Transaction_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Transaction_Stddev_Samp_Fields>;\n  sum?: Maybe<Transaction_Sum_Fields>;\n  var_pop?: Maybe<Transaction_Var_Pop_Fields>;\n  var_samp?: Maybe<Transaction_Var_Samp_Fields>;\n  variance?: Maybe<Transaction_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"transaction\" */\nexport type Transaction_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Transaction_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"transaction\" */\nexport type Transaction_Aggregate_Order_By = {\n  avg?: InputMaybe<Transaction_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Transaction_Max_Order_By>;\n  min?: InputMaybe<Transaction_Min_Order_By>;\n  stddev?: InputMaybe<Transaction_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Transaction_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Transaction_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Transaction_Sum_Order_By>;\n  var_pop?: InputMaybe<Transaction_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Transaction_Var_Samp_Order_By>;\n  variance?: InputMaybe<Transaction_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Transaction_Avg_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"transaction\" */\nexport type Transaction_Avg_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"transaction\". All fields are combined with a logical 'AND'. */\nexport type Transaction_Bool_Exp = {\n  _and?: InputMaybe<Array<Transaction_Bool_Exp>>;\n  _not?: InputMaybe<Transaction_Bool_Exp>;\n  _or?: InputMaybe<Array<Transaction_Bool_Exp>>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  cyberlinks?: InputMaybe<Cyberlinks_Bool_Exp>;\n  fee?: InputMaybe<Jsonb_Comparison_Exp>;\n  gas_used?: InputMaybe<Bigint_Comparison_Exp>;\n  gas_wanted?: InputMaybe<Bigint_Comparison_Exp>;\n  hash?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  investmints?: InputMaybe<Investmints_Bool_Exp>;\n  logs?: InputMaybe<Jsonb_Comparison_Exp>;\n  memo?: InputMaybe<String_Comparison_Exp>;\n  messages?: InputMaybe<Jsonb_Comparison_Exp>;\n  messagesByTransactionHash?: InputMaybe<Message_Bool_Exp>;\n  particles?: InputMaybe<Particles_Bool_Exp>;\n  raw_log?: InputMaybe<String_Comparison_Exp>;\n  routes?: InputMaybe<Routes_Bool_Exp>;\n  signatures?: InputMaybe<_Text_Comparison_Exp>;\n  signer_infos?: InputMaybe<Jsonb_Comparison_Exp>;\n  success?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Transaction_Max_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"transaction\" */\nexport type Transaction_Max_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Transaction_Min_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"transaction\" */\nexport type Transaction_Min_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"transaction\". */\nexport type Transaction_Order_By = {\n  block?: InputMaybe<Block_Order_By>;\n  cyberlinks_aggregate?: InputMaybe<Cyberlinks_Aggregate_Order_By>;\n  fee?: InputMaybe<Order_By>;\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  investmints_aggregate?: InputMaybe<Investmints_Aggregate_Order_By>;\n  logs?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  messages?: InputMaybe<Order_By>;\n  messagesByTransactionHash_aggregate?: InputMaybe<Message_Aggregate_Order_By>;\n  particles_aggregate?: InputMaybe<Particles_Aggregate_Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  routes_aggregate?: InputMaybe<Routes_Aggregate_Order_By>;\n  signatures?: InputMaybe<Order_By>;\n  signer_infos?: InputMaybe<Order_By>;\n  success?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"transaction\" */\nexport enum Transaction_Select_Column {\n  /** column name */\n  Fee = 'fee',\n  /** column name */\n  GasUsed = 'gas_used',\n  /** column name */\n  GasWanted = 'gas_wanted',\n  /** column name */\n  Hash = 'hash',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Logs = 'logs',\n  /** column name */\n  Memo = 'memo',\n  /** column name */\n  Messages = 'messages',\n  /** column name */\n  RawLog = 'raw_log',\n  /** column name */\n  Signatures = 'signatures',\n  /** column name */\n  SignerInfos = 'signer_infos',\n  /** column name */\n  Success = 'success'\n}\n\n/** aggregate stddev on columns */\nexport type Transaction_Stddev_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"transaction\" */\nexport type Transaction_Stddev_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Transaction_Stddev_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"transaction\" */\nexport type Transaction_Stddev_Pop_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Transaction_Stddev_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"transaction\" */\nexport type Transaction_Stddev_Samp_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n};\n\n/** aggregate sum on columns */\nexport type Transaction_Sum_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"transaction\" */\nexport type Transaction_Sum_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Transaction_Var_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"transaction\" */\nexport type Transaction_Var_Pop_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Transaction_Var_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"transaction\" */\nexport type Transaction_Var_Samp_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Transaction_Variance_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"transaction\" */\nexport type Transaction_Variance_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"tweets_stats\" */\nexport type Tweets_Stats = {\n  date?: Maybe<Scalars['date']['output']>;\n  tweets?: Maybe<Scalars['numeric']['output']>;\n  tweets_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregated selection of \"tweets_stats\" */\nexport type Tweets_Stats_Aggregate = {\n  aggregate?: Maybe<Tweets_Stats_Aggregate_Fields>;\n  nodes: Array<Tweets_Stats>;\n};\n\n/** aggregate fields of \"tweets_stats\" */\nexport type Tweets_Stats_Aggregate_Fields = {\n  avg?: Maybe<Tweets_Stats_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Tweets_Stats_Max_Fields>;\n  min?: Maybe<Tweets_Stats_Min_Fields>;\n  stddev?: Maybe<Tweets_Stats_Stddev_Fields>;\n  stddev_pop?: Maybe<Tweets_Stats_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Tweets_Stats_Stddev_Samp_Fields>;\n  sum?: Maybe<Tweets_Stats_Sum_Fields>;\n  var_pop?: Maybe<Tweets_Stats_Var_Pop_Fields>;\n  var_samp?: Maybe<Tweets_Stats_Var_Samp_Fields>;\n  variance?: Maybe<Tweets_Stats_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"tweets_stats\" */\nexport type Tweets_Stats_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Tweets_Stats_Avg_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"tweets_stats\". All fields are combined with a logical 'AND'. */\nexport type Tweets_Stats_Bool_Exp = {\n  _and?: InputMaybe<Array<Tweets_Stats_Bool_Exp>>;\n  _not?: InputMaybe<Tweets_Stats_Bool_Exp>;\n  _or?: InputMaybe<Array<Tweets_Stats_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  tweets?: InputMaybe<Numeric_Comparison_Exp>;\n  tweets_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Tweets_Stats_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  tweets?: Maybe<Scalars['numeric']['output']>;\n  tweets_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Tweets_Stats_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  tweets?: Maybe<Scalars['numeric']['output']>;\n  tweets_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"tweets_stats\". */\nexport type Tweets_Stats_Order_By = {\n  date?: InputMaybe<Order_By>;\n  tweets?: InputMaybe<Order_By>;\n  tweets_per_day?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"tweets_stats\" */\nexport enum Tweets_Stats_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  Tweets = 'tweets',\n  /** column name */\n  TweetsPerDay = 'tweets_per_day'\n}\n\n/** aggregate stddev on columns */\nexport type Tweets_Stats_Stddev_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Tweets_Stats_Stddev_Pop_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Tweets_Stats_Stddev_Samp_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Tweets_Stats_Sum_Fields = {\n  tweets?: Maybe<Scalars['numeric']['output']>;\n  tweets_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Tweets_Stats_Var_Pop_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Tweets_Stats_Var_Samp_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Tweets_Stats_Variance_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"txs_ranked\" */\nexport type Txs_Ranked = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"txs_ranked\" */\nexport type Txs_Ranked_Aggregate = {\n  aggregate?: Maybe<Txs_Ranked_Aggregate_Fields>;\n  nodes: Array<Txs_Ranked>;\n};\n\n/** aggregate fields of \"txs_ranked\" */\nexport type Txs_Ranked_Aggregate_Fields = {\n  avg?: Maybe<Txs_Ranked_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Txs_Ranked_Max_Fields>;\n  min?: Maybe<Txs_Ranked_Min_Fields>;\n  stddev?: Maybe<Txs_Ranked_Stddev_Fields>;\n  stddev_pop?: Maybe<Txs_Ranked_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Txs_Ranked_Stddev_Samp_Fields>;\n  sum?: Maybe<Txs_Ranked_Sum_Fields>;\n  var_pop?: Maybe<Txs_Ranked_Var_Pop_Fields>;\n  var_samp?: Maybe<Txs_Ranked_Var_Samp_Fields>;\n  variance?: Maybe<Txs_Ranked_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"txs_ranked\" */\nexport type Txs_Ranked_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Txs_Ranked_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"txs_ranked\". All fields are combined with a logical 'AND'. */\nexport type Txs_Ranked_Bool_Exp = {\n  _and?: InputMaybe<Array<Txs_Ranked_Bool_Exp>>;\n  _not?: InputMaybe<Txs_Ranked_Bool_Exp>;\n  _or?: InputMaybe<Array<Txs_Ranked_Bool_Exp>>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n  rank?: InputMaybe<Bigint_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Txs_Ranked_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Txs_Ranked_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"txs_ranked\". */\nexport type Txs_Ranked_Order_By = {\n  height?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  rank?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"txs_ranked\" */\nexport enum Txs_Ranked_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Neuron = 'neuron',\n  /** column name */\n  Rank = 'rank',\n  /** column name */\n  Type = 'type',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Txs_Ranked_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Txs_Ranked_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Txs_Ranked_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Txs_Ranked_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Txs_Ranked_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Txs_Ranked_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Txs_Ranked_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"txs_stats\" */\nexport type Txs_Stats = {\n  date?: Maybe<Scalars['date']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  msg_type?: Maybe<Scalars['String']['output']>;\n  pubkey?: Maybe<Scalars['jsonb']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n};\n\n\n/** columns and relationships of \"txs_stats\" */\nexport type Txs_StatsPubkeyArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregated selection of \"txs_stats\" */\nexport type Txs_Stats_Aggregate = {\n  aggregate?: Maybe<Txs_Stats_Aggregate_Fields>;\n  nodes: Array<Txs_Stats>;\n};\n\n/** aggregate fields of \"txs_stats\" */\nexport type Txs_Stats_Aggregate_Fields = {\n  avg?: Maybe<Txs_Stats_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Txs_Stats_Max_Fields>;\n  min?: Maybe<Txs_Stats_Min_Fields>;\n  stddev?: Maybe<Txs_Stats_Stddev_Fields>;\n  stddev_pop?: Maybe<Txs_Stats_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Txs_Stats_Stddev_Samp_Fields>;\n  sum?: Maybe<Txs_Stats_Sum_Fields>;\n  var_pop?: Maybe<Txs_Stats_Var_Pop_Fields>;\n  var_samp?: Maybe<Txs_Stats_Var_Samp_Fields>;\n  variance?: Maybe<Txs_Stats_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"txs_stats\" */\nexport type Txs_Stats_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Txs_Stats_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Txs_Stats_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"txs_stats\". All fields are combined with a logical 'AND'. */\nexport type Txs_Stats_Bool_Exp = {\n  _and?: InputMaybe<Array<Txs_Stats_Bool_Exp>>;\n  _not?: InputMaybe<Txs_Stats_Bool_Exp>;\n  _or?: InputMaybe<Array<Txs_Stats_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  msg_type?: InputMaybe<String_Comparison_Exp>;\n  pubkey?: InputMaybe<Jsonb_Comparison_Exp>;\n  rank?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Txs_Stats_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  msg_type?: Maybe<Scalars['String']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Txs_Stats_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  msg_type?: Maybe<Scalars['String']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"txs_stats\". */\nexport type Txs_Stats_Order_By = {\n  date?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  msg_type?: InputMaybe<Order_By>;\n  pubkey?: InputMaybe<Order_By>;\n  rank?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"txs_stats\" */\nexport enum Txs_Stats_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  MsgType = 'msg_type',\n  /** column name */\n  Pubkey = 'pubkey',\n  /** column name */\n  Rank = 'rank'\n}\n\n/** aggregate stddev on columns */\nexport type Txs_Stats_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Txs_Stats_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Txs_Stats_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Txs_Stats_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Txs_Stats_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Txs_Stats_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Txs_Stats_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"uptime\" */\nexport type Uptime = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  uptime?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"uptime\" */\nexport type Uptime_Aggregate = {\n  aggregate?: Maybe<Uptime_Aggregate_Fields>;\n  nodes: Array<Uptime>;\n};\n\n/** aggregate fields of \"uptime\" */\nexport type Uptime_Aggregate_Fields = {\n  avg?: Maybe<Uptime_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Uptime_Max_Fields>;\n  min?: Maybe<Uptime_Min_Fields>;\n  stddev?: Maybe<Uptime_Stddev_Fields>;\n  stddev_pop?: Maybe<Uptime_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Uptime_Stddev_Samp_Fields>;\n  sum?: Maybe<Uptime_Sum_Fields>;\n  var_pop?: Maybe<Uptime_Var_Pop_Fields>;\n  var_samp?: Maybe<Uptime_Var_Samp_Fields>;\n  variance?: Maybe<Uptime_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"uptime\" */\nexport type Uptime_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Uptime_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Uptime_Avg_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"uptime\". All fields are combined with a logical 'AND'. */\nexport type Uptime_Bool_Exp = {\n  _and?: InputMaybe<Array<Uptime_Bool_Exp>>;\n  _not?: InputMaybe<Uptime_Bool_Exp>;\n  _or?: InputMaybe<Array<Uptime_Bool_Exp>>;\n  consensus_address?: InputMaybe<String_Comparison_Exp>;\n  consensus_pubkey?: InputMaybe<String_Comparison_Exp>;\n  pre_commits?: InputMaybe<Bigint_Comparison_Exp>;\n  uptime?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Uptime_Max_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  uptime?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Uptime_Min_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  uptime?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"uptime\". */\nexport type Uptime_Order_By = {\n  consensus_address?: InputMaybe<Order_By>;\n  consensus_pubkey?: InputMaybe<Order_By>;\n  pre_commits?: InputMaybe<Order_By>;\n  uptime?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"uptime\" */\nexport enum Uptime_Select_Column {\n  /** column name */\n  ConsensusAddress = 'consensus_address',\n  /** column name */\n  ConsensusPubkey = 'consensus_pubkey',\n  /** column name */\n  PreCommits = 'pre_commits',\n  /** column name */\n  Uptime = 'uptime'\n}\n\n/** aggregate stddev on columns */\nexport type Uptime_Stddev_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Uptime_Stddev_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Uptime_Stddev_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Uptime_Sum_Fields = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  uptime?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Uptime_Var_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Uptime_Var_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Uptime_Variance_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"validator\" */\nexport type Validator = {\n  /** An array relationship */\n  blocks: Array<Block>;\n  /** An aggregate relationship */\n  blocks_aggregate: Block_Aggregate;\n  consensus_address: Scalars['String']['output'];\n  consensus_pubkey: Scalars['String']['output'];\n  /** An array relationship */\n  pre_commits: Array<Pre_Commit>;\n  /** An aggregate relationship */\n  pre_commits_aggregate: Pre_Commit_Aggregate;\n};\n\n\n/** columns and relationships of \"validator\" */\nexport type ValidatorBlocksArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"validator\" */\nexport type ValidatorBlocks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"validator\" */\nexport type ValidatorPre_CommitsArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"validator\" */\nexport type ValidatorPre_Commits_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n/** aggregated selection of \"validator\" */\nexport type Validator_Aggregate = {\n  aggregate?: Maybe<Validator_Aggregate_Fields>;\n  nodes: Array<Validator>;\n};\n\n/** aggregate fields of \"validator\" */\nexport type Validator_Aggregate_Fields = {\n  count: Scalars['Int']['output'];\n  max?: Maybe<Validator_Max_Fields>;\n  min?: Maybe<Validator_Min_Fields>;\n};\n\n\n/** aggregate fields of \"validator\" */\nexport type Validator_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Validator_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** Boolean expression to filter rows from the table \"validator\". All fields are combined with a logical 'AND'. */\nexport type Validator_Bool_Exp = {\n  _and?: InputMaybe<Array<Validator_Bool_Exp>>;\n  _not?: InputMaybe<Validator_Bool_Exp>;\n  _or?: InputMaybe<Array<Validator_Bool_Exp>>;\n  blocks?: InputMaybe<Block_Bool_Exp>;\n  consensus_address?: InputMaybe<String_Comparison_Exp>;\n  consensus_pubkey?: InputMaybe<String_Comparison_Exp>;\n  pre_commits?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Validator_Max_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Validator_Min_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"validator\". */\nexport type Validator_Order_By = {\n  blocks_aggregate?: InputMaybe<Block_Aggregate_Order_By>;\n  consensus_address?: InputMaybe<Order_By>;\n  consensus_pubkey?: InputMaybe<Order_By>;\n  pre_commits_aggregate?: InputMaybe<Pre_Commit_Aggregate_Order_By>;\n};\n\n/** select columns of table \"validator\" */\nexport enum Validator_Select_Column {\n  /** column name */\n  ConsensusAddress = 'consensus_address',\n  /** column name */\n  ConsensusPubkey = 'consensus_pubkey'\n}\n\n/** columns and relationships of \"volts_demand\" */\nexport type Volts_Demand = {\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n  volts?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregated selection of \"volts_demand\" */\nexport type Volts_Demand_Aggregate = {\n  aggregate?: Maybe<Volts_Demand_Aggregate_Fields>;\n  nodes: Array<Volts_Demand>;\n};\n\n/** aggregate fields of \"volts_demand\" */\nexport type Volts_Demand_Aggregate_Fields = {\n  avg?: Maybe<Volts_Demand_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Volts_Demand_Max_Fields>;\n  min?: Maybe<Volts_Demand_Min_Fields>;\n  stddev?: Maybe<Volts_Demand_Stddev_Fields>;\n  stddev_pop?: Maybe<Volts_Demand_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Volts_Demand_Stddev_Samp_Fields>;\n  sum?: Maybe<Volts_Demand_Sum_Fields>;\n  var_pop?: Maybe<Volts_Demand_Var_Pop_Fields>;\n  var_samp?: Maybe<Volts_Demand_Var_Samp_Fields>;\n  variance?: Maybe<Volts_Demand_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"volts_demand\" */\nexport type Volts_Demand_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Volts_Demand_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Volts_Demand_Avg_Fields = {\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n  volts?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"volts_demand\". All fields are combined with a logical 'AND'. */\nexport type Volts_Demand_Bool_Exp = {\n  _and?: InputMaybe<Array<Volts_Demand_Bool_Exp>>;\n  _not?: InputMaybe<Volts_Demand_Bool_Exp>;\n  _or?: InputMaybe<Array<Volts_Demand_Bool_Exp>>;\n  cyberlinks_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  volts?: InputMaybe<Float8_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Volts_Demand_Max_Fields = {\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n  volts?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Volts_Demand_Min_Fields = {\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n  volts?: Maybe<Scalars['float8']['output']>;\n};\n\n/** Ordering options when selecting data from \"volts_demand\". */\nexport type Volts_Demand_Order_By = {\n  cyberlinks_per_day?: InputMaybe<Order_By>;\n  date?: InputMaybe<Order_By>;\n  volts?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"volts_demand\" */\nexport enum Volts_Demand_Select_Column {\n  /** column name */\n  CyberlinksPerDay = 'cyberlinks_per_day',\n  /** column name */\n  Date = 'date',\n  /** column name */\n  Volts = 'volts'\n}\n\n/** aggregate stddev on columns */\nexport type Volts_Demand_Stddev_Fields = {\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n  volts?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Volts_Demand_Stddev_Pop_Fields = {\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n  volts?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Volts_Demand_Stddev_Samp_Fields = {\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n  volts?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Volts_Demand_Sum_Fields = {\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  volts?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Volts_Demand_Var_Pop_Fields = {\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n  volts?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Volts_Demand_Var_Samp_Fields = {\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n  volts?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Volts_Demand_Variance_Fields = {\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n  volts?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"volts_stats\" */\nexport type Volts_Stats = {\n  date?: Maybe<Scalars['date']['output']>;\n  volts?: Maybe<Scalars['float8']['output']>;\n  volts_per_day?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregated selection of \"volts_stats\" */\nexport type Volts_Stats_Aggregate = {\n  aggregate?: Maybe<Volts_Stats_Aggregate_Fields>;\n  nodes: Array<Volts_Stats>;\n};\n\n/** aggregate fields of \"volts_stats\" */\nexport type Volts_Stats_Aggregate_Fields = {\n  avg?: Maybe<Volts_Stats_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Volts_Stats_Max_Fields>;\n  min?: Maybe<Volts_Stats_Min_Fields>;\n  stddev?: Maybe<Volts_Stats_Stddev_Fields>;\n  stddev_pop?: Maybe<Volts_Stats_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Volts_Stats_Stddev_Samp_Fields>;\n  sum?: Maybe<Volts_Stats_Sum_Fields>;\n  var_pop?: Maybe<Volts_Stats_Var_Pop_Fields>;\n  var_samp?: Maybe<Volts_Stats_Var_Samp_Fields>;\n  variance?: Maybe<Volts_Stats_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"volts_stats\" */\nexport type Volts_Stats_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Volts_Stats_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Volts_Stats_Avg_Fields = {\n  volts?: Maybe<Scalars['Float']['output']>;\n  volts_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"volts_stats\". All fields are combined with a logical 'AND'. */\nexport type Volts_Stats_Bool_Exp = {\n  _and?: InputMaybe<Array<Volts_Stats_Bool_Exp>>;\n  _not?: InputMaybe<Volts_Stats_Bool_Exp>;\n  _or?: InputMaybe<Array<Volts_Stats_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  volts?: InputMaybe<Float8_Comparison_Exp>;\n  volts_per_day?: InputMaybe<Float8_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Volts_Stats_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  volts?: Maybe<Scalars['float8']['output']>;\n  volts_per_day?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Volts_Stats_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  volts?: Maybe<Scalars['float8']['output']>;\n  volts_per_day?: Maybe<Scalars['float8']['output']>;\n};\n\n/** Ordering options when selecting data from \"volts_stats\". */\nexport type Volts_Stats_Order_By = {\n  date?: InputMaybe<Order_By>;\n  volts?: InputMaybe<Order_By>;\n  volts_per_day?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"volts_stats\" */\nexport enum Volts_Stats_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  Volts = 'volts',\n  /** column name */\n  VoltsPerDay = 'volts_per_day'\n}\n\n/** aggregate stddev on columns */\nexport type Volts_Stats_Stddev_Fields = {\n  volts?: Maybe<Scalars['Float']['output']>;\n  volts_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Volts_Stats_Stddev_Pop_Fields = {\n  volts?: Maybe<Scalars['Float']['output']>;\n  volts_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Volts_Stats_Stddev_Samp_Fields = {\n  volts?: Maybe<Scalars['Float']['output']>;\n  volts_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate sum on columns */\nexport type Volts_Stats_Sum_Fields = {\n  volts?: Maybe<Scalars['float8']['output']>;\n  volts_per_day?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Volts_Stats_Var_Pop_Fields = {\n  volts?: Maybe<Scalars['Float']['output']>;\n  volts_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Volts_Stats_Var_Samp_Fields = {\n  volts?: Maybe<Scalars['Float']['output']>;\n  volts_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Volts_Stats_Variance_Fields = {\n  volts?: Maybe<Scalars['Float']['output']>;\n  volts_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\nexport type TransactionsSubscriptionVariables = Exact<{ [key: string]: never; }>;\n\n\nexport type TransactionsSubscription = { transaction: Array<{ success: boolean, messages: any, height: any, hash: string }> };\n\nexport type AccountCountQueryVariables = Exact<{ [key: string]: never; }>;\n\n\nexport type AccountCountQuery = { account_aggregate: { aggregate?: { count: number } | null } };\n\nexport type BlockByHeightQueryVariables = Exact<{\n  blockId?: InputMaybe<Scalars['bigint']['input']>;\n}>;\n\n\nexport type BlockByHeightQuery = { block: Array<{ hash: string, height: any, proposer_address?: string | null, timestamp: any, transactions: Array<{ messages: any, hash: string, height: any, success: boolean }> }> };\n\nexport type BlocksQueryVariables = Exact<{\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  where?: InputMaybe<Block_Bool_Exp>;\n}>;\n\n\nexport type BlocksQuery = { block: Array<{ hash: string, height: any, proposer_address?: string | null, timestamp: any, transactions_aggregate: { aggregate?: { count: number } | null } }> };\n\nexport type ContractsCountQueryVariables = Exact<{ [key: string]: never; }>;\n\n\nexport type ContractsCountQuery = { contracts_aggregate: { aggregate?: { count: number } | null } };\n\nexport type CyberlinksByParticleQueryVariables = Exact<{\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  orderBy?: InputMaybe<Array<Cyberlinks_Order_By> | Cyberlinks_Order_By>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n}>;\n\n\nexport type CyberlinksByParticleQuery = { cyberlinks: Array<{ timestamp: any, neuron: string, transaction_hash: string, from: string, to: string }> };\n\nexport type CyberlinksCountByNeuronQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['String']['input']>;\n  particles_from?: InputMaybe<Array<Scalars['String']['input']> | Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n}>;\n\n\nexport type CyberlinksCountByNeuronQuery = { cyberlinks_aggregate: { aggregate?: { count: number } | null } };\n\nexport type CyberlinksCountByParticleQueryVariables = Exact<{\n  cid?: InputMaybe<Scalars['String']['input']>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n}>;\n\n\nexport type CyberlinksCountByParticleQuery = { cyberlinks_aggregate: { aggregate?: { count: number } | null } };\n\nexport type MessagesByAddressCountQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['_text']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n}>;\n\n\nexport type MessagesByAddressCountQuery = { messages_by_address_aggregate: { aggregate?: { count: number } | null } };\n\nexport type MessagesByAddressSenseQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['_text']['input']>;\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  timestamp_from?: InputMaybe<Scalars['timestamp']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n  order_direction?: InputMaybe<Order_By>;\n}>;\n\n\nexport type MessagesByAddressSenseQuery = { messages_by_address: Array<{ transaction_hash: string, index: any, value: any, type: string, transaction: { success: boolean, memo?: string | null, block: { timestamp: any, height: any } } }> };\n\nexport type MessagesByAddressSenseWsSubscriptionVariables = Exact<{\n  address?: InputMaybe<Scalars['_text']['input']>;\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  timestamp_from?: InputMaybe<Scalars['timestamp']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n  order_direction?: InputMaybe<Order_By>;\n}>;\n\n\nexport type MessagesByAddressSenseWsSubscription = { messages_by_address: Array<{ transaction_hash: string, index: any, value: any, type: string, transaction: { success: boolean, memo?: string | null, block: { timestamp: any, height: any } } }> };\n\nexport type TransactionCountQueryVariables = Exact<{ [key: string]: never; }>;\n\n\nexport type TransactionCountQuery = { transaction_aggregate: { aggregate?: { count: number } | null } };\n\nexport type UptimeByAddressQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['String']['input']>;\n}>;\n\n\nexport type UptimeByAddressQuery = { uptime: Array<{ uptime?: any | null }> };\n\nexport type WasmDashboardPageQueryVariables = Exact<{\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n}>;\n\n\nexport type WasmDashboardPageQuery = { contracts: Array<{ address: string, admin: string, code_id: any, creator: string, fees: any, gas: any, label: string, tx: any }>, contracts_aggregate: { aggregate?: { count: number, sum?: { gas?: any | null, fees?: any | null, tx?: any | null } | null } | null } };\n\nexport type MessagesByAddressQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['_text']['input']>;\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n}>;\n\n\nexport type MessagesByAddressQuery = { messages_by_address: Array<{ transaction_hash: string, value: any, type: string, transaction: { success: boolean, height: any, logs?: any | null, memo?: string | null, block: { timestamp: any } } }> };\n\n\nexport const TransactionsDocument = gql`\n    subscription Transactions {\n  transaction(offset: 0, limit: 200, order_by: {height: desc}) {\n    success\n    messages\n    height\n    hash\n  }\n}\n    `;\n\n/**\n * __useTransactionsSubscription__\n *\n * To run a query within a React component, call `useTransactionsSubscription` and pass it any options that fit your needs.\n * When your component renders, `useTransactionsSubscription` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the subscription, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useTransactionsSubscription({\n *   variables: {\n *   },\n * });\n */\nexport function useTransactionsSubscription(baseOptions?: Apollo.SubscriptionHookOptions<TransactionsSubscription, TransactionsSubscriptionVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useSubscription<TransactionsSubscription, TransactionsSubscriptionVariables>(TransactionsDocument, options);\n      }\nexport type TransactionsSubscriptionHookResult = ReturnType<typeof useTransactionsSubscription>;\nexport type TransactionsSubscriptionResult = Apollo.SubscriptionResult<TransactionsSubscription>;\nexport const AccountCountDocument = gql`\n    query accountCount {\n  account_aggregate {\n    aggregate {\n      count(columns: address)\n    }\n  }\n}\n    `;\n\n/**\n * __useAccountCountQuery__\n *\n * To run a query within a React component, call `useAccountCountQuery` and pass it any options that fit your needs.\n * When your component renders, `useAccountCountQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useAccountCountQuery({\n *   variables: {\n *   },\n * });\n */\nexport function useAccountCountQuery(baseOptions?: Apollo.QueryHookOptions<AccountCountQuery, AccountCountQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<AccountCountQuery, AccountCountQueryVariables>(AccountCountDocument, options);\n      }\nexport function useAccountCountLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<AccountCountQuery, AccountCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<AccountCountQuery, AccountCountQueryVariables>(AccountCountDocument, options);\n        }\nexport function useAccountCountSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<AccountCountQuery, AccountCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<AccountCountQuery, AccountCountQueryVariables>(AccountCountDocument, options);\n        }\nexport type AccountCountQueryHookResult = ReturnType<typeof useAccountCountQuery>;\nexport type AccountCountLazyQueryHookResult = ReturnType<typeof useAccountCountLazyQuery>;\nexport type AccountCountSuspenseQueryHookResult = ReturnType<typeof useAccountCountSuspenseQuery>;\nexport type AccountCountQueryResult = Apollo.QueryResult<AccountCountQuery, AccountCountQueryVariables>;\nexport const BlockByHeightDocument = gql`\n    query blockByHeight($blockId: bigint) {\n  block(where: {height: {_eq: $blockId}}) {\n    hash\n    height\n    proposer_address\n    timestamp\n    transactions {\n      messages\n      hash\n      height\n      success\n    }\n  }\n}\n    `;\n\n/**\n * __useBlockByHeightQuery__\n *\n * To run a query within a React component, call `useBlockByHeightQuery` and pass it any options that fit your needs.\n * When your component renders, `useBlockByHeightQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useBlockByHeightQuery({\n *   variables: {\n *      blockId: // value for 'blockId'\n *   },\n * });\n */\nexport function useBlockByHeightQuery(baseOptions?: Apollo.QueryHookOptions<BlockByHeightQuery, BlockByHeightQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<BlockByHeightQuery, BlockByHeightQueryVariables>(BlockByHeightDocument, options);\n      }\nexport function useBlockByHeightLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<BlockByHeightQuery, BlockByHeightQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<BlockByHeightQuery, BlockByHeightQueryVariables>(BlockByHeightDocument, options);\n        }\nexport function useBlockByHeightSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<BlockByHeightQuery, BlockByHeightQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<BlockByHeightQuery, BlockByHeightQueryVariables>(BlockByHeightDocument, options);\n        }\nexport type BlockByHeightQueryHookResult = ReturnType<typeof useBlockByHeightQuery>;\nexport type BlockByHeightLazyQueryHookResult = ReturnType<typeof useBlockByHeightLazyQuery>;\nexport type BlockByHeightSuspenseQueryHookResult = ReturnType<typeof useBlockByHeightSuspenseQuery>;\nexport type BlockByHeightQueryResult = Apollo.QueryResult<BlockByHeightQuery, BlockByHeightQueryVariables>;\nexport const BlocksDocument = gql`\n    query blocks($limit: Int, $offset: Int, $where: block_bool_exp) {\n  block(where: $where, limit: $limit, offset: $offset, order_by: {height: desc}) {\n    hash\n    height\n    proposer_address\n    transactions_aggregate {\n      aggregate {\n        count\n      }\n    }\n    timestamp\n  }\n}\n    `;\n\n/**\n * __useBlocksQuery__\n *\n * To run a query within a React component, call `useBlocksQuery` and pass it any options that fit your needs.\n * When your component renders, `useBlocksQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useBlocksQuery({\n *   variables: {\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      where: // value for 'where'\n *   },\n * });\n */\nexport function useBlocksQuery(baseOptions?: Apollo.QueryHookOptions<BlocksQuery, BlocksQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<BlocksQuery, BlocksQueryVariables>(BlocksDocument, options);\n      }\nexport function useBlocksLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<BlocksQuery, BlocksQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<BlocksQuery, BlocksQueryVariables>(BlocksDocument, options);\n        }\nexport function useBlocksSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<BlocksQuery, BlocksQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<BlocksQuery, BlocksQueryVariables>(BlocksDocument, options);\n        }\nexport type BlocksQueryHookResult = ReturnType<typeof useBlocksQuery>;\nexport type BlocksLazyQueryHookResult = ReturnType<typeof useBlocksLazyQuery>;\nexport type BlocksSuspenseQueryHookResult = ReturnType<typeof useBlocksSuspenseQuery>;\nexport type BlocksQueryResult = Apollo.QueryResult<BlocksQuery, BlocksQueryVariables>;\nexport const ContractsCountDocument = gql`\n    query contractsCount {\n  contracts_aggregate {\n    aggregate {\n      count\n    }\n  }\n}\n    `;\n\n/**\n * __useContractsCountQuery__\n *\n * To run a query within a React component, call `useContractsCountQuery` and pass it any options that fit your needs.\n * When your component renders, `useContractsCountQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useContractsCountQuery({\n *   variables: {\n *   },\n * });\n */\nexport function useContractsCountQuery(baseOptions?: Apollo.QueryHookOptions<ContractsCountQuery, ContractsCountQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<ContractsCountQuery, ContractsCountQueryVariables>(ContractsCountDocument, options);\n      }\nexport function useContractsCountLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<ContractsCountQuery, ContractsCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<ContractsCountQuery, ContractsCountQueryVariables>(ContractsCountDocument, options);\n        }\nexport function useContractsCountSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<ContractsCountQuery, ContractsCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<ContractsCountQuery, ContractsCountQueryVariables>(ContractsCountDocument, options);\n        }\nexport type ContractsCountQueryHookResult = ReturnType<typeof useContractsCountQuery>;\nexport type ContractsCountLazyQueryHookResult = ReturnType<typeof useContractsCountLazyQuery>;\nexport type ContractsCountSuspenseQueryHookResult = ReturnType<typeof useContractsCountSuspenseQuery>;\nexport type ContractsCountQueryResult = Apollo.QueryResult<ContractsCountQuery, ContractsCountQueryVariables>;\nexport const CyberlinksByParticleDocument = gql`\n    query CyberlinksByParticle($limit: Int, $offset: Int, $orderBy: [cyberlinks_order_by!], $where: cyberlinks_bool_exp) {\n  cyberlinks(limit: $limit, offset: $offset, order_by: $orderBy, where: $where) {\n    from: particle_from\n    to: particle_to\n    timestamp\n    neuron\n    transaction_hash\n  }\n}\n    `;\n\n/**\n * __useCyberlinksByParticleQuery__\n *\n * To run a query within a React component, call `useCyberlinksByParticleQuery` and pass it any options that fit your needs.\n * When your component renders, `useCyberlinksByParticleQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useCyberlinksByParticleQuery({\n *   variables: {\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      orderBy: // value for 'orderBy'\n *      where: // value for 'where'\n *   },\n * });\n */\nexport function useCyberlinksByParticleQuery(baseOptions?: Apollo.QueryHookOptions<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>(CyberlinksByParticleDocument, options);\n      }\nexport function useCyberlinksByParticleLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>(CyberlinksByParticleDocument, options);\n        }\nexport function useCyberlinksByParticleSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>(CyberlinksByParticleDocument, options);\n        }\nexport type CyberlinksByParticleQueryHookResult = ReturnType<typeof useCyberlinksByParticleQuery>;\nexport type CyberlinksByParticleLazyQueryHookResult = ReturnType<typeof useCyberlinksByParticleLazyQuery>;\nexport type CyberlinksByParticleSuspenseQueryHookResult = ReturnType<typeof useCyberlinksByParticleSuspenseQuery>;\nexport type CyberlinksByParticleQueryResult = Apollo.QueryResult<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>;\nexport const CyberlinksCountByNeuronDocument = gql`\n    query CyberlinksCountByNeuron($address: String, $particles_from: [String!], $timestamp: timestamp) {\n  cyberlinks_aggregate(\n    where: {_and: [{neuron: {_eq: $address}}, {particle_from: {_in: $particles_from}}, {timestamp: {_gt: $timestamp}}]}\n  ) {\n    aggregate {\n      count\n    }\n  }\n}\n    `;\n\n/**\n * __useCyberlinksCountByNeuronQuery__\n *\n * To run a query within a React component, call `useCyberlinksCountByNeuronQuery` and pass it any options that fit your needs.\n * When your component renders, `useCyberlinksCountByNeuronQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useCyberlinksCountByNeuronQuery({\n *   variables: {\n *      address: // value for 'address'\n *      particles_from: // value for 'particles_from'\n *      timestamp: // value for 'timestamp'\n *   },\n * });\n */\nexport function useCyberlinksCountByNeuronQuery(baseOptions?: Apollo.QueryHookOptions<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>(CyberlinksCountByNeuronDocument, options);\n      }\nexport function useCyberlinksCountByNeuronLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>(CyberlinksCountByNeuronDocument, options);\n        }\nexport function useCyberlinksCountByNeuronSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>(CyberlinksCountByNeuronDocument, options);\n        }\nexport type CyberlinksCountByNeuronQueryHookResult = ReturnType<typeof useCyberlinksCountByNeuronQuery>;\nexport type CyberlinksCountByNeuronLazyQueryHookResult = ReturnType<typeof useCyberlinksCountByNeuronLazyQuery>;\nexport type CyberlinksCountByNeuronSuspenseQueryHookResult = ReturnType<typeof useCyberlinksCountByNeuronSuspenseQuery>;\nexport type CyberlinksCountByNeuronQueryResult = Apollo.QueryResult<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>;\nexport const CyberlinksCountByParticleDocument = gql`\n    query cyberlinksCountByParticle($cid: String, $where: cyberlinks_bool_exp) {\n  cyberlinks_aggregate(where: $where) {\n    aggregate {\n      count\n    }\n  }\n}\n    `;\n\n/**\n * __useCyberlinksCountByParticleQuery__\n *\n * To run a query within a React component, call `useCyberlinksCountByParticleQuery` and pass it any options that fit your needs.\n * When your component renders, `useCyberlinksCountByParticleQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useCyberlinksCountByParticleQuery({\n *   variables: {\n *      cid: // value for 'cid'\n *      where: // value for 'where'\n *   },\n * });\n */\nexport function useCyberlinksCountByParticleQuery(baseOptions?: Apollo.QueryHookOptions<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>(CyberlinksCountByParticleDocument, options);\n      }\nexport function useCyberlinksCountByParticleLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>(CyberlinksCountByParticleDocument, options);\n        }\nexport function useCyberlinksCountByParticleSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>(CyberlinksCountByParticleDocument, options);\n        }\nexport type CyberlinksCountByParticleQueryHookResult = ReturnType<typeof useCyberlinksCountByParticleQuery>;\nexport type CyberlinksCountByParticleLazyQueryHookResult = ReturnType<typeof useCyberlinksCountByParticleLazyQuery>;\nexport type CyberlinksCountByParticleSuspenseQueryHookResult = ReturnType<typeof useCyberlinksCountByParticleSuspenseQuery>;\nexport type CyberlinksCountByParticleQueryResult = Apollo.QueryResult<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>;\nexport const MessagesByAddressCountDocument = gql`\n    query MessagesByAddressCount($address: _text, $timestamp: timestamp) {\n  messages_by_address_aggregate(\n    args: {addresses: $address, limit: \"100000000\", offset: \"0\", types: \"{}\"}\n    where: {transaction: {block: {timestamp: {_gt: $timestamp}}}}\n  ) {\n    aggregate {\n      count\n    }\n  }\n}\n    `;\n\n/**\n * __useMessagesByAddressCountQuery__\n *\n * To run a query within a React component, call `useMessagesByAddressCountQuery` and pass it any options that fit your needs.\n * When your component renders, `useMessagesByAddressCountQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useMessagesByAddressCountQuery({\n *   variables: {\n *      address: // value for 'address'\n *      timestamp: // value for 'timestamp'\n *   },\n * });\n */\nexport function useMessagesByAddressCountQuery(baseOptions?: Apollo.QueryHookOptions<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>(MessagesByAddressCountDocument, options);\n      }\nexport function useMessagesByAddressCountLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>(MessagesByAddressCountDocument, options);\n        }\nexport function useMessagesByAddressCountSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>(MessagesByAddressCountDocument, options);\n        }\nexport type MessagesByAddressCountQueryHookResult = ReturnType<typeof useMessagesByAddressCountQuery>;\nexport type MessagesByAddressCountLazyQueryHookResult = ReturnType<typeof useMessagesByAddressCountLazyQuery>;\nexport type MessagesByAddressCountSuspenseQueryHookResult = ReturnType<typeof useMessagesByAddressCountSuspenseQuery>;\nexport type MessagesByAddressCountQueryResult = Apollo.QueryResult<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>;\nexport const MessagesByAddressSenseDocument = gql`\n    query MessagesByAddressSense($address: _text, $limit: bigint, $offset: bigint, $timestamp_from: timestamp, $types: _text, $order_direction: order_by) {\n  messages_by_address(\n    args: {addresses: $address, limit: $limit, offset: $offset, types: $types}\n    order_by: {transaction: {block: {timestamp: $order_direction}}}\n    where: {transaction: {block: {timestamp: {_gt: $timestamp_from}}}}\n  ) {\n    transaction_hash\n    index\n    value\n    transaction {\n      success\n      block {\n        timestamp\n        height\n      }\n      memo\n    }\n    type\n  }\n}\n    `;\n\n/**\n * __useMessagesByAddressSenseQuery__\n *\n * To run a query within a React component, call `useMessagesByAddressSenseQuery` and pass it any options that fit your needs.\n * When your component renders, `useMessagesByAddressSenseQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useMessagesByAddressSenseQuery({\n *   variables: {\n *      address: // value for 'address'\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      timestamp_from: // value for 'timestamp_from'\n *      types: // value for 'types'\n *      order_direction: // value for 'order_direction'\n *   },\n * });\n */\nexport function useMessagesByAddressSenseQuery(baseOptions?: Apollo.QueryHookOptions<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>(MessagesByAddressSenseDocument, options);\n      }\nexport function useMessagesByAddressSenseLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>(MessagesByAddressSenseDocument, options);\n        }\nexport function useMessagesByAddressSenseSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>(MessagesByAddressSenseDocument, options);\n        }\nexport type MessagesByAddressSenseQueryHookResult = ReturnType<typeof useMessagesByAddressSenseQuery>;\nexport type MessagesByAddressSenseLazyQueryHookResult = ReturnType<typeof useMessagesByAddressSenseLazyQuery>;\nexport type MessagesByAddressSenseSuspenseQueryHookResult = ReturnType<typeof useMessagesByAddressSenseSuspenseQuery>;\nexport type MessagesByAddressSenseQueryResult = Apollo.QueryResult<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>;\nexport const MessagesByAddressSenseWsDocument = gql`\n    subscription MessagesByAddressSenseWs($address: _text, $limit: bigint, $offset: bigint, $timestamp_from: timestamp, $types: _text, $order_direction: order_by) {\n  messages_by_address(\n    args: {addresses: $address, limit: $limit, offset: $offset, types: $types}\n    order_by: {transaction: {block: {timestamp: $order_direction}}}\n    where: {transaction: {block: {timestamp: {_gt: $timestamp_from}}}}\n  ) {\n    transaction_hash\n    index\n    value\n    transaction {\n      success\n      block {\n        timestamp\n        height\n      }\n      memo\n    }\n    type\n  }\n}\n    `;\n\n/**\n * __useMessagesByAddressSenseWsSubscription__\n *\n * To run a query within a React component, call `useMessagesByAddressSenseWsSubscription` and pass it any options that fit your needs.\n * When your component renders, `useMessagesByAddressSenseWsSubscription` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the subscription, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useMessagesByAddressSenseWsSubscription({\n *   variables: {\n *      address: // value for 'address'\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      timestamp_from: // value for 'timestamp_from'\n *      types: // value for 'types'\n *      order_direction: // value for 'order_direction'\n *   },\n * });\n */\nexport function useMessagesByAddressSenseWsSubscription(baseOptions?: Apollo.SubscriptionHookOptions<MessagesByAddressSenseWsSubscription, MessagesByAddressSenseWsSubscriptionVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useSubscription<MessagesByAddressSenseWsSubscription, MessagesByAddressSenseWsSubscriptionVariables>(MessagesByAddressSenseWsDocument, options);\n      }\nexport type MessagesByAddressSenseWsSubscriptionHookResult = ReturnType<typeof useMessagesByAddressSenseWsSubscription>;\nexport type MessagesByAddressSenseWsSubscriptionResult = Apollo.SubscriptionResult<MessagesByAddressSenseWsSubscription>;\nexport const TransactionCountDocument = gql`\n    query transactionCount {\n  transaction_aggregate {\n    aggregate {\n      count(columns: hash)\n    }\n  }\n}\n    `;\n\n/**\n * __useTransactionCountQuery__\n *\n * To run a query within a React component, call `useTransactionCountQuery` and pass it any options that fit your needs.\n * When your component renders, `useTransactionCountQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useTransactionCountQuery({\n *   variables: {\n *   },\n * });\n */\nexport function useTransactionCountQuery(baseOptions?: Apollo.QueryHookOptions<TransactionCountQuery, TransactionCountQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<TransactionCountQuery, TransactionCountQueryVariables>(TransactionCountDocument, options);\n      }\nexport function useTransactionCountLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<TransactionCountQuery, TransactionCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<TransactionCountQuery, TransactionCountQueryVariables>(TransactionCountDocument, options);\n        }\nexport function useTransactionCountSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<TransactionCountQuery, TransactionCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<TransactionCountQuery, TransactionCountQueryVariables>(TransactionCountDocument, options);\n        }\nexport type TransactionCountQueryHookResult = ReturnType<typeof useTransactionCountQuery>;\nexport type TransactionCountLazyQueryHookResult = ReturnType<typeof useTransactionCountLazyQuery>;\nexport type TransactionCountSuspenseQueryHookResult = ReturnType<typeof useTransactionCountSuspenseQuery>;\nexport type TransactionCountQueryResult = Apollo.QueryResult<TransactionCountQuery, TransactionCountQueryVariables>;\nexport const UptimeByAddressDocument = gql`\n    query uptimeByAddress($address: String) {\n  uptime(where: {consensus_address: {_eq: $address}}) {\n    uptime\n  }\n}\n    `;\n\n/**\n * __useUptimeByAddressQuery__\n *\n * To run a query within a React component, call `useUptimeByAddressQuery` and pass it any options that fit your needs.\n * When your component renders, `useUptimeByAddressQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useUptimeByAddressQuery({\n *   variables: {\n *      address: // value for 'address'\n *   },\n * });\n */\nexport function useUptimeByAddressQuery(baseOptions?: Apollo.QueryHookOptions<UptimeByAddressQuery, UptimeByAddressQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<UptimeByAddressQuery, UptimeByAddressQueryVariables>(UptimeByAddressDocument, options);\n      }\nexport function useUptimeByAddressLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<UptimeByAddressQuery, UptimeByAddressQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<UptimeByAddressQuery, UptimeByAddressQueryVariables>(UptimeByAddressDocument, options);\n        }\nexport function useUptimeByAddressSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<UptimeByAddressQuery, UptimeByAddressQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<UptimeByAddressQuery, UptimeByAddressQueryVariables>(UptimeByAddressDocument, options);\n        }\nexport type UptimeByAddressQueryHookResult = ReturnType<typeof useUptimeByAddressQuery>;\nexport type UptimeByAddressLazyQueryHookResult = ReturnType<typeof useUptimeByAddressLazyQuery>;\nexport type UptimeByAddressSuspenseQueryHookResult = ReturnType<typeof useUptimeByAddressSuspenseQuery>;\nexport type UptimeByAddressQueryResult = Apollo.QueryResult<UptimeByAddressQuery, UptimeByAddressQueryVariables>;\nexport const WasmDashboardPageDocument = gql`\n    query wasmDashboardPage($offset: Int, $limit: Int) {\n  contracts(limit: $limit, offset: $offset, order_by: {tx: desc}) {\n    address\n    admin\n    code_id\n    creator\n    fees\n    gas\n    label\n    tx\n  }\n  contracts_aggregate {\n    aggregate {\n      sum {\n        gas\n        fees\n        tx\n      }\n      count(columns: address)\n    }\n  }\n}\n    `;\n\n/**\n * __useWasmDashboardPageQuery__\n *\n * To run a query within a React component, call `useWasmDashboardPageQuery` and pass it any options that fit your needs.\n * When your component renders, `useWasmDashboardPageQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useWasmDashboardPageQuery({\n *   variables: {\n *      offset: // value for 'offset'\n *      limit: // value for 'limit'\n *   },\n * });\n */\nexport function useWasmDashboardPageQuery(baseOptions?: Apollo.QueryHookOptions<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>(WasmDashboardPageDocument, options);\n      }\nexport function useWasmDashboardPageLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>(WasmDashboardPageDocument, options);\n        }\nexport function useWasmDashboardPageSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>(WasmDashboardPageDocument, options);\n        }\nexport type WasmDashboardPageQueryHookResult = ReturnType<typeof useWasmDashboardPageQuery>;\nexport type WasmDashboardPageLazyQueryHookResult = ReturnType<typeof useWasmDashboardPageLazyQuery>;\nexport type WasmDashboardPageSuspenseQueryHookResult = ReturnType<typeof useWasmDashboardPageSuspenseQuery>;\nexport type WasmDashboardPageQueryResult = Apollo.QueryResult<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>;\nexport const MessagesByAddressDocument = gql`\n    query MessagesByAddress($address: _text, $limit: bigint, $offset: bigint, $types: _text) {\n  messages_by_address(\n    args: {addresses: $address, limit: $limit, offset: $offset, types: $types}\n    order_by: {transaction: {block: {height: desc}}}\n  ) {\n    transaction_hash\n    value\n    transaction {\n      success\n      height\n      logs\n      memo\n      block {\n        timestamp\n      }\n    }\n    type\n  }\n}\n    `;\n\n/**\n * __useMessagesByAddressQuery__\n *\n * To run a query within a React component, call `useMessagesByAddressQuery` and pass it any options that fit your needs.\n * When your component renders, `useMessagesByAddressQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useMessagesByAddressQuery({\n *   variables: {\n *      address: // value for 'address'\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      types: // value for 'types'\n *   },\n * });\n */\nexport function useMessagesByAddressQuery(baseOptions?: Apollo.QueryHookOptions<MessagesByAddressQuery, MessagesByAddressQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<MessagesByAddressQuery, MessagesByAddressQueryVariables>(MessagesByAddressDocument, options);\n      }\nexport function useMessagesByAddressLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<MessagesByAddressQuery, MessagesByAddressQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<MessagesByAddressQuery, MessagesByAddressQueryVariables>(MessagesByAddressDocument, options);\n        }\nexport function useMessagesByAddressSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<MessagesByAddressQuery, MessagesByAddressQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<MessagesByAddressQuery, MessagesByAddressQueryVariables>(MessagesByAddressDocument, options);\n        }\nexport type MessagesByAddressQueryHookResult = ReturnType<typeof useMessagesByAddressQuery>;\nexport type MessagesByAddressLazyQueryHookResult = ReturnType<typeof useMessagesByAddressLazyQuery>;\nexport type MessagesByAddressSuspenseQueryHookResult = ReturnType<typeof useMessagesByAddressSuspenseQuery>;\nexport type MessagesByAddressQueryResult = Apollo.QueryResult<MessagesByAddressQuery, MessagesByAddressQueryVariables>;","import { Coin } from 'cosmjs-types/cosmos/base/v1beta1/coin';\nimport { CyberLinkSimple, NeuronAddress } from 'src/types/base';\n\ninterface GenericIndexerTransaction<T> {\n  value: T;\n  type: string;\n  transaction_hash: string;\n  index: number;\n  transaction: {\n    memo?: string;\n    success: boolean;\n    block: {\n      timestamp: string;\n    };\n  };\n}\nexport const MSG_SEND_TRANSACTION_TYPE = 'cosmos.bank.v1beta1.MsgSend';\n\nexport const MSG_MULTI_SEND_TRANSACTION_TYPE =\n  'cosmos.bank.v1beta1.MsgMultiSend';\n\nexport const CYBER_LINK_TRANSACTION_TYPE = 'cyber.graph.v1beta1.MsgCyberlink';\n\ninterface Input {\n  address: NeuronAddress;\n  coins: Coin[];\n}\n\ninterface Output {\n  address: NeuronAddress;\n  coins: Coin[];\n}\n\nexport interface MsgMultiSendValue {\n  inputs: Input[];\n  outputs: Output[];\n}\n\nexport interface MsgSendValue {\n  amount: Coin[];\n  from_address: NeuronAddress;\n  to_address: NeuronAddress;\n}\n\ninterface MsgDelegateValue {\n  amount: Coin;\n  delegator_address: NeuronAddress;\n  validator_address: NeuronAddress;\n}\n\nexport interface CyberLinkValue {\n  neuron: NeuronAddress;\n  links: CyberLinkSimple[];\n}\n\nexport interface CyberLinkTransaction\n  extends GenericIndexerTransaction<CyberLinkValue> {\n  type: typeof CYBER_LINK_TRANSACTION_TYPE;\n}\n\nexport interface MsgMultiSendTransaction\n  extends GenericIndexerTransaction<MsgMultiSendValue> {\n  type: typeof MSG_MULTI_SEND_TRANSACTION_TYPE;\n}\n\nexport interface MsgSendTransaction\n  extends GenericIndexerTransaction<MsgSendValue> {\n  type: typeof MSG_SEND_TRANSACTION_TYPE;\n}\n\nexport type Transaction =\n  // | DelegateTransaction\n  CyberLinkTransaction | MsgMultiSendTransaction | MsgSendTransaction;\n","import { Tx } from 'cosmjs-types/cosmos/tx/v1beta1/tx';\nimport { MsgSend, MsgMultiSend } from 'cosmjs-types/cosmos/bank/v1beta1/tx';\n\nimport { fromBase64 } from '@cosmjs/encoding';\nimport {\n  MSG_MULTI_SEND_TRANSACTION_TYPE,\n  MSG_SEND_TRANSACTION_TYPE,\n} from 'src/services/backend/services/indexer/types';\nimport { NeuronAddress } from 'src/types/base';\nimport { TransactionDto } from 'src/services/CozoDb/types/dto';\nimport { getNowUtcNumber } from 'src/utils/date';\n\n// eslint-disable-next-line import/no-unused-modules\nexport const extractTxData = (data: string) => {\n  const result = Tx.decode(fromBase64(data));\n  const memo = result.body?.memo;\n  const messages = result.body?.messages\n    .map((message) => {\n      const msgType = message.typeUrl.slice(1);\n      if (msgType === MSG_SEND_TRANSACTION_TYPE) {\n        return MsgSend.decode(message.value);\n      }\n\n      if (msgType === MSG_MULTI_SEND_TRANSACTION_TYPE) {\n        return MsgMultiSend.decode(message.value);\n      }\n      return undefined;\n    })\n    .filter((message) => message !== undefined);\n\n  return { memo, messages };\n};\n\n// eslint-disable-next-line import/no-unused-modules\nexport const mapWebsocketTxToTransactions = (\n  neuron: NeuronAddress,\n  result: any\n) => {\n  const { data, events } = result;\n\n  const hash = events['tx.hash'][0];\n  const transactionType = events['message.action'][0].slice(1);\n  const timestamp = getNowUtcNumber();\n  const blockHeight = events['tx.height'][0];\n\n  const { memo = '', messages } = extractTxData(data.value.TxResult.tx);\n\n  const transactions: TransactionDto[] = [];\n  messages!.forEach((message, index) => {\n    transactions.push({\n      hash,\n      index,\n      type: transactionType,\n      timestamp,\n      success: true,\n      value: message!,\n      memo,\n      neuron,\n      blockHeight,\n    });\n  });\n\n  return transactions;\n};\n","import { ApolloClient, DocumentNode, InMemoryCache } from '@apollo/client';\n\nimport { GraphQLWsLink } from '@apollo/client/link/subscriptions';\nimport { GraphQLClient } from 'graphql-request';\nimport { createClient } from 'graphql-ws';\nimport { Observable } from 'rxjs';\nimport { INDEX_WEBSOCKET, INDEX_HTTPS } from 'src/constants/config';\n\nconst cyberGraphQLWsLink = new GraphQLWsLink(\n  createClient({\n    url: INDEX_WEBSOCKET,\n    shouldRetry: (errOrCloseEvent: unknown) => true,\n    retryAttempts: 10,\n    retryWait: async (retries: number): Promise<void> => {\n      setTimeout(() => Promise.resolve(), Math.min(1000 * 2 ** retries, 10000));\n    },\n    // on: {\n    //   error: (err) => {\n    //     console.log('---ws errr', err);\n    //   },\n    //   message: (msg) => {\n    //     console.log('---ws message', msg);\n    //   },\n    //   // Handle connection opened event\n    //   opened: () => {\n    //     console.log('---ws opened');\n    //   },\n    //   // Handle connection closed event\n    //   closed: () => {\n    //     console.log('---ws closed');\n    //   },\n    // },\n  })\n);\n\nexport const createIndexerClient = (abortSignal: AbortSignal) =>\n  new GraphQLClient(INDEX_HTTPS, {\n    signal: abortSignal,\n  });\n\n// eslint-disable-next-line import/no-unused-modules\nexport function createIndexerWebsocket<T>(\n  query: DocumentNode,\n  variables: object\n): Observable<T> {\n  const client = new ApolloClient({\n    link: cyberGraphQLWsLink,\n    cache: new InMemoryCache(),\n  });\n\n  const apolloObservable = client.subscribe({ query, variables });\n  return new Observable((subscriber) => {\n    const subscription = apolloObservable.subscribe({\n      next(result) {\n        subscriber.next(result.data as T);\n      },\n      error(err) {\n        subscriber.error(err);\n      },\n      complete() {\n        subscriber.complete();\n      },\n    });\n\n    // Cleanup subscription on unsubscribe\n    return () => subscription.unsubscribe();\n  });\n}\n","/* eslint-disable import/no-unused-modules */\n\nimport { ParticleCid, NeuronAddress } from 'src/types/base';\nimport { numberToUtcDate } from 'src/utils/date';\n\nimport { CYBERLINKS_BATCH_LIMIT } from './consts';\nimport { createIndexerClient } from './utils/graphqlClient';\nimport { fetchIterableByOffset } from 'src/utils/async/iterable';\nimport {\n  CyberlinksByParticleDocument,\n  CyberlinksByParticleQuery,\n  CyberlinksByParticleQueryVariables,\n  CyberlinksCountByNeuronDocument,\n  CyberlinksCountByNeuronQuery,\n  CyberlinksCountByNeuronQueryVariables,\n  Order_By,\n} from 'src/generated/graphql';\n\nconst fetchCyberlinks = async ({\n  particleCid,\n  timestampFrom,\n  offset = 0,\n  abortSignal,\n}: {\n  particleCid: ParticleCid;\n  timestampFrom: number;\n  offset?: number;\n  abortSignal: AbortSignal;\n}) => {\n  const res = await createIndexerClient(abortSignal).request<\n    CyberlinksByParticleQuery,\n    CyberlinksByParticleQueryVariables\n  >(CyberlinksByParticleDocument, {\n    limit: CYBERLINKS_BATCH_LIMIT,\n    offset,\n    orderBy: [{ timestamp: Order_By.Asc }],\n    where: {\n      _or: [\n        { particle_to: { _eq: particleCid } },\n        { particle_from: { _eq: particleCid } },\n      ],\n      timestamp: { _gt: numberToUtcDate(timestampFrom) },\n    },\n  });\n\n  return res.cyberlinks;\n};\n\nconst fetchCyberlinksCount = async (\n  address: NeuronAddress,\n  particlesFrom: ParticleCid[],\n  timestampFrom: number,\n  abortSignal: AbortSignal\n) => {\n  const res = await createIndexerClient(abortSignal).request<\n    CyberlinksCountByNeuronQuery,\n    CyberlinksCountByNeuronQueryVariables\n  >(CyberlinksCountByNeuronDocument, {\n    address,\n    particles_from: particlesFrom,\n    timestamp: numberToUtcDate(timestampFrom),\n  });\n\n  return res.cyberlinks_aggregate.aggregate?.count;\n};\n\nconst fetchCyberlinksByNeroun = async ({\n  neuron,\n  particlesFrom,\n  timestampFrom,\n  batchSize,\n  offset = 0,\n  abortSignal,\n}: {\n  neuron: NeuronAddress;\n  particlesFrom: ParticleCid[];\n  timestampFrom: number;\n  batchSize: number;\n  offset: number;\n  abortSignal: AbortSignal;\n}) => {\n  const where = {\n    _and: [\n      {\n        timestamp: {\n          _gt: numberToUtcDate(timestampFrom),\n        },\n      },\n      {\n        neuron: {\n          _eq: neuron,\n        },\n      },\n      { particle_from: { _in: particlesFrom } },\n    ],\n  };\n\n  const res = await createIndexerClient(abortSignal).request<\n    CyberlinksByParticleQuery,\n    CyberlinksByParticleQueryVariables\n  >(CyberlinksByParticleDocument, {\n    limit: batchSize,\n    offset,\n    orderBy: [\n      {\n        timestamp: Order_By.Asc,\n      },\n    ],\n    where,\n  });\n\n  return res.cyberlinks;\n};\n\nexport const fetchCyberlinksByNerounIterable = async (\n  neuron: NeuronAddress,\n  particlesFrom: ParticleCid[],\n  timestampFrom: number,\n  batchSize: number,\n  abortSignal: AbortSignal\n) =>\n  fetchIterableByOffset(fetchCyberlinksByNeroun, {\n    neuron,\n    particlesFrom,\n    timestampFrom,\n    batchSize,\n    abortSignal,\n  });\n\nconst fetchCyberlinksIterable = (\n  particleCid: ParticleCid,\n  timestampFrom: number,\n  abortSignal: AbortSignal\n) =>\n  fetchIterableByOffset(fetchCyberlinks, {\n    particleCid,\n    timestampFrom,\n    abortSignal,\n  });\n\nexport { fetchCyberlinksIterable, fetchCyberlinksCount };\n","const TRANSACTIONS_BATCH_LIMIT = 500;\nconst CYBERLINKS_BATCH_LIMIT = 200;\n\nexport { TRANSACTIONS_BATCH_LIMIT, CYBERLINKS_BATCH_LIMIT };\n","import { CyberLinkSimple, CyberlinkTxHash, ParticleCid } from 'src/types/base';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { asyncIterableBatchProcessor } from 'src/utils/async/iterable';\nimport { CID_TWEET } from 'src/constants/app';\nimport { LinkDto, TransactionDto } from 'src/services/CozoDb/types/dto';\n\nimport { fetchCyberlinksIterable } from '../../../indexer/cyberlinks';\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { MAX_LINKS_RESOLVE_BATCH } from '../consts';\nimport {\n  CYBER_LINK_TRANSACTION_TYPE,\n  CyberLinkValue,\n} from '../../../indexer/types';\nimport { SyncQueueJobType } from 'src/services/CozoDb/types/entities';\n\nconst getUniqueParticlesFromLinks = (links: CyberLinkSimple[]) =>\n  [\n    ...new Set([\n      ...links.map((link) => link.to),\n      ...links.map((link) => link.from),\n    ]),\n  ] as ParticleCid[];\n\n// eslint-disable-next-line import/no-unused-modules\nexport const fetchCyberlinksAndResolveParticles = async (\n  cid: ParticleCid,\n  timestampUpdate: number,\n  particlesResolver: ParticlesResolverQueue,\n  queuePriority: QueuePriority,\n  abortSignal: AbortSignal\n) => {\n  const cyberlinksIterable = fetchCyberlinksIterable(\n    cid,\n    timestampUpdate,\n    abortSignal\n  );\n  const links = [];\n  // eslint-disable-next-line no-restricted-syntax\n  for await (const batch of cyberlinksIterable) {\n    links.push(...batch);\n    const particles = getUniqueParticlesFromLinks(batch);\n    if (particles.length > 0) {\n      await asyncIterableBatchProcessor(\n        particles,\n        (cids: ParticleCid[]) =>\n          particlesResolver!.enqueueBatch(\n            cids,\n            SyncQueueJobType.particle,\n            queuePriority\n          ),\n        MAX_LINKS_RESOLVE_BATCH\n      );\n    }\n  }\n\n  return links;\n};\n\nexport function extractCybelinksFromTransaction(batch: TransactionDto[]) {\n  const cyberlinks = batch.filter(\n    (l) => l.type === CYBER_LINK_TRANSACTION_TYPE\n  );\n  const particlesFound = new Set<string>();\n  const links: LinkDto[] = [];\n  // Get links: only from TWEETS\n  const tweets: Record<ParticleCid, LinkDto> = cyberlinks.reduce<\n    Record<ParticleCid, LinkDto>\n  >((acc, { value, hash, timestamp }: TransactionDto) => {\n    (value as CyberLinkValue).links.forEach((link) => {\n      particlesFound.add(link.to);\n      particlesFound.add(link.from);\n      const txLink = {\n        ...link,\n        timestamp,\n        neuron: (value as CyberLinkValue).neuron,\n        transactionHash: hash,\n      };\n      links.push(txLink);\n\n      if (link.from === CID_TWEET) {\n        acc[txLink.to] = txLink;\n      }\n    });\n    return acc;\n  }, {});\n\n  return {\n    tweets,\n    particlesFound: [...particlesFound],\n    links,\n  };\n}\n","import { NeuronAddress } from 'src/types/base';\nimport { numberToUtcDate } from 'src/utils/date';\nimport { fetchIterableByOffset } from 'src/utils/async/iterable';\nimport {\n  MessagesByAddressCountDocument,\n  MessagesByAddressCountQuery,\n  MessagesByAddressCountQueryVariables,\n  MessagesByAddressSenseDocument,\n  MessagesByAddressSenseQuery,\n  MessagesByAddressSenseQueryVariables,\n} from 'src/generated/graphql';\n\nimport { createIndexerClient } from './utils/graphqlClient';\nimport { Transaction } from './types';\n\ntype OrderDirection = 'desc' | 'asc';\ntype Abortable = { abortSignal: AbortSignal };\n\nexport type MessagesByAddressVariables = {\n  neuron: NeuronAddress;\n  timestampFrom: number;\n  offset?: number;\n  types: Transaction['type'][];\n  orderDirection: OrderDirection;\n  limit: number;\n} & Abortable;\n\nexport const mapMessagesByAddressVariables = ({\n  neuron,\n  timestampFrom,\n  offset = 0,\n  types = [],\n  orderDirection = 'desc',\n  limit,\n}: MessagesByAddressVariables) => ({\n  address: `{${neuron}}`,\n  limit,\n  timestamp_from: numberToUtcDate(timestampFrom),\n  offset,\n  types: `{${types.map((t) => `\"${t}\"`).join(' ,')}}`,\n  order_direction: orderDirection,\n});\n\nconst fetchTransactions = async ({\n  neuron,\n  timestampFrom,\n  offset = 0,\n  types = [],\n  orderDirection = 'desc',\n  limit,\n  abortSignal,\n}: MessagesByAddressVariables) => {\n  const res = await createIndexerClient(abortSignal).request<\n    MessagesByAddressSenseQuery,\n    MessagesByAddressSenseQueryVariables\n  >(\n    MessagesByAddressSenseDocument,\n    mapMessagesByAddressVariables({\n      neuron,\n      timestampFrom,\n      offset,\n      types,\n      orderDirection,\n      limit,\n      abortSignal,\n    }) as MessagesByAddressSenseQueryVariables\n  );\n\n  return res?.messages_by_address as Transaction[];\n};\n\nexport const fetchTransactionMessagesCount = async (\n  address: NeuronAddress,\n  timestampFrom: number,\n  abortSignal: AbortSignal\n) => {\n  const res = await createIndexerClient(abortSignal).request<\n    MessagesByAddressCountQuery,\n    MessagesByAddressCountQueryVariables\n  >(MessagesByAddressCountDocument, {\n    address: `{${address}}`,\n    timestamp: numberToUtcDate(timestampFrom),\n  });\n\n  return res?.messages_by_address_aggregate.aggregate?.count;\n};\n\nexport const fetchTransactionsIterable = ({\n  neuron,\n  timestampFrom,\n  types,\n  orderDirection,\n  limit,\n  abortSignal,\n}: MessagesByAddressVariables) =>\n  fetchIterableByOffset(fetchTransactions, {\n    neuron,\n    timestampFrom,\n    types,\n    orderDirection,\n    limit,\n    abortSignal,\n  });\n","import { TransactionDto } from 'src/services/CozoDb/types/dto';\nimport { SenseChat } from 'src/services/backend/types/sense';\nimport { NeuronAddress } from 'src/types/base';\nimport { Coin } from 'cosmjs-types/cosmos/base/v1beta1/coin';\n\nimport {\n  MSG_SEND_TRANSACTION_TYPE,\n  MSG_MULTI_SEND_TRANSACTION_TYPE,\n  MsgSendTransaction,\n} from '../../../indexer/types';\n\nexport const extractSenseChats = (\n  myAddress: NeuronAddress,\n  transactions: TransactionDto[]\n) => {\n  const sendTransactions =\n    transactions!.filter(\n      (t) =>\n        t.type === MSG_SEND_TRANSACTION_TYPE ||\n        t.type === MSG_MULTI_SEND_TRANSACTION_TYPE\n    ) || [];\n\n  if (sendTransactions.length === 0) {\n    return [];\n  }\n  const chats = new Map<NeuronAddress, SenseChat>();\n  transactions.forEach((t) => {\n    let userAddress = '';\n    if (t.type === MSG_MULTI_SEND_TRANSACTION_TYPE) {\n      const { inputs, outputs } = t.value;\n      const isSender = inputs.find((i) => i.address === myAddress);\n      const userMessages = isSender ? outputs : inputs;\n      userMessages.forEach((msg) =>\n        updateSenseChat(chats, msg.address, t, msg.coins, isSender)\n      );\n    } else if (t.type === MSG_SEND_TRANSACTION_TYPE) {\n      const { fromAddress, toAddress, amount } =\n        t.value as MsgSendTransaction['value'];\n      const isSender = fromAddress === myAddress;\n      userAddress = isSender ? toAddress : fromAddress;\n      updateSenseChat(chats, userAddress, t, amount, isSender);\n    }\n  });\n\n  return chats;\n};\n\nconst updateSenseChat = (\n  chats: Map<NeuronAddress, SenseChat>,\n  addr: string,\n  t: TransactionDto,\n  amount: Coin[],\n  isSender: boolean\n): Map<string, SenseChat> => {\n  const chat = chats.get(addr);\n  const transactions = chat?.transactions || [];\n\n  transactions.push(t);\n  chats.set(addr, {\n    userAddress: addr,\n    lastSendTimestamp: isSender ? t.timestamp : chat?.lastSendTimestamp || 0,\n    last: { amount, memo: t.memo, direction: isSender ? 'to' : 'from' },\n    transactions,\n  });\n  return chats;\n};\n","import { EntryType } from 'src/services/CozoDb/types/entities';\nimport DbApiWrapper from 'src/services/backend/services/DbApi/DbApi';\nimport { NeuronAddress } from 'src/types/base';\nimport {\n  SenseListItem,\n  SenseTransactionMeta,\n} from 'src/services/backend/types/sense';\nimport { throwIfAborted } from 'src/utils/async/promise';\nimport { extractSenseChats } from '../../utils/sense';\n\n// eslint-disable-next-line import/prefer-default-export\nexport const syncMyChats = async (\n  db: DbApiWrapper,\n  myAddress: NeuronAddress,\n  timestampFrom: number,\n  signal: AbortSignal,\n  shouldUpdateTimestamp = true\n) => {\n  const syncItems = await db.findSyncStatus({\n    ownerId: myAddress,\n    entryType: EntryType.chat,\n  });\n\n  const syncItemsMap = new Map(syncItems?.map((i) => [i.id, i]));\n\n  const myTransactions = await db.getTransactions(myAddress, {\n    order: 'asc',\n    timestampFrom,\n  });\n\n  const myChats = extractSenseChats(myAddress, myTransactions!);\n\n  const results: SenseListItem[] = [];\n\n  // eslint-disable-next-line no-restricted-syntax\n  for (const chat of myChats.values()) {\n    const syncItem = syncItemsMap.get(chat.userAddress);\n    const lastTransaction = chat.transactions.at(-1)!;\n\n    const { timestamp: transactionTimestamp, hash, index } = lastTransaction;\n    const syncItemHeader = {\n      entryType: EntryType.chat,\n      ownerId: myAddress,\n      meta: {\n        transactionHash: hash,\n        index,\n      } as SenseTransactionMeta,\n    };\n\n    // if no sync item(first message/initial)\n    if (!syncItem) {\n      const unreadCount = chat.transactions.filter(\n        (t) => t.timestamp > chat.lastSendTimestamp\n      ).length; // uread count on top of my last send message\n\n      const newItem = {\n        ...syncItemHeader,\n        id: chat.userAddress,\n        unreadCount,\n        // if 'fast' then no shift update poiter till 'slow' reupdate\n        timestampUpdate: shouldUpdateTimestamp ? transactionTimestamp : 0,\n        timestampRead: chat.lastSendTimestamp,\n        disabled: false,\n      };\n\n      // eslint-disable-next-line no-await-in-loop\n      await throwIfAborted(db.putSyncStatus.bind(db), signal)(newItem);\n\n      results.push({ ...newItem, meta: lastTransaction });\n    } else {\n      const {\n        id,\n        timestampRead,\n        timestampUpdate,\n        meta,\n        unreadCount: prevUnreadCount,\n      } = syncItem;\n\n      const lastTimestampRead = Math.max(\n        timestampRead!,\n        chat.lastSendTimestamp\n      );\n      const { timestampUpdateContent = 0, timestampUpdateChat = 0 } = meta;\n      const timestampUnreadFrom = Math.max(\n        chat.lastSendTimestamp,\n        timestampUpdateChat\n      );\n      const unreadCount =\n        prevUnreadCount +\n        chat.transactions.filter((t) => t.timestamp > timestampUnreadFrom) // + new messages count\n          .length;\n\n      if (timestampUpdate < transactionTimestamp) {\n        // if message source is 'fast' then no update till 'slow' reupdate\n        const newTimestampUpdateChat = shouldUpdateTimestamp\n          ? transactionTimestamp\n          : timestampUpdateChat;\n\n        const syncStatusChanges = {\n          ...syncItemHeader,\n          id: id!,\n          unreadCount,\n          timestampRead: lastTimestampRead,\n          // show max timestamp to use in sorting, in sense list\n          // real timestamp shold be resynced with 'slow' data source by timestampUpdateChat\n          timestampUpdate: Math.max(\n            transactionTimestamp,\n            timestampUpdateContent,\n            newTimestampUpdateChat\n          ),\n\n          meta: {\n            ...syncItemHeader.meta,\n            timestampUpdateChat: newTimestampUpdateChat,\n            timestampUpdateContent,\n          },\n        };\n\n        // eslint-disable-next-line no-await-in-loop\n        await throwIfAborted(\n          db.updateSyncStatus.bind(db),\n          signal\n        )(syncStatusChanges);\n\n        results.push({\n          ...syncItem,\n          ...syncStatusChanges,\n          meta: lastTransaction,\n        } as SenseListItem);\n      }\n    }\n  }\n  return results;\n};\n","import { ProgressTracking } from 'src/services/backend/types/services';\n\nconst ROLLING_WINDOW = 10;\n\ntype onProgressUpdateFunc = (progress: ProgressTracking) => void;\n\ntype RequestRecord = {\n  timestamp: number;\n  itemCount: number;\n};\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport class ProgressTracker {\n  private requestRecords: RequestRecord[] = [];\n\n  private totalRequests = 0;\n\n  private completedRequests = 0;\n\n  private estimatedTime = -1;\n\n  private batchSize = 1;\n\n  private onProgressUpdate?: onProgressUpdateFunc;\n\n  public get progress(): ProgressTracking {\n    return {\n      totalCount: this.totalRequests,\n      completeCount: this.completedRequests,\n      estimatedTime: this.estimatedTime,\n    };\n  }\n\n  constructor(onProgressUpdate?: onProgressUpdateFunc) {\n    this.onProgressUpdate = onProgressUpdate;\n  }\n\n  public start(totalRequests: number, batchSize = 1) {\n    this.totalRequests = totalRequests;\n    this.requestRecords = [];\n    this.completedRequests = 0;\n    this.estimatedTime = -1;\n    this.batchSize = batchSize;\n\n    return this.progress;\n  }\n\n  public add(extraRequests: number) {\n    this.totalRequests += extraRequests;\n\n    return this.progress;\n  }\n\n  public trackProgress(processedCount: number) {\n    this.addRequestRecord(processedCount);\n\n    if (this.requestRecords.length > ROLLING_WINDOW) {\n      this.requestRecords.shift();\n    }\n\n    if (this.requestRecords.length > 1) {\n      const averageTimePerItem = this.calculateAverageTimePerItem();\n      const remainingRequests = this.totalRequests - this.completedRequests;\n      const estimatedRemainingItems = remainingRequests * processedCount; // Assuming remaining requests will process the same number of items\n      const estimatedRemainingTime =\n        averageTimePerItem * estimatedRemainingItems;\n\n      this.completedRequests += processedCount;\n      this.estimatedTime = Math.round(estimatedRemainingTime); // Convert to seconds;\n      this.onProgressUpdate && this.onProgressUpdate(this.progress);\n    }\n\n    return this.progress;\n  }\n\n  private addRequestRecord(itemCount: number) {\n    this.requestRecords.push({ timestamp: Date.now(), itemCount });\n  }\n\n  private calculateAverageTimePerItem(): number {\n    let totalDiff = 0;\n    let totalItems = 0;\n\n    for (let i = 1; i < this.requestRecords.length; i++) {\n      const timeDiff =\n        this.requestRecords[i].timestamp - this.requestRecords[i - 1].timestamp;\n      const { itemCount } = this.requestRecords[i];\n\n      totalDiff += timeDiff * itemCount;\n      totalItems += itemCount;\n    }\n\n    return totalItems === 0 ? 0 : totalDiff / totalItems;\n  }\n}\n","import {\n  Observable,\n  filter,\n  distinctUntilChanged,\n  map,\n  switchMap,\n  take,\n  tap,\n} from 'rxjs';\n\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport { broadcastStatus } from 'src/services/backend/channels/broadcastStatus';\nimport { SyncEntryName } from 'src/services/backend/types/services';\nimport { CyblogChannel, createCyblogChannel } from 'src/utils/logging/cyblog';\n\nimport DbApiWrapper from '../../../DbApi/DbApi';\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { ProgressTracker } from '../ProgressTracker/ProgressTracker';\nimport { ServiceDeps } from '../types';\nimport { SyncServiceParams } from '../../types';\n\nabstract class BaseSync {\n  protected name: string;\n\n  protected abortController: AbortController;\n\n  protected db: DbApiWrapper | undefined;\n\n  protected progressTracker = new ProgressTracker();\n\n  protected channelApi = new BroadcastChannelSender();\n\n  protected particlesResolver: ParticlesResolverQueue | undefined;\n\n  protected statusApi: ReturnType<typeof broadcastStatus>;\n\n  protected params: SyncServiceParams = {\n    myAddress: null,\n  };\n\n  protected readonly isInitialized$: Observable<boolean>;\n\n  protected cyblogCh: CyblogChannel;\n\n  constructor(\n    name: SyncEntryName,\n    deps: ServiceDeps,\n    particlesResolver: ParticlesResolverQueue\n  ) {\n    this.name = name;\n\n    this.abortController = new AbortController();\n\n    this.statusApi = broadcastStatus(name, this.channelApi);\n    this.particlesResolver = particlesResolver;\n    this.cyblogCh = createCyblogChannel({ thread: 'bckd', module: name });\n    if (!deps.params$) {\n      throw new Error('params$ is not defined');\n    }\n\n    deps.dbInstance$.subscribe((db) => {\n      this.db = db;\n    });\n\n    this.particlesResolver = particlesResolver;\n\n    this.isInitialized$ = this.createIsInitializedObserver(deps);\n\n    this.isInitialized$.subscribe((isInitialized) => {\n      this.cyblogCh.info(\n        `>>> ${this.name} - ${isInitialized ? 'initialized' : 'inactive'}`\n      );\n      this.statusApi.sendStatus(isInitialized ? 'initialized' : 'inactive');\n    });\n\n    this.isInitialized$\n      .pipe(switchMap(() => deps.params$!))\n      .subscribe((params) => {\n        this.params = params;\n        this.cyblogCh.info(`>>> ${this.name} - params updated`, {\n          data: params,\n        });\n      });\n\n    // Restart observer\n    this.isInitialized$\n      .pipe(\n        filter((isInitialized) => !!isInitialized),\n        switchMap(() => this.createRestartObserver(deps.params$!))\n      )\n      .subscribe(() => {\n        this.restart();\n      });\n  }\n\n  protected initAbortController() {\n    this.abortController = new AbortController();\n  }\n\n  protected abstract createIsInitializedObserver(\n    deps: ServiceDeps\n  ): Observable<boolean>;\n\n  // eslint-disable-next-line class-methods-use-this\n  protected createRestartObserver(params$: Observable<SyncServiceParams>) {\n    return params$.pipe(\n      map((params) => params.myAddress),\n      distinctUntilChanged((addrBefore, addrAfter) => addrBefore === addrAfter),\n      map((v) => !!v),\n      filter((v) => !!v)\n    );\n  }\n\n  public abstract restart(): void;\n\n  public abstract start(): void;\n}\n\nexport default BaseSync;\n","/* eslint-disable import/prefer-default-export */\nimport {\n  distinctUntilChanged,\n  filter,\n  Observable,\n  share,\n  switchMap,\n  tap,\n} from 'rxjs';\n\nexport const switchWhenInitialized = (\n  isInitialized$: Observable<boolean>,\n  actionObservable$: Observable<any>,\n  onChange?: (isInitialized: boolean) => void\n) =>\n  isInitialized$.pipe(\n    distinctUntilChanged(),\n    tap((isInitialized) => onChange?.(isInitialized)),\n    filter((initialized) => initialized),\n    switchMap(() => actionObservable$),\n    share()\n  );\n","import { Observable, Subject, from, startWith, switchMap, tap } from 'rxjs';\n\nimport { SyncEntryName } from 'src/services/backend/types/services';\n\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { ServiceDeps } from '../types';\nimport BaseSync from './BaseSync';\nimport { switchWhenInitialized } from '../utils/rxjs/withInitializer';\nimport { SyncServiceParams } from '../../types';\n\nabstract class BaseSyncClient extends BaseSync {\n  protected readonly source$: Observable<any>;\n\n  protected readonly reloadTrigger$ = new Subject<void>();\n\n  constructor(\n    name: SyncEntryName,\n    deps: ServiceDeps,\n    particlesResolver: ParticlesResolverQueue\n  ) {\n    super(name, deps, particlesResolver);\n\n    const source$ = switchWhenInitialized(\n      this.isInitialized$!,\n      this.reloadTrigger$.pipe(\n        startWith(null),\n        tap(() => {\n          // initialize abort conteoller for restart strategy\n          this.initAbortController();\n        }),\n        switchMap(() =>\n          this.createInitObservable().pipe(\n            switchMap((timestampFrom: number) =>\n              this.createClientObservable(timestampFrom).pipe(\n                tap(() => this.statusApi.sendStatus('listen')),\n                switchMap((data) => from(this.onUpdate(data, this.params)))\n              )\n            )\n          )\n        )\n      ),\n      (isInitialized) => {\n        console.log(`>>> ${name} isInitialized`, isInitialized);\n        this.statusApi.sendStatus(isInitialized ? 'initialized' : 'inactive');\n      }\n    );\n\n    source$.subscribe({\n      next: () => {\n        this.statusApi.sendStatus('listen');\n      },\n      error: (err) => {\n        this.statusApi.sendStatus('error', err);\n      },\n    });\n    this.source$ = source$;\n  }\n\n  protected abstract createClientObservable(\n    timestampFrom: number\n  ): Observable<any>;\n\n  protected abstract createInitObservable(): Observable<number>;\n\n  public restart() {\n    this.abortController?.abort();\n    this.reloadTrigger$.next();\n    console.log(`>>> ${this.name} client restart`);\n  }\n\n  protected abstract onUpdate(\n    data: any,\n    params: SyncServiceParams\n  ): Promise<void>;\n\n  public start() {\n    this.source$.subscribe(() => {\n      // dummy subscriber to keep pipeline running - don't remove\n    });\n    return this;\n  }\n}\n\nexport default BaseSyncClient;\n","/* eslint-disable camelcase */\nimport {\n  map,\n  combineLatest,\n  Observable,\n  from,\n  defer,\n  distinctUntilChanged,\n  merge,\n  filter,\n} from 'rxjs';\nimport { isEmpty } from 'lodash';\n\nimport {\n  EntryType,\n  SyncQueueJobType,\n} from 'src/services/CozoDb/types/entities';\nimport { mapIndexerTransactionToEntity } from 'src/services/CozoDb/mapping';\nimport { numberToUtcDate } from 'src/utils/date';\nimport { NeuronAddress } from 'src/types/base';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { SyncStatusDto, TransactionDto } from 'src/services/CozoDb/types/dto';\nimport { asyncIterableBatchProcessor } from 'src/utils/async/iterable';\nimport { throwIfAborted } from 'src/utils/async/promise';\nimport {\n  createNodeWebsocketObservable,\n  getIncomingTransfersQuery,\n} from 'src/services/lcd/websocket';\nimport {\n  MessagesByAddressSenseQueryVariables,\n  MessagesByAddressSenseWsDocument,\n  MessagesByAddressSenseWsSubscription,\n} from 'src/generated/graphql';\n\nimport { mapWebsocketTxToTransactions } from 'src/services/lcd/utils/mapping';\n\nimport { ServiceDeps } from '../types';\nimport { extractCybelinksFromTransaction } from '../utils/links';\n\nimport {\n  fetchTransactionsIterable,\n  mapMessagesByAddressVariables,\n  fetchTransactionMessagesCount,\n} from '../../../indexer/transactions';\nimport { syncMyChats } from './services/chat';\nimport { TRANSACTIONS_BATCH_LIMIT } from '../../../indexer/consts';\nimport BaseSyncClient from '../BaseSyncLoop/BaseSyncClient';\nimport { createIndexerWebsocket } from '../../../indexer/utils/graphqlClient';\nimport { SyncServiceParams } from '../../types';\nimport { MAX_DATABASE_PUT_SIZE } from '../consts';\n\ntype DataStreamResult = {\n  source: 'indexer' | 'node';\n  transactions: TransactionDto[];\n};\n\nclass SyncTransactionsLoop extends BaseSyncClient {\n  protected createIsInitializedObserver(deps: ServiceDeps) {\n    const isInitialized$ = combineLatest([\n      deps.dbInstance$,\n      deps.params$!.pipe(\n        map((params) => params.myAddress),\n        distinctUntilChanged()\n      ),\n      this.particlesResolver!.isInitialized$,\n    ]).pipe(\n      map(\n        ([dbInstance, myAddress, syncQueueInitialized]) =>\n          !!dbInstance && !!syncQueueInitialized && !!myAddress\n      )\n    );\n\n    return isInitialized$;\n  }\n\n  // eslint-disable-next-line class-methods-use-this\n  protected createClientObservable(\n    timestampFrom: number\n  ): Observable<DataStreamResult> {\n    const { myAddress } = this.params;\n    this.cyblogCh.info(\n      `>>> ${this.name} subscribe ${myAddress} from ${numberToUtcDate(\n        timestampFrom\n      )}`\n    );\n\n    const variables = mapMessagesByAddressVariables({\n      neuron: myAddress!,\n      timestampFrom,\n      types: [],\n      orderDirection: 'desc',\n      limit: 100,\n    }) as MessagesByAddressSenseQueryVariables;\n\n    const indexerObservable$ =\n      createIndexerWebsocket<MessagesByAddressSenseWsSubscription>(\n        MessagesByAddressSenseWsDocument,\n        variables\n      ).pipe(\n        map((response: MessagesByAddressSenseWsSubscription) => {\n          return {\n            source: 'indexer',\n            transactions: response.messages_by_address.map((i) =>\n              mapIndexerTransactionToEntity(myAddress!, i)\n            ),\n          };\n        })\n      );\n\n    const nodeObservample$ = createNodeWebsocketObservable(\n      myAddress!,\n      getIncomingTransfersQuery(myAddress!),\n      (message, ctx) => this.cyblogCh.info(message, { unit: 'node-ws', ...ctx })\n    ).pipe(\n      filter((data) => !isEmpty(data)),\n      map((data) => {\n        return {\n          source: 'node',\n          transactions: mapWebsocketTxToTransactions(myAddress!, data),\n        };\n      })\n    );\n\n    return merge(\n      indexerObservable$,\n      nodeObservample$\n    ) as Observable<DataStreamResult>;\n  }\n\n  protected createInitObservable() {\n    return defer(() => from(this.initSync()));\n    // return from(this.initSync());\n  }\n\n  public async initSync() {\n    const { myAddress } = this.params;\n    const { signal } = this.abortController;\n    const syncItem = await this.db!.getSyncStatus(myAddress!, myAddress!);\n\n    const lastTransactionTimestamp = await this.syncTransactions(\n      myAddress!,\n      myAddress!,\n      syncItem\n    );\n\n    this.statusApi.sendStatus('in-progress', `sync my chats`);\n    const syncStatusItems = await syncMyChats(\n      this.db!,\n      myAddress!,\n      syncItem.timestampUpdate,\n      signal\n    );\n\n    this.channelApi.postSenseUpdate(syncStatusItems);\n    this.statusApi.sendStatus('active');\n\n    return lastTransactionTimestamp;\n  }\n\n  protected async onUpdate(\n    { source, transactions }: DataStreamResult,\n    params: SyncServiceParams\n  ) {\n    const { myAddress } = params;\n    const { signal } = this.abortController;\n    if (transactions.length === 0) {\n      this.cyblogCh.info(`>>> ${this.name} ${myAddress} recived 0 updates `);\n      return;\n    }\n    const syncItem = await this.db!.getSyncStatus(myAddress!, myAddress!);\n\n    await this.processBatchTransactions(\n      myAddress!,\n      myAddress!,\n      transactions,\n      syncItem,\n      source\n    );\n\n    this.statusApi.sendStatus('in-progress', `sync my chats`);\n    const syncStatusItems = await syncMyChats(\n      this.db!,\n      myAddress!,\n      syncItem.timestampUpdate,\n      signal,\n      source !== 'node'\n    );\n\n    this.channelApi.postSenseUpdate(syncStatusItems);\n    this.statusApi.sendStatus('listen');\n  }\n\n  public async processBatchTransactions(\n    myAddress: NeuronAddress,\n    address: NeuronAddress,\n    transactions: TransactionDto[],\n    { timestampRead, unreadCount, timestampUpdate }: SyncStatusDto,\n    source: DataStreamResult['source']\n  ) {\n    const { signal } = this.abortController;\n\n    // node transaction is limited by incoming messages,\n    // to prevent missing of other msg types let's avoid to change ts\n    const shouldUpdateTimestamp = source !== 'node';\n\n    this.cyblogCh.info(\n      `   syncTransactions - process ${address}[${source}],  count: ${\n        transactions.length\n      }, from: ${transactions.at(0)?.timestamp}, to: ${\n        transactions.at(-1)?.timestamp\n      }`\n    );\n\n    // save transaction\n    await throwIfAborted(this.db!.putTransactions, signal)(transactions);\n\n    // save links\n    this.syncLinks(transactions, signal);\n\n    const {\n      hash,\n      index,\n\n      timestamp,\n    } = transactions.at(-1)!;\n\n    const lastTimestampFrom = timestamp;\n\n    // Update transaction sync items\n    const newSyncItem = {\n      ownerId: myAddress,\n      entryType: EntryType.transactions,\n      id: address,\n      timestampUpdate: shouldUpdateTimestamp\n        ? lastTimestampFrom\n        : timestampUpdate!,\n      unreadCount: unreadCount! + transactions.length,\n      timestampRead: timestampRead || 0,\n      disabled: false,\n      meta: {\n        transactionHash: hash,\n        index,\n      },\n    };\n\n    await throwIfAborted(this.db!.putSyncStatus, signal)(newSyncItem);\n\n    return lastTimestampFrom;\n  }\n\n  public async syncTransactions(\n    myAddress: NeuronAddress,\n    address: NeuronAddress,\n    syncItem: SyncStatusDto\n  ) {\n    const { unreadCount, timestampUpdate } = syncItem;\n    const timestampFrom = timestampUpdate + 1; // ofsset + 1 to fix milliseconds precision bug\n\n    this.statusApi.sendStatus('estimating');\n\n    const totalMessageCount = await fetchTransactionMessagesCount(\n      address,\n      timestampFrom,\n      this.abortController!.signal\n    );\n\n    this.cyblogCh.info(\n      `>>> syncTransactions - start ${address},  count: ${totalMessageCount}, from: ${timestampFrom}`\n    );\n\n    if (totalMessageCount === 0) {\n      return timestampFrom;\n    }\n\n    this.statusApi.sendStatus(\n      'in-progress',\n      `sync ${address}...`,\n      this.progressTracker.start(\n        Math.ceil(totalMessageCount / TRANSACTIONS_BATCH_LIMIT)\n      )\n    );\n\n    const transactionsAsyncIterable = fetchTransactionsIterable({\n      neuron: address,\n      timestampFrom,\n      types: [], // SENSE_TRANSACTIONS,\n      orderDirection: 'asc',\n      limit: TRANSACTIONS_BATCH_LIMIT,\n      abortSignal: this.abortController?.signal,\n    });\n\n    let transactionCount = 0;\n    let lastTimestampFrom = timestampFrom;\n\n    // eslint-disable-next-line no-restricted-syntax\n    for await (const batch of transactionsAsyncIterable) {\n      this.statusApi.sendStatus(\n        'in-progress',\n        `sync ${address}...`,\n        this.progressTracker.trackProgress(1)\n      );\n\n      transactionCount += batch.length;\n\n      const transactions = batch.map((i) =>\n        mapIndexerTransactionToEntity(address, i)\n      );\n\n      lastTimestampFrom = await this.processBatchTransactions(\n        myAddress,\n        address,\n        transactions,\n        {\n          ...syncItem,\n          unreadCount: unreadCount + transactionCount,\n        },\n        'indexer'\n      );\n    }\n\n    return lastTimestampFrom;\n  }\n\n  private async syncLinks(batch: TransactionDto[], signal: AbortSignal) {\n    const { tweets, particlesFound, links } =\n      extractCybelinksFromTransaction(batch);\n    if (links.length > 0) {\n      await asyncIterableBatchProcessor(\n        links,\n        (links) => throwIfAborted(this.db!.putCyberlinks, signal)(links),\n        MAX_DATABASE_PUT_SIZE\n      );\n    }\n\n    const tweetParticles = Object.keys(tweets);\n\n    const nonTweetParticles = particlesFound.filter(\n      (cid) => !tweetParticles.includes(cid)\n    );\n\n    // pre-resolve 'tweets' particles\n    await this.particlesResolver!.enqueueBatch(\n      tweetParticles,\n      SyncQueueJobType.particle,\n      QueuePriority.HIGH\n    );\n\n    // pre-resolve all the rest particles\n    if (nonTweetParticles.length > 0) {\n      await this.particlesResolver!.enqueueBatch(\n        nonTweetParticles,\n        SyncQueueJobType.particle,\n        QueuePriority.LOW\n      );\n    }\n  }\n}\n\nexport default SyncTransactionsLoop;\n","import { Observable } from 'rxjs';\nimport { WEBSOCKET_URL } from 'src/constants/config';\nimport { NeuronAddress } from 'src/types/base';\nimport { LogFunc } from 'src/utils/logging/cyblog';\n\nexport const getIncomingTransfersQuery = (address: NeuronAddress) =>\n  `tm.event='Tx' AND transfer.recipient='${address}'`;\n\n// eslint-disable-next-line import/no-unused-modules\nexport function createNodeWebsocketObservable(\n  address: NeuronAddress,\n  query: string,\n  log: LogFunc\n) {\n  return new Observable((subscriber) => {\n    const ws = new WebSocket(WEBSOCKET_URL);\n\n    ws.onopen = () => {\n      log(`node ws connected to ${WEBSOCKET_URL} with ${query}`);\n      ws.send(\n        JSON.stringify({\n          jsonrpc: '2.0',\n          method: 'subscribe',\n          id: '0',\n          params: { query },\n        })\n      );\n    };\n\n    ws.onmessage = (event) => {\n      const message = JSON.parse(event.data);\n      log(`node ws ${address} onmessage`, message);\n      subscriber.next(message.result);\n    };\n\n    ws.onerror = (event) => {\n      log(`node ws ${address} error`, { error: event });\n      subscriber.error(event);\n    };\n\n    ws.onclose = () => {\n      log(`node ws ${address} closed`);\n      subscriber.complete();\n    };\n\n    return () => {\n      ws.close();\n    };\n  });\n}\n","import { EntityToDto, DtoToEntity } from 'src/types/dto';\nimport { deserializeString } from './string';\n\nexport const snakeToCamel = (str: string) =>\n  str.replace(/([-_][a-z])/g, (group) =>\n    group.toUpperCase().replace('-', '').replace('_', '')\n  );\n\nexport const camelToSnake = (str: string) =>\n  str.replace(/[A-Z]/g, (letter) => `_${letter.toLowerCase()}`);\n// Function to transform a DB entity to a DTO\n\n// eslint-disable-next-line import/no-unused-modules\nexport function entityToDto<T extends Record<string, any>>(\n  dbEntity: T\n): EntityToDto<T> {\n  if (!dbEntity || typeof dbEntity !== 'object') {\n    return dbEntity;\n  }\n  const dto: Record<string, any> = {}; // Specify the type for dto\n  Object.keys(dbEntity).forEach((key) => {\n    if (Object.prototype.hasOwnProperty.call(dbEntity, key)) {\n      const camelCaseKey = snakeToCamel(key);\n      let value = dbEntity[key];\n      if (Array.isArray(dbEntity[key])) {\n        value = dbEntity[key].map((item) => entityToDto(item));\n      } else if (typeof dbEntity[key] === 'object') {\n        value = entityToDto(dbEntity[key]);\n      } else if (typeof dbEntity[key] === 'string') {\n        value = deserializeString(value);\n      }\n      dto[camelCaseKey] = value;\n    }\n  });\n  return dto as EntityToDto<T>;\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport function dtoToEntity<T extends Record<string, any>>(\n  dto: T\n): DtoToEntity<T> {\n  // in case of recursive calls\n  if (!dto || typeof dto !== 'object') {\n    return dto;\n  }\n  const dbEntity: any = {};\n\n  Object.keys(dto).forEach((key) => {\n    if (Object.prototype.hasOwnProperty.call(dto, key)) {\n      const snakeCaseKey = camelToSnake(key);\n      let value = dto[key];\n      if (Array.isArray(value)) {\n        value = value.map((item) => dtoToEntity(item));\n      } else if (typeof value === 'object') {\n        value = dtoToEntity(value);\n      }\n      //  else if (typeof value === 'string') {\n      //   value = replaceQuotes(value);\n      // }\n      dbEntity[snakeCaseKey] = value;\n    }\n  });\n  return dbEntity as DtoToEntity<T>; // Replace T with the appropriate DB Entity type if known\n}\n\nexport function dtoListToEntity<T extends Record<string, any>>(\n  array: T[]\n): DtoToEntity<T>[] {\n  return array.map((dto) => dtoToEntity(dto));\n}\n\nexport function entityListToDto<T extends Record<string, any>>(\n  array: T[]\n): EntityToDto<T>[] {\n  return array.map((dto) => entityToDto(dto));\n}\n\nexport function removeUndefinedFields(entity: Record<string, any>) {\n  Object.keys(entity).forEach((key) => {\n    if (entity[key] === undefined) {\n      delete entity[key];\n    }\n  });\n  return entity;\n}\n","import { NeuronAddress } from 'src/types/base';\nimport { LinkDto, SyncStatusDto } from 'src/services/CozoDb/types/dto';\nimport { EntryType } from 'src/services/CozoDb/types/entities';\n\nimport { findLastIndex } from 'lodash';\nimport { entityToDto } from 'src/utils/dto';\n\nimport { SenseItemLinkMeta } from '../../types/sense';\nimport { SyncEntryName } from '../../types/services';\n\nexport function getLastReadInfo(\n  links: LinkDto[],\n  ownerId: NeuronAddress,\n  prevTimestampRead = 0,\n  prevUnreadCount = 0\n) {\n  const lastUnreadLinks = links.filter(\n    (link) => link.timestamp > prevTimestampRead\n  );\n  const lastMyLinkIndex = findLastIndex(\n    lastUnreadLinks,\n    (link) => link.neuron === ownerId\n  );\n\n  const unreadCount =\n    lastMyLinkIndex < 0\n      ? prevUnreadCount + lastUnreadLinks.length\n      : lastUnreadLinks.length - lastMyLinkIndex - 1;\n\n  const timestampRead =\n    lastMyLinkIndex < 0 ? prevTimestampRead : links[lastMyLinkIndex].timestamp;\n\n  return {\n    timestampRead,\n    unreadCount,\n  };\n}\n\nexport function changeParticleSyncStatus(\n  syncStatus: Partial<SyncStatusDto>,\n  links: LinkDto[],\n  ownerId: NeuronAddress,\n  shouldUpdateTimestamp = true\n) {\n  const { timestampRead, unreadCount } = getLastReadInfo(\n    links,\n    ownerId,\n    syncStatus.timestampRead,\n    syncStatus.unreadCount\n  );\n\n  const lastLink = entityToDto(links[links.length - 1]);\n  const timestampUpdate = lastLink.timestamp;\n  return {\n    ...syncStatus,\n    ownerId,\n    entryType: EntryType.particle,\n    disabled: false,\n    unreadCount,\n    meta: {\n      ...lastLink,\n      timestamp: timestampUpdate,\n    } as SenseItemLinkMeta,\n    timestampRead,\n    timestampUpdate: shouldUpdateTimestamp\n      ? timestampUpdate\n      : syncStatus.timestampUpdate,\n  } as SyncStatusDto;\n}\n\nconst mapSyncEntryReadable: Record<SyncEntryName, string> = {\n  'my-friends': \"friend's logs\",\n  particles: 'log cyberlinks',\n  resolver: 'particles',\n  transactions: 'transactions',\n  pin: 'ipfs pins',\n};\n\nexport const syncEntryNameToReadable = (name: SyncEntryName) =>\n  mapSyncEntryReadable[name] || name;\n","export const isAbortException = (e: Error) =>\n  e instanceof DOMException && e.name === 'AbortError';\n","import { Observable, defer, filter, from, tap } from 'rxjs';\n\nimport { SyncEntryName } from 'src/services/backend/types/services';\nimport { isAbortException } from 'src/utils/exceptions/helpers';\nimport { clone } from 'ramda';\n\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { ServiceDeps } from '../types';\nimport { createLoopObservable } from '../utils/rxjs/loop';\nimport BaseSync from './BaseSync';\nimport { SyncServiceParams } from '../../types';\n\nabstract class BaseSyncLoop extends BaseSync {\n  private restartLoop: (() => void) | undefined;\n\n  public readonly loop$: Observable<boolean>;\n\n  constructor(\n    name: SyncEntryName,\n    intervalMs: number,\n    deps: ServiceDeps,\n    particlesResolver: ParticlesResolverQueue,\n    {\n      warmupMs,\n    }: {\n      warmupMs: number;\n    } = { warmupMs: 0 }\n  ) {\n    super(name, deps, particlesResolver);\n\n    const { loop$, restartLoop } = createLoopObservable(\n      this.isInitialized$,\n      // defer(() => from(this.sync())),\n      defer(() => from(this.doSync())),\n      {\n        intervalMs,\n        warmupMs,\n        // onStartInterval: () => this.initAbortController(),\n        onError: (error) => {\n          this.cyblogCh.info(`>>> ${name} error`, error.toString());\n          this.statusApi.sendStatus('error', error.toString());\n        },\n        onChange: (isInitialized) => {\n          this.cyblogCh.info(`>>> ${name} initialized: ${isInitialized}`);\n          this.statusApi.sendStatus(isInitialized ? 'initialized' : 'inactive');\n        },\n      }\n    );\n\n    this.loop$ = loop$;\n    this.restartLoop = restartLoop;\n  }\n\n  public restart() {\n    this.abortController?.abort();\n    this.restartLoop?.();\n    this.cyblogCh.info(`>>> ${this.name} loop restart`);\n  }\n\n  public start() {\n    this.loop$.subscribe(() => this.statusApi.sendStatus('active'));\n    return this;\n  }\n\n  private async doSync() {\n    const params = clone(this.params);\n    this.initAbortController();\n    try {\n      await this.sync(params);\n    } catch (e) {\n      const isAborted = isAbortException(e);\n      this.cyblogCh.info(\n        `>>> ${this.name} ${params.myAddress} sync error [abrt:${isAborted}]:`,\n        {\n          error: e,\n        }\n      );\n\n      if (!isAborted) {\n        throw e;\n      }\n    }\n  }\n\n  protected abstract sync(params: SyncServiceParams): Promise<void>;\n}\n\nexport default BaseSyncLoop;\n","/* eslint-disable import/prefer-default-export */\nimport {\n  Observable,\n  switchMap,\n  interval,\n  startWith,\n  tap,\n  retry,\n  delay,\n  exhaustMap,\n  Subject,\n} from 'rxjs';\nimport { switchWhenInitialized } from './withInitializer';\n\ntype LoopObservableOptions = {\n  warmupMs?: number;\n  retryDelayMs?: number;\n  onStartInterval?: () => void;\n  onError?: (error: any) => void;\n  onChange?: (isInitialized: boolean) => void;\n  intervalMs?: number;\n};\n\nexport const createLoopObservable = (\n  isInitialized$: Observable<boolean>,\n  actionObservable$: Observable<any>,\n  options: LoopObservableOptions = {}\n) => {\n  const {\n    intervalMs,\n    warmupMs = 0,\n    onStartInterval,\n    onError,\n    retryDelayMs = 0,\n    onChange,\n  } = options;\n\n  const restartTrigger$ = new Subject<void>();\n\n  const intervalOrRestart$ = restartTrigger$.pipe(\n    startWith(null),\n    switchMap(() => interval(intervalMs).pipe(startWith(0), delay(warmupMs)))\n  );\n\n  const source$ = switchWhenInitialized(\n    isInitialized$,\n    intervalOrRestart$.pipe(\n      tap(() => onStartInterval && onStartInterval()),\n      exhaustMap(() =>\n        actionObservable$.pipe(\n          retry({\n            delay: (error) => {\n              console.log('retry', error);\n              onError && onError(error);\n              return interval(retryDelayMs);\n            },\n          })\n        )\n      )\n    ),\n    (isInitialized) => onChange?.(isInitialized)\n  );\n\n  return {\n    loop$: source$,\n    restartLoop: () => {\n      // console.log('>>> createLoopObservable restart');\n      // Trigger a restart by emitting a new value\n      restartTrigger$.next();\n    },\n  };\n};\n","import { map, combineLatest, distinctUntilChanged } from 'rxjs';\nimport { EntryType } from 'src/services/CozoDb/types/entities';\nimport { SyncStatusDto } from 'src/services/CozoDb/types/dto';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { NeuronAddress } from 'src/types/base';\n\nimport { mapLinkFromIndexerToDto } from 'src/services/CozoDb/mapping';\nimport { CID_TWEET } from 'src/constants/app';\nimport { dateToUtcNumber } from 'src/utils/date';\nimport { SenseListItem } from 'src/services/backend/types/sense';\nimport { asyncIterableBatchProcessor } from 'src/utils/async/iterable';\nimport { throwIfAborted } from 'src/utils/async/promise';\nimport { entityToDto } from 'src/utils/dto';\n\nimport { ServiceDeps } from '../types';\nimport { fetchCyberlinksAndResolveParticles } from '../utils/links';\n\nimport { changeParticleSyncStatus } from '../../utils';\nimport {\n  fetchCyberlinksByNerounIterable,\n  fetchCyberlinksCount,\n} from '../../../indexer/cyberlinks';\nimport { CYBERLINKS_BATCH_LIMIT } from '../../../indexer/consts';\nimport BaseSyncLoop from '../BaseSyncLoop/BaseSyncLoop';\nimport { MAX_DATABASE_PUT_SIZE } from '../consts';\nimport { SyncServiceParams } from '../../types';\n\nclass SyncParticlesLoop extends BaseSyncLoop {\n  protected createIsInitializedObserver(deps: ServiceDeps) {\n    const isInitialized$ = combineLatest([\n      deps.dbInstance$,\n      deps.ipfsInstance$,\n      deps.params$!.pipe(\n        map((params) => params.myAddress),\n        distinctUntilChanged()\n      ),\n      this.particlesResolver!.isInitialized$,\n    ]).pipe(\n      map(\n        ([dbInstance, ipfsInstance, myAddress, particleResolverInitialized]) =>\n          !!ipfsInstance &&\n          !!dbInstance &&\n          !!particleResolverInitialized &&\n          !!myAddress\n      )\n    );\n\n    return isInitialized$;\n  }\n\n  protected async sync(params: SyncServiceParams): Promise<void> {\n    const { myAddress } = params;\n    const { signal } = this.abortController;\n    this.statusApi.sendStatus('estimating');\n\n    const syncItemParticles = await this.db!.findSyncStatus({\n      ownerId: myAddress!,\n      entryType: EntryType.particle,\n    });\n\n    const timestampUpdate = syncItemParticles.at(0)?.timestampUpdate || 0;\n\n    // Get count of new links after last update\n    const newLinkCount = await fetchCyberlinksCount(\n      myAddress!,\n      [CID_TWEET],\n      timestampUpdate,\n      signal\n    );\n\n    this.cyblogCh.info(\n      `>>> syncMyParticles ${myAddress} count ${newLinkCount}`\n    );\n    this.progressTracker.start(newLinkCount + syncItemParticles.length);\n    this.statusApi.sendStatus(\n      'in-progress',\n      'preparing...',\n      this.progressTracker.progress\n    );\n\n    if (newLinkCount > 0) {\n      // fetch and save new particles\n      const newSyncItemParticles = await this.fetchNewTweets(\n        myAddress!,\n        timestampUpdate,\n        signal\n      );\n\n      // add to fetch-sync linked particles\n      syncItemParticles.push(...newSyncItemParticles);\n    }\n    await this.syncParticles(myAddress!, syncItemParticles, signal);\n  }\n\n  private async fetchNewTweets(\n    myAddress: NeuronAddress,\n    timestampUpdate: number,\n    signal: AbortSignal\n  ) {\n    const tweetsAsyncIterable = await fetchCyberlinksByNerounIterable(\n      myAddress,\n      [CID_TWEET],\n      timestampUpdate,\n      CYBERLINKS_BATCH_LIMIT,\n      this.abortController?.signal\n    );\n\n    const newTweets: SyncStatusDto[] = [];\n    const existingParticles = await this.db!.findSyncStatus({\n      ownerId: myAddress,\n      entryType: EntryType.particle,\n    });\n    const existingParticlesMap = new Map(\n      existingParticles.map((i) => [i.id, i])\n    );\n    // eslint-disable-next-line no-await-in-loop, no-restricted-syntax\n    for await (const tweetsBatch of tweetsAsyncIterable) {\n      this.statusApi.sendStatus(\n        'in-progress',\n        `fetching new tweets...`,\n        this.progressTracker.trackProgress(1)\n      );\n      const syncStatusEntities = tweetsBatch.map(entityToDto).map((item) => {\n        const { timestamp, to } = item;\n        const timestampUpdate = dateToUtcNumber(timestamp);\n\n        // In case my tweet already linked from other neuron, resync from beginning\n        const timestampSyncFrom = existingParticlesMap.get(to)\n          ? dateToUtcNumber(timestamp)\n          : 0;\n\n        // Initial state\n        return {\n          ownerId: myAddress,\n          id: to,\n          entryType: EntryType.particle,\n          timestampUpdate: timestampSyncFrom,\n          timestampRead: timestampUpdate,\n          unreadCount: 0,\n          disabled: false,\n          meta: { ...item, timestamp: timestampUpdate },\n        } as SyncStatusDto;\n      });\n\n      if (syncStatusEntities.length > 0) {\n        await throwIfAborted(\n          this.db!.putSyncStatus,\n          signal\n        )(syncStatusEntities);\n        newTweets.push(...syncStatusEntities);\n      }\n    }\n\n    return newTweets;\n  }\n\n  private async syncParticles(\n    myAddress: NeuronAddress,\n    syncItems: SyncStatusDto[],\n    signal: AbortSignal\n  ) {\n    const updatedSyncItems: SyncStatusDto[] = [];\n\n    // eslint-disable-next-line no-restricted-syntax\n    for (const syncItem of syncItems) {\n      const { id, timestampUpdate } = syncItem;\n\n      this.statusApi.sendStatus(\n        'in-progress',\n        `fetching tweet updates...`,\n        this.progressTracker.trackProgress(1)\n      );\n      // eslint-disable-next-line no-await-in-loop\n      const linksIndexer = await fetchCyberlinksAndResolveParticles(\n        id,\n        timestampUpdate,\n        this.particlesResolver!,\n        QueuePriority.MEDIUM,\n        this.abortController?.signal\n      );\n\n      if (linksIndexer.length > 0) {\n        const links = linksIndexer.map(mapLinkFromIndexerToDto);\n\n        // save links\n        // eslint-disable-next-line no-await-in-loop\n        await asyncIterableBatchProcessor(\n          links,\n          (links) => throwIfAborted(this.db!.putCyberlinks, signal)(links),\n          MAX_DATABASE_PUT_SIZE\n        );\n\n        const newItem = changeParticleSyncStatus(syncItem, links, myAddress);\n\n        updatedSyncItems.push(newItem);\n      }\n    }\n\n    if (updatedSyncItems.length > 0) {\n      await throwIfAborted(this.db!.putSyncStatus, signal)(updatedSyncItems);\n    }\n    this.channelApi.postSenseUpdate(updatedSyncItems as SenseListItem[]);\n  }\n}\n\nexport default SyncParticlesLoop;\n","/* eslint-disable camelcase */\nimport {\n  map,\n  combineLatest,\n  distinctUntilChanged,\n  BehaviorSubject,\n} from 'rxjs';\n\nimport {\n  EntryType,\n  SyncQueueJobType,\n} from 'src/services/CozoDb/types/entities';\n\nimport { NeuronAddress } from 'src/types/base';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { isAbortException } from 'src/utils/exceptions/helpers';\n\nimport { mapLinkFromIndexerToDto } from 'src/services/CozoDb/mapping';\nimport { throwIfAborted } from 'src/utils/async/promise';\n\nimport { SyncEntryName } from 'src/services/backend/types/services';\nimport { SenseItemLinkMeta } from 'src/services/backend/types/sense';\nimport { entityToDto } from 'src/utils/dto';\nimport { ServiceDeps } from '../types';\n\nimport { fetchCyberlinksByNerounIterable } from '../../../indexer/cyberlinks';\nimport { CYBERLINKS_BATCH_LIMIT } from '../../../indexer/consts';\nimport BaseSyncLoop from '../BaseSyncLoop/BaseSyncLoop';\nimport { SyncServiceParams } from '../../types';\nimport { getLastReadInfo } from '../../utils';\n\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { SENSE_FRIEND_PARTICLES } from '../consts';\n\nclass SyncMyFriendsLoop extends BaseSyncLoop {\n  protected followings: NeuronAddress[] = [];\n\n  constructor(\n    name: SyncEntryName,\n    intervalMs: number,\n    deps: ServiceDeps,\n    particlesResolver: ParticlesResolverQueue,\n    { warmupMs }: { warmupMs: number } = { warmupMs: 0 }\n  ) {\n    if (!deps.followings$) {\n      throw new Error('followings$ is required');\n    }\n\n    super(name, intervalMs, deps, particlesResolver, {\n      warmupMs,\n    });\n  }\n\n  protected createIsInitializedObserver(deps: ServiceDeps) {\n    const followingsInitialized$ = new BehaviorSubject<boolean>(false);\n    deps.params$\n      ?.pipe(\n        map((params) => params.myAddress),\n        distinctUntilChanged()\n      )\n      .subscribe(() => {\n        followingsInitialized$.next(false);\n      });\n\n    deps.followings$!.subscribe((followings) => {\n      this.followings = followings;\n      followingsInitialized$.next(true);\n\n      this.restart();\n    });\n\n    const isInitialized$ = combineLatest([\n      deps.dbInstance$,\n      deps.params$!,\n      this.particlesResolver!.isInitialized$,\n      followingsInitialized$!,\n    ]).pipe(\n      map(\n        ([dbInstance, params, syncQueueInitialized, followingsInitialized]) =>\n          !!dbInstance &&\n          !!params.myAddress &&\n          !!syncQueueInitialized &&\n          followingsInitialized\n      )\n    );\n\n    return isInitialized$;\n  }\n\n  protected async sync(params: SyncServiceParams) {\n    const { signal } = this.abortController;\n\n    this.statusApi.sendStatus('in-progress', 'preparing...');\n    const { myAddress } = params;\n\n    const { followings } = this;\n\n    this.statusApi.sendStatus('estimating');\n\n    this.cyblogCh.info(\n      `>>> syncMyFriends ${myAddress} count ${followings.length}`,\n      {\n        unit: 'friends-sync',\n        data: followings,\n      }\n    );\n\n    this.progressTracker.start(followings.length);\n    this.statusApi.sendStatus(\n      'in-progress',\n      `sync...`,\n      this.progressTracker.progress\n    );\n\n    // eslint-disable-next-line no-restricted-syntax\n    for (const addr of followings) {\n      // eslint-disable-next-line no-await-in-loop\n      await this.syncLinks(myAddress!, addr, signal);\n    }\n  }\n\n  public async syncLinks(\n    myAddress: NeuronAddress,\n    address: NeuronAddress,\n    signal: AbortSignal\n  ) {\n    let syncUpdates = [];\n    try {\n      this.statusApi.sendStatus(\n        'in-progress',\n        `starting sync ${address}...`,\n        this.progressTracker.progress\n      );\n      const { timestampRead, unreadCount, meta } = await this.db!.getSyncStatus(\n        myAddress,\n        address\n      );\n\n      const { timestampUpdateChat = 0, timestampUpdateContent = 0 } =\n        meta || {};\n\n      const timestampFrom = timestampUpdateContent + 1; // ofsset + 1 to fix milliseconds precision bug\n\n      const linksAsyncIterable = await fetchCyberlinksByNerounIterable(\n        address,\n        SENSE_FRIEND_PARTICLES,\n        timestampFrom,\n        CYBERLINKS_BATCH_LIMIT,\n        signal\n      );\n\n      // eslint-disable-next-line no-restricted-syntax\n      for await (const linksBatch of linksAsyncIterable) {\n        this.statusApi.sendStatus(\n          'in-progress',\n          `sync ${address}...`,\n          this.progressTracker.trackProgress(1)\n        );\n\n        const links = linksBatch.map(mapLinkFromIndexerToDto);\n\n        const { timestampRead: newTimestampRead, unreadCount: newUnreadCount } =\n          getLastReadInfo(links, myAddress, timestampRead, unreadCount);\n\n        // const unreadItemsCount = unreadCount + links.length;\n\n        if (links.length > 0) {\n          const lastLink = entityToDto(links.at(-1)!);\n          const newTimestampUpdateContent = lastLink!.timestamp;\n\n          await throwIfAborted(this.db!.putCyberlinks, signal)(links);\n\n          const particles = links.map((t) => t.to);\n          await this.particlesResolver!.enqueueBatch(\n            particles,\n            SyncQueueJobType.particle,\n            QueuePriority.HIGH\n          );\n\n          const newSyncItem = {\n            ownerId: myAddress,\n            entryType: EntryType.chat,\n            id: address,\n            timestampUpdate: Math.max(\n              newTimestampUpdateContent,\n              timestampUpdateChat\n            ),\n            unreadCount: newUnreadCount,\n            timestampRead: newTimestampRead,\n            disabled: false,\n            meta: {\n              ...lastLink!,\n              timestampUpdateContent: newTimestampUpdateContent,\n              timestampUpdateChat,\n            } as SenseItemLinkMeta,\n          };\n          // Update transaction\n          await throwIfAborted(this.db!.putSyncStatus, signal)(newSyncItem);\n\n          syncUpdates.push(newSyncItem);\n        }\n      }\n    } catch (err) {\n      this.cyblogCh.error(`>>> SyncMyFriends ${address} error`, {\n        error: err,\n      });\n      if (!isAbortException(err)) {\n        this.statusApi.sendStatus('error', err.toString());\n      } else {\n        syncUpdates = [];\n        throw err;\n      }\n    } finally {\n      // console.log('-----syncUpdates with redux', syncUpdates);\n      this.channelApi.postSenseUpdate(syncUpdates);\n    }\n  }\n\n  // eslint-disable-next-line class-methods-use-this\n  // protected createRestartObserver(\n  //   params$: Observable<SyncServiceParams>\n  // ): Observable<boolean> {\n  //   return super\n  //     .createRestartObserver(params$)\n  //     .pipe(switchMap((addressChanged) => this.isInitialized$));\n  // }\n}\n\nexport default SyncMyFriendsLoop;\n","import { NeuronAddress, ParticleCid } from 'src/types/base';\nimport { getIpfsHash } from 'src/utils/ipfs/helpers';\nimport { PATTERN_CYBER } from 'src/constants/patterns';\nimport { Subject, Observable } from 'rxjs';\n\nimport DbApiWrapper from '../backend/services/DbApi/DbApi';\nimport { getFollowsAsCid, getFollowers } from './lcd';\nimport { FetchParticleAsync, QueuePriority } from '../QueueManager/types';\nimport { CommunityDto } from '../CozoDb/types/dto';\nimport { FetchIpfsFunc } from '../backend/services/sync/types';\nimport { createCyblogChannel } from 'src/utils/logging/cyblog';\n\nexport type SyncCommunityResult = {\n  action: 'reset' | 'add' | 'complete';\n  items: CommunityDto[];\n};\n\nconst cyblogCh = createCyblogChannel({\n  thread: 'bckd',\n  unit: 'fetchStoredSyncCommunity',\n});\n\n// eslint-disable-next-line import/prefer-default-export, import/no-unused-modules\nexport const fetchStoredSyncCommunity$ = (\n  dbApi: DbApiWrapper,\n  address: NeuronAddress,\n  fetchParticleAsync?: FetchIpfsFunc,\n  signal?: AbortSignal\n): Observable<SyncCommunityResult> => {\n  return new Observable<SyncCommunityResult>((subscriber) => {\n    subscriber.next({ action: 'reset', items: [] });\n\n    (async () => {\n      const storedCommunity = await dbApi.getCommunity(address);\n\n      subscriber.next({ action: 'add', items: storedCommunity });\n\n      const communityUpdatesMap = new Map<ParticleCid, CommunityDto>(\n        storedCommunity.map((c) => [c.particle, c])\n      );\n\n      const getExistingOrDefault = (cid: ParticleCid): Partial<CommunityDto> =>\n        communityUpdatesMap.get(cid) || {\n          ownerId: address,\n          name: '',\n          following: false,\n          follower: false,\n        };\n\n      const followsCids = await getFollowsAsCid(address, signal);\n      const followers = await getFollowers(address, signal);\n\n      const newFollowerCids = followsCids.filter(\n        (cid) => !storedCommunity.some((i) => i.particle === cid && i.following)\n      );\n\n      const newFollowingNeurons = followers.filter(\n        (addr) => !storedCommunity.some((i) => i.neuron === addr && i.follower)\n      );\n\n      cyblogCh.info(\n        `>>>$ sync community ${address} processing, stored ${storedCommunity.length} new followers: ${newFollowerCids.length} new following: ${newFollowingNeurons.length}`\n      );\n\n      const followersCommunity = await Promise.all(\n        newFollowingNeurons.map(async (neuron) => {\n          const cid = await getIpfsHash(neuron);\n\n          const communityItem = {\n            ...getExistingOrDefault(cid),\n            particle: cid,\n            neuron,\n            follower: true,\n          } as CommunityDto;\n\n          await dbApi.putCommunity(communityItem);\n          communityUpdatesMap.set(cid, communityItem);\n          return communityItem;\n        })\n      );\n\n      subscriber.next({ action: 'add', items: followersCommunity });\n\n      await Promise.all(\n        newFollowerCids.map(async (cid: ParticleCid) => {\n          const neuron = (await fetchParticleAsync!(cid, QueuePriority.URGENT))\n            ?.result?.textPreview;\n          if (neuron && neuron.match(PATTERN_CYBER)) {\n            const communityItem = {\n              ...getExistingOrDefault(cid),\n              neuron,\n              particle: cid,\n              following: true,\n            } as CommunityDto;\n\n            await dbApi.putCommunity(communityItem);\n            communityUpdatesMap.set(cid, communityItem);\n            subscriber.next({ action: 'add', items: [communityItem] });\n          }\n        })\n      );\n\n      cyblogCh.info(`>>>$ sync community ${address}, done`);\n      // const communityUpdates = [...communityUpdatesMap.values()];\n\n      // if (communityUpdates.length > 0) {\n      //   subscriber.next(communityUpdates);\n      // }\n      subscriber.next({ action: 'complete', items: [] });\n\n      subscriber.complete();\n    })().catch((err) => {\n      cyblogCh.error(`>>>$ sync community ${address}, error`, { error: err });\n      subscriber.error(err);\n    });\n  });\n};\n\n// eslint-disable-next-line import/no-unused-modules\nexport const fetchCommunity = async (\n  address: NeuronAddress,\n  fetchParticleAsync?: FetchParticleAsync,\n  onResolve?: (community: CommunityDto[]) => void,\n  signal?: AbortSignal\n) => {\n  const communityUpdatesMap = new Map<ParticleCid, CommunityDto>();\n\n  const getExistingOrDefault = (cid: ParticleCid): Partial<CommunityDto> =>\n    communityUpdatesMap.get(cid) || {\n      ownerId: address,\n      name: '',\n      following: false,\n      follower: false,\n    };\n\n  const followsCids = await getFollowsAsCid(address, signal);\n  const followers = await getFollowers(address, signal);\n\n  console.log(`>>> sync community ${address} processing without store`);\n\n  const followsPromise = Promise.all(\n    followsCids.map(async (cid) => {\n      const neuron = (await fetchParticleAsync!(cid))?.result?.textPreview;\n      if (neuron && neuron.match(PATTERN_CYBER)) {\n        const communityItem = {\n          ...getExistingOrDefault(cid),\n          neuron,\n          particle: cid,\n          following: true,\n        } as CommunityDto;\n        communityUpdatesMap.set(cid, communityItem);\n        onResolve && !signal?.aborted && onResolve([communityItem]);\n      }\n    })\n  );\n\n  const followersPromise = Promise.all(\n    followers.map(async (neuron) => {\n      const cid = await getIpfsHash(neuron);\n\n      const communityItem = {\n        ...getExistingOrDefault(cid),\n        particle: cid,\n        neuron,\n        follower: true,\n      } as CommunityDto;\n\n      communityUpdatesMap.set(cid, communityItem);\n      onResolve && !signal?.aborted && onResolve([communityItem]);\n    })\n  );\n\n  await Promise.all([followersPromise, followsPromise]);\n};\n","import axios from 'axios';\nimport { NeuronAddress, ParticleCid } from 'src/types/base';\nimport { CID_FOLLOW } from 'src/constants/app';\nimport { getIpfsHash } from 'src/utils/ipfs/helpers';\nimport { LCD_URL } from 'src/constants/config';\n// import { Api } from 'src/generated/lcd';\n\n// const lcdApi = new Api({ baseURL: LCD_URL });\n\nexport const getFollowsAsCid = async (\n  address: NeuronAddress,\n  signal?: AbortSignal\n): Promise<ParticleCid[]> => {\n  // const response = await lcdApi.cosmos.getTxsEvent(\n  //   {\n  //     events: [\n  //       `cyberlink.neuron=${address}`,\n  //       `cyberlink.particleFrom=${CID_FOLLOW}`,\n  //     ],\n  //     paginationLimit: '1000000000',\n  //   },\n  //   { signal }\n  // );\n\n  const response = await axios({\n    method: 'get',\n    url: `${LCD_URL}/txs?cyberlink.neuron=${address}&cyberlink.particleFrom=${CID_FOLLOW}&limit=1000000000`,\n    signal,\n  });\n\n  if (!response.data.txs) {\n    return [];\n  }\n  return response.data.txs.map(\n    (item) => item.tx.value.msg[0].value.links[0].to\n  );\n};\n\nexport const getFollowers = async (\n  address: NeuronAddress,\n  signal?: AbortSignal\n): Promise<NeuronAddress[]> => {\n  const addressHash = await getIpfsHash(address);\n\n  const response = await axios({\n    method: 'get',\n    url: `${LCD_URL}/txs?cyberlink.particleFrom=${CID_FOLLOW}&cyberlink.particleTo=${addressHash}&limit=1000000000`,\n    signal,\n  });\n\n  if (!response.data.txs) {\n    return [];\n  }\n  return response.data.txs.map((item) => item.tx.value.msg[0].value.neuron);\n};\n","import { BehaviorSubject, Observable, first } from 'rxjs';\nimport { LinkDto } from 'src/services/CozoDb/types/dto';\nimport { IPFSContent } from 'src/services/ipfs/types';\nimport { mapParticleToEntity } from 'src/services/CozoDb/mapping';\nimport { QueueChannelMessage } from './types';\nimport { CYB_QUEUE_CHANNEL } from '../consts';\n\nimport { enqueueParticleEmbeddingMaybe } from './backendQueueSenders';\nimport ParticlesResolverQueue from '../../services/sync/services/ParticlesResolverQueue/ParticlesResolverQueue';\nimport DbApi from '../../services/DbApi/DbApi';\n\nimport { SyncQueueItem } from '../../services/sync/services/ParticlesResolverQueue/types';\nimport { Option } from 'src/types';\n\nclass BackendQueueChannelListener {\n  private channel = new BroadcastChannel(CYB_QUEUE_CHANNEL);\n\n  private particlesResolver: ParticlesResolverQueue;\n\n  private dbInstance$: BehaviorSubject<Option<DbApi>>;\n\n  constructor(\n    particlesResolver: ParticlesResolverQueue,\n    dbInstance$: Observable<DbApi | undefined>\n  ) {\n    this.particlesResolver = particlesResolver;\n    dbInstance$.subscribe((v) => {\n      this.dbInstance$.next(v);\n    });\n    this.dbInstance$ = new BehaviorSubject<Option<DbApi>>(undefined);\n\n    this.channel.onmessage = (event) => this.onMessage(event);\n\n    this.channel.onmessageerror = (event) =>\n      console.error(`${CYB_QUEUE_CHANNEL} error`, event);\n  }\n\n  private async getDeffredDbApi(): Promise<DbApi> {\n    return new Promise((resolve) => {\n      const dbApi = this.dbInstance$.getValue();\n      if (dbApi) {\n        resolve(dbApi);\n      }\n\n      this.dbInstance$\n        .pipe(\n          first((value) => value !== undefined) // Automatically unsubscribes after the first valid value\n        )\n        .subscribe((value) => {\n          resolve(value as DbApi);\n        });\n    });\n  }\n\n  private async saveLinks(links: LinkDto[]) {\n    const dbApi = await this.getDeffredDbApi();\n    const res = await dbApi.putCyberlinks(links);\n    // console.log('---saveLinks done', links, res);\n  }\n\n  private async saveParticles(content: IPFSContent) {\n    try {\n      const dbApi = await this.getDeffredDbApi();\n      const entity = mapParticleToEntity(content);\n      const result = await dbApi.putParticles(entity);\n      if (result.ok) {\n        await enqueueParticleEmbeddingMaybe(content);\n      }\n    } catch (e) {\n      console.log(\n        '---saveParticle e',\n        content,\n        content.textPreview,\n        e.toString()\n      );\n      throw e;\n    }\n  }\n\n  private async enquueSync(data: SyncQueueItem | SyncQueueItem[]) {\n    // TODO: TMP ASYNC WAIT TO INIT DB\n    await this.getDeffredDbApi();\n\n    this.particlesResolver.enqueue(Array.isArray(data) ? data : [data]);\n  }\n\n  private onMessage(msg: MessageEvent<QueueChannelMessage>) {\n    const { type, data } = msg.data;\n    if (type === 'link') {\n      this.saveLinks(data);\n    } else if (type === 'particle') {\n      this.saveParticles(data);\n    } else if (type === 'sync') {\n      this.enquueSync(data);\n    }\n  }\n}\n\nexport default BackendQueueChannelListener;\n","/* eslint-disable no-restricted-syntax */\nimport { Observable, combineLatest } from 'rxjs';\nimport { map } from 'rxjs/operators';\n\nimport BroadcastChannelSender from '../../channels/BroadcastChannelSender';\n\nimport ParticlesResolverQueue from './services/ParticlesResolverQueue/ParticlesResolverQueue';\n\n// import SyncIpfsLoop from './services/SyncIpfsLoop/SyncIpfsLoop';\nimport SyncTransactionsLoop from './services/SyncTransactionsLoop/SyncTransactionsLoop';\nimport SyncParticlesLoop from './services/SyncParticlesLoop/SyncParticlesLoop';\n\nimport { ServiceDeps } from './services/types';\nimport {\n  MY_FRIENDS_SYNC_INTERVAL,\n  MY_PARTICLES_SYNC_INTERVAL,\n} from './services/consts';\nimport SyncMyFriendsLoop from './services/SyncMyFriendsLoop/SyncMyFriendsLoop';\nimport { SyncEntryName } from '../../types/services';\nimport BaseSyncLoop from './services/BaseSyncLoop/BaseSyncLoop';\nimport createCommunitySync$ from './services/CommunitySync/CommunitySync';\nimport { createCyblogChannel } from 'src/utils/logging/cyblog';\nimport BackendQueueChannelListener from '../../channels/BackendQueueChannel/BackendQueueChannel';\n\nconst cyblogCh = createCyblogChannel({ thread: 'bckd' });\n\n// eslint-disable-next-line import/prefer-default-export\nexport class SyncService {\n  private isInitialized$: Observable<boolean>;\n\n  private channelApi = new BroadcastChannelSender();\n\n  private loops: Partial<Record<SyncEntryName, BaseSyncLoop>> = {};\n\n  constructor(deps: ServiceDeps) {\n    const { dbInstance$, ipfsInstance$ } = deps;\n\n    const particlesResolver = new ParticlesResolverQueue(deps).start();\n\n    const queueListener = new BackendQueueChannelListener(\n      particlesResolver,\n      dbInstance$\n    );\n\n    this.isInitialized$ = combineLatest([dbInstance$, ipfsInstance$]).pipe(\n      map(([dbInstance, ipfsInstance]) => !!dbInstance && !!ipfsInstance)\n    );\n    // subscribe when started\n    this.isInitialized$.subscribe({\n      next: (result) => {\n        return result && this.channelApi.postServiceStatus('sync', 'started');\n      },\n      error: (err) => this.channelApi.postServiceStatus('sync', 'error', err),\n    });\n\n    const communitySync$ = createCommunitySync$(deps);\n    communitySync$.subscribe((community) => {\n      cyblogCh.info('--> community fetched', {\n        unit: 'community',\n        data: community,\n      });\n    });\n\n    const followings$ = communitySync$.pipe(\n      map((c) => c.filter((i) => i.following)),\n      map((c) => c.map((i) => i.neuron))\n    );\n\n    // new SyncIpfsLoop(deps, particlesResolver).start();\n\n    new SyncTransactionsLoop('transactions', deps, particlesResolver).start();\n\n    new SyncParticlesLoop(\n      'particles',\n      MY_PARTICLES_SYNC_INTERVAL,\n      deps,\n      particlesResolver\n    ).start();\n\n    new SyncMyFriendsLoop(\n      'my-friends',\n      MY_FRIENDS_SYNC_INTERVAL,\n      { ...deps, followings$ },\n      particlesResolver\n      // { warmupMs: 1000 }\n    ).start();\n  }\n\n  public restart(name: SyncEntryName) {\n    this.loops[name]?.restart();\n  }\n}\n","import {\n  Observable,\n  combineLatest,\n  defer,\n  distinctUntilChanged,\n  filter,\n  map,\n  switchMap,\n} from 'rxjs';\n\nimport {\n  SyncCommunityResult,\n  fetchStoredSyncCommunity$,\n} from 'src/services/community/community';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport { CommunityDto } from 'src/services/CozoDb/types/dto';\nimport { ServiceDeps } from '../types';\n\n// eslint-disable-next-line import/no-unused-modules\nexport default function createCommunitySync$(\n  deps: ServiceDeps\n): Observable<CommunityDto[]> {\n  const { dbInstance$, ipfsInstance$, params$ } = deps;\n  const channel = new BroadcastChannelSender();\n\n  return combineLatest([\n    dbInstance$,\n    params$!.pipe(\n      map((params) => params.myAddress),\n      distinctUntilChanged()\n    ),\n    ipfsInstance$,\n  ]).pipe(\n    filter(\n      ([dbInstance, myAddress, ipfsInstance]) =>\n        !!dbInstance && !!ipfsInstance && !!myAddress\n    ),\n    switchMap(([dbApi, myAddress, ipfsInstance]) => {\n      const { waitForParticleResolve } = deps;\n      let community: CommunityDto[] = []; // Fix: Add type declaration for community array\n      return new Observable<CommunityDto[]>((observer) => {\n        observer.next([]);\n\n        fetchStoredSyncCommunity$(\n          dbApi!,\n          myAddress!,\n          waitForParticleResolve!\n        ).subscribe(({ action, items }: SyncCommunityResult) => {\n          channel.post({ type: 'load_community', value: { action, items } });\n\n          if (action === 'reset') {\n            community = [];\n          } else if (['add', 'complete'].some((s) => s === action)) {\n            community.push(...items);\n          }\n\n          if (action === 'complete') {\n            observer.next(community);\n            observer.complete();\n          }\n        });\n      });\n    })\n  );\n}\n","/* eslint-disable valid-jsdoc */\n/* eslint-disable import/no-unused-modules */\nimport { fileTypeFromBuffer } from 'file-type';\nimport { concat as uint8ArrayConcat } from 'uint8arrays/concat';\nimport { Uint8ArrayLike } from '../types';\n\ntype ResultWithMime = {\n  result: Uint8ArrayLike;\n  mime: string | undefined;\n  firstChunk: Uint8Array | undefined;\n};\n\ntype StreamDoneCallback = (\n  chunks: Array<Uint8Array>,\n  mime: string | undefined\n) => Promise<void> | void;\n\n// interface AsyncIterableWithReturn<T> extends AsyncIterable<T> {\n//   return?: (value?: unknown) => Promise<IteratorResult<T>>;\n// }\n\nexport const getMimeFromUint8Array = async (\n  raw: Uint8Array | undefined\n): Promise<string | undefined> => {\n  if (!raw) {\n    return 'unknown';\n  }\n  // TODO: try to pass only first N-bytes\n  const fileType = await fileTypeFromBuffer(raw);\n\n  return fileType?.mime || 'text/plain';\n};\n\nexport async function toAsyncIterableWithMime(\n  stream: ReadableStream<Uint8Array>,\n  flush?: StreamDoneCallback\n): Promise<ResultWithMime> {\n  const [firstChunkStream, fullStream] = stream.tee();\n  const chunks: Array<Uint8Array> = []; // accumulate all the data to pim/save\n\n  // Read the first chunk from the stream\n  const firstReader = firstChunkStream.getReader();\n  const { value } = await firstReader.read();\n  const mime = value ? await getMimeFromUint8Array(value) : undefined;\n\n  const restReader = fullStream.getReader();\n\n  const asyncIterable: AsyncIterable<Uint8Array> = {\n    async *[Symbol.asyncIterator]() {\n      while (true) {\n        const { done, value } = await restReader.read();\n        if (done) {\n          flush && flush(chunks, mime);\n          return; // Exit the loop when done\n        }\n        flush && chunks.push(value);\n        yield value; // Yield the value to the consumer\n      }\n    },\n  };\n\n  return { mime, result: asyncIterable, firstChunk: value };\n}\n\nexport async function toReadableStreamWithMime(\n  stream: ReadableStream<Uint8Array>,\n  flush?: StreamDoneCallback\n): Promise<ResultWithMime> {\n  const [firstChunkStream, fullStream] = stream.tee();\n  const chunks: Array<Uint8Array> = []; // accumulate all the data to pim/save\n\n  // Read the first chunk from the stream\n  const firstReader = firstChunkStream.getReader();\n  const { value } = await firstReader.read();\n  const mime = value ? await getMimeFromUint8Array(value) : undefined;\n\n  const modifiedStream = new ReadableStream<Uint8Array>({\n    async pull(controller) {\n      const restReader = fullStream.getReader();\n      const { done, value } = await restReader.read();\n      if (done) {\n        controller.close();\n        flush && flush(chunks, mime);\n      } else {\n        controller.enqueue(value);\n        flush && chunks.push(value);\n      }\n      restReader.releaseLock();\n    },\n    cancel() {\n      firstChunkStream.cancel();\n      fullStream.cancel();\n    },\n  });\n\n  return { mime, result: modifiedStream, firstChunk: value };\n}\n\nexport type onProgressCallback = (progress: number) => void;\n\nexport const getResponseResult = async (\n  response: Uint8ArrayLike,\n  onProgress?: onProgressCallback\n) => {\n  let bytesDownloaded = 0;\n  try {\n    if (response instanceof Uint8Array) {\n      onProgress && onProgress(response.byteLength);\n      return response;\n    }\n    const chunks: Array<Uint8Array> = [];\n\n    if (response instanceof ReadableStream) {\n      const reader = response.getReader();\n\n      const readStream = async ({\n        done,\n        value,\n      }: ReadableStreamReadResult<Uint8Array>): Promise<Uint8Array> => {\n        if (done) {\n          return uint8ArrayConcat(chunks);\n        }\n\n        chunks.push(value!);\n        bytesDownloaded += value!.byteLength;\n        onProgress && onProgress(bytesDownloaded);\n        return reader.read().then(readStream);\n      };\n\n      const readArray: Uint8Array = await reader.read().then(readStream);\n\n      return readArray;\n    }\n\n    if (Symbol.asyncIterator in response) {\n      const reader = response[Symbol.asyncIterator]();\n\n      // if (cid === 'QmRqms6Utkk6L4mtyLQXY2spcQ8Pk7fBBTNjvxa9jTNrXp') {\n      //   debugger;\n      // }\n      // eslint-disable-next-line no-restricted-syntax\n      for await (const chunk of reader) {\n        if (chunk instanceof Uint8Array) {\n          chunks.push(chunk);\n          bytesDownloaded += chunk.byteLength;\n          onProgress && onProgress(bytesDownloaded);\n        }\n      }\n      const result = uint8ArrayConcat(chunks);\n      return result;\n    }\n    return undefined;\n  } catch (error) {\n    console.error(\n      `Error reading stream/iterable.\\r\\n Probably Hot reload error!`,\n      error\n    );\n\n    // throw error;\n\n    return undefined;\n  }\n};\n","import Dexie from 'dexie';\n\nconst db = new Dexie('cyber-page-cash');\ndb.version(3).stores({\n  cid: 'cid',\n  following: 'cid',\n});\n\nexport default db;\n","import db from 'src/db';\n\nconst ipfsCacheDb = () => {\n  const add = async (cid: string, raw: Uint8Array): Promise<void> => {\n    const dbValue = await db.table('cid').get({ cid });\n\n    if (!dbValue) {\n      const ipfsContentAddtToInddexdDB = {\n        cid,\n        data: raw,\n      };\n      db.table('cid').add(ipfsContentAddtToInddexdDB);\n    }\n  };\n\n  const get = async (cid: string): Promise<Uint8Array | undefined> => {\n    // TODO: use cursor\n    const dbValue = await db.table('cid').get({ cid });\n\n    // backward compatibility\n    return dbValue?.data || dbValue?.content;\n  };\n\n  return { add, get };\n};\n\nexport default ipfsCacheDb();\n","import { IPFSNodes, IpfsOptsType } from './types';\n\nexport const CYBER_NODE_SWARM_PEER_ID =\n  'QmUgmRxoLtGERot7Y6G7UyF6fwvnusQZfGR15PuE6pY3aB';\n\nexport const CYBERNODE_SWARM_ADDR_WSS = `/dns4/swarm.io.cybernode.ai/tcp/443/wss/p2p/${CYBER_NODE_SWARM_PEER_ID}`;\nexport const CYBERNODE_SWARM_ADDR_TCP = `/ip4/88.99.105.146/tcp/4001/p2p/${CYBER_NODE_SWARM_PEER_ID}`;\n\nexport const IPFS_CLUSTER_URL = 'https://io.cybernode.ai';\n\nexport const CYBER_GATEWAY_URL = 'https://gateway.ipfs.cybernode.ai';\n\nexport const FILE_SIZE_DOWNLOAD = 20 * 10 ** 6;\n\nexport const getIpfsOpts = () => {\n  let ipfsOpts: IpfsOptsType = {\n    ipfsNodeType: IPFSNodes.HELIA,\n    urlOpts: '/ip4/127.0.0.1/tcp/5001', // default url\n    userGateway: 'http://127.0.0.1:8080',\n  };\n\n  // get type ipfs\n  const lsTypeIpfs = localStorage.getItem('ipfsState');\n  if (lsTypeIpfs !== null) {\n    const lsTypeIpfsData = JSON.parse(lsTypeIpfs);\n    ipfsOpts = { ...ipfsOpts, ...lsTypeIpfsData };\n  }\n\n  localStorage.setItem('ipfsState', JSON.stringify(ipfsOpts));\n\n  return ipfsOpts as IpfsOptsType;\n};\n","import {\n  AddResponse,\n  PinResponse,\n} from '@nftstorage/ipfs-cluster/dist/src/interface';\n\nimport { Cluster } from '@nftstorage/ipfs-cluster';\nimport { IPFS_CLUSTER_URL } from '../config';\n\nconst cyberCluster = () => {\n  const cluster = new Cluster(IPFS_CLUSTER_URL);\n\n  const add = async (\n    file: File | string\n  ): Promise<AddResponse | PinResponse | undefined> => {\n    const dataFile =\n      typeof file === 'string' ? new File([file], 'file.txt') : file;\n    return cluster.add(dataFile, { cidVersion: 0, rawLeaves: false });\n  };\n\n  const status = async (cid: string) => cluster.status(cid);\n  return { add, status };\n};\n\nexport default cyberCluster();\n","import { toString as uint8ArrayToAsciiString } from 'uint8arrays/to-string';\nimport isSvg from 'is-svg';\nimport { PATTERN_HTTP, PATTERN_IPFS_HASH } from 'src/constants/patterns';\nimport { Option } from 'src/types';\n\nimport { shortenString } from 'src/utils/string';\nimport {\n  IPFSContentDetails,\n  IPFSContent,\n  IpfsContentType,\n  IpfsGatewayContentType,\n} from '../types';\nimport { getResponseResult, onProgressCallback } from './stream';\n\nfunction createObjectURL(rawData: Uint8Array, type: string) {\n  const blob = new Blob([rawData], { type });\n  return URL.createObjectURL(blob);\n}\n\nfunction createImgData(rawData: Uint8Array, type: string) {\n  const imgBase64 = uint8ArrayToAsciiString(rawData, 'base64');\n  const file = `data:${type};base64,${imgBase64}`;\n  return file;\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport const detectGatewayContentType = (\n  mime: string | undefined\n): Option<IpfsGatewayContentType> => {\n  if (mime) {\n    if (mime.includes('video')) {\n      return 'video';\n    }\n\n    if (mime.includes('audio')) {\n      return 'audio';\n    }\n\n    if (mime.includes('epub')) {\n      return 'epub';\n    }\n  }\n  return undefined;\n};\n\nconst basic = /\\s?<!doctype html>|(<html\\b[^>]*>|<body\\b[^>]*>|<x-[^>]+>)+/i;\n\nfunction isHtml(string: string) {\n  const newString = string.trim().slice(0, 1000);\n  return basic.test(newString);\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport const chunksToBlob = (\n  chunks: Array<Uint8Array>,\n  mime: string | undefined\n) => new Blob(chunks, mime ? { type: mime } : {});\n\n// eslint-disable-next-line import/no-unused-modules\nexport const mimeToBaseContentType = (\n  mime: string | undefined\n): IpfsContentType => {\n  if (!mime) {\n    return 'other';\n  }\n\n  const initialType = detectGatewayContentType(mime);\n  if (initialType) {\n    return initialType;\n  }\n\n  if (\n    mime.indexOf('text/plain') !== -1 ||\n    mime.indexOf('application/xml') !== -1\n  ) {\n    return 'text';\n  }\n  if (mime.indexOf('image') !== -1) {\n    return 'image';\n  }\n  if (mime.indexOf('application/pdf') !== -1) {\n    return 'pdf';\n  }\n  return 'other';\n};\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport const parseArrayLikeToDetails = async (\n  content: Option<IPFSContent>,\n  cid: string,\n  onProgress?: onProgressCallback\n): Promise<IPFSContentDetails> => {\n  // try {\n  if (!content || !content?.result) {\n    return {\n      gateway: true,\n      text: cid.toString(),\n      cid,\n    };\n  }\n\n  const { result, meta } = content;\n\n  const { mime, contentType } = meta;\n\n  if (!mime) {\n    return {\n      cid,\n      gateway: true,\n      text: `Can't detect MIME for ${cid.toString()}`,\n    };\n  }\n  const contentCid = content.cid;\n\n  const response: IPFSContentDetails = {\n    link: `/ipfs/${cid}`,\n    gateway: false,\n    cid: contentCid,\n    type: contentType,\n  };\n\n  if (detectGatewayContentType(mime)) {\n    return { ...response, gateway: true };\n  }\n\n  const rawData =\n    typeof result !== 'string'\n      ? await getResponseResult(result, onProgress)\n      : result;\n\n  const isStringData = typeof rawData === 'string';\n\n  // console.log(rawData);\n  if (!rawData) {\n    return {\n      ...response,\n      gateway: true,\n      text: `Can't parse content for ${cid.toString()}`,\n    };\n  }\n\n  // clarify text-content subtypes\n  if (response.type === 'text') {\n    // render svg as image\n    if (!isStringData && isSvg(Buffer.from(rawData))) {\n      return {\n        ...response,\n        type: 'image',\n        content: createImgData(rawData, 'image/svg+xml'),\n      };\n    }\n\n    const str = isStringData ? rawData : uint8ArrayToAsciiString(rawData);\n\n    if (str.match(PATTERN_IPFS_HASH)) {\n      return {\n        ...response,\n        type: 'cid',\n        content: str,\n      };\n    }\n    if (str.match(PATTERN_HTTP)) {\n      return {\n        ...response,\n        type: 'link',\n        content: str,\n      };\n    }\n    if (isHtml(str)) {\n      return {\n        ...response,\n        type: 'html',\n        gateway: true,\n        content: cid.toString(),\n      };\n    }\n\n    // TODO: search can bel longer for 42???!\n    // also cover ipns links\n    return {\n      ...response,\n      link: str.length > 42 ? `/ipfs/${cid}` : `/search/${str}`,\n      type: 'text',\n      text: shortenString(str),\n      content: str,\n    };\n  }\n\n  if (!isStringData) {\n    if (response.type === 'image') {\n      return { ...response, content: createImgData(rawData, mime) }; // file\n    }\n    if (response.type === 'pdf') {\n      return {\n        ...response,\n        content: createObjectURL(rawData, mime),\n        gateway: true,\n      }; // file\n    }\n  }\n\n  return response;\n  // } catch (e) {\n  //   console.log('----parseRawIpfsData', e, cid);\n  //   return undefined;\n  // }\n};\n\nexport const contentToUint8Array = async (\n  content: File | string\n): Promise<Uint8Array> => {\n  return new Uint8Array(\n    typeof content === 'string'\n      ? Buffer.from(content)\n      : await content.arrayBuffer()\n  );\n};\n\nexport const createTextPreview = (\n  array: Uint8Array | undefined | string,\n  contentType: IpfsContentType,\n  previewLength = 150\n) => {\n  if (!array) {\n    return undefined;\n  }\n  if (typeof array === 'string') {\n    return array.slice(0, previewLength);\n  }\n  return contentType && contentType === 'text'\n    ? uint8ArrayToAsciiString(array).slice(0, previewLength)\n    : undefined;\n};\n","/* eslint-disable import/no-unused-modules */\nimport { concat as uint8ArrayConcat } from 'uint8arrays/concat';\n\nimport { Option } from 'src/types';\nimport {\n  // getIpfsUserGatewanAndNodeType,\n  IPFSContentMeta,\n  CallBackFuncStatus,\n  IpfsContentSource,\n  IpfsNode,\n  IpfsFileStats,\n  IPFSContent,\n} from '../types';\n\nimport { getMimeFromUint8Array, toAsyncIterableWithMime } from './stream';\n\nimport ipfsCacheDb from './ipfsCacheDb';\nimport cyberCluster from './cluster';\n\nimport {\n  contentToUint8Array,\n  createTextPreview,\n  mimeToBaseContentType,\n} from './content';\n\nimport { CYBER_GATEWAY_URL, FILE_SIZE_DOWNLOAD } from '../config';\n\n// Get data by CID from local storage\nconst loadIPFSContentFromDb = async (\n  cid: string\n): Promise<Option<IPFSContent>> => {\n  // TODO: enable, disabled for tests\n\n  // TODO: use cursor\n  const data = await ipfsCacheDb.get(cid);\n  if (data && data.length) {\n    // TODO: use cursor\n    const mime = await getMimeFromUint8Array(data);\n    const contentType = mimeToBaseContentType(mime);\n\n    const textPreview = createTextPreview(data, contentType);\n\n    const meta: IPFSContentMeta = {\n      type: 'file', // `TODO: ipfs refactor dir support ?\n      size: data.length,\n      sizeLocal: data.length,\n      mime,\n      contentType,\n    };\n    return { result: data, cid, meta, source: 'db', textPreview };\n  }\n\n  return undefined;\n};\n\nconst emptyStats: IpfsFileStats = {\n  type: 'file',\n  size: undefined,\n  sizeLocal: undefined,\n  blocks: undefined,\n};\n\nconst fetchIPFSContentStat = async (\n  cid: string,\n  node?: IpfsNode,\n  signal?: AbortSignal\n): Promise<IpfsFileStats> => {\n  if (node) {\n    const stats = await node.stat(cid, { signal });\n    return stats;\n  }\n  return emptyStats;\n};\n\nconst fetchIPFSContentFromNode = async (\n  cid: string,\n  node?: IpfsNode,\n  controller?: AbortController\n): Promise<Option<IPFSContent>> => {\n  const controllerLegacy = controller || new AbortController();\n  const { signal } = controllerLegacy;\n  let timer: NodeJS.Timeout | undefined;\n\n  if (!node) {\n    console.log('--------fetchIPFSContentFromNode NO NODE INTIALIZED--------');\n    return undefined;\n  }\n\n  if (!controller) {\n    timer = setTimeout(() => {\n      controllerLegacy.abort();\n    }, 1000 * 60 * 1);\n  } // 1 min\n\n  // TODO: cover ipns case\n  try {\n    // const stat = await node.files.stat(path, { signal });\n    const startTime = Date.now();\n    const stats = await fetchIPFSContentStat(cid, node, signal);\n    const meta = stats as IPFSContentMeta;\n    const statsDoneTime = Date.now();\n    meta.statsTime = statsDoneTime - startTime;\n    const allowedSize = stats.size ? stats.size < FILE_SIZE_DOWNLOAD : false;\n    timer && clearTimeout(timer);\n\n    switch (stats.type) {\n      case 'directory': {\n        // TODO: return directory structure\n        return { cid, availableDownload: true, source: 'node', meta: stats };\n      }\n      default: {\n        // Get sample of content\n        const { value: firstChunk } = await node\n          .cat(cid, { signal, length: 2048, offset: 0 })\n          [Symbol.asyncIterator]()\n          .next();\n\n        meta.mime = await getMimeFromUint8Array(firstChunk);\n        meta.contentType = mimeToBaseContentType(meta.mime);\n        const fullyDownloaded =\n          stats.size && stats.size > -1 && firstChunk.length >= stats.size;\n\n        const textPreview = createTextPreview(firstChunk, meta.contentType);\n\n        if (fullyDownloaded) {\n          await ipfsCacheDb.add(cid, uint8ArrayConcat([firstChunk]));\n        }\n\n        // If all content fits in first chunk return byte-array instead iterable\n        const stream = fullyDownloaded\n          ? firstChunk\n          : allowedSize\n          ? node.cat(cid, { signal })\n          : undefined;\n\n        meta.catTime = Date.now() - statsDoneTime;\n\n        // TODO: add to db flag that content is pinned TO local node\n        // if already pinned skip pin\n        if (!meta.local && allowedSize) {\n          node.pin(cid);\n\n          meta.pinTime = Date.now() - meta.catTime;\n        } else {\n          meta.pinTime = -1;\n        }\n\n        return {\n          result: stream,\n          textPreview,\n          cid,\n          meta,\n          source: 'node',\n        };\n        // }\n      }\n    }\n  } catch (error) {\n    console.debug('error fetchIPFSContentFromNode', error);\n    return {\n      cid,\n      availableDownload: true,\n      source: 'node',\n      meta: { ...emptyStats } as IPFSContentMeta,\n    };\n  }\n};\n\nconst fetchIPFSContentFromGateway = async (\n  cid: string,\n  node?: IpfsNode,\n  controller?: AbortController,\n  headers?: Record<string, string>\n): Promise<Option<IPFSContent>> => {\n  // fetch META only from external node(toooo slow), TODO: fetch meta from cybernode\n  const isExternalNode = node?.nodeType === 'external';\n\n  const stats = isExternalNode\n    ? await fetchIPFSContentStat(cid, node, controller?.signal)\n    : emptyStats;\n\n  const contentUrl = `${CYBER_GATEWAY_URL}/ipfs/${cid}`;\n  const response = await fetch(contentUrl, {\n    method: 'GET',\n    signal: controller?.signal,\n    headers,\n  });\n  if (response && response.body) {\n    // fetch doesn't provide any headers in our case :(\n\n    // const contentLength = parseInt(\n    //   response.headers['content-length'] || '-1',\n    //   10\n    // );\n    // const contentType = response.headers['content-type'];\n\n    // Extract meta if ipfs prob/node not started yet\n    // if (!meta.mime) {\n    //   meta = { ...meta, mime: contentType };\n    // }\n\n    // TODO: fix\n    const flushResults = (chunks: Uint8Array[]) =>\n      !isExternalNode\n        ? ipfsCacheDb.add(cid, uint8ArrayConcat(chunks))\n        : Promise.resolve();\n\n    const { mime, result, firstChunk } = await toAsyncIterableWithMime(\n      response.body,\n      flushResults\n    );\n\n    const contentType = mimeToBaseContentType(mime);\n\n    const textPreview = createTextPreview(firstChunk, contentType);\n    return {\n      cid,\n      textPreview,\n      meta: { ...stats, mime, contentType },\n      result,\n      source: 'gateway',\n      contentUrl,\n    };\n  }\n\n  return undefined;\n};\n\ntype fetchContentOptions = {\n  controller?: AbortController;\n  node?: IpfsNode;\n  headers?: Record<string, string>;\n};\n\nasync function fetchIpfsContent(\n  cid: string,\n  source: IpfsContentSource,\n  options: fetchContentOptions\n): Promise<Option<IPFSContent>> {\n  const { node, controller, headers } = options;\n\n  try {\n    switch (source) {\n      case 'db':\n        return loadIPFSContentFromDb(cid);\n      case 'node':\n        return fetchIPFSContentFromNode(cid, node, controller);\n      case 'gateway':\n        return fetchIPFSContentFromGateway(cid, node, controller, headers);\n      default:\n        return undefined;\n    }\n  } catch (e) {\n    console.log('----fetchIpfsContent error', e);\n    return undefined;\n  }\n}\n\nconst getIPFSContent = async (\n  cid: string,\n  node?: IpfsNode,\n  controller?: AbortController,\n  callBackFuncStatus?: CallBackFuncStatus\n): Promise<Option<IPFSContent>> => {\n  const dataRsponseDb = await loadIPFSContentFromDb(cid);\n  if (dataRsponseDb !== undefined) {\n    return dataRsponseDb;\n  }\n\n  if (node) {\n    callBackFuncStatus && callBackFuncStatus('trying to get with a node');\n    // console.log('----Fetch from node', cid);\n    const ipfsContent = await fetchIPFSContentFromNode(cid, node, controller);\n\n    return ipfsContent;\n  }\n\n  callBackFuncStatus && callBackFuncStatus('trying to get with a gatway');\n  // console.log('----Fetch from gateway', cid);\n  const respnseGateway = await fetchIPFSContentFromGateway(\n    cid,\n    node,\n    controller\n  );\n\n  return respnseGateway;\n};\n\nconst catIPFSContentFromNode = (\n  cid: string,\n  node?: IpfsNode,\n  offset?: number,\n  controller?: AbortController\n): AsyncIterable<Uint8Array> | undefined => {\n  if (!node) {\n    console.log(\n      '--------fetchIPFSContentFromNode NO NODE INTIALIZED TODO: cover case--------'\n    );\n    return undefined;\n  }\n\n  // TODO: cover ipns case\n\n  return node.cat(cid, { offset, signal: controller?.signal });\n};\n\n// const nodeContentFindProvs = async (\n//   node: AppIPFS,\n//   cid: string,\n//   offset: number,\n//   controller?: AbortController\n// ): AsyncIterable<number> | undefined => {\n//   if (!node) {\n//     console.log(\n//       '--------fetchIPFSContentFromNode NO NODE INTIALIZED TODO: cover case--------'\n//     );\n//     return undefined;\n//   }\n\n//   // TODO: cover ipns case\n//   const path = `/ipfs/${cid}`;\n\n//   const providers = node.dht.findProvs(path, {\n//     signal: controller?.signal,\n//   });\n\n//   let count = 0;\n//   for await (const provider of providers) {\n//     //  console.log(provider.id.toString())\n//     //  id: PeerId\n//     // multiaddrs: Multiaddr[]\n//     // protocols: string[]\n//     count++;\n//   }\n\n//   return count;\n// };\n\nconst addContenToIpfs = async (\n  node: IpfsNode,\n  content: File | string\n): Promise<Option<string>> => {\n  let cid;\n  if (node) {\n    cid = await node.add(content);\n  }\n  // TODO: WARN - TMP solution make cluster call non-awaitable\n  cyberCluster.add(content);\n  // Save to local cache\n  cid && (await ipfsCacheDb.add(cid, await contentToUint8Array(content)));\n  return cid;\n};\n\nexport {\n  getIPFSContent,\n  catIPFSContentFromNode,\n  fetchIpfsContent,\n  addContenToIpfs,\n};\n","import { IQueueStrategy, QueueSettings, QueueSource } from './types';\n\nexport class QueueStrategy implements IQueueStrategy {\n  settings: QueueSettings;\n\n  order: QueueSource[];\n\n  constructor(settings: QueueSettings, order: QueueSource[]) {\n    this.settings = settings;\n    this.order = order;\n  }\n\n  getNextSource(source: QueueSource): QueueSource | undefined {\n    const index = this.order.indexOf(source);\n    return index < this.order.length ? this.order[index + 1] : undefined;\n  }\n}\n","export class QueueItemTimeoutError extends Error {\n  constructor(timeoutMs: number) {\n    super(`Timeout after ${timeoutMs}`);\n    Object.setPrototypeOf(this, QueueItemTimeoutError.prototype);\n  }\n}\n","/* eslint-disable import/prefer-default-export */\nexport const CustomHeaders = {\n  XCybSource: 'X-Cyb-Source',\n};\n\nexport enum XCybSourceValues {\n  sharedWorker = 'shared-worker',\n}\n","import {\n  BehaviorSubject,\n  EMPTY,\n  Observable,\n  catchError,\n  combineLatest,\n  debounceTime,\n  distinctUntilChanged,\n  filter,\n  interval,\n  map,\n  merge,\n  mergeMap,\n  of,\n  share,\n  tap,\n  throwError,\n  timeout,\n  withLatestFrom,\n} from 'rxjs';\n\nimport * as R from 'ramda';\n\nimport { CybIpfsNode, IpfsContentSource } from 'src/services/ipfs/types';\nimport { fetchIpfsContent } from 'src/services/ipfs/utils/utils-ipfs';\nimport { ParticleCid } from 'src/types/base';\n\nimport { promiseToObservable } from '../../utils/rxjs/helpers';\n\nimport type {\n  QueueItem,\n  QueueItemAsyncResult,\n  QueueItemCallback,\n  QueueItemOptions,\n  QueueItemResult,\n  QueueSource,\n  QueueStats,\n} from './types';\n\nimport { QueueStrategy } from './QueueStrategy';\n\nimport { enqueueParticleSave } from '../backend/channels/BackendQueueChannel/backendQueueSenders';\nimport BroadcastChannelSender from '../backend/channels/BroadcastChannelSender';\nimport { RuneEngine } from '../scripting/engine';\n\nimport { QueueItemTimeoutError } from './QueueItemTimeoutError';\nimport { CustomHeaders, XCybSourceValues } from './constants';\n\nconst QUEUE_DEBOUNCE_MS = 33;\nconst CONNECTION_KEEPER_RETRY_MS = 15000;\n\nfunction getQueueItemTotalPriority(item: QueueItem): number {\n  return (item.priority || 0) + (item.viewPortPriority || 0);\n}\n\nconst debugCid = (cid: ParticleCid, prefix: string, ...args) => {\n  console.log(`>>> ${prefix}: ${cid}`, ...args);\n};\n\nconst strategies = {\n  external: new QueueStrategy(\n    {\n      db: { timeout: 5000, maxConcurrentExecutions: 999 },\n      node: { timeout: 60 * 1000, maxConcurrentExecutions: 30 },\n      gateway: { timeout: 10000, maxConcurrentExecutions: 11 },\n    },\n    ['db', 'node', 'gateway']\n  ),\n  embedded: new QueueStrategy(\n    {\n      db: { timeout: 5000, maxConcurrentExecutions: 999 },\n      node: { timeout: 60 * 1000, maxConcurrentExecutions: 30 },\n      gateway: { timeout: 21000, maxConcurrentExecutions: 11 },\n    },\n    ['db', 'gateway', 'node']\n  ),\n  helia: new QueueStrategy(\n    {\n      db: { timeout: 5000, maxConcurrentExecutions: 999 },\n      node: { timeout: 60 * 1000, maxConcurrentExecutions: 50 },\n      gateway: { timeout: 10000, maxConcurrentExecutions: 11 },\n    },\n    ['db', 'node', 'gateway']\n  ),\n};\n\ntype QueueMap = Map<ParticleCid, QueueItem>;\n\nclass QueueManager {\n  private queue$ = new BehaviorSubject<QueueMap>(new Map());\n\n  private node: CybIpfsNode | undefined = undefined;\n\n  private strategy: QueueStrategy;\n\n  private queueDebounceMs: number;\n\n  private lastNodeCallTime: number = Date.now();\n\n  private channel = new BroadcastChannelSender();\n\n  private executing: Record<QueueSource, Set<ParticleCid>> = {\n    db: new Set(),\n    node: new Set(),\n    gateway: new Set(),\n  };\n\n  private switchStrategy(strategy: QueueStrategy): void {\n    this.strategy = strategy;\n  }\n\n  public async setNode(node: CybIpfsNode, customStrategy?: QueueStrategy) {\n    console.log(\n      `* switch node from ${this.node?.nodeType || '<none>'} to ${\n        node.nodeType\n      }`\n    );\n    this.node = node;\n    this.switchStrategy(customStrategy || strategies[node.nodeType]);\n  }\n\n  private getItemBySourceAndPriority(queue: QueueMap) {\n    const pendingItems = [...queue.values()].filter(\n      (i) => i.status === 'pending'\n    );\n\n    const pendingBySource = R.groupBy((i) => i.source, pendingItems);\n\n    const itemsToExecute: QueueItem[] = [];\n    // eslint-disable-next-line no-loop-func, no-restricted-syntax\n    for (const [queueSource, items] of Object.entries(pendingBySource)) {\n      const settings = this.strategy.settings[queueSource as IpfsContentSource];\n\n      const executeCount =\n        settings.maxConcurrentExecutions -\n        this.executing[queueSource as IpfsContentSource].size;\n      const itemsByPriority = items\n        .sort(\n          (a, b) => getQueueItemTotalPriority(b) - getQueueItemTotalPriority(a)\n        )\n        .slice(0, executeCount);\n\n      itemsToExecute.push(...itemsByPriority);\n    }\n\n    return itemsToExecute;\n  }\n\n  private postSummary() {\n    const summary = `(total: ${this.queue$.value.size} |  db - ${this.executing.db.size} node - ${this.executing.node.size} gateway - ${this.executing.gateway.size})`;\n    this.channel.postServiceStatus('ipfs', 'started', summary);\n  }\n\n  private fetchData$(item: QueueItem) {\n    const { cid, source, callbacks, controller } = item;\n    // const abortController = controller || new AbortController();\n    const settings = this.strategy.settings[source];\n    this.executing[source].add(cid);\n    this.postSummary();\n    const queueItem = this.queue$.value.get(cid);\n    // Mutate item without next\n    this.queue$.value.set(cid, {\n      ...queueItem,\n      status: 'executing',\n      executionTime: Date.now(),\n      controller: new AbortController(),\n    } as QueueItem);\n    // debugCid(cid, 'fetchData', cid, source);\n    callbacks.map((callback) => callback(cid, 'executing', source));\n\n    return promiseToObservable(async () => {\n      return fetchIpfsContent(cid, source, {\n        controller,\n        node: this.node,\n        headers: {\n          [CustomHeaders.XCybSource]: XCybSourceValues.sharedWorker,\n        },\n      }).then((content) => {\n        // put saveto db msg into bus\n        if (content && source !== 'db') {\n          enqueueParticleSave(content);\n        }\n\n        return content;\n      });\n    }).pipe(\n      timeout({\n        each: settings.timeout,\n        with: () =>\n          throwError(() => {\n            controller?.abort('timeout');\n\n            return new QueueItemTimeoutError(settings.timeout);\n          }),\n      }),\n      map((result): QueueItemResult => {\n        return {\n          item,\n          status: result ? 'completed' : 'error',\n          source,\n          result,\n        };\n      }),\n      catchError((error): Observable<QueueItemResult> => {\n        // debugCid(cid, 'fetchData - fetchIpfsContent catchErr', error);\n        if (error instanceof QueueItemTimeoutError) {\n          return of({\n            item,\n            status: 'timeout',\n            source,\n          });\n        }\n\n        if (error?.name === 'AbortError') {\n          return of({ item, status: 'cancelled', source });\n        }\n        return of({ item, status: 'error', source });\n      })\n    );\n  }\n\n  /**\n   * Mutate queue item, and return new queue\n   * @param cid\n   * @param changes\n   * @returns\n   */\n  private mutateQueueItem(cid: string, changes: Partial<QueueItem>) {\n    const queue = this.queue$.value;\n    const item = queue.get(cid);\n    if (item) {\n      queue.set(cid, { ...item, ...changes });\n    }\n\n    return this.queue$.next(queue);\n  }\n\n  private removeAndNext(cid: string): void {\n    const queue = this.queue$.value;\n    queue.delete(cid);\n    this.queue$.next(queue);\n  }\n\n  // reset status and switch to next source\n  private switchSourceAndNext(item: QueueItem, nextSource: QueueSource): void {\n    item.callbacks.map((callback) => callback(item.cid, 'pending', nextSource));\n\n    this.mutateQueueItem(item.cid, { status: 'pending', source: nextSource });\n  }\n\n  private cancelDeprioritizedItems(queue: QueueMap): QueueMap {\n    (['node', 'gateway'] as IpfsContentSource[]).forEach((source) => {\n      Array.from(this.executing[source]).forEach((cid) => {\n        const item = queue.get(cid);\n        if (item && getQueueItemTotalPriority(item) < 0 && item.controller) {\n          // abort request and move to pending\n          item.controller.abort('cancelled');\n          item.callbacks.map((callback) =>\n            callback(item.cid, 'pending', item.source)\n          );\n\n          queue.set(cid, { ...item, status: 'pending' });\n          // console.log('-----cancel item', item, queue);\n\n          this.executing[source].delete(cid);\n        }\n      });\n    });\n\n    return queue;\n  }\n\n  private releaseExecution(cid: string) {\n    // eslint-disable-next-line no-restricted-syntax\n    Object.keys(this.executing).forEach((key) =>\n      this.executing[key as IpfsContentSource].delete(cid)\n    );\n  }\n\n  constructor(\n    ipfsInstance$: Observable<CybIpfsNode | undefined>,\n    {\n      strategy,\n      queueDebounceMs,\n    }: {\n      strategy?: QueueStrategy;\n      queueDebounceMs?: number;\n    }\n  ) {\n    ipfsInstance$.subscribe((node) => {\n      if (node) {\n        this.setNode(node);\n      }\n    });\n\n    this.strategy = strategy || strategies.embedded;\n    this.queueDebounceMs = queueDebounceMs || QUEUE_DEBOUNCE_MS;\n\n    // Little hack to handle keep-alive connection to swarm cyber node\n    // Fix some lag with node peers(when it shown swarm node in peers but not  connected anymore)\n    interval(CONNECTION_KEEPER_RETRY_MS)\n      .pipe(\n        filter(\n          () =>\n            !!this.node &&\n            !![...this.queue$.value.values()].find((i) => i.source === 'node')\n        )\n      )\n      .subscribe(() => {\n        // console.log(\n        //   '-----reconnect cnt',\n        //   this.queue$.value.size,\n        //   this.queue$.value\n        // );\n        this.node!.reconnectToSwarm(true);\n      });\n\n    const isInitialized$ = combineLatest([ipfsInstance$]).pipe(\n      map(([ipfsInstance]) => !!ipfsInstance && ipfsInstance?.isStarted),\n      distinctUntilChanged(),\n      share()\n    );\n\n    isInitialized$.subscribe((isInitialized) => {\n      isInitialized && console.log(' ipfs queue initialized');\n      this.node?.reconnectToSwarm(true);\n    });\n\n    this.queue$\n      .pipe(\n        withLatestFrom(isInitialized$),\n        filter(([, isInitialized]) => isInitialized),\n        debounceTime(this.queueDebounceMs),\n        map(([queue]) => this.cancelDeprioritizedItems(queue)),\n        mergeMap((queue) => {\n          const workItems = this.getItemBySourceAndPriority(queue);\n          // console.log('---workItems', workItems);\n          if (workItems.length > 0) {\n            // wake up connnection to swarm cyber node\n            this.node?.reconnectToSwarm(false);\n\n            return merge(...workItems.map((item) => this.fetchData$(item)));\n          }\n          return EMPTY;\n        })\n      )\n      .subscribe(({ item, status, source, result }) => {\n        const { cid } = item;\n        const callbacks = this.queue$.value.get(cid)?.callbacks || [];\n        // fix to process dublicated items\n        // debugCid(cid, 'subscribe', cid, source, status, result, callbacks);\n\n        callbacks.map((callback) => callback(cid, status, source, result));\n\n        // HACK to use with GracePeriod for reconnection\n        if (source === 'node') {\n          this.lastNodeCallTime = Date.now();\n        }\n\n        this.executing[source].delete(cid);\n\n        // success execution -> next\n        if (status === 'completed' || status === 'cancelled') {\n          // debugCid(cid, '------done', item, status, source, result);\n          this.removeAndNext(cid);\n        } else {\n          // debugCid(cid, '------error', item, status, source, result);\n\n          // Retry -> (next sources) or -> next\n          const nextSource = this.strategy.getNextSource(source);\n\n          if (nextSource) {\n            this.switchSourceAndNext(item, nextSource);\n          } else {\n            this.removeAndNext(cid);\n            // notify thatn nothing found from all sources\n            callbacks.map((callback) =>\n              callback(cid, 'not_found', source, result)\n            );\n          }\n        }\n\n        this.postSummary();\n      });\n  }\n\n  public enqueue(\n    cid: string,\n    callback: QueueItemCallback,\n    options: QueueItemOptions = {}\n  ): void {\n    const queue = this.queue$.value;\n    const existingItem = queue.get(cid);\n    // debugCid(cid, '----/--enqueue ', cid, existingItem);\n\n    // In case if item already in queue,\n    // just attach one more callback to quieued item\n    if (existingItem) {\n      this.mutateQueueItem(cid, {\n        callbacks: [...existingItem.callbacks, callback],\n      });\n    } else {\n      const source = options.initialSource || this.strategy.order[0];\n      const item: QueueItem = {\n        cid,\n        callbacks: [callback],\n        source, // initial method to fetch\n        status: 'pending',\n        postProcessing: true, // by default rune-post-processing enabled\n        ...options,\n      };\n\n      callback(cid, 'pending', source);\n\n      queue.set(cid, item);\n      this.queue$.next(queue);\n    }\n  }\n\n  public enqueueAndWait(\n    cid: string,\n    options: QueueItemOptions = {}\n  ): Promise<QueueItemAsyncResult> {\n    return new Promise((resolve) => {\n      const callback = ((cid, status, source, result) => {\n        if (status === 'completed' || status === 'not_found') {\n          resolve({ status, source, result });\n        }\n      }) as QueueItemCallback;\n\n      this.enqueue(cid, callback, options);\n    });\n  }\n\n  public updateViewPortPriority(cid: string, viewPortPriority: number) {\n    this.mutateQueueItem(cid, { viewPortPriority });\n  }\n\n  public cancel(cid: string): void {\n    const queue = this.queue$.value;\n    const item = queue.get(cid);\n    // console.log('-----cancel item', item, item?.controller);\n    if (item) {\n      // If item has no abortController we can just remove it,\n      // otherwise abort&keep-to-finalize\n      if (!item.controller) {\n        this.removeAndNext(cid);\n      } else {\n        item.controller.abort('cancelled');\n      }\n    }\n  }\n\n  public cancelByParent(parent: string): void {\n    const queue = this.queue$.value;\n\n    queue.forEach((item, cid) => {\n      if (item.parent === parent) {\n        this.releaseExecution(cid);\n        item.controller?.abort('cancelled');\n        queue.delete(cid);\n      }\n    });\n\n    this.queue$.next(queue);\n  }\n\n  public clear(): void {\n    const queue = this.queue$.value;\n\n    queue.forEach((item, cid) => {\n      this.releaseExecution(cid);\n      item.controller?.abort('cancelled');\n      queue.delete(cid);\n    });\n\n    this.queue$.next(new Map());\n  }\n\n  public getQueueMap(): QueueMap {\n    return this.queue$.value;\n  }\n\n  public getQueueList(): QueueItem[] {\n    return Array.from(this.queue$.value.values());\n  }\n\n  public getStats(): QueueStats[] {\n    const fn = R.pipe(\n      R.countBy<QueueItem>(R.prop('status')),\n      R.toPairs,\n      R.map(R.zipObj(['status', 'count']))\n    );\n\n    return fn(this.getQueueList()) as QueueStats[];\n  }\n}\n\nexport default QueueManager;\n","import { Observable } from 'rxjs';\n\n/**\n * Convert promise to observable\n * @param promiseFactory\n * @returns\n */\nexport function promiseToObservable<T>(promiseFactory: () => Promise<T>) {\n  return new Observable<T>((observer) => {\n    promiseFactory()\n      .then((response) => {\n        observer.next(response);\n        observer.complete();\n      })\n      .catch((error) => {\n        console.debug('----promiseToObservable error', error); //, error\n        observer.error(error);\n      });\n  });\n}\n","import { CID } from 'multiformats/cid';\n\nexport const stringToCid = (s: string) => CID.parse(s);\nexport const stringToIpfsPath = (s: string) => `/ipfs/${s}`;\n","import { IPFSHTTPClient, create as createKuboClient } from 'kubo-rpc-client';\nimport { multiaddr } from '@multiformats/multiaddr';\n\nimport { stringToCid, stringToIpfsPath } from '../../utils/cid';\nimport {\n  AbortOptions,\n  CatOptions,\n  IpfsNodeType,\n  InitOptions,\n  IpfsFileStats,\n  IpfsNode,\n  IpfsNodePrperties,\n} from '../../types';\nimport { CYBER_GATEWAY_URL } from '../../config';\n\nclass KuboNode implements IpfsNode {\n  readonly nodeType: IpfsNodeType = 'external';\n\n  private node?: IPFSHTTPClient;\n\n  private _config: IpfsNodePrperties = {};\n\n  get config() {\n    return this._config;\n  }\n\n  private _isStarted: boolean = false;\n\n  get isStarted() {\n    return this._isStarted;\n  }\n\n  private async initConfig() {\n    const response = await this.node!.config.get('Addresses.Gateway');\n    if (!response) {\n      return { gatewayUrl: CYBER_GATEWAY_URL };\n    }\n    const address = multiaddr(response as string).nodeAddress();\n\n    return { gatewayUrl: `http://${address.address}:${address.port}` };\n  }\n\n  async init(options?: InitOptions) {\n    this.node = createKuboClient(options);\n    this._config = await this.initConfig();\n\n    if (typeof window !== 'undefined') {\n      window.node = this.node;\n      window.toCid = stringToCid;\n    }\n    console.log(\n      'IPFS - Kubo addrs',\n      (await this.node.swarm.localAddrs()).map((a) => a.toString())\n    );\n    this._isStarted = true;\n  }\n\n  async stat(cid: string, options: AbortOptions = {}): Promise<IpfsFileStats> {\n    return this.node!.files.stat(stringToIpfsPath(cid), {\n      ...options,\n      withLocal: true,\n      size: true,\n    }).then((result) => {\n      const { type, size, sizeLocal, local, blocks } = result;\n      return {\n        type,\n        size: size || -1,\n        sizeLocal: sizeLocal || -1,\n        blocks,\n      };\n    });\n  }\n\n  cat(cid: string, options: CatOptions = {}) {\n    return this.node!.cat(stringToCid(cid), options);\n  }\n\n  async add(content: File | string, options: AbortOptions = {}) {\n    return (await this.node!.add(content, options)).cid.toString();\n  }\n\n  async pin(cid: string, options: AbortOptions = {}) {\n    return (await this.node!.pin.add(stringToCid(cid), options)).toString();\n  }\n\n  async getPeers() {\n    return (await this.node!.swarm.peers()).map((c) => c.peer.toString());\n  }\n\n  async stop() {}\n  async start() {}\n\n  async connectPeer(address: string) {\n    const addr = multiaddr(address);\n    await this.node!.bootstrap.add(addr);\n\n    await this.node!.swarm.connect(addr);\n    return true;\n  }\n\n  ls() {\n    return this.node!.pin.ls();\n  }\n\n  async info() {\n    const { repoSize } = await this.node!.stats.repo();\n\n    const responseId = await this.node!.id();\n    const { agentVersion, id } = responseId;\n    return { id: id.toString(), agentVersion, repoSize };\n  }\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport default KuboNode;\n","import { Helia, Pin, createHelia } from 'helia';\nimport { IDBBlockstore } from 'blockstore-idb';\nimport { IDBDatastore } from 'datastore-idb';\nimport { Libp2p, createLibp2p } from 'libp2p';\nimport { noise } from '@chainsafe/libp2p-noise';\nimport { yamux } from '@chainsafe/libp2p-yamux';\n// import { mplex } from '@libp2p/mplex';\n\nimport { circuitRelayTransport } from 'libp2p/circuit-relay';\nimport { UnixFS, unixfs, AddOptions } from '@helia/unixfs';\nimport { bootstrap } from '@libp2p/bootstrap';\nimport { webRTC, webRTCDirect } from '@libp2p/webrtc';\nimport { webSockets } from '@libp2p/websockets';\nimport { webTransport } from '@libp2p/webtransport';\nimport { identifyService } from 'libp2p/identify';\nimport { multiaddr, protocols } from '@multiformats/multiaddr';\nimport { LsResult } from 'ipfs-core-types/src/pin';\n\nimport {\n  AbortOptions,\n  CatOptions,\n  IpfsNodeType,\n  IpfsFileStats,\n  IpfsNode,\n} from '../../types';\n// import { all } from '@libp2p/websockets/filters';\nimport { stringToCid } from '../../utils/cid';\nimport { CYBER_GATEWAY_URL } from '../../config';\n\nasync function* mapToLsResult(\n  iterable: AsyncIterable<Pin>\n): AsyncIterable<LsResult> {\n  // eslint-disable-next-line no-restricted-syntax\n  for await (const item of iterable) {\n    const { cid, metadata } = item;\n    yield { cid: cid.toV0(), metadata, type: 'recursive' };\n  }\n}\n\nconst libp2pFactory = async (\n  datastore: IDBDatastore,\n  bootstrapList: string[] = []\n) => {\n  const libp2p = await createLibp2p({\n    datastore,\n    // addresses: {\n    //   listen: [\n    //     '/ip4/127.0.0.1/tcp/0',\n    //     '/dns4/swarm.io.cybernode.ai/tcp/443/wss/p2p/QmUgmRxoLtGERot7Y6G7UyF6fwvnusQZfGR15PuE6pY3aB',\n    //   ],\n    // },\n    transports: [\n      webSockets(),\n      webTransport(),\n      webRTC({\n        rtcConfiguration: {\n          iceServers: [\n            {\n              urls: [\n                'stun:stun.l.google.com:19302',\n                'stun:global.stun.twilio.com:3478',\n                'STUN:freestun.net:3479',\n                'STUN:stun.bernardoprovenzano.net:3478',\n                'STUN:stun.aa.net.uk:3478',\n              ],\n            },\n            {\n              credential: 'free',\n              username: 'free',\n              urls: ['TURN:freestun.net:3479', 'TURNS:freestun.net:5350'],\n            },\n          ],\n        },\n      }),\n      webRTCDirect(),\n      circuitRelayTransport({\n        discoverRelays: 1,\n      }),\n    ],\n    connectionEncryption: [noise()],\n    streamMuxers: [yamux()],\n    connectionGater: {\n      denyDialMultiaddr: () => {\n        return false;\n        // by default we refuse to dial local addresses from the browser since they\n        // are usually sent by remote peers broadcasting undialable multiaddrs but\n        // here we are explicitly connecting to a local node so do not deny dialing\n        // any discovered address\n      },\n    },\n    peerDiscovery: [\n      bootstrap({\n        list: bootstrapList,\n      }),\n    ],\n    services: {\n      identify: identifyService(),\n    },\n  });\n  return libp2p;\n};\n\nconst addOptionsV0: Partial<AddOptions> = {\n  cidVersion: 0,\n  rawLeaves: false,\n};\n\nclass HeliaNode implements IpfsNode {\n  readonly nodeType: IpfsNodeType = 'helia';\n\n  get config() {\n    return { gatewayUrl: CYBER_GATEWAY_URL };\n  }\n\n  private _isStarted = false;\n\n  get isStarted() {\n    return this._isStarted;\n  }\n\n  private node?: Helia;\n\n  private fs?: UnixFS;\n\n  async init() {\n    const blockstore = new IDBBlockstore('helia-bs');\n    await blockstore.open();\n\n    const datastore = new IDBDatastore('helia-ds');\n    await datastore.open();\n\n    const bootstrapList = [\n      '/dnsaddr/bootstrap.libp2p.io/p2p/QmNnooDu7bfjPFoTZYxMNLWUQJyrVwtbZg5gBMjTezGAJN',\n      '/dnsaddr/bootstrap.libp2p.io/p2p/QmQCU2EcMqAqQPR2i9bChDtGNJchTbq5TbXJJ16u19uLTa',\n      '/dnsaddr/bootstrap.libp2p.io/p2p/QmbLHAnMoJPWSCR5Zhtx6BHJX9KiKNN6tpvbUcqanj75Nb',\n      '/dnsaddr/bootstrap.libp2p.io/p2p/QmcZf59bWwK5XFi76CZX8cbJ4BhTzzA3gU1ZjYZcYW3dwt',\n      '/dns4/swarm.io.cybernode.ai/tcp/443/wss/p2p/QmUgmRxoLtGERot7Y6G7UyF6fwvnusQZfGR15PuE6pY3aB',\n    ];\n    const libp2p = await libp2pFactory(datastore, bootstrapList);\n\n    this.node = await createHelia({ blockstore, datastore, libp2p });\n\n    this.fs = unixfs(this.node);\n\n    if (typeof window !== 'undefined') {\n      window.libp2p = libp2p;\n      window.node = this.node;\n      window.fs = this.fs;\n      window.toCid = stringToCid;\n    }\n\n    // DEBUG\n    libp2p.addEventListener('peer:connect', (evt) => {\n      const peerId = evt.detail.toString();\n      const conn = libp2p.getConnections(peerId) || [];\n      const transportsByAddr = Object.fromEntries(\n        conn.map((c) => [\n          c.remoteAddr.toString(),\n          c.remoteAddr.protoCodes().map((v) => protocols(v)?.name),\n        ])\n      );\n      console.debug(`Connected to ${peerId}`, transportsByAddr);\n\n      // console.log(\n      //   '---------ppppp',\n      //   peerId,\n      //   conn,\n      //   conn?.remoteAddr.protoCodes().map((v) => protocols(v)?.name)\n      // ); //.includes(WEBRTC_CODE)\n      // if (conn && conn.stat) {\n      //   const transport = conn.stat.transport; // This might vary based on libp2p version\n      //   console.log(`Connected to ${peerId} using transport ${transport}`);\n      // } else {\n      //   console.log(`Connected to ${peerId}`);\n      // }\n    });\n    libp2p.addEventListener('peer:disconnect', (evt) => {\n      console.debug(`Disconnected from ${evt.detail.toString()}`);\n    });\n    console.log(\n      'IPFS - Helia addrs',\n      libp2p.getMultiaddrs().map((a) => a.toString())\n    );\n    // const webrtcConn = await libp2p.dial(\n    //   multiaddr(\n    //     '/ip4/127.0.0.1/udp/4001/quic-v1/webtransport/certhash/uEiDHumbyZRFV1Av7qH9-2l5HGgU2a2UqM6eloqO0vYz5pQ/certhash/uEiDD_TuVgih5_ua31Z4MVbNq7WSw095UAQmZqdUFMDTVRA/p2p/12D3KooWEYGfgK4dEY3spfuDKVq6Jpiyj4KxP1r6HS5RFp5WHebz'\n    //   )\n    // );\n    // console.log('----webrtcConn', webrtcConn);\n\n    this._isStarted = true;\n  }\n\n  async stat(cid: string, options: AbortOptions = {}): Promise<IpfsFileStats> {\n    return this.fs!.stat(stringToCid(cid), options).then((result) => {\n      const { type, fileSize, localFileSize, blocks, dagSize, mtime } = result;\n      return {\n        type,\n        size: fileSize || -1,\n        sizeLocal: localFileSize || -1,\n        blocks,\n      };\n    });\n  }\n\n  cat(cid: string, options: CatOptions = {}) {\n    return this.fs!.cat(stringToCid(cid), options);\n  }\n\n  async add(content: File | string, options: AbortOptions = {}) {\n    // Options to keep CID in V0 format 'Qm....';\n    const optionsV0 = {\n      ...options,\n      ...addOptionsV0,\n    } as Partial<AddOptions>;\n\n    let cid;\n\n    if (content instanceof File) {\n      const fileName = content.name;\n      const arrayBuffer = await content.arrayBuffer();\n      const data = new Uint8Array(arrayBuffer);\n      cid = await this.fs!.addFile(\n        { path: fileName, content: data },\n        optionsV0\n      );\n    } else {\n      const data = new TextEncoder().encode(content);\n      cid = await this.fs!.addBytes(data, optionsV0);\n    }\n    // console.log('----added to helia', cid.toString());\n    this.pin(cid.toString(), options);\n    return cid.toString();\n  }\n\n  async pin(cid: string, options: AbortOptions = {}) {\n    const cid_ = stringToCid(cid);\n    const isPinned = await this.node?.pins.isPinned(cid_, options);\n    if (!isPinned) {\n      const pinResult = (\n        await this.node?.pins.add(cid_, options)\n      )?.cid.toString();\n      // console.log('------pin', pinResult);\n    }\n    // console.log('------pinned', cid, isPinned);\n    return undefined;\n  }\n\n  async getPeers() {\n    return this.node!.libp2p!.getConnections().map((c) =>\n      c.remotePeer.toString()\n    );\n  }\n\n  async stop() {\n    await this.node?.stop();\n  }\n\n  async start() {\n    await this.node?.start();\n  }\n\n  async connectPeer(address: string) {\n    const conn = await this.node!.libp2p!.dial(multiaddr(address));\n    return true;\n  }\n\n  ls() {\n    const result = mapToLsResult(this.node!.pins.ls());\n    return result;\n  }\n\n  async info() {\n    const id = this.node!.libp2p.peerId.toString();\n    const agentVersion = this.node!.libp2p!.services!.identify!.host!\n      .agentVersion as string;\n    return { id, agentVersion, repoSize: -1 };\n  }\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport default HeliaNode;\n","// eslint-disable-next-line import/no-unresolved\nimport { webSockets } from '@libp2p/websockets';\nimport * as filters from '@libp2p/websockets/filters';\nimport { Options } from 'ipfs-core/dist/src/types';\n\nconst configIpfs = (): Options => ({\n  start: true,\n  repo: 'ipfs-repo-cyber-v2',\n  relay: {\n    enabled: false,\n    hop: {\n      enabled: false,\n    },\n  },\n  preload: {\n    enabled: false,\n  },\n  config: {\n    API: {\n      HTTPHeaders: {\n        'Access-Control-Allow-Methods': ['PUT', 'POST'],\n        'Access-Control-Allow-Origin': [\n          'http://localhost:3000',\n          'http://127.0.0.1:5001',\n          'http://127.0.0.1:8888',\n          'http://localhost:8888',\n        ],\n      },\n    },\n    Addresses: {\n      Gateway: '/ip4/127.0.0.1/tcp/8080',\n      Swarm: [\n        // '/dns4/ws-star.discovery.cybernode.ai/tcp/443/wss/p2p-webrtc-star',\n        // '/dns4/wrtc-star1.par.dwebops.pub/tcp/443/wss/p2p-webrtc-star',\n        // '/dns4/wrtc-star2.sjc.dwebops.pub/tcp/443/wss/p2p-webrtc-star',\n      ],\n      Delegates: [\n        // '/dns4/node0.delegate.ipfs.io/tcp/443/https',\n        // '/dns4/node1.delegate.ipfs.io/tcp/443/https',\n        // '/dns4/node2.delegate.ipfs.io/tcp/443/https',\n      ],\n    },\n    Discovery: {\n      MDNS: {\n        Enabled: true,\n        Interval: 10,\n      },\n      webRTCStar: {\n        Enabled: false,\n      },\n    },\n    Bootstrap: [\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmNnooDu7bfjPFoTZYxMNLWUQJyrVwtbZg5gBMjTezGAJN',\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmbLHAnMoJPWSCR5Zhtx6BHJX9KiKNN6tpvbUcqanj75Nb',\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmZa1sAxajnQjVM8WjWXoMbmPd7NsWhfKsPkErzpm9wGkp',\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmQCU2EcMqAqQPR2i9bChDtGNJchTbq5TbXJJ16u19uLTa',\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmcZf59bWwK5XFi76CZX8cbJ4BhTzzA3gU1ZjYZcYW3dwt',\n      // '/dns4/ws-star.discovery.cybernode.ai/tcp/4430/wss/p2p/QmUgmRxoLtGERot7Y6G7UyF6fwvnusQZfGR15PuE6pY3aB',\n    ],\n    Pubsub: {\n      Enabled: false,\n    },\n    Swarm: {\n      ConnMgr: {\n        HighWater: 300,\n        LowWater: 50,\n      },\n      DisableNatPortMap: false,\n    },\n    Routing: {\n      Type: 'dhtclient',\n    },\n  },\n  libp2p: {\n    transports: [\n      // This is added for local demo!\n      // In a production environment the default filter should be used\n      // where only DNS + WSS addresses will be dialed by websockets in the browser.\n      webSockets({\n        filter: filters.dnsWss,\n      }),\n    ],\n    nat: {\n      enabled: false,\n    },\n  },\n  EXPERIMENTAL: {\n    ipnsPubsub: false,\n  },\n});\n\nexport default configIpfs;\n","import {\n  AbortOptions,\n  CatOptions,\n  IpfsNodeType,\n  IpfsFileStats,\n  IpfsNode,\n  IpfsNodePrperties,\n} from '../../types';\nimport { create as createJsIpfsClient, IPFS } from 'ipfs-core';\nimport { stringToCid, stringToIpfsPath } from '../../utils/cid';\nimport { multiaddr } from '@multiformats/multiaddr';\n\nimport configIpfs from './configs/jsIpfsConfig';\nimport { CYBER_GATEWAY_URL } from '../../config';\n\nclass JsIpfsNode implements IpfsNode {\n  readonly nodeType: IpfsNodeType = 'embedded';\n\n  get config() {\n    return { gatewayUrl: CYBER_GATEWAY_URL };\n  }\n\n  private _isStarted = false;\n\n  get isStarted() {\n    return this._isStarted;\n  }\n\n  private node?: IPFS;\n\n  async init() {\n    this.node = await createJsIpfsClient(configIpfs());\n    if (typeof window !== 'undefined') {\n      window.node = this.node;\n      window.toCid = stringToCid;\n    }\n\n    this._isStarted = true;\n  }\n\n  async stat(cid: string, options: AbortOptions = {}): Promise<IpfsFileStats> {\n    return this.node!.files.stat(stringToIpfsPath(cid), {\n      ...options,\n      withLocal: true,\n      size: true,\n    }).then((result) => {\n      const { type, size, sizeLocal, local, blocks } = result;\n      return {\n        type,\n        size: size || -1,\n        sizeLocal: sizeLocal || -1,\n        blocks,\n      };\n    });\n  }\n\n  cat(cid: string, options: CatOptions = {}) {\n    return this.node!.cat(stringToCid(cid), options);\n  }\n\n  async add(content: File | string, options: AbortOptions = {}) {\n    return (await this.node!.add(content, options)).cid.toString();\n  }\n\n  async pin(cid: string, options: AbortOptions = {}) {\n    return (await this.node!.pin.add(stringToCid(cid), options)).toString();\n  }\n\n  async getPeers() {\n    return (await this.node!.swarm.peers()).map((c) => c.peer.toString());\n  }\n\n  async stop() {}\n  async start() {}\n\n  async connectPeer(address: string) {\n    const addr = multiaddr(address);\n    await this.node!.bootstrap.add(addr);\n\n    await this.node!.swarm.connect(addr);\n    return true;\n  }\n\n  ls() {\n    return this.node!.pin.ls();\n  }\n\n  async info() {\n    const response = await this.node!.stats.repo();\n    const repoSize = Number(response.repoSize);\n\n    const responseId = await this.node!.id();\n    const { agentVersion, id } = responseId;\n    return { id: id.toString(), agentVersion, repoSize };\n  }\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport default JsIpfsNode;\n","// import { getNodeAutoDialInterval } from './utils-ipfs';\nimport { IpfsNodeType, IpfsNode, CybIpfsNode, IpfsOptsType } from '../types';\nimport KuboNode from './impl/kubo';\nimport HeliaNode from './impl/helia';\nimport JsIpfsNode from './impl/js-ipfs';\n// import EnhancedIpfsNode from './node/enhancedNode';\nimport {\n  CYBERNODE_SWARM_ADDR_TCP,\n  CYBERNODE_SWARM_ADDR_WSS,\n  CYBER_NODE_SWARM_PEER_ID,\n} from '../config';\nimport { withCybFeatures } from './mixins/withCybFeatures';\n\nconst nodeClassMap: Record<IpfsNodeType, new () => IpfsNode> = {\n  helia: HeliaNode,\n  embedded: JsIpfsNode,\n  external: KuboNode,\n};\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport async function initIpfsNode(\n  options: IpfsOptsType\n): Promise<CybIpfsNode> {\n  const { ipfsNodeType, ...restOptions } = options;\n\n  const swarmPeerId = CYBER_NODE_SWARM_PEER_ID;\n\n  const swarmPeerAddress =\n    ipfsNodeType === 'external'\n      ? CYBERNODE_SWARM_ADDR_TCP\n      : CYBERNODE_SWARM_ADDR_WSS;\n\n  const EnhancedClass = withCybFeatures(nodeClassMap[ipfsNodeType], {\n    swarmPeerId,\n    swarmPeerAddress,\n  });\n\n  const instance = new EnhancedClass();\n\n  await instance.init({ url: restOptions.urlOpts });\n  // TODO: REFACT\n  //   instance.connMgrGracePeriod = await getNodeAutoDialInterval(instance);\n  // window.ipfs = instance;\n\n  await instance.reconnectToSwarm();\n  return instance;\n}\n","import { IpfsNode, CybIpfsNode, IpfsContentType } from '../../types';\nimport { parseArrayLikeToDetails } from '../../utils/content';\nimport { addContenToIpfs, getIPFSContent } from '../../utils/utils-ipfs';\n\ntype WithCybFeaturesOptions = {\n  swarmPeerId: string;\n  swarmPeerAddress: string;\n};\n\nfunction withCybFeatures<TBase extends new (...args: any[]) => IpfsNode>(\n  Base: TBase,\n  options: WithCybFeaturesOptions\n) {\n  return class CybIpfsNodeMixin extends Base implements CybIpfsNode {\n    async fetchWithDetails(\n      cid: string,\n      parseAs?: IpfsContentType,\n      abortController?: AbortController\n    ) {\n      const content = await getIPFSContent(cid, this, abortController);\n\n      const details = await parseArrayLikeToDetails(content, cid);\n      return !parseAs\n        ? details\n        : details?.type === parseAs\n        ? details\n        : undefined;\n    }\n\n    async addContent(content: File | string) {\n      return addContenToIpfs(this, content);\n    }\n\n    async isConnectedToSwarm() {\n      const peers = await super.getPeers();\n      return !!peers.find((peerId) => peerId === options.swarmPeerId);\n    }\n\n    async reconnectToSwarm(forced = false) {\n      const isConnectedToSwarm = await this.isConnectedToSwarm();\n      if (!isConnectedToSwarm || forced) {\n        // TODO: refactor using timeout for node config\n\n        //   const needToReconnect =\n        //     Date.now() - lastConnectedTimestamp <\n        //     DEFAULT_CONNECTION_LIFETIME_SECONDS;\n        super\n          .connectPeer(options.swarmPeerAddress)\n          .then(() => {\n            console.log(` connected to swarm - ${options.swarmPeerAddress}`);\n            return true;\n          })\n          .catch((err) => {\n            console.log(\n              `Can't connect to swarm ${options.swarmPeerAddress}: ${err.message}`\n            );\n            return false;\n          });\n      }\n    }\n  };\n}\n\nexport { withCybFeatures };\n","import {\n  PipelineType,\n  pipeline,\n  env,\n  FeatureExtractionPipeline,\n} from '@xenova/transformers';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport DbApiWrapper from 'src/services/backend/services/DbApi/DbApi';\nimport {\n  Subject,\n  combineLatest,\n  Observable,\n  shareReplay,\n  ReplaySubject,\n  filter,\n} from 'rxjs';\nimport { proxy } from 'comlink';\n\nenv.allowLocalModels = false;\n\ntype MlModelParams = {\n  name: PipelineType;\n  model: string;\n};\nconst mlModelMap: Record<string, MlModelParams> = {\n  featureExtractor: {\n    name: 'feature-extraction',\n    model: 'Xenova/all-MiniLM-L6-v2',\n  },\n  // summarization: {\n  //   name: 'summarization',\n  //   model: 'ahmedaeb/distilbart-cnn-6-6-optimised',\n  // },\n  // qa: {\n  //   name: 'question-answering',\n  //   model: 'Xenova/distilbert-base-uncased-distilled-squad',\n  // },\n};\n\nconst loadPipeline = (\n  name: PipelineType,\n  model: string,\n  onProgress: (data: any) => void\n) => {\n  return pipeline(name, model, {\n    progress_callback: (progressData: any) => {\n      try {\n        const {\n          status,\n          progress,\n          // name: modelName,\n          loaded,\n          total,\n        } = progressData;\n\n        const message = loaded ? `${model} - ${loaded}/${total} bytes` : model;\n\n        const done = ['ready', 'error'].some((s) => s === status);\n\n        const progrssStateItem = {\n          status,\n          message,\n          done,\n          progress: progress ? Math.round(progress) : 0,\n        };\n        // console.log('progress_callback', name, progressData);\n\n        onProgress(progrssStateItem);\n      } catch (e) {\n        console.log('-------progresss error', model, e.toString());\n      }\n    },\n  });\n};\n\nexport type EmbeddingApi = {\n  createEmbedding: (text: string) => Promise<number[]>;\n  searchByEmbedding: (\n    text: string,\n    count?: number\n  ) => ReturnType<DbApiWrapper['searchByEmbedding']>;\n};\n\nconst createEmbeddingApi$ = (\n  dbInstance$: Subject<DbApiWrapper>,\n  featureExtractor$: Subject<FeatureExtractionPipeline>\n) => {\n  const replaySubject = new ReplaySubject(1);\n\n  combineLatest([dbInstance$, featureExtractor$]).subscribe(\n    ([dbInstance, featureExtractor]) => {\n      if (dbInstance && featureExtractor) {\n        const createEmbedding = async (text: string) => {\n          const output = await featureExtractor(text, {\n            pooling: 'mean',\n            normalize: true,\n          });\n\n          return output.data as number[];\n        };\n\n        const searchByEmbedding = async (text: string, count?: number) => {\n          const vec = await createEmbedding(text);\n          // console.log('----searchByEmbedding', vec);\n\n          const rows = await dbInstance.searchByEmbedding(vec, count);\n          //   console.log('----searcByEmbedding rows', rows);\n\n          return rows;\n        };\n\n        const api = {\n          createEmbedding,\n          searchByEmbedding,\n        };\n        replaySubject.next(proxy(api));\n      }\n    }\n  );\n  // .pipe(filter((v) => !!v))\n  return replaySubject as Observable<EmbeddingApi>;\n};\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport const createMlApi = (\n  dbInstance$: Subject<DbApiWrapper>,\n  broadcastApi: BroadcastChannelSender\n) => {\n  const featureExtractor$ = new Subject<FeatureExtractionPipeline>();\n  const embeddingApi$ = createEmbeddingApi$(dbInstance$, featureExtractor$);\n\n  const initPipelineInstance = async (alias: keyof typeof mlModelMap) => {\n    const { name, model } = mlModelMap[alias];\n\n    const pipeline = await loadPipeline(name, model, (data) =>\n      broadcastApi.postMlSyncEntryProgress(alias, data)\n    );\n    if (name === 'feature-extraction') {\n      featureExtractor$.next(pipeline as FeatureExtractionPipeline);\n    }\n    console.log(`${alias} - loaded`);\n  };\n\n  const init = async () => {\n    broadcastApi.postServiceStatus('ml', 'starting');\n    console.time(' ml initialized');\n\n    return Promise.all([\n      initPipelineInstance('featureExtractor'),\n      // initMlInstance('summarization'),\n      // initMlInstance('qa'),\n    ])\n      .then((result) => {\n        setTimeout(() => broadcastApi.postServiceStatus('ml', 'started'), 0);\n        console.timeEnd(' ml initialized');\n\n        return result;\n      })\n      .catch((e) =>\n        broadcastApi.postServiceStatus('ml', 'error', e.toString())\n      );\n  };\n\n  init();\n\n  return { embeddingApi$, init };\n};\n","import initAsync, { compile } from 'cyb-rune-wasm';\n\nimport { v4 as uuidv4 } from 'uuid';\n\nimport { TabularKeyValues } from 'src/types/data';\nimport { keyValuesToObject } from 'src/utils/localStorage';\nimport { entityToDto } from 'src/utils/dto';\n\nimport { mapObjIndexed } from 'ramda';\nimport { removeBrokenUnicode } from 'src/utils/string';\n\nimport {\n  BehaviorSubject,\n  ReplaySubject,\n  combineLatest,\n  distinctUntilChanged,\n  map,\n} from 'rxjs';\n\nimport defaultParticleScript from 'src/services/scripting/rune/default/particle.rn';\nimport runtimeScript from 'src/services/scripting/rune/runtime.rn';\n\nimport {\n  ScriptCallback,\n  ScriptParticleParams,\n  ScriptContext,\n  ScriptParticleResult,\n  // ScriptMyParticleParams,\n  ScriptEntrypoints,\n  EntrypointParams,\n  EngineContext,\n  ScriptMyCampanion,\n} from './types';\n\nimport { extractRuneScript } from './helpers';\n\ntype RuneEntrypoint = {\n  readOnly: boolean;\n  execute: boolean;\n  funcName: string;\n  funcParams: any[];\n  params: Object; // context data\n  input: string; // main code\n  script: string; // runtime code\n};\n\ntype RuneRunResult = {\n  output: string;\n  diagnosticsOutput: string;\n  diagnostics: any[];\n  error: string;\n  result: any;\n};\n\nconst compileConfig = {\n  budget: 1_000_000,\n  experimental: false,\n  instructions: false,\n  options: [],\n};\n\nconst defaultRuneEntrypoint: RuneEntrypoint = {\n  readOnly: false,\n  execute: true,\n  funcName: 'main',\n  funcParams: {},\n  params: {},\n  input: defaultParticleScript,\n  script: runtimeScript,\n};\n\nconst toRecord = (item: TabularKeyValues) =>\n  keyValuesToObject(Object.values(item));\n\nexport type LoadParams = {\n  entrypoints: ScriptEntrypoints;\n  secrets: TabularKeyValues;\n};\n\n// eslint-disable-next-line import/prefer-default-export\nfunction enigine() {\n  let entrypoints: Partial<ScriptEntrypoints> = {};\n  let context: EngineContext = { params: {}, user: {}, secrets: {} };\n  const isInitialized$ = new BehaviorSubject<boolean>(false);\n  const entrypoints$ = new BehaviorSubject<Partial<ScriptEntrypoints>>({});\n\n  const scriptCallbacks = new Map<string, ScriptCallback>();\n\n  const isSoulInitialized$ = new ReplaySubject(1);\n  combineLatest([isInitialized$, entrypoints$])\n    .pipe(\n      map(\n        ([isInitialized, entrypoints]) =>\n          !!(isInitialized && entrypoints.particle)\n      ),\n      distinctUntilChanged()\n    )\n    .subscribe((v) => {\n      isSoulInitialized$.next(v);\n    });\n\n  entrypoints$.subscribe((v) => {\n    entrypoints = v;\n  });\n\n  const init = async () => {\n    console.time(' rune initialized');\n    await initAsync();\n    // window.rune = rune; // debug\n    console.timeEnd(' rune initialized');\n    isInitialized$.next(true);\n  };\n\n  const pushContext = <K extends keyof ScriptContext>(\n    name: K,\n    value: ScriptContext[K] //| TabularKeyValues\n  ) => {\n    // context[name] =  toRecord(value as TabularKeyValues);\n    context[name] = value;\n  };\n\n  const popContext = (names: (keyof ScriptContext)[]) => {\n    const newContext = context;\n    names.forEach((name) => {\n      newContext[name] = {};\n    });\n    context = newContext;\n  };\n\n  const setEntrypoints = (scriptEntrypoints: ScriptEntrypoints) => {\n    entrypoints = mapObjIndexed(\n      (v) => ({ ...v, script: extractRuneScript(v.script) }),\n      scriptEntrypoints\n    );\n    entrypoints$.next(entrypoints);\n  };\n\n  const run = async (\n    script: string,\n    compileParams: Partial<RuneEntrypoint>,\n    callback?: ScriptCallback,\n    scripts: Record<string, string> = {}\n  ) => {\n    const refId = uuidv4().toString();\n\n    callback && scriptCallbacks.set(refId, callback);\n    const scriptParams = {\n      app: context,\n      refId,\n    };\n    const compilerParams = {\n      ...defaultRuneEntrypoint,\n      ...compileParams,\n      input: script,\n      scripts: { ...scripts, runtime: runtimeScript },\n      params: scriptParams,\n    };\n    // console.log('-----run', scriptParams);\n    const outputData = await compile(compilerParams, compileConfig);\n\n    // Parse the JSON string\n    const { result, error } = outputData;\n\n    try {\n      scriptCallbacks.delete(refId);\n\n      return {\n        ...entityToDto(outputData),\n        error,\n        result: result\n          ? result === '()'\n            ? ''\n            : JSON.parse(removeBrokenUnicode(result))\n          : { action: 'error', message: 'No result' },\n      } as RuneRunResult;\n    } catch (e) {\n      scriptCallbacks.delete(refId);\n\n      console.log(\n        `engine.run err ${compilerParams.funcName}`,\n        e,\n        outputData,\n        compilerParams\n      );\n      return {\n        diagnosticsOutput: `scripting engine error ${e}`,\n        ...outputData,\n        result: { action: 'error', message: e?.toString() || 'Unknown error' },\n      } as RuneRunResult;\n    }\n  };\n\n  const getParticleScriptOrAction = ():\n    | ['error' | 'pass' | 'script', string] => {\n    if (!entrypoints.particle) {\n      return ['error', ''];\n    }\n\n    const { script, enabled } = entrypoints.particle;\n\n    if (!enabled) {\n      return ['pass', ''];\n    }\n\n    return ['script', script];\n  };\n\n  const personalProcessor = async (\n    params: ScriptParticleParams\n  ): Promise<ScriptParticleResult> => {\n    const [resultType, script] = getParticleScriptOrAction();\n\n    if (resultType === 'error') {\n      return { action: 'error', message: 'No particle entrypoint' };\n    }\n\n    if (resultType !== 'script') {\n      return { action: 'pass' };\n    }\n\n    const { cid, contentType, content } = params;\n    const output = await run(script, {\n      funcName: 'personal_processor',\n      funcParams: [cid, contentType, content], //params as EntrypointParams,\n    });\n    const { action, content: outputContent } = output.result;\n\n    if (action === 'error') {\n      console.error(\n        `RUNE: personalProcessor error: ${params.cid}`,\n        params,\n        output\n      );\n    }\n\n    if (outputContent) {\n      return { ...output.result, content: outputContent };\n    }\n\n    return output.result;\n  };\n\n  const executeFunction = async (\n    script: string,\n    funcName: string,\n    funcParams: EntrypointParams\n  ): Promise<ScriptParticleResult> => {\n    console.log('-----executeFunction rune', funcName, funcParams);\n    const output = await run(script, {\n      funcName,\n      funcParams,\n      readOnly: true, // block to sign tx and add to ipfs\n    });\n\n    return output.result;\n  };\n\n  // const particleInference = async (\n  //   userScript: string,\n  //   funcParams: EntrypointParams\n  // ): Promise<ScriptMyParticleResult> => {\n  //   const output = await run(userScript, {\n  //     funcName: 'particle_inference',\n  //     funcParams,\n  //   });\n\n  //   return output.result;\n  // };\n\n  const askCompanion = async (\n    cid: string,\n    contentType: string,\n    content: string,\n    callback?: ScriptCallback\n  ): Promise<ScriptMyCampanion> => {\n    const [resultType, script] = getParticleScriptOrAction();\n    if (resultType === 'error') {\n      return {\n        action: 'error',\n        metaItems: [[{ type: 'text', text: 'No particle entrypoint' }]],\n      };\n    }\n\n    if (resultType === 'pass') {\n      return { action: 'pass', metaItems: [] };\n    }\n\n    const output = await run(\n      script,\n      {\n        funcName: 'ask_companion',\n        funcParams: [cid, contentType, content],\n      },\n      callback\n    );\n\n    if (output.result.action === 'error') {\n      console.error('---askCompanion error', output);\n      return {\n        action: 'error',\n        metaItems: [[{ type: 'text', text: output.error }]],\n      };\n    }\n\n    return { action: 'answer', metaItems: output.result.content };\n  };\n\n  const executeCallback = async (refId: string, data: any) => {\n    const callback = scriptCallbacks.get(refId);\n\n    if (callback) {\n      await callback(data);\n    }\n  };\n\n  return {\n    init,\n    isSoulInitialized$,\n    run,\n    // particleInference,\n    askCompanion,\n    personalProcessor,\n    setEntrypoints,\n    pushContext,\n    popContext,\n    executeFunction,\n    executeCallback,\n    getDebug: () => ({\n      context,\n      entrypoints,\n    }),\n  };\n}\n\nconst scriptEngine = enigine();\n\nexport type RuneEngine = typeof scriptEngine;\n\nexport default scriptEngine;\n","import { Observable } from 'rxjs';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport rune from 'src/services/scripting/engine';\nimport runeDeps, { RuneInnerDeps } from 'src/services/scripting/runeDeps';\nimport DbApiWrapper from 'src/services/backend/services/DbApi/DbApi';\nimport { EmbeddingApi } from './mlApi';\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport const createRuneApi = (\n  embeddingApi$: Observable<EmbeddingApi>,\n  dbInstance$: Observable<DbApiWrapper>,\n  broadcastApi: BroadcastChannelSender\n) => {\n  const setInnerDeps = (deps: Partial<RuneInnerDeps>) =>\n    runeDeps.setInnerDeps(deps);\n\n  embeddingApi$.subscribe((embeddingApi) => {\n    setInnerDeps({ embeddingApi });\n  });\n\n  dbInstance$.subscribe((dbApi) => {\n    setInnerDeps({ dbApi });\n  });\n\n  rune.isSoulInitialized$.subscribe((value) => {\n    value\n      ? setTimeout(() => broadcastApi.postServiceStatus('rune', 'started'), 0)\n      : broadcastApi.postServiceStatus('rune', 'inactive');\n  });\n\n  const init = async () => {\n    broadcastApi.postServiceStatus('rune', 'starting');\n\n    await rune.init();\n    setInnerDeps({ rune });\n  };\n\n  init();\n\n  return { rune, setInnerDeps, abort: runeDeps.abort };\n};\n","import { proxy } from 'comlink';\n\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { ParticleCid } from 'src/types/base';\nimport { BehaviorSubject, Subject } from 'rxjs';\nimport { RuneInnerDeps } from 'src/services/scripting/runeDeps';\n\nimport { exposeWorkerApi } from '../factoryMethods';\n\nimport { SyncService } from '../../services/sync/sync';\nimport { SyncServiceParams } from '../../services/sync/types';\n\nimport DbApi from '../../services/DbApi/DbApi';\n\nimport BroadcastChannelSender from '../../channels/BroadcastChannelSender';\nimport { createIpfsApi } from './api/ipfsApi';\nimport { createMlApi } from './api/mlApi';\nimport { createRuneApi } from './api/runeApi';\n\n// import { initRuneDeps } from 'src/services/scripting/wasmBindings';\n\nconst createBackgroundWorkerApi = () => {\n  const broadcastApi = new BroadcastChannelSender();\n\n  const dbInstance$ = new Subject<DbApi>();\n\n  const injectDb = (db: DbApi) => dbInstance$.next(db);\n\n  const params$ = new BehaviorSubject<SyncServiceParams>({\n    myAddress: null,\n  });\n\n  const { embeddingApi$ } = createMlApi(dbInstance$, broadcastApi);\n\n  const { setInnerDeps, rune } = createRuneApi(\n    embeddingApi$,\n    dbInstance$,\n    broadcastApi\n  );\n\n  const {\n    ipfsQueue,\n    ipfsInstance$,\n    api: ipfsApi,\n  } = createIpfsApi(rune, broadcastApi);\n\n  const waitForParticleResolve = (\n    cid: ParticleCid,\n    priority: QueuePriority = QueuePriority.MEDIUM\n  ) => ipfsQueue.enqueueAndWait(cid, { postProcessing: false, priority });\n\n  const serviceDeps = {\n    waitForParticleResolve,\n    dbInstance$,\n    ipfsInstance$,\n    embeddingApi$,\n    params$,\n  };\n\n  // service to sync updates about cyberlinks, transactions, swarm etc.\n  const syncService = new SyncService(serviceDeps);\n\n  // INITIALIZATION\n  setInnerDeps({ ipfsApi });\n\n  return {\n    injectDb,\n    isIpfsInitialized: () => !!ipfsInstance$.getValue(),\n    // syncDrive,\n    ipfsApi: proxy(ipfsApi),\n    rune: proxy(rune),\n    embeddingApi$,\n    // ipfsInstance$,\n    ipfsQueue: proxy(ipfsQueue),\n    setRuneDeps: (\n      deps: Partial<Omit<RuneInnerDeps, 'embeddingApi' | 'dbApi'>>\n    ) => setInnerDeps(deps),\n    // restartSync: (name: SyncEntryName) => syncService.restart(name),\n    setParams: (params: Partial<SyncServiceParams>) =>\n      params$.next({ ...params$.value, ...params }),\n  };\n};\n\nconst backgroundWorker = createBackgroundWorkerApi();\n\nexport type BackgroundWorker = typeof backgroundWorker;\n\n// Expose the API to the main thread as shared/regular worker\nexposeWorkerApi(self, backgroundWorker);\n","import { proxy } from 'comlink';\nimport { BehaviorSubject, Subject } from 'rxjs';\nimport QueueManager from 'src/services/QueueManager/QueueManager';\nimport {\n  QueueItemCallback,\n  QueueItemOptions,\n} from 'src/services/QueueManager/types';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport { initIpfsNode } from 'src/services/ipfs/node/factory';\n\nimport {\n  CybIpfsNode,\n  IpfsContentType,\n  IpfsOptsType,\n} from 'src/services/ipfs/types';\nimport { RuneEngine } from 'src/services/scripting/engine';\n\n// eslint-disable-next-line import/prefer-default-export\nexport const createIpfsApi = (\n  rune: RuneEngine,\n  broadcastApi: BroadcastChannelSender\n) => {\n  const ipfsInstance$ = new BehaviorSubject<CybIpfsNode | undefined>(undefined);\n  const ipfsQueue = new QueueManager(ipfsInstance$, {\n    rune,\n  });\n  const stopIpfs = async () => {\n    const ipfsNode = ipfsInstance$.getValue();\n\n    if (ipfsNode) {\n      await ipfsNode.stop();\n    }\n    ipfsInstance$.next(undefined);\n    broadcastApi.postServiceStatus('ipfs', 'inactive');\n  };\n\n  const startIpfs = async (ipfsOpts: IpfsOptsType) => {\n    try {\n      const ipfsNode = ipfsInstance$.getValue();\n      if (ipfsNode) {\n        // console.log('Ipfs node already started!');\n        setTimeout(() => broadcastApi.postServiceStatus('ipfs', 'started'), 0);\n        return Promise.resolve();\n        // await ipfsNode.stop();\n      }\n      broadcastApi.postServiceStatus('ipfs', 'starting');\n      console.time(' ipfs initialized');\n\n      const newIpfsNode = await initIpfsNode(ipfsOpts);\n      console.timeEnd(' ipfs initialized');\n\n      ipfsInstance$.next(newIpfsNode);\n      setTimeout(() => broadcastApi.postServiceStatus('ipfs', 'started'), 0);\n      return true;\n    } catch (err) {\n      console.log('----ipfs node init error ', err);\n      const msg = err instanceof Error ? err.message : (err as string);\n      broadcastApi.postServiceStatus('ipfs', 'error', msg);\n      throw Error(msg);\n    }\n  };\n\n  const api = {\n    start: startIpfs,\n    stop: stopIpfs,\n    config: async () => ipfsInstance$.getValue()?.config,\n    info: async () => ipfsInstance$.getValue()?.info(),\n    fetchWithDetails: async (\n      cid: string,\n      parseAs?: IpfsContentType,\n      controller?: AbortController\n    ) => {\n      const ipfsNode = ipfsInstance$.getValue();\n      if (!ipfsNode) {\n        throw new Error('ipfs node not initialized');\n      }\n      return ipfsNode.fetchWithDetails(cid, parseAs, controller);\n    },\n    enqueue: async (\n      cid: string,\n      callback: QueueItemCallback,\n      options: QueueItemOptions\n    ) => ipfsQueue.enqueue(cid, callback, options),\n    enqueueAndWait: async (cid: string, options?: QueueItemOptions) =>\n      ipfsQueue!.enqueueAndWait(cid, options),\n    dequeue: async (cid: string) => ipfsQueue.cancel(cid),\n    dequeueByParent: async (parent: string) => ipfsQueue.cancelByParent(parent),\n    clearQueue: async () => ipfsQueue.clear(),\n    addContent: async (content: string | File) =>\n      ipfsInstance$.getValue()?.addContent(content),\n  };\n\n  return { ipfsInstance$, ipfsQueue, api };\n};\n\nexport type IpfsApi = ReturnType<typeof createIpfsApi>['api'];\n","import { Nullable } from 'src/types';\nimport { v4 as uuidv4 } from 'uuid';\n\nexport async function getScriptFromParticle(cid?: Nullable<string>) {\n  throw new Error('Not implemented');\n  // if (!cid || !isCID(cid)) {\n  //   // throw new Error('cid is not valid');\n  //   return undefined;\n  // }\n\n  // const queueResult = await queueManager.enqueueAndWait(cid, {\n  //   postProcessing: false,\n  // });\n  // const result = queueResult?.result;\n  // if (!result?.result || result?.contentType !== 'text') {\n  //   // throw new Error('content is not valid');\n  //   return undefined;\n  // }\n\n  // return getTextFromIpfsContent(result.result);\n}\n\nexport function extractRuneContent(markdown: string) {\n  // Regular expression to match the content between ```rune``` tags\n  const runeRegex = /```rune\\s*([\\s\\S]*?)```/g;\n\n  let match;\n  let runeScript = '';\n  let modifiedMarkdown = markdown;\n  let hasRune = false;\n  // Iterate through all matches of the regular expression\n  while ((match = runeRegex.exec(markdown)) !== null) {\n    hasRune = true;\n    // Append the matched content between ```rune``` tags to runeContent variable\n    runeScript += match[1] + '\\n';\n\n    // Replace the entire matched block, including the tags, with an empty string\n    modifiedMarkdown = modifiedMarkdown.replace(match[0], '');\n  }\n\n  // Returning both the extracted content and the modified markdown without the tags\n  return {\n    script: runeScript.trim(),\n    markdown: modifiedMarkdown,\n    hasRune,\n  };\n}\n\nexport function extractRuneScript(markdown: string) {\n  const { script, markdown: md, hasRune } = extractRuneContent(markdown);\n  // if no rune tag, consider this like pure script\n  return hasRune ? script : md;\n}\n\nexport const generateRefId = () => uuidv4().toString();\n","import React, { useContext } from 'react';\nimport { CyberClient } from '@cybercongress/cyber-js';\nimport { Option } from 'src/types';\nimport { useQuery } from '@tanstack/react-query';\nimport { RPC_URL } from 'src/constants/config';\n\nconst QueryClientContext = React.createContext<Option<CyberClient>>(undefined);\n\nexport function useQueryClient() {\n  return useContext(QueryClientContext);\n}\n\nfunction QueryClientProvider({ children }: { children: React.ReactNode }) {\n  const { data, error, isFetching } = useQuery({\n    queryKey: ['cyberClient', 'connect'],\n    queryFn: async () => {\n      return CyberClient.connect(RPC_URL);\n    },\n  });\n\n  if (isFetching) {\n    return null;\n  }\n\n  if (error) {\n    console.error('Error queryClient connect: ', error.message);\n  }\n\n  return (\n    <QueryClientContext.Provider value={data}>\n      {children}\n    </QueryClientContext.Provider>\n  );\n}\n\nexport default QueryClientProvider;\n","import { useEffect, useState, useMemo } from 'react';\nimport axios from 'axios';\nimport { useQueryClient } from 'src/contexts/queryClient';\nimport { AccountValue } from 'src/types/defaultAccount';\nimport { useQuery } from '@tanstack/react-query';\nimport { Nullable } from 'src/types';\nimport { Citizenship } from 'src/types/citizenship';\nimport { CyberClient } from '@cybercongress/cyber-js';\nimport { getPassport } from 'src/services/passports/lcd';\n\nconst AMOUNT_ALL_STAGE = 90;\nconst NEW_RELEASE = 1000; // release 1% every 1k claims\nconst CONSTITUTION_HASH = 'QmcHB9GKHAKCLQhmSj71qNJhENJJg8Gymd1PvvsCQBhG7M';\n\n// test root\n// const CONTRACT_ADDRESS_GIFT =\n//   'bostrom1dwzfa74hzpt6393czajlldnxjup8zk3xh3skegnm67yzqx33k2cssyduk8';\n// const CONTRACT_ADDRESS_PASSPORT =\n//   'bostrom1fzm6gzyccl8jvdv3qq6hp9vs6ylaruervs4m06c7k0ntzn2f8faq7ha2z2';\n\n// prod root\nconst CONTRACT_ADDRESS_GIFT =\n  'bostrom16t6tucgcqdmegye6c9ltlkr237z8yfndmasrhvh7ucrfuqaev6xq7cpvek';\nconst CONTRACT_ADDRESS_PASSPORT =\n  'bostrom1xut80d09q0tgtch8p0z4k5f88d3uvt8cvtzm5h3tu3tsy4jk9xlsfzhxel';\n\nconst DICTIONARY = {\n  Astronauts: 'Astronaut',\n  'Average Citizens. ETH Analysis': 'Average Citizens',\n  'Cyberpunks. ERC20 and ERC721 Analysis': 'Cyberpunk',\n  'Extraordinary Hackers. Gas Analysis': 'Extraordinary Hacker',\n  'Key Opinion Leaders. ERC20 Analysis': 'Key Opinion Leader',\n  'Masters of the Great Web. Gas and ERC721 Analysis':\n    'Master of the Great Web',\n  'Passionate Investors. ERC20 Analysis': 'Passionate Investor',\n  'Heroes of the Great Web. Genesis and ETH2 Stakers':\n    'True Hero of the Great Web',\n  Leeches: 'Devil',\n};\n\nconst GIFT_ICON = '';\nconst BOOT_ICON = '';\n\nconst useGetActivePassport = (\n  addressActive: Nullable<AccountValue>,\n  updateFunc?: number\n) => {\n  const queryClient = useQueryClient();\n  const data = useQuery(\n    ['active_passport', addressActive?.bech32],\n    () => activePassport(queryClient, addressActive?.bech32),\n    {\n      enabled: Boolean(addressActive) && Boolean(queryClient),\n    }\n  );\n\n  useEffect(() => {\n    if (updateFunc) {\n      data.refetch();\n    }\n  }, [updateFunc]);\n\n  return {\n    citizenship: data.data,\n    loading: data.isFetching,\n  };\n};\n\n// TODO: replace with hook\nconst activePassport = async (\n  client: CyberClient,\n  address: string\n): Promise<Nullable<Citizenship>> => {\n  try {\n    const query = {\n      active_passport: {\n        address,\n      },\n    };\n    const response = await client.queryContractSmart(\n      CONTRACT_ADDRESS_PASSPORT,\n      query\n    );\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst parseValue = (data) => {\n  if (data.length > 0) {\n    const newData = data.replace(/'/g, '\"');\n    return JSON.parse(newData);\n  }\n  return null;\n};\n\nconst parseValueDetails = (data) => {\n  const value = parseValue(data);\n  if (value !== null) {\n    const details = {};\n    value.forEach((item) => {\n      details[DICTIONARY[item.audience]] = { gift: item.gift };\n    });\n    return details;\n  }\n  return null;\n};\n\nconst parseResponse = (obj) => {\n  return {\n    ...obj,\n    details: parseValueDetails(obj.details),\n    proof: parseValue(obj.proof),\n  };\n};\n\nconst checkGift = async (address) => {\n  try {\n    const response = await axios({\n      method: 'GET',\n      url: `https://titan.cybernode.ai/graphql/api/rest/get-cybergift/${address}`, // prod root\n      // url: `https://titan.cybernode.ai/graphql/api/rest/get-test-gift/${address}`, // test root\n    });\n\n    if (response && response.data) {\n      const { data } = response;\n      if (\n        Object.prototype.hasOwnProperty.call(data, 'cyber_gift_proofs') &&\n        Object.keys(data.cyber_gift_proofs).length > 0\n      ) {\n        const { cyber_gift_proofs: cyberGiftData } = data;\n        return parseResponse(cyberGiftData[0]);\n      }\n      if (\n        Object.prototype.hasOwnProperty.call(data, 'test_gift') &&\n        Object.keys(data.test_gift).length > 0\n      ) {\n        const { test_gift: cyberGiftData } = data;\n        return parseResponse(cyberGiftData[0]);\n      }\n    }\n    return null;\n  } catch (error) {\n    return null;\n  }\n};\n\nconst queryContractSmartPassport = async (client, query) => {\n  try {\n    const response = await getPassport(query);\n    // const response = await client.queryContractSmart(\n    //   CONTRACT_ADDRESS_PASSPORT,\n    //   query\n    // );\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst queryContractSmartGift = async (client, query) => {\n  try {\n    const response = await client.queryContractSmart(\n      CONTRACT_ADDRESS_GIFT,\n      query\n    );\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getStateGift = async (client) => {\n  try {\n    const query = {\n      state: {},\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getConfigGift = async (client) => {\n  try {\n    const query = {\n      config: {},\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getReleaseState = async (client, address) => {\n  try {\n    const query = {\n      release_state: {\n        address,\n      },\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getClaimedAmount = async (client, address) => {\n  try {\n    const query = {\n      claim: {\n        address,\n      },\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getIsClaimed = async (client, address) => {\n  try {\n    const query = {\n      is_claimed: {\n        address,\n      },\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getPassportByNickname = async (client, nickname) => {\n  try {\n    const query = {\n      passport_by_nickname: {\n        nickname,\n      },\n    };\n    const response = await queryContractSmartPassport(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getNumTokens = async (client) => {\n  try {\n    const query = {\n      num_tokens: {},\n    };\n    const response = await queryContractSmartPassport(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst tooMuthAddressError =\n  'failed to execute message; message index: 0: Address is not eligible to claim airdrop, Too many addresses: execute wasm contract failed';\n\nconst canProve8AddressNewError =\n  'You can prove only 8 addresses for one passport';\n\nconst parseRowLog = (rawlog) => {\n  if (rawlog === tooMuthAddressError) {\n    return canProve8AddressNewError;\n  }\n\n  return rawlog;\n};\n\nexport {\n  activePassport,\n  CONTRACT_ADDRESS_PASSPORT,\n  useGetActivePassport,\n  CONSTITUTION_HASH,\n  CONTRACT_ADDRESS_GIFT,\n  GIFT_ICON,\n  BOOT_ICON,\n  AMOUNT_ALL_STAGE,\n  NEW_RELEASE,\n  checkGift,\n  getConfigGift,\n  getStateGift,\n  getReleaseState,\n  getClaimedAmount,\n  getIsClaimed,\n  getPassportByNickname,\n  parseRowLog,\n  getNumTokens,\n};\n","import { CONTRACT_ADDRESS_PASSPORT } from 'src/containers/portal/utils';\nimport { toAscii, toBase64 } from '@cosmjs/encoding';\nimport { PassportContractQuery } from 'src/services/soft.js/api/passport';\n\nimport { LCD_URL } from 'src/constants/config';\nimport axios from 'axios';\nimport defaultNetworks from 'src/constants/defaultNetworks';\n\n// need this request to query passports with any queryClient chain\nexport async function getPassport(query: PassportContractQuery) {\n  const response = await axios.get(\n    `${\n      defaultNetworks.bostrom.LCD_URL\n    }/cosmwasm/wasm/v1/contract/${CONTRACT_ADDRESS_PASSPORT}/smart/${toBase64(\n      toAscii(JSON.stringify(query))\n    )}`\n  );\n  return response.data.data;\n}\n","import { DeliverTxResponse } from '@cosmjs/stargate';\n\nexport class SigningCyberClientError extends Error {\n  public code: number;\n\n  constructor(response: string[] | DeliverTxResponse) {\n    let message = '';\n    let code = -1;\n    if (response instanceof Array) {\n      message = response.join('\\r\\n');\n    } else if (response.rawLog) {\n      message = response.rawLog.toString();\n      code = response.code;\n    } else {\n      message = message?.error;\n    }\n\n    super(message);\n    cyblog.error(message, { error: response });\n\n    this.code = code;\n  }\n}\n\nexport const throwErrorOrResponse = (\n  response: string[] | DeliverTxResponse\n) => {\n  const isResponseError = response instanceof Array || response.code !== 0;\n  if (isResponseError) {\n    throw new SigningCyberClientError(response);\n  }\n  return response as DeliverTxResponse;\n};\n","/* eslint-disable import/no-unused-modules */\nimport { Coin, OfflineSigner, StdFee } from '@cosmjs/launchpad';\nimport { SigningCyberClient } from '@cybercongress/cyber-js';\nimport { SenseApi } from 'src/contexts/backend/services/senseApi';\nimport { NeuronAddress, ParticleCid } from 'src/types/base';\nimport { getNowUtcNumber } from 'src/utils/date';\n\nimport { DEFAULT_GAS_LIMITS } from 'src/constants/config';\nimport { LinkDto } from '../CozoDb/types/dto';\nimport { throwErrorOrResponse } from './errors';\n\nimport { CONTRACT_ADDRESS_PASSPORT } from 'src/containers/portal/utils';\n\nconst defaultFee = {\n  amount: [],\n  gas: DEFAULT_GAS_LIMITS.toString(),\n} as StdFee;\n\nexport const sendCyberlink = async (\n  neuron: NeuronAddress,\n  from: ParticleCid,\n  to: ParticleCid,\n  {\n    senseApi,\n    signingClient,\n  }: {\n    senseApi: SenseApi;\n    signingClient: SigningCyberClient;\n  },\n  fee: StdFee = defaultFee\n) => {\n  const response = await signingClient!.cyberlink(neuron, from, to, fee);\n  const result = throwErrorOrResponse(response);\n\n  const { transactionHash } = result;\n  const link = {\n    from,\n    to,\n    transactionHash,\n    timestamp: getNowUtcNumber(),\n    neuron,\n  } as LinkDto;\n\n  // TODO: add from/toparticle to DB ??\n  await senseApi?.putCyberlink(link);\n  await senseApi?.addCyberlinkLocal(link);\n\n  return transactionHash;\n};\n\nexport const sendTokensWithMessage = async (\n  address: NeuronAddress,\n  recipient: string,\n  offerCoin: Coin[],\n  memo: string | ParticleCid,\n  {\n    senseApi,\n    signingClient,\n  }: { signingClient: SigningCyberClient; senseApi: SenseApi }\n) => {\n  const response = await signingClient.sendTokens(\n    address,\n    recipient,\n    offerCoin,\n    'auto',\n    memo\n  );\n  const result = throwErrorOrResponse(response);\n  const { transactionHash } = result;\n\n  await senseApi?.addMsgSendAsLocal({\n    transactionHash,\n    fromAddress: address,\n    toAddress: recipient,\n    amount: offerCoin,\n    memo,\n  });\n\n  return transactionHash;\n};\n\nexport const investmint = async (\n  address: NeuronAddress,\n  amount: Coin,\n  resource: string,\n  length: number,\n  signingClient: SigningCyberClient\n) => {\n  const response = await signingClient.investmint(\n    address,\n    amount,\n    resource,\n    length,\n    'auto'\n  );\n\n  const { transactionHash } = throwErrorOrResponse(response);\n  return transactionHash;\n};\n\nexport const updatePassportParticle = async (\n  nickname: string,\n  particle: ParticleCid,\n  {\n    signer,\n    signingClient,\n  }: {\n    signer: OfflineSigner;\n    signingClient: SigningCyberClient;\n  }\n) => {\n  const [{ address }] = await signer.getAccounts();\n\n  const msgObject = {\n    update_particle: {\n      nickname,\n      particle,\n    },\n  };\n  return signingClient.execute(\n    address,\n    CONTRACT_ADDRESS_PASSPORT,\n    msgObject,\n    'auto'\n  );\n};\n","import { ProxyMarked, Remote } from 'comlink';\n\nimport { BehaviorSubject, Subject, first, tap } from 'rxjs';\nimport { CyberClient, SigningCyberClient } from '@cybercongress/cyber-js';\nimport { RPC_URL } from 'src/constants/config';\nimport { SenseApi } from 'src/contexts/backend/services/senseApi';\nimport { Option } from 'src/types';\nimport { getSearchQuery, searchByHash } from 'src/utils/search/utils';\nimport { NeuronAddress, ParticleCid } from 'src/types/base';\nimport { getPassportByNickname } from 'src/containers/portal/utils';\nimport { sendCyberlink } from '../neuron/neuronApi';\n\nimport { extractRuneScript } from './helpers';\nimport { RuneEngine } from './engine';\nimport DbApiWrapper from '../backend/services/DbApi/DbApi';\nimport { IpfsApi } from '../backend/workers/background/api/ipfsApi';\nimport { EmbeddingApi } from '../backend/workers/background/api/mlApi';\n\nexport type RuneInnerDeps = {\n  ipfsApi: Option<IpfsApi>;\n  rune: Option<RuneEngine>;\n  queryClient: Option<CyberClient>;\n  embeddingApi: Option<EmbeddingApi>;\n  dbApi: Option<DbApiWrapper>;\n  signingClient: Option<SigningCyberClient & ProxyMarked>;\n  // signer?: Option<OfflineSigner>;\n  senseApi: Option<SenseApi & ProxyMarked>;\n  address: Option<NeuronAddress>;\n};\n\ntype SubjectDeps<T> = {\n  [K in keyof T]: BehaviorSubject<T[K]> | Subject<T[K]>;\n};\n\nconst createRuneDeps = () => {\n  const subjectDeps: SubjectDeps<RuneInnerDeps> = {\n    // Initialize subjects for each dependency\n    ipfsApi: new BehaviorSubject<RuneInnerDeps['ipfsApi']>(undefined),\n    rune: new BehaviorSubject<RuneInnerDeps['rune']>(undefined),\n    queryClient: new BehaviorSubject<RuneInnerDeps['queryClient']>(undefined),\n    embeddingApi: new BehaviorSubject<Option<EmbeddingApi>>(undefined),\n    signingClient: new BehaviorSubject<RuneInnerDeps['signingClient']>(\n      undefined\n    ),\n    senseApi: new BehaviorSubject<RuneInnerDeps['senseApi']>(undefined),\n    address: new BehaviorSubject<RuneInnerDeps['address']>(undefined),\n    dbApi: new BehaviorSubject<RuneInnerDeps['dbApi']>(undefined),\n  };\n\n  let abortController: Option<AbortController>;\n\n  const defferedDependency = (\n    name: keyof RuneInnerDeps\n  ): Promise<RuneInnerDeps[typeof name]> => {\n    return new Promise((resolve) => {\n      const item$ = subjectDeps[name] as BehaviorSubject<\n        RuneInnerDeps[typeof name]\n      >;\n      if (item$.getValue()) {\n        resolve(item$.getValue());\n      }\n\n      item$\n        .pipe(\n          first((value) => !!value) // Automatically unsubscribes after the first valid value\n          // tap((v) => console.log('------defferedDependency', name, v))\n        )\n        .subscribe((value) => {\n          resolve(value);\n        });\n    });\n  };\n\n  CyberClient.connect(RPC_URL).then((client) => {\n    subjectDeps.queryClient?.next(client);\n  });\n\n  const setInnerDeps = (externalDeps: Partial<RuneInnerDeps>) => {\n    Object.keys(externalDeps)\n      .filter((name) => externalDeps[name as keyof RuneInnerDeps] !== undefined)\n      .forEach((name) => {\n        const item = externalDeps[name as keyof RuneInnerDeps];\n        subjectDeps[name as keyof RuneInnerDeps].next(item);\n      });\n  };\n\n  const graphSearch = async (query: string, page = 0) => {\n    const queryClient = (await defferedDependency(\n      'queryClient'\n    )) as CyberClient;\n\n    const keywordHash = await getSearchQuery(query);\n\n    return searchByHash(queryClient, keywordHash, page);\n  };\n\n  const getIpfsTextConent = async (cid: string) => {\n    const ipfsApi = (await defferedDependency('ipfsApi')) as IpfsApi;\n    return ipfsApi.fetchWithDetails(cid, 'text');\n  };\n\n  const evalScriptFromIpfs = async (\n    cid: ParticleCid,\n    funcName: string,\n    params = {}\n  ) => {\n    try {\n      const result = await getIpfsTextConent(cid);\n      if (result?.content === undefined) {\n        return { action: 'error', message: 'Particle not found' };\n      }\n      // in case of soul script is mixed with markdown\n      // need to extract pure script\n      const pureScript = extractRuneScript(result.content);\n\n      const rune = (await defferedDependency('rune')) as RuneEngine;\n\n      return rune.executeFunction(pureScript, funcName, params);\n    } catch (e) {\n      return { action: 'error', message: e.toString() };\n    }\n  };\n\n  const executeScriptCallback = async (refId: string, data = {}) => {\n    try {\n      const rune = (await defferedDependency('rune')) as RuneEngine;\n      return rune.executeCallback(refId, data);\n    } catch (e) {\n      return { action: 'error', message: e.toString() };\n    }\n  };\n\n  const createAbortController = () => {\n    abortController = new AbortController();\n    return abortController;\n  };\n\n  const abort = () => {\n    abortController?.abort();\n  };\n\n  const cybApi = {\n    createAbortController,\n    graphSearch,\n    cyberlink: async (from: string, to: string) => {\n      const address = subjectDeps.address.getValue();\n      if (!address) {\n        throw new Error('Connect your wallet first');\n      }\n      const senseApi = (await defferedDependency('senseApi')) as SenseApi;\n      const signingClient = (await defferedDependency(\n        'signingClient'\n      )) as SigningCyberClient;\n\n      return sendCyberlink(address, from, to, {\n        senseApi,\n        signingClient,\n      });\n    },\n    getPassportByNickname: async (nickname: string) => {\n      const queryClient = await defferedDependency('queryClient');\n      const passport = await getPassportByNickname(queryClient, nickname);\n\n      return passport;\n    },\n    searcByEmbedding: async (text: string, count = 10) => {\n      const embeddingApi = (await defferedDependency(\n        'embeddingApi'\n      )) as EmbeddingApi;\n      await defferedDependency('dbApi');\n      // console.log('----searcByEmbedding', text);\n      return embeddingApi.searchByEmbedding(text, count);\n    },\n    evalScriptFromIpfs,\n    getIpfsTextConent,\n    addContenToIpfs: async (content: string) => {\n      const ipfsApi = (await defferedDependency('ipfsApi')) as IpfsApi;\n\n      return ipfsApi.addContent(content);\n    },\n    executeScriptCallback,\n  };\n\n  return { setInnerDeps, cybApi, abort };\n};\n\nconst runeDeps = createRuneDeps();\n\nexport type RuneDeps = typeof runeDeps;\n\n// export type EngineDeps = ReturnType<typeof createRuneDeps>;\n\nexport default runeDeps;\n","/* eslint-disable import/prefer-default-export */\n/* eslint-disable import/no-unused-modules */\n\n// https://platform.openai.com/docs/models/overview\n// gpt-3.5-turbo\n\ntype OpenAiMessage = {\n  role: 'system' | 'user' | 'assistant';\n  content: string;\n};\n\ninterface OpenAIParams {\n  model: string;\n  messages: OpenAiMessage[];\n  stream?: boolean;\n  [key: string]: any;\n}\n\nconst defaultOpenAIParams: Partial<OpenAIParams> = {\n  model: 'gpt-3.5-turbo',\n};\n\nexport const openAICompletion = async (\n  messages: OpenAiMessage[],\n  apiKey: string,\n  params: Partial<OpenAIParams> = {},\n  cb: (s: string) => Promise<void>,\n  abortController?: AbortController\n): Promise<string> => {\n  const body = JSON.stringify({\n    messages,\n    ...defaultOpenAIParams,\n    ...params,\n  });\n\n  const headers = {\n    'Content-Type': 'application/json',\n    Authorization: `Bearer ${apiKey}`,\n  };\n\n  const response = await fetch('https://api.openai.com/v1/chat/completions', {\n    method: 'POST',\n    signal: abortController?.signal,\n    headers,\n    body,\n  });\n\n  if (!params.stream) {\n    // Non-streaming request\n    const data = await response.json();\n    return data.choices[0].message.content;\n  }\n  // Streaming request\n  const reader = response.body?.getReader();\n  const decoder = new TextDecoder();\n  let result = '';\n  let buffer = '';\n\n  if (reader) {\n    // eslint-disable-next-line no-constant-condition\n    while (true) {\n      // eslint-disable-next-line no-await-in-loop\n      const { done, value } = await reader.read();\n      if (done || abortController?.signal.aborted) {\n        break;\n      }\n\n      buffer += decoder.decode(value, { stream: true });\n      const lines = buffer.split('\\n');\n\n      // Keep the last partial line in the buffer\n      buffer = lines.pop() || '';\n\n      // eslint-disable-next-line no-restricted-syntax\n      for (const line of lines) {\n        const message = line.replace(/^data: /, '');\n        if (message === '[DONE]') {\n          return result;\n        }\n        try {\n          const parsed = JSON.parse(message);\n          if (parsed.choices && parsed.choices.length > 0) {\n            const { content } = parsed.choices[0].delta;\n            result += content;\n            if (content) {\n              // eslint-disable-next-line no-await-in-loop\n              await cb(content);\n            }\n          }\n        } catch (error) {\n          console.error('Error parsing stream message:', message, error);\n        }\n      }\n    }\n  }\n\n  return result;\n};\n","/* eslint-disable import/no-unused-modules */\nimport { getFromLink, getToLink } from 'src/utils/search/utils';\nimport runeDeps from './runeDeps';\nimport { openAICompletion } from './services/llmRequests/openai';\n\n// let runeDeps;\n\n// export const initRuneDeps = (deps) => {\n//   console.log('---initRuneDeps', deps);\n\n//   runeDeps = deps;\n// };\nexport async function jsCyberSearch(query) {\n  return runeDeps.cybApi.graphSearch(query);\n}\n\nexport async function jsCyberLink(fromCid, toCid) {\n  return runeDeps.cybApi.cyberlink(fromCid, toCid);\n}\n\nexport async function jsGetPassportByNickname(nickname) {\n  return runeDeps.cybApi.getPassportByNickname(nickname);\n}\n\nexport async function jsEvalScriptFromIpfs(cid, funcName, params = {}) {\n  return runeDeps.cybApi.evalScriptFromIpfs(cid, funcName, params);\n}\n\nexport async function jsGetIpfsTextContent(cid) {\n  return runeDeps.getIpfsTextConent(cid);\n}\n\nexport async function jsAddContenToIpfs(content) {\n  return runeDeps.addContenToIpfs(content);\n}\n\nexport async function jsExecuteScriptCallback(refId, data) {\n  console.log('exec deps callback', refId);\n  return runeDeps.cybApi.executeScriptCallback(refId, data);\n}\n\nexport async function jsOpenAICompletions(messages, apiKey, params, refId) {\n  const callback = async (data) => jsExecuteScriptCallback(refId, data);\n  const result = await openAICompletion(\n    messages,\n    apiKey,\n    params,\n    callback,\n    runeDeps.cybApi.createAbortController()\n  );\n  return result;\n}\n\nexport async function jsSearchByEmbedding(text, count) {\n  return runeDeps.cybApi.searcByEmbedding(text, count);\n}\n\nexport async function jsCyberLinksFrom(cid) {\n  const result = await getFromLink(cid);\n  return result;\n}\n\nexport async function jsCyberLinksTo(cid) {\n  const result = await getToLink(cid);\n  return result;\n}\n","export const enum Networks {\n  BOSTROM = 'bostrom',\n  SPACE_PUSSY = 'space-pussy',\n  ETH = 'eth',\n  OSMO = 'osmo',\n  TERRA = 'terra',\n  COSMOS = 'cosmoshub-4',\n}\n\nexport type NetworkConfig = {\n  CHAIN_ID: Networks;\n  BASE_DENOM: string;\n  DENOM_LIQUID: string;\n  RPC_URL: string;\n  LCD_URL: string;\n  WEBSOCKET_URL: string;\n  INDEX_HTTPS: string;\n  INDEX_WEBSOCKET: string;\n  BECH32_PREFIX: string;\n  MEMO_KEPLR: string;\n};\n\nexport type NetworksList = {\n  [key in Networks]: NetworkConfig;\n};\n","const LEDGER = {\n  STAGE_INIT: 0,\n  STAGE_SELECTION: 1,\n  STAGE_LEDGER_INIT: 2,\n  STAGE_READY: 3,\n  STAGE_WAIT: 4,\n  STAGE_GENERATED: 5,\n  STAGE_SUBMITTED: 6,\n  STAGE_CONFIRMING: 7,\n  STAGE_CONFIRMED: 8,\n  STAGE_ERROR: 15,\n  HDPATH: [44, 118, 0, 0, 0],\n};\n\nconst GENESIS_SUPPLY = 1000000000000000;\nconst TOTAL_GOL_GENESIS_SUPPLY = 50000000000000;\n\nconst POCKET = {\n  STAGE_TWEET_ACTION_BAR: {\n    ADD_AVATAR: 'addAvatar',\n    FOLLOW: 'follow',\n    TWEET: 'tweet',\n  },\n};\n\nexport { LEDGER, GENESIS_SUPPLY, TOTAL_GOL_GENESIS_SUPPLY, POCKET };\n","import dateFormat from 'dateformat';\n\nexport const numberToUtcDate = (timestamp: number) =>\n  dateFormat(new Date(timestamp), 'yyyy-mm-dd\"T\"HH:MM:ss.l', true);\n\nexport const dateToUtcNumber = (isoString: string) =>\n  Date.parse(isoString.endsWith('Z') ? isoString : `${isoString}Z`);\n\nexport const getNowUtcNumber = () => Date.now();\n\nfunction roundMilliseconds(dateTimeString: string) {\n  const date = new Date(dateTimeString);\n  const roundedMilliseconds = Math.round(date.getMilliseconds() / 1000) * 1000;\n  date.setMilliseconds(roundedMilliseconds);\n  return dateFormat(date, 'yyyy-mm-dd\"T\"HH:MM:ss.l');\n}\nfunction getCurrentTimezoneOffset() {\n  const now = new Date();\n  return -now.getTimezoneOffset() / 60;\n}\n\nfunction pluralizeUnit(quantity: number, unit: string): string {\n  return quantity === 1 ? unit : `${unit}s`;\n}\n\nconst minuteInMs = 60000; // 60 seconds * 1000 milliseconds\nconst hourInMs = 3600000; // 60 minutes * 60 seconds * 1000 milliseconds\nconst dayInMs = 86400000; // 24 hours * 60 minutes * 60 seconds * 1000 milliseconds\n\nfunction convertTimestampToString(timestamp: number): string {\n  if (timestamp < minuteInMs) {\n    const seconds = Math.floor(timestamp / 1000);\n    return `${seconds} ${pluralizeUnit(seconds, 'second')}`;\n  }\n  if (timestamp < hourInMs) {\n    const minutes = Math.floor(timestamp / minuteInMs);\n    return `${minutes} ${pluralizeUnit(minutes, 'minute')}`;\n  }\n  if (timestamp < dayInMs) {\n    const hours = Math.floor(timestamp / hourInMs);\n    return `${hours} ${pluralizeUnit(hours, 'hour')}`;\n  }\n\n  const days = Math.floor(timestamp / dayInMs);\n  return `${days} ${pluralizeUnit(days, 'day')}`;\n}\n\nexport { roundMilliseconds, convertTimestampToString };\n","import Unixfs from 'ipfs-unixfs';\nimport { DAGNode, util as DAGUtil } from 'ipld-dag-pb';\nimport { isString } from 'lodash';\nimport { RemoteIpfsApi } from 'src/services/backend/workers/background/worker';\nimport { ParticleCid } from 'src/types/base';\nimport { PATTERN_IPFS_HASH } from 'src/constants/patterns';\nimport { Remote } from 'comlink';\nimport { IpfsApi } from 'src/services/backend/workers/background/api/ipfsApi';\n\nexport const isCID = (cid: string): boolean => {\n  return cid.match(PATTERN_IPFS_HASH) !== null;\n};\n\n// eslint-disable-next-line import/prefer-default-export\nexport const getIpfsHash = (string: string): Promise<ParticleCid> =>\n  new Promise((resolve, reject) => {\n    const unixFsFile = new Unixfs('file', Buffer.from(string));\n\n    const buffer = unixFsFile.marshal();\n    DAGNode.create(buffer, (err, dagNode) => {\n      if (err) {\n        reject(new Error('Cannot create ipfs DAGNode'));\n      }\n\n      DAGUtil.cid(dagNode, (error, cid) => {\n        resolve(cid.toBaseEncodedString());\n      });\n    });\n  });\nexport const addIfpsMessageOrCid = async (\n  message: string | ParticleCid | File,\n  { ipfsApi }: { ipfsApi: Remote<IpfsApi> | null }\n) => {\n  if (!ipfsApi) {\n    throw Error('IpfsApi is not initialized');\n  }\n\n  return (\n    isString(message) && message.match(PATTERN_IPFS_HASH)\n      ? message\n      : ((await ipfsApi!.addContent(message)) as string)\n  ) as ParticleCid;\n};\n","export const CYBLOG_LOG_SHOW = 'cyblog_show';\n\nexport const CYBLOG_BROADCAST_CHANNEL_NAME = 'CYBLOG_BROADCST_CHANNEL';\n\nexport const CYBLOG_CONSOLE_PARAMS_DEFAULT = {\n  thread: 'all',\n  unit: 'all',\n  module: 'all',\n};\n","import _, { isEmpty } from 'lodash';\nimport { ConsoleLogParams, LogContext, LogItem, LogLevel } from './types';\nimport { CYBLOG_BROADCAST_CHANNEL_NAME } from './constants';\n\nconst logList: LogItem[] = [];\n\nfunction createCybLog<T>(defaultContext: Partial<LogContext<T>> = {}) {\n  function appendLog(logItem: LogItem, truncate = true) {\n    logList.push(logItem);\n\n    while (truncate && logList.length > 1000) {\n      logList.shift(); // Remove the first element to keep the list size <= 1000\n    }\n  }\n  let consoleLogParams = {} as ConsoleLogParams;\n\n  const channel = new BroadcastChannel(CYBLOG_BROADCAST_CHANNEL_NAME);\n\n  channel.onmessage = (event) => {\n    if (event.data.type === 'params') {\n      consoleLogParams = { ...consoleLogParams, ...event.data.value };\n    }\n  };\n\n  const getConsoleLogParams = () => consoleLogParams;\n\n  function consoleLog<T>(\n    level: LogLevel,\n    message: T,\n    context: Partial<LogContext<T>>\n  ) {\n    const ctx = _.omit(context, [\n      'formatter',\n      'thread',\n      'module',\n      'unit',\n      'data',\n    ]);\n    const { thread = '', module = '', unit = '', data = '' } = context;\n    const ctxItem = isEmpty(ctx) ? '' : ctx;\n\n    if (Array.isArray(message)) {\n      console[level](...message, ctxItem);\n      return;\n    }\n\n    if (context?.formatter) {\n      console[level](context?.formatter(message), ctxItem);\n      return;\n    }\n\n    console[level](`[${thread}:${module}:${unit}] ${message}`, data, ctxItem);\n  }\n\n  // eslint-disable-next-line import/no-unused-modules\n  function log<T>(\n    level: LogLevel,\n    message: string | T,\n    context: LogContext<any> = defaultContext\n  ) {\n    try {\n      const formattedMessage = context?.formatter\n        ? context?.formatter(message)\n        : message;\n\n      const logEntry = {\n        timestamp: new Date(),\n        level,\n        message: formattedMessage,\n        stacktrace: context?.stacktrace,\n        context: _.omit(context, ['formatter', 'stacktrace']),\n      };\n\n      appendLog(logEntry);\n      // !!localStorage.getItem(LOCAL_STORAGE_USE_CONSOLE_LOG_KEY) &&\n      const showConsoleLog = Object.keys(consoleLogParams).reduce(\n        (acc: boolean, key: string) => {\n          const params = consoleLogParams[key];\n          const contextItem = context[key];\n          if (params && contextItem) {\n            return (\n              acc ||\n              params === 'all' ||\n              params.length === 0 ||\n              params.some((p) => p === contextItem)\n            );\n          }\n          return acc;\n        },\n        false\n      );\n\n      if (showConsoleLog) {\n        consoleLog(level, message, context);\n      }\n    } catch (error) {\n      console.log('cyblog error', error);\n    }\n  }\n\n  function info<T>(message: T, context?: LogContext<string | T>) {\n    return log('info', message, context);\n  }\n\n  function error<T>(message: T, context?: LogContext<string | T>) {\n    return log('error', message, context);\n  }\n\n  function warn<T>(message: T, context?: LogContext<string | T>) {\n    return log('warn', message, context);\n  }\n\n  function trace<T>(message: T, context?: LogContext<string | T>) {\n    return log('warn', message, context);\n  }\n\n  function normalizeLog() {\n    return logList.map((logItem) => {\n      const { context, ...rest } = logItem;\n      const {\n        unit = '',\n        module = '',\n        thread = '',\n        data = '',\n        error = '',\n        stacktrace = '',\n      } = context || {};\n      return {\n        ...rest,\n        unit,\n        module,\n        thread,\n        data, //: JSON.stringify(data),\n        error,\n        stacktrace,\n      };\n    });\n  }\n\n  return {\n    log,\n    info,\n    error,\n    warn,\n    trace,\n    logList,\n    getLogs: () => normalizeLog(),\n    clear: () => logList.splice(0, logList.length),\n    getConsoleLogParams,\n  };\n}\n\nexport const createCyblogChannel = (\n  defaultContext: Partial<LogContext<T>> = {}\n) => {\n  const channel = new BroadcastChannel(CYBLOG_BROADCAST_CHANNEL_NAME);\n\n  function postLogToChannel<T>(\n    level: LogLevel,\n    message: T,\n    context?: LogContext<string | T>\n  ) {\n    const ctx = { ...defaultContext, ...context };\n    if (context?.error) {\n      ctx.error = JSON.stringify(context.error);\n    }\n    channel.postMessage({\n      type: 'log',\n      value: { level, message, context: ctx },\n    });\n  }\n\n  function info<T>(message: T, context?: LogContext<string | T>) {\n    return postLogToChannel('info', message, context);\n  }\n\n  function error<T>(message: T, context?: LogContext<string | T>) {\n    return postLogToChannel('error', message, context);\n  }\n\n  function warn<T>(message: T, context?: LogContext<string | T>) {\n    return postLogToChannel('warn', message, context);\n  }\n\n  function trace<T>(message: T, context?: LogContext<string | T>) {\n    return postLogToChannel('warn', message, context);\n  }\n\n  return { info, error, warn, trace };\n};\n\nconst cyblog = createCybLog({ thread: 'main' });\n\nexport type LogFunc = (message: T, context?: LogContext<string | T>) => void;\n\nexport type CyblogChannel = ReturnType<typeof createCyblogChannel>;\n\nexport default cyblog;\n","import axios from 'axios';\n\nimport { CyberClient } from '@cybercongress/cyber-js';\nimport { DelegationResponse } from 'cosmjs-types/cosmos/staking/v1beta1/staking';\nimport { CID_TWEET } from 'src/constants/app';\nimport { LCD_URL } from 'src/constants/config';\nimport { LinksType, LinksTypeFilter } from 'src/containers/Search/types';\nimport { ParticleCid } from 'src/types/base';\nimport { PATTERN_IPFS_HASH } from 'src/constants/patterns';\nimport { getIpfsHash } from '../ipfs/helpers';\nimport { encodeSlash } from '../utils';\n\nexport const formatNumber = (number, toFixed) => {\n  let formatted = +number;\n\n  if (toFixed) {\n    formatted = +formatted.toFixed(toFixed);\n  }\n\n  return formatted.toLocaleString('en').replace(/,/g, ' ');\n};\n\nexport const getRankGrade = (rank) => {\n  let from;\n  let to;\n  let value;\n\n  if (rank > 0.00000276) {\n    from = 0.00000276;\n    to = 0.01;\n    value = 1;\n  } else if (rank > 0.00000254879356777504 && rank <= 0.00000276) {\n    from = 0.00000254879356777504;\n    to = 0.00000276;\n    value = 2;\n  } else if (rank > 0.00000233758713555007 && rank <= 0.00000254879356777504) {\n    from = 0.00000233758713555007;\n    to = 0.00000254879356777504;\n    value = 3;\n  } else if (rank > 0.00000191517427110014 && rank <= 0.00000233758713555007) {\n    from = 0.00000191517427110014;\n    to = 0.00000233758713555007;\n    value = 4;\n  } else if (rank > 0.00000128155497442525 && rank <= 0.00000191517427110014) {\n    from = 0.00000128155497442525;\n    to = 0.00000191517427110014;\n    value = 5;\n  } else if (rank > 0.00000022552281330043 && rank <= 0.00000128155497442525) {\n    from = 0.00000022552281330043;\n    to = 0.00000128155497442525;\n    value = 6;\n  } else if (rank > 0 && rank <= 0.00000022552281330043) {\n    from = 0;\n    to = 0.00000022552281330043;\n    value = 7;\n  } else {\n    from = 'n/a';\n    to = 'n/a';\n    value = 'n/a';\n  }\n\n  return {\n    from,\n    to,\n    value,\n  };\n};\n\nexport const selfDelegationShares = async (\n  delegatorAddress,\n  operatorAddress\n) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/staking/delegators/${delegatorAddress}/delegations/${operatorAddress}`,\n    });\n    return response.data.result.balance.amount;\n  } catch (e) {\n    console.log(e);\n    return 0;\n  }\n};\n\nexport const stakingPool = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/staking/pool`,\n    });\n\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return 0;\n  }\n};\n\nexport const getRelevance = async (page = 0, limit = 50) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/rank/top?page=${page}&limit=${limit}`,\n    });\n    return response.data.result;\n  } catch (error) {\n    return {};\n  }\n};\n\nexport const getTxs = async (txs) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/txs/${txs}`,\n    });\n    return response.data;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const getValidatorsInfo = async (address) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/staking/validators/${address}`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const keybaseCheck = async (identity) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `https://keybase.io/_/api/1.0/user/lookup.json?key_suffix=${identity}&fields=basics`,\n    });\n    return response.data;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const keybaseAvatar = async (identity) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `https://keybase.io/_/api/1.0/user/lookup.json?key_suffix=${identity}&fields=pictures`,\n    });\n    return response.data;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const getDelegators = async (validatorAddr) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/staking/validators/${validatorAddr}/delegations`,\n    });\n    return response.data;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamSlashing = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/slashing/parameters`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamDistribution = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/distribution/parameters`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamBandwidth = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/bandwidth/parameters`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamGov = async () => {\n  try {\n    const responseGovDeposit = await axios({\n      method: 'get',\n      url: `${LCD_URL}/gov/parameters/deposit`,\n    });\n\n    const responseGovTallying = await axios({\n      method: 'get',\n      url: `${LCD_URL}/gov/parameters/tallying`,\n    });\n\n    const responseGovVoting = await axios({\n      method: 'get',\n      url: `${LCD_URL}/gov/parameters/voting`,\n    });\n\n    const response = {\n      deposit: responseGovDeposit.data.result,\n      voting: responseGovTallying.data.result,\n      tallying: responseGovVoting.data.result,\n    };\n\n    return response;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamRank = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/rank/parameters`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamInlfation = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/minting/parameters`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamResources = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cyber/resources/v1beta1/resources/params`,\n    });\n    return response.data.params;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamStaking = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/staking/parameters`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamLiquidity = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cosmos/liquidity/v1beta1/params`,\n    });\n    return response.data.params;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamGrid = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cyber/grid/v1beta1/grid/params`,\n    });\n    return response.data.params;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamDmn = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cyber/dmn/v1beta1/dmn/params`,\n    });\n    return response.data.params;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const getParamNetwork = async (address, node) => {\n  try {\n    let staking = null;\n    let slashing = null;\n    let distribution = null;\n    let bandwidth = null;\n    let gov = null;\n    let rank = null;\n    let mint = null;\n    let resources = null;\n    let liquidity = null;\n    let grid = null;\n    let dmn = null;\n\n    const dataStaking = await getParamStaking();\n    if (dataStaking !== null) {\n      staking = dataStaking;\n    }\n    const dataSlashing = await getParamSlashing();\n    if (dataSlashing !== null) {\n      slashing = dataSlashing;\n    }\n    const dataDistribution = await getParamDistribution();\n    if (dataDistribution !== null) {\n      distribution = dataDistribution;\n    }\n    const dataGov = await getParamGov();\n    if (dataGov !== null) {\n      gov = dataGov;\n    }\n    const dataBandwidth = await getParamBandwidth();\n    if (dataBandwidth !== null) {\n      bandwidth = dataBandwidth;\n    }\n\n    const dataRank = await getParamRank();\n    if (dataRank !== null) {\n      rank = dataRank;\n    }\n\n    const dataInlfation = await getParamInlfation();\n    if (dataInlfation !== null) {\n      mint = dataInlfation;\n    }\n\n    const dataResources = await getParamResources();\n    if (dataResources !== null) {\n      resources = dataResources;\n    }\n\n    const dataLiquidity = await getParamLiquidity();\n    if (dataLiquidity !== null) {\n      liquidity = dataLiquidity;\n    }\n\n    const dataGrid = await getParamGrid();\n    if (dataGrid !== null) {\n      grid = dataGrid;\n    }\n\n    const dataDmn = await getParamDmn();\n    if (dataDmn !== null) {\n      dmn = dataDmn;\n    }\n\n    const response = {\n      staking,\n      slashing,\n      distribution,\n      bandwidth,\n      gov,\n      rank,\n      mint,\n      resources,\n      liquidity,\n      grid,\n      dmn,\n    };\n\n    return response;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const getInlfation = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/minting/inflation`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nenum Order {\n  ASC = 'ORDER_BY_ASC',\n  DESC = 'ORDER_BY_DESC',\n}\n\nconst getLink = async (\n  cid: string,\n  type: LinksType = LinksTypeFilter.from,\n  { offset, limit, order = Order.DESC }\n) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cosmos/tx/v1beta1/txs`,\n      params: {\n        'pagination.offset': offset,\n        'pagination.limit': limit,\n        orderBy: Order.DESC,\n        events: `cyberlink.particle${\n          type === LinksTypeFilter.to ? 'To' : 'From'\n        }='${cid}'`,\n      },\n    });\n    return response.data;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const getFromLink = async (cid, offset, limit) => {\n  return getLink(cid, LinksTypeFilter.from, { offset, limit });\n};\n\nexport const getToLink = async (cid, offset, limit) => {\n  return getLink(cid, LinksTypeFilter.to, { offset, limit });\n};\n\nexport const getSendBySenderRecipient = async (\n  address,\n  offset = 0,\n  limit = 5\n) => {\n  try {\n    const { recipient, sender } = address;\n    const response = await axios({\n      method: 'get',\n      url: `https://lcd.bostrom.cybernode.ai/cosmos/tx/v1beta1/txs?pagination.offset=${offset}&pagination.limit=${limit}&orderBy=ORDER_BY_DESC&events=message.action%3D%27%2Fcosmos.bank.v1beta1.MsgSend%27&events=transfer.sender%3D%27${sender}%27&events=transfer.recipient%3D%27${recipient}%27`,\n    });\n    return response.data;\n  } catch (e) {\n    console.log(e);\n    return undefined;\n  }\n};\n\nexport const getFollows = async (address) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/txs?cyberlink.neuron=${address}&cyberlink.particleFrom=QmPLSA5oPqYxgc8F7EwrM8WS9vKrr1zPoDniSRFh8HSrxx&limit=1000000000`,\n    });\n    return response.data;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const getTweet = async (address) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/txs?cyberlink.neuron=${address}&cyberlink.particleFrom=${CID_TWEET}&limit=1000000000`,\n    });\n    return response.data;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\nexport const chekFollow = async (address, addressFollowHash) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/txs?cyberlink.neuron=${address}&cyberlink.particleFrom=QmPLSA5oPqYxgc8F7EwrM8WS9vKrr1zPoDniSRFh8HSrxx&cyberlink.particleTo=${addressFollowHash}&limit=1000000000`,\n    });\n    return response.data;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\ntype PropsTx = {\n  events: ReadonlyArray<{ key: string; value: string }>;\n  pagination?: {\n    limit: number;\n    offset: number;\n  };\n  orderBy?: 'ORDER_BY_UNSPECIFIED' | 'ORDER_BY_ASC' | 'ORDER_BY_DESC';\n};\n\n// // TODO: add types\nexport async function getTransactions({\n  events,\n  pagination = { limit: 20, offset: 0 },\n  orderBy = 'ORDER_BY_UNSPECIFIED',\n}: PropsTx) {\n  const { offset, limit } = pagination;\n  return axios.get(`${LCD_URL}/cosmos/tx/v1beta1/txs`, {\n    params: {\n      'pagination.offset': offset,\n      'pagination.limit': limit,\n      orderBy,\n      events: events.map((evn) => `${evn.key}='${evn.value}'`),\n    },\n    paramsSerializer: {\n      indexes: null,\n    },\n  });\n}\n\n// export async function getCyberlinks(address) {\n//   getTransactions({\n//     events: [\n//       \"message.action='/cyber.graph.v1beta1.MsgCyberlink'\",\n//       \"cyberlink.neuron=' + address\",\n//     ],\n//   });\n// }\nexport async function getCyberlinksTotal(address: string) {\n  try {\n    const response = await getTransactions({\n      events: [\n        { key: 'message.action', value: '/cyber.graph.v1beta1.MsgCyberlink' },\n        { key: 'cyberlink.neuron', value: address },\n      ],\n      pagination: { limit: 5, offset: 0 },\n    });\n\n    return response.data?.pagination?.total;\n  } catch (error) {\n    console.log(error);\n    return undefined;\n  }\n}\n\nexport const getAvatar = async (address) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/txs?cyberlink.neuron=${address}&cyberlink.particleFrom=Qmf89bXkJH9jw4uaLkHmZkxQ51qGKfUPtAMxA8rTwBrmTs&limit=1000000000`,\n    });\n    return response.data;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\nexport const getFollowers = async (addressHash) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/txs?cyberlink.particleFrom=QmPLSA5oPqYxgc8F7EwrM8WS9vKrr1zPoDniSRFh8HSrxx&cyberlink.particleTo=${addressHash}&limit=1000000000`,\n    });\n    return response.data;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\nexport const getCreator = async (cid) => {\n  try {\n    // TODO: refactor this\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cosmos/tx/v1beta1/txs?pagination.offset=0&pagination.limit=1&orderBy=ORDER_BY_ASC&events=cyberlink.particleTo%3D%27${cid}%27`,\n    });\n\n    const response2 = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cosmos/tx/v1beta1/txs?pagination.offset=0&pagination.limit=1&orderBy=ORDER_BY_ASC&events=cyberlink.particleFrom%3D%27${cid}%27`,\n    });\n\n    const h1 = Number(response.data.tx_responses?.[0]?.height || 0);\n    const h2 = Number(response2.data.tx_responses?.[0]?.height || 0);\n\n    if (h1 === 0) {\n      return response2.data;\n    }\n    if (h2 === 0) {\n      return response.data;\n    }\n\n    return h1 < h2 ? response.data : response2.data;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\nexport const authAccounts = async (address) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/auth/accounts/${address}`,\n    });\n    return response.data;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\n// export const getAvatarIpfs = async (cid, ipfs) => {\n//   try {\n//     // TODO: ipfs refactor\n//     const response = await getIPFSContent(cid, ipfs);\n//     console.log('--------getAvatarIpfs', cid, response);\n//     if (response?.result) {\n//       // const rawData = await getResponseResult(response.result);\n//       const details = await parseArrayLikeToDetails(\n//         response.result,\n//         response.meta.mime,\n//         cid\n//       );\n//       if (details.type === 'image') {\n//         return details.content;\n//       }\n\n//       return undefined;\n//     }\n\n//     return undefined;\n//   } catch (error) {\n//     return undefined;\n//   }\n// };\n\n// Access-Control-Allow-Origin\nexport const getCredit = async (address) => {\n  try {\n    const headers = {\n      'Content-Type': 'application/json',\n    };\n    const fromData = {\n      denom: 'boot',\n      address,\n    };\n    const response = await axios({\n      method: 'post',\n      // url: 'http://localhost:8000/credit',\n      url: 'https://titan.cybernode.ai/credit',\n      headers,\n      data: JSON.stringify(fromData),\n    });\n\n    return response;\n  } catch (error) {\n    return null;\n  }\n};\n\nexport const getSearchQuery = async (query: ParticleCid | string) =>\n  query.match(PATTERN_IPFS_HASH) ? query : getIpfsHash(encodeSlash(query));\n\nexport const searchByHash = async (\n  client: CyberClient,\n  hash: string,\n  page: number\n) => {\n  try {\n    const results = await client.search(hash, page);\n\n    return results;\n  } catch (error) {\n    // TODO: handle\n    console.error(error);\n    return undefined;\n  }\n};\n\nexport const getDelegatorDelegations = async (\n  client: CyberClient,\n  addressBech32: string\n): Promise<DelegationResponse[]> => {\n  let nextKey;\n  const delegationData: DelegationResponse[] = [];\n\n  let done = false;\n  while (!done) {\n    // eslint-disable-next-line no-await-in-loop\n    const responsedelegatorDelegations = await client.delegatorDelegations(\n      addressBech32,\n      nextKey\n    );\n\n    delegationData.push(...responsedelegatorDelegations.delegationResponses);\n\n    const key = responsedelegatorDelegations?.pagination?.nextKey;\n\n    if (key) {\n      nextKey = key;\n    } else {\n      done = true;\n    }\n  }\n\n  return delegationData;\n};\n","/* eslint-disable no-await-in-loop */\nimport bech32 from 'bech32';\nimport { fromBase64, fromUtf8, toBech32 } from '@cosmjs/encoding';\nimport { Sha256 } from '@cosmjs/crypto';\nimport BigNumber from 'bignumber.js';\nimport { ObjKeyValue } from 'src/types/data';\nimport { Pool } from '@cybercongress/cyber-js/build/codec/tendermint/liquidity/v1beta1/liquidity';\nimport { Option } from 'src/types';\nimport { Key } from '@keplr-wallet/types';\nimport { AccountValue } from 'src/types/defaultAccount';\nimport { BECH32_PREFIX, BECH32_PREFIX_VAL_CONS } from 'src/constants/config';\nimport { LEDGER } from './config';\n\nimport cyberSpace from '../image/large-purple-circle.png';\nimport customNetwork from '../image/large-orange-circle.png';\nimport cyberBostrom from '../image/large-green.png';\n\nconst DEFAULT_DECIMAL_DIGITS = 3;\nconst DEFAULT_CURRENCY = 'GoL';\n\nconst roundNumber = (num, scale) => {\n  if (!`${num}`.includes('e')) {\n    return +`${Math.floor(`${num}e+${scale}`)}e-${scale}`;\n  }\n  const arr = `${num}`.split('e');\n  let sig = '';\n  if (+arr[1] + scale > 0) {\n    sig = '+';\n  }\n  const i = `${+arr[0]}e${sig}${+arr[1] + scale}`;\n  const j = Math.floor(i);\n  const k = +`${j}e-${scale}`;\n  return k;\n};\n\nfunction numberWithCommas(x) {\n  const parts = x.split('.');\n  parts[0] = parts[0].replace(/\\B(?=(\\d{3})+(?!\\d))/g, ' ');\n  return parts.join('.');\n}\n\nconst formatNumber = (number: number | string, toFixed?: number): string => {\n  let formatted = number;\n\n  if (toFixed) {\n    formatted = roundNumber(formatted, toFixed);\n    formatted = formatted.toFixed(toFixed + 1);\n  }\n\n  if (typeof number === 'string') {\n    return numberWithCommas(formatted);\n  }\n\n  return formatted\n    .toLocaleString('en')\n    .replace(/(\\.\\d{0,})0+$/, '$1')\n    .replace(/,/g, ' ');\n};\n\nconst PREFIXES = [\n  {\n    prefix: 'T',\n    power: 10 ** 12,\n  },\n  {\n    prefix: 'G',\n    power: 10 ** 9,\n  },\n  {\n    prefix: 'M',\n    power: 10 ** 6,\n  },\n  {\n    prefix: 'K',\n    power: 10 ** 3,\n  },\n];\n\nexport function formatCurrency(\n  value,\n  currency = DEFAULT_CURRENCY,\n  decimalDigits = DEFAULT_DECIMAL_DIGITS,\n  prefixCustom = PREFIXES\n) {\n  const { prefix = '', power = 1 } =\n    prefixCustom.find((obj) => value >= obj.power) || {};\n\n  return `${roundNumber(\n    Number(value) / power,\n    decimalDigits\n  )} ${prefix}${currency.toLocaleUpperCase()}`;\n}\n\nconst getDecimal = (number, toFixed) => {\n  const nstring = number.toString();\n  const narray = nstring.split('.');\n  const result = narray.length > 1 ? narray[1] : '000';\n  return result;\n};\n\nconst asyncForEach = async (array, callback) => {\n  for (let index = 0; index < array.length; index++) {\n    await callback(array[index], index, array);\n  }\n};\n\nconst fromBech32 = (operatorAddr, prefix = BECH32_PREFIX) => {\n  const address = bech32.decode(operatorAddr);\n  return bech32.encode(prefix, address.words);\n};\n\nexport const consensusPubkey = (pubKey: string) => {\n  const ed25519PubkeyRaw = fromBase64(pubKey);\n  const addressData = sha256(ed25519PubkeyRaw).slice(0, 20);\n  return toBech32(BECH32_PREFIX_VAL_CONS, addressData);\n};\n\nconst trimString = (address: string, first = 3, second = 8) => {\n  if (address && address.length > 11) {\n    return `${address.substring(0, first)}...${address.substring(\n      address.length - second\n    )}`;\n  }\n  if (address && address.length < 11) {\n    return address;\n  }\n  return '';\n};\n\nconst exponentialToDecimal = (exponential) => {\n  let decimal = exponential.toString().toLowerCase();\n  if (decimal.includes('e+')) {\n    const exponentialSplitted = decimal.split('e+');\n    let postfix = '';\n    for (\n      let i = 0;\n      i <\n      +exponentialSplitted[1] -\n        (exponentialSplitted[0].includes('.')\n          ? exponentialSplitted[0].split('.')[1].length\n          : 0);\n      i++\n    ) {\n      postfix += '0';\n    }\n    decimal = exponentialSplitted[0].replace('.', '') + postfix;\n  }\n  if (decimal.toLowerCase().includes('e-')) {\n    const exponentialSplitted = decimal.split('e-');\n    let prefix = '0.';\n    for (let i = 0; i < +exponentialSplitted[1] - 1; i++) {\n      prefix += '0';\n    }\n    decimal = prefix + exponentialSplitted[0].replace('.', '');\n  }\n  return decimal;\n};\n\nfunction dhm(t) {\n  const cd = 24 * 60 * 60 * 1000;\n  const ch = 60 * 60 * 1000;\n  let d = Math.floor(t / cd);\n  let h = Math.floor((t - d * cd) / ch);\n  let m = Math.round((t - d * cd - h * ch) / 60000);\n  const pad = (n, unit) => {\n    return n < 10 ? `0${n}${unit}` : `${n}${unit}`;\n  };\n  if (m === 60) {\n    h += 1;\n    m = 0;\n  }\n  if (h === 24) {\n    d += 1;\n    h = 0;\n  }\n  return [`${d}d`, pad(h, 'h'), pad(m, 'm')].join(':');\n}\n\nconst downloadObjectAsJson = (exportObj, exportName) => {\n  const dataStr = `data:text/json;charset=utf-8,${encodeURIComponent(\n    JSON.stringify(exportObj)\n  )}`;\n  const downloadAnchorNode = document.createElement('a');\n\n  downloadAnchorNode.setAttribute('href', dataStr);\n  downloadAnchorNode.setAttribute('download', `${exportName}.json`);\n  document.body.appendChild(downloadAnchorNode);\n  downloadAnchorNode.click();\n  downloadAnchorNode.remove();\n};\n\nconst isMobileTablet = () => {\n  let hasTouchScreen = false;\n  if ('maxTouchPoints' in navigator) {\n    hasTouchScreen = navigator.maxTouchPoints > 0;\n  } else if ('msMaxTouchPoints' in navigator) {\n    hasTouchScreen = navigator.msMaxTouchPoints > 0;\n  } else {\n    const mQ = window.matchMedia && matchMedia('(pointer:coarse)');\n    if (mQ && mQ.media === '(pointer:coarse)') {\n      hasTouchScreen = !!mQ.matches;\n    } else if ('orientation' in window) {\n      hasTouchScreen = true; // deprecated, but good fallback\n    } else {\n      // Only as a last resort, fall back to user agent sniffing\n      const UA = navigator.userAgent;\n      hasTouchScreen =\n        /\\b(BlackBerry|webOS|iPhone|IEMobile)\\b/i.test(UA) ||\n        /\\b(Android|Windows Phone|iPad|iPod)\\b/i.test(UA);\n    }\n  }\n  return hasTouchScreen;\n};\n\nconst coinDecimals = (number) => {\n  return number * 10 ** -18;\n};\n\nconst convertResources = (number) => {\n  return Math.floor(number * 10 ** -3);\n};\n\nfunction timeSince(timeMS: number) {\n  const seconds = Math.floor(timeMS / 1000);\n\n  if (seconds === 0) {\n    return 'now';\n  }\n\n  let interval = Math.floor(seconds / 31536000);\n\n  if (interval > 1) {\n    return `${interval} years`;\n  }\n  interval = Math.floor(seconds / 2592000);\n  if (interval > 1) {\n    return `${interval} months`;\n  }\n  interval = Math.floor(seconds / 86400);\n  if (interval > 1) {\n    return `${interval} days`;\n  }\n  interval = Math.floor(seconds / 3600);\n  if (interval > 1) {\n    return `${interval} hours`;\n  }\n  interval = Math.floor(seconds / 60);\n  if (interval > 1) {\n    return `${interval} min`;\n  }\n  return `${Math.floor(seconds)} sec`;\n}\n\nconst reduceBalances = (data): ObjKeyValue => {\n  try {\n    let balances = {};\n    if (Object.keys(data).length > 0) {\n      balances = data.reduce(\n        (obj, item) => ({\n          ...obj,\n          [item.denom]: parseFloat(item.amount),\n        }),\n        {}\n      );\n    }\n    return balances;\n  } catch (error) {\n    console.log(`error reduceBalances`, error);\n    return {};\n  }\n};\n\n// example: oneLiner -> message.module=wasm&message.action=/cosmwasm.wasm.v1.MsgStoreCode&store_code.code_id=${codeId}\nfunction makeTags(oneLiner) {\n  return oneLiner.split('&').map((pair) => {\n    if (pair.indexOf('=') === -1) {\n      throw new Error('Parsing error: Equal sign missing');\n    }\n    const parts = pair.split('=');\n    if (parts.length > 2) {\n      throw new Error(\n        'Parsing error: Multiple equal signs found. If you need escaping support, please create a PR.'\n      );\n    }\n    const [key, value] = parts;\n    if (!key) {\n      throw new Error('Parsing error: Key must not be empty');\n    }\n    return { key, value };\n  });\n}\n\nfunction parseMsgContract(msg) {\n  const json = fromUtf8(msg);\n\n  return JSON.parse(json);\n}\nconst replaceSlash = (text) => text.replace(/\\//g, '%2F');\n\nconst encodeSlash = (text) => text.replace(/%2F/g, '/');\n\nconst groupMsg = (ArrMsg, size = 2) => {\n  const link = [];\n  for (let i = 0; i < Math.ceil(ArrMsg.length / size); i += 1) {\n    link[i] = ArrMsg.slice(i * size, i * size + size);\n  }\n  return link;\n};\n\nconst selectNetworkImg = (network) => {\n  switch (network) {\n    case 'bostrom':\n      return cyberBostrom;\n    case 'space-pussy':\n      return cyberSpace;\n\n    default:\n      return customNetwork;\n  }\n};\n\nconst sha256 = (data) => {\n  return new Uint8Array(new Sha256().update(data).digest());\n};\n\nfunction getDenomHash(path, baseDenom) {\n  const parts = path.split('/');\n  parts.push(baseDenom);\n  const newPath = parts.slice().join('/');\n  return `ibc/${Buffer.from(sha256(Buffer.from(newPath)))\n    .toString('hex')\n    .toUpperCase()}`;\n}\n\nfunction convertAmount(rawAmount, precision) {\n  return new BigNumber(rawAmount)\n    .shiftedBy(-precision)\n    .dp(precision, BigNumber.ROUND_FLOOR)\n    .toNumber();\n}\n\nfunction convertAmountReverce(rawAmount, precision) {\n  return new BigNumber(rawAmount)\n    .shiftedBy(precision)\n    .dp(precision, BigNumber.ROUND_FLOOR)\n    .toNumber();\n}\n\nfunction getDisplayAmount(\n  rawAmount: number | string,\n  precision: number\n): number {\n  return parseFloat(\n    new BigNumber(rawAmount)\n      .shiftedBy(-precision)\n      .dp(precision, BigNumber.ROUND_FLOOR)\n      .toFixed(precision > 0 ? 3 : 0, BigNumber.ROUND_FLOOR)\n  );\n}\n\nfunction getDisplayAmountReverce(rawAmount, precision) {\n  return new BigNumber(rawAmount)\n    .shiftedBy(precision)\n    .dp(precision, BigNumber.ROUND_FLOOR)\n    .toFixed(precision > 0 ? 3 : 0, BigNumber.ROUND_FLOOR);\n}\n\nfunction isNative(denom) {\n  if (denom && denom.includes('ibc')) {\n    return false;\n  }\n  return true;\n}\n\nconst findPoolDenomInArr = (\n  baseDenom: string,\n  dataPools: Pool[]\n): Option<Pool> => {\n  const findObj = dataPools.find((item) => item.poolCoinDenom === baseDenom);\n  return findObj;\n};\n\n// REFACTOR: Probably wrong timestamp\nconst getNowUtcTime = (): number => {\n  const now = new Date();\n  const utcTime = new Date(\n    now.getUTCFullYear(),\n    now.getUTCMonth(),\n    now.getUTCDate(),\n    now.getUTCHours(),\n    now.getUTCMinutes(),\n    now.getUTCSeconds()\n  );\n\n  return utcTime.getTime();\n};\n\nconst accountsKeplr = (accounts: Key): AccountValue => {\n  const { pubKey, bech32Address, name } = accounts;\n  const pk = Buffer.from(pubKey).toString('hex');\n\n  return {\n    bech32: bech32Address,\n    keys: 'keplr',\n    pk,\n    path: LEDGER.HDPATH,\n    name,\n  };\n};\n\nexport {\n  formatNumber,\n  asyncForEach,\n  getDecimal,\n  fromBech32,\n  trimString,\n  exponentialToDecimal,\n  dhm,\n  downloadObjectAsJson,\n  isMobileTablet,\n  coinDecimals,\n  convertResources,\n  timeSince,\n  reduceBalances,\n  makeTags,\n  parseMsgContract,\n  replaceSlash,\n  encodeSlash,\n  groupMsg,\n  selectNetworkImg,\n  getDenomHash,\n  getDisplayAmount,\n  getDisplayAmountReverce,\n  convertAmount,\n  convertAmountReverce,\n  isNative,\n  findPoolDenomInArr,\n  getNowUtcTime,\n  accountsKeplr,\n};\n","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\tid: moduleId,\n\t\tloaded: false,\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n\t// Flag the module as loaded\n\tmodule.loaded = true;\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n// expose the modules object (__webpack_modules__)\n__webpack_require__.m = __webpack_modules__;\n\n// the startup function\n__webpack_require__.x = function() {\n\t// Load entry module and return exports\n\t// This entry module depends on other loaded chunks and execution need to be delayed\n\tvar __webpack_exports__ = __webpack_require__.O(undefined, [1,373,60,48,112,411,565], function() { return __webpack_require__(55723); })\n\t__webpack_exports__ = __webpack_require__.O(__webpack_exports__);\n\treturn __webpack_exports__;\n};\n\n","__webpack_require__.amdO = {};","// getDefaultExport function for compatibility with non-harmony modules\n__webpack_require__.n = function(module) {\n\tvar getter = module && module.__esModule ?\n\t\tfunction() { return module['default']; } :\n\t\tfunction() { return module; };\n\t__webpack_require__.d(getter, { a: getter });\n\treturn getter;\n};","// define getter functions for harmony exports\n__webpack_require__.d = function(exports, definition) {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.f = {};\n// This file contains only the entry chunk.\n// The chunk loading function for additional chunks\n__webpack_require__.e = function(chunkId) {\n\treturn Promise.all(Object.keys(__webpack_require__.f).reduce(function(promises, key) {\n\t\t__webpack_require__.f[key](chunkId, promises);\n\t\treturn promises;\n\t}, []));\n};","// This function allow to reference async chunks and sibling chunks for the entrypoint\n__webpack_require__.u = function(chunkId) {\n\t// return url for filenames not based on template\n\tif (chunkId === 1) return \"1.fe5a6992.js\";\n\tif (chunkId === 373) return \"373.ba079c84.js\";\n\tif (chunkId === 60) return \"60.9116b30d.js\";\n\t// return url for filenames based on template\n\treturn \"\" + chunkId + \".\" + {\"48\":\"10a10ae3\",\"112\":\"e74bb3b2\",\"198\":\"44cc2170\",\"411\":\"42674ed3\",\"565\":\"3e8b9a62\"}[chunkId] + \".chunk.js\";\n};","// This function allow to reference async chunks and sibling chunks for the entrypoint\n__webpack_require__.miniCssF = function(chunkId) {\n\t// return url for filenames based on template\n\treturn undefined;\n};","__webpack_require__.g = (function() {\n\tif (typeof globalThis === 'object') return globalThis;\n\ttry {\n\t\treturn this || new Function('return this')();\n\t} catch (e) {\n\t\tif (typeof window === 'object') return window;\n\t}\n})();","__webpack_require__.hmd = function(module) {\n\tmodule = Object.create(module);\n\tif (!module.children) module.children = [];\n\tObject.defineProperty(module, 'exports', {\n\t\tenumerable: true,\n\t\tset: function() {\n\t\t\tthrow new Error('ES Modules may not assign module.exports or exports.*, Use ESM export syntax, instead: ' + module.id);\n\t\t}\n\t});\n\treturn module;\n};","__webpack_require__.o = function(obj, prop) { return Object.prototype.hasOwnProperty.call(obj, prop); }","// define __esModule on exports\n__webpack_require__.r = function(exports) {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","__webpack_require__.nmd = function(module) {\n\tmodule.paths = [];\n\tif (!module.children) module.children = [];\n\treturn module;\n};","__webpack_require__.p = \"/\";","__webpack_require__.b = self.location + \"\";\n\n// object to store loaded chunks\n// \"1\" means \"already loaded\"\nvar installedChunks = {\n\t719: 1\n};\n\n// importScripts chunk loading\nvar installChunk = function(data) {\n\tvar chunkIds = data[0];\n\tvar moreModules = data[1];\n\tvar runtime = data[2];\n\tfor(var moduleId in moreModules) {\n\t\tif(__webpack_require__.o(moreModules, moduleId)) {\n\t\t\t__webpack_require__.m[moduleId] = moreModules[moduleId];\n\t\t}\n\t}\n\tif(runtime) runtime(__webpack_require__);\n\twhile(chunkIds.length)\n\t\tinstalledChunks[chunkIds.pop()] = 1;\n\tparentChunkLoadingFunction(data);\n};\n__webpack_require__.f.i = function(chunkId, promises) {\n\t// \"1\" is the signal for \"already loaded\"\n\tif(!installedChunks[chunkId]) {\n\t\tif(true) { // all chunks have JS\n\t\t\timportScripts(__webpack_require__.p + __webpack_require__.u(chunkId));\n\t\t}\n\t}\n};\n\nvar chunkLoadingGlobal = self[\"webpackChunkcyb\"] = self[\"webpackChunkcyb\"] || [];\nvar parentChunkLoadingFunction = chunkLoadingGlobal.push.bind(chunkLoadingGlobal);\nchunkLoadingGlobal.push = installChunk;\n\n// no HMR\n\n// no HMR manifest","// run startup\nvar __webpack_exports__ = __webpack_require__.x();\n"],"names":["deferred","leafPrototypes","getProto","next","DEFAULT_CHAIN_ID","WorkerGlobalScope","self","localStorage","getItem","LCD_URL","RPC_URL","WEBSOCKET_URL","INDEX_HTTPS","INDEX_WEBSOCKET","BECH32_PREFIX","BECH32_PREFIX_VAL","BECH32_PREFIX_VALOPER","DEFAULT_GAS_LIMITS","BASE_DENOM","DENOM_LIQUID","MEMO_KEPLR","defaultNetworks","bostrom","CHAIN_ID","BOSTROM","process","env","IS_DEV","SPACE_PUSSY","PATTERN_CYBER","RegExp","PATTERN_IPFS_HASH","PATTERN_COSMOS","PATTERN_HTTP","LinksTypeFilter","QueuePriority","createAsyncIterable","port","async","Symbol","asyncIterator","done","promise","Promise","resolve","onmessage","event","data","value","IPFSContentTransferHandler","canHandle","obj","result","serialize","rest","port1","port2","MessageChannel","postMessage","close","deserialize","serializedObj","SharedWorker","installTransferHandlers","set","Observable","observer","remote","get","subscribe","error","complete","then","subscription","add","unsubscribe","Subscription","overrideLogging","worker","consoleLogMap","log","original","console","warn","replaceConsoleLog","method","args","apply","serializableArgs","map","arg","JSON","stringify","String","safeStringify","type","Object","keys","forEach","isParticle","Boolean","match","EntryType","SyncQueueStatus","SyncQueueJobType","initialState","list","isLoading","chats","summary","unreadCount","total","particles","neurons","formatApiData","item","entryType","chat","meta","to","particle","formatted","timestamp","Date","toISOString","transactionHash","hash","transaction_hash","memo","senseChatId","id","transactions","from","ownerId","fromAddress","inputs","address","assign","neuron","fromLog","getSenseList","senseApi","getList","getSenseChat","getLinks","filter","getFriendItems","markAsRead","newChatStructure","checkIfMessageExists","newMessage","slice","some","msg","name","reducers","updateSenseList","reducer","state","action","payload","message","concat","caseReducers","orderSenseList","prepare","addSenseItem","push","status","newList","unshift","updateSenseItem","chatId","txHash","isSuccess","find","sorted","reduce","acc","length","lastMsg","sort","a","b","parse","i","reset","extraReducers","builder","addCase","pending","fulfilled","rejected","sense","unreadCountParticle","unreadCountNeuron","values","actions","POCKET","POCKET_ACCOUNT","actionBar","tweet","STAGE_TWEET_ACTION_BAR","TWEET","defaultAccount","account","accounts","saveToLocalStorage","setItem","setDefaultAccount","setAccounts","setStageTweetActionBar","deleteAddress","accountKey","networkKey","bech32","cyber","entryCyber","entries","CYB_QUEUE_CHANNEL","constructor","this","channel","BroadcastChannel","postServiceStatus","postSyncEntryProgress","entry","postMlSyncEntryProgress","postSenseUpdate","senseList","postSetDefaultAccount","post","broadcastStatus","channelApi","sendStatus","progress","s","asyncIterableBatchProcessor","items","batchProcess","batchSize","batch","fetchIterableByOffset","fetchFunction","params","offset","busSender","enqueue","createBackendQueueSender","enqueueParticleEmbeddingMaybe","content","contentToEmbed","getTextContentIfShouldEmbed","cid","jobType","embedding","priority","MEDIUM","CID_TWEET","CID_FOLLOW","SENSE_FRIEND_PARTICLES","contentType","textPreview","getContentToEmbed","shouldEmbed","deps","statusApi","_syncQueue$","BehaviorSubject","Map","waitForParticleResolve","Error","embeddingApi$","embeddingApi","queue","size","dbInstance$","pipe","first","db","loadSyncQueue","isInitialized$","combineLatest","ipfsInstance$","dbInstance","ipfsInstance","canEmbed","getValue","loop$","_loop$","catch","text","existEmbedding","vec","createEmbedding","putEmbedding","err","toString","pendingItems","all","jobPromise","saveEmbedding","resolveIpfsParticle","removeSyncQueue","updateSyncQueue","delete","start","source$","tap","q","isInitialized","mergeMap","executing","jobTypeFilter","processSyncQueue","share","cids","putSyncQueue","getSyncQueue","statuses","shortenString","string","specialCharsRegexe","mapIndexerTransactionToEntity","tx","index","transaction","block","height","success","date","blockHeight","mapLinkFromIndexerToDto","throwIfAborted","func","signal","aborted","DOMException","Order_By","CyberlinksByParticleDocument","CyberlinksCountByNeuronDocument","MessagesByAddressCountDocument","MessagesByAddressSenseDocument","MessagesByAddressSenseWsDocument","MSG_SEND_TRANSACTION_TYPE","MSG_MULTI_SEND_TRANSACTION_TYPE","mapWebsocketTxToTransactions","events","transactionType","messages","Tx","decode","body","msgType","typeUrl","MsgSend","MsgMultiSend","extractTxData","TxResult","cyberGraphQLWsLink","url","shouldRetry","errOrCloseEvent","retryAttempts","retryWait","retries","setTimeout","Math","min","createIndexerClient","abortSignal","fetchCyberlinks","particleCid","timestampFrom","request","limit","orderBy","Asc","where","_or","particle_to","_eq","particle_from","_gt","cyberlinks","fetchCyberlinksByNeroun","particlesFrom","_and","_in","fetchCyberlinksByNerounIterable","getUniqueParticlesFromLinks","links","Set","link","fetchCyberlinksAndResolveParticles","timestampUpdate","particlesResolver","queuePriority","cyberlinksIterable","fetchCyberlinksIterable","enqueueBatch","mapMessagesByAddressVariables","types","orderDirection","timestamp_from","t","join","order_direction","fetchTransactions","res","messages_by_address","updateSenseChat","addr","amount","isSender","userAddress","lastSendTimestamp","last","direction","syncMyChats","myAddress","shouldUpdateTimestamp","syncItems","findSyncStatus","syncItemsMap","myChats","outputs","coins","toAddress","extractSenseChats","getTransactions","order","results","syncItem","lastTransaction","at","transactionTimestamp","syncItemHeader","timestampRead","prevUnreadCount","lastTimestampRead","max","timestampUpdateContent","timestampUpdateChat","timestampUnreadFrom","newTimestampUpdateChat","syncStatusChanges","updateSyncStatus","bind","newItem","disabled","putSyncStatus","ProgressTracker","onProgressUpdate","requestRecords","totalRequests","completedRequests","estimatedTime","totalCount","completeCount","extraRequests","trackProgress","processedCount","addRequestRecord","shift","estimatedRemainingTime","calculateAverageTimePerItem","round","itemCount","now","totalDiff","totalItems","timeDiff","progressTracker","abortController","AbortController","cyblogCh","thread","module","params$","createIsInitializedObserver","info","switchMap","createRestartObserver","restart","initAbortController","distinctUntilChanged","addrBefore","addrAfter","v","switchWhenInitialized","actionObservable$","onChange","initialized","super","reloadTrigger$","Subject","startWith","createInitObservable","createClientObservable","onUpdate","abort","syncQueueInitialized","variables","indexerObservable$","query","apolloObservable","ApolloClient","cache","subscriber","createIndexerWebsocket","response","source","nodeObservample$","ws","WebSocket","onopen","send","jsonrpc","onerror","onclose","createNodeWebsocketObservable","ctx","unit","isEmpty","merge","defer","initSync","getSyncStatus","lastTransactionTimestamp","syncTransactions","syncStatusItems","processBatchTransactions","putTransactions","syncLinks","lastTimestampFrom","newSyncItem","totalMessageCount","messages_by_address_aggregate","aggregate","count","fetchTransactionMessagesCount","ceil","transactionsAsyncIterable","fetchTransactionsIterable","transactionCount","tweets","particlesFound","l","txLink","extractCybelinksFromTransaction","putCyberlinks","tweetParticles","nonTweetParticles","includes","HIGH","LOW","snakeToCamel","str","replace","group","toUpperCase","entityToDto","dbEntity","dto","key","prototype","hasOwnProperty","call","camelCaseKey","Array","isArray","getLastReadInfo","prevTimestampRead","lastUnreadLinks","lastMyLinkIndex","findLastIndex","changeParticleSyncStatus","syncStatus","lastLink","isAbortException","e","intervalMs","warmupMs","restartLoop","options","onStartInterval","onError","retryDelayMs","restartTrigger$","intervalOrRestart$","interval","delay","exhaustMap","retry","createLoopObservable","doSync","sync","isAborted","particleResolverInitialized","syncItemParticles","newLinkCount","particles_from","cyberlinks_aggregate","fetchCyberlinksCount","newSyncItemParticles","fetchNewTweets","syncParticles","tweetsAsyncIterable","newTweets","existingParticles","existingParticlesMap","tweetsBatch","syncStatusEntities","timestampSyncFrom","updatedSyncItems","linksIndexer","followings$","followings","followingsInitialized$","followingsInitialized","syncUpdates","linksAsyncIterable","linksBatch","newTimestampRead","newUnreadCount","newTimestampUpdateContent","fetchStoredSyncCommunity$","dbApi","fetchParticleAsync","storedCommunity","getCommunity","communityUpdatesMap","c","getExistingOrDefault","following","follower","followsCids","axios","txs","followers","addressHash","newFollowerCids","newFollowingNeurons","followersCommunity","communityItem","putCommunity","URGENT","onMessage","onmessageerror","getDeffredDbApi","entity","mime","blocks","sizeLocal","markdown","removeMarkdownFormatting","size_local","mapParticleToEntity","putParticles","ok","saveLinks","saveParticles","enquueSync","SyncService","loops","BackendQueueChannel","communitySync$","community","createCommunitySync$","getMimeFromUint8Array","raw","fileType","version","stores","table","ipfsContentAddtToInddexdDB","dbValue","CYBER_NODE_SWARM_PEER_ID","CYBERNODE_SWARM_ADDR_WSS","CYBERNODE_SWARM_ADDR_TCP","CYBER_GATEWAY_URL","cluster","file","dataFile","File","cidVersion","rawLeaves","createObjectURL","rawData","blob","Blob","URL","createImgData","detectGatewayContentType","basic","mimeToBaseContentType","initialType","indexOf","parseArrayLikeToDetails","onProgress","gateway","bytesDownloaded","Uint8Array","byteLength","chunks","ReadableStream","reader","getReader","readStream","read","chunk","getResponseResult","isStringData","Buffer","newString","trim","test","isHtml","createTextPreview","array","previewLength","loadIPFSContentFromDb","emptyStats","fetchIPFSContentStat","node","stat","fetchIPFSContentFromNode","controller","controllerLegacy","timer","startTime","stats","statsDoneTime","statsTime","allowedSize","clearTimeout","availableDownload","firstChunk","cat","fullyDownloaded","stream","catTime","local","pin","pinTime","debug","fetchIPFSContentFromGateway","headers","isExternalNode","nodeType","contentUrl","fetch","flushResults","flush","firstChunkStream","fullStream","tee","firstReader","restReader","asyncIterable","toAsyncIterableWithMime","getIPFSContent","callBackFuncStatus","dataRsponseDb","addContenToIpfs","arrayBuffer","contentToUint8Array","QueueStrategy","settings","getNextSource","QueueItemTimeoutError","timeoutMs","setPrototypeOf","CustomHeaders","XCybSourceValues","getQueueItemTotalPriority","viewPortPriority","strategies","external","timeout","maxConcurrentExecutions","embedded","helia","strategy","queueDebounceMs","queue$","lastNodeCallTime","setNode","reconnectToSwarm","isStarted","withLatestFrom","debounceTime","cancelDeprioritizedItems","workItems","getItemBySourceAndPriority","fetchData$","callbacks","callback","removeAndNext","nextSource","switchSourceAndNext","postSummary","switchStrategy","customStrategy","pendingBySource","itemsToExecute","queueSource","executeCount","itemsByPriority","queueItem","executionTime","promiseFactory","fetchIpfsContent","sharedWorker","enqueueParticleSave","each","with","throwError","catchError","of","mutateQueueItem","changes","releaseExecution","existingItem","initialSource","postProcessing","enqueueAndWait","updateViewPortPriority","cancel","cancelByParent","parent","clear","getQueueMap","getQueueList","getStats","fn","stringToCid","stringToIpfsPath","_config","_isStarted","config","gatewayUrl","nodeAddress","initConfig","window","toCid","swarm","localAddrs","files","withLocal","peers","peer","bootstrap","connect","ls","repoSize","repo","responseId","agentVersion","addOptionsV0","blockstore","open","datastore","libp2p","bootstrapList","transports","rtcConfiguration","iceServers","urls","credential","username","discoverRelays","connectionEncryption","streamMuxers","connectionGater","denyDialMultiaddr","peerDiscovery","services","identify","libp2pFactory","fs","addEventListener","evt","peerId","detail","conn","getConnections","transportsByAddr","fromEntries","remoteAddr","protoCodes","getMultiaddrs","fileSize","localFileSize","dagSize","mtime","optionsV0","fileName","addFile","path","TextEncoder","encode","addBytes","cid_","pins","isPinned","remotePeer","stop","dial","iterable","metadata","toV0","mapToLsResult","host","relay","enabled","hop","preload","API","HTTPHeaders","Addresses","Gateway","Swarm","Delegates","Discovery","MDNS","Enabled","Interval","webRTCStar","Bootstrap","Pubsub","ConnMgr","HighWater","LowWater","DisableNatPortMap","Routing","Type","filters","nat","EXPERIMENTAL","ipnsPubsub","Number","nodeClassMap","initIpfsNode","ipfsNodeType","restOptions","EnhancedClass","Base","parseAs","details","getPeers","swarmPeerId","forced","isConnectedToSwarm","connectPeer","swarmPeerAddress","withCybFeatures","instance","init","urlOpts","allowLocalModels","mlModelMap","featureExtractor","model","createMlApi","broadcastApi","featureExtractor$","replaySubject","ReplaySubject","pooling","normalize","searchByEmbedding","api","createEmbeddingApi$","initPipelineInstance","alias","pipeline","progress_callback","progressData","loaded","progrssStateItem","loadPipeline","time","timeEnd","compileConfig","budget","experimental","instructions","defaultRuneEntrypoint","readOnly","execute","funcName","funcParams","input","script","scriptEngine","entrypoints","context","user","secrets","entrypoints$","scriptCallbacks","isSoulInitialized$","run","compileParams","scripts","refId","compilerParams","runtime","app","outputData","diagnosticsOutput","getParticleScriptOrAction","askCompanion","resultType","metaItems","output","personalProcessor","outputContent","setEntrypoints","scriptEntrypoints","pushContext","popContext","names","newContext","executeFunction","executeCallback","getDebug","enigine","backgroundWorker","setInnerDeps","rune","runeDeps","createRuneApi","ipfsQueue","ipfsApi","ipfsOpts","newIpfsNode","ipfsNode","fetchWithDetails","dequeue","dequeueByParent","clearQueue","addContent","createIpfsApi","serviceDeps","injectDb","isIpfsInitialized","setRuneDeps","setParams","createBackgroundWorkerApi","onconnect","ports","extractRuneScript","md","hasRune","runeRegex","runeScript","modifiedMarkdown","exec","extractRuneContent","queryContractSmartPassport","client","getPassport","SigningCyberClientError","code","rawLog","cyblog","defaultFee","gas","sendCyberlink","signingClient","fee","cyberlink","putCyberlink","addCyberlinkLocal","subjectDeps","queryClient","defferedDependency","item$","CyberClient","getIpfsTextConent","cybApi","createAbortController","graphSearch","page","keywordHash","getPassportByNickname","nickname","passport","passport_by_nickname","searcByEmbedding","evalScriptFromIpfs","pureScript","executeScriptCallback","externalDeps","createRuneDeps","defaultOpenAIParams","openAICompletion","apiKey","cb","Authorization","json","choices","decoder","TextDecoder","buffer","lines","split","pop","line","parsed","delta","jsCyberSearch","jsCyberLink","fromCid","jsGetPassportByNickname","jsEvalScriptFromIpfs","jsGetIpfsTextContent","jsAddContenToIpfs","jsExecuteScriptCallback","jsOpenAICompletions","jsSearchByEmbedding","jsCyberLinksFrom","jsCyberLinksTo","Networks","ADD_AVATAR","FOLLOW","numberToUtcDate","dateToUtcNumber","isoString","endsWith","getNowUtcNumber","getIpfsHash","reject","marshal","DAGNode","dagNode","toBaseEncodedString","CYBLOG_BROADCAST_CHANNEL_NAME","logList","createCyblogChannel","defaultContext","postLogToChannel","level","trace","consoleLogParams","formattedMessage","formatter","logItem","truncate","appendLog","stacktrace","contextItem","p","ctxItem","consoleLog","getLogs","splice","getConsoleLogParams","createCybLog","getLink","getFromLink","getToLink","getSearchQuery","searchByHash","search","encodeSlash","__webpack_module_cache__","__webpack_require__","moduleId","cachedModule","undefined","exports","__webpack_modules__","m","x","__webpack_exports__","O","amdO","chunkIds","notFulfilled","Infinity","j","every","r","n","getter","__esModule","d","getPrototypeOf","__proto__","mode","ns","create","def","current","getOwnPropertyNames","definition","o","defineProperty","enumerable","f","chunkId","promises","u","miniCssF","g","globalThis","Function","hmd","children","prop","toStringTag","nmd","paths","location","installedChunks","importScripts","chunkLoadingGlobal","parentChunkLoadingFunction","moreModules"],"sourceRoot":""}