{"version":3,"file":"621.d193b556.chunk.js","mappings":"gBAAIA,ECCAC,EADAC,ECAAC,E,uQCWJ,MAEMC,IAP2B,oBAAtBC,mBACPC,gBAAgBD,oBAIyBE,aAAaC,QAAQ,YAGvC,UAIdC,EACY,IAAgBL,GAAkBK,QAE9CC,EACY,IAAgBN,GAAkBM,QAE9CC,EACkB,IAAgBP,GAAkBO,cAEpDC,EACgB,IAAgBR,GAAkBQ,YAElDC,EAEX,IAAgBT,GAAkBS,gBAEvBC,EACkB,IAAgBV,GAAkBU,cAE3DC,EAAoB,GAAGD,OAEhBE,EAAwB,GAAGD,QAe3BE,GAVe,IAAgBb,GAAkBc,WAGhC,IAAgBd,GAAkBe,aAO9B,MAIrB,WAAEC,GAAe,IAAgBhB,E,oDCpD9C,MAAMiB,EAAgC,CACpCC,QAAS,CACPC,SAAU,IAASC,QACnBN,WAAY,OACZC,aAAc,WACdT,QAAS,mCACTD,QAAS,mCACTE,cAAe,2CACfC,YAAa,gDACbC,gBAAiB,8CACjBC,cAAe,UACfM,WAAY,iCAqBd,cAAe,CACbG,SAAU,IAASE,YACnBP,WAAY,QACZC,aAAc,cACdT,QAAS,wCACTD,QAAS,uCACTE,cAAe,+CACfC,YAAa,oDACbC,gBAAiB,kDACjBC,cAAe,QACfM,WAAY,sCAIhB,K,6JClDO,MAAMM,EAAgB,IAAIC,OAC/B,IAAI,uBACJ,KAKWC,EAAoB,uBAYpBC,GAVyB,IAAIF,OACxC,IAAI,uBACJ,KAGmC,IAAIA,OACvC,IAAI,8BACJ,KAG4B,4BAYjBG,EAAe,yB,qECjCrB,IAAKC,EAAL,CAAKA,IACVA,EAAA,GAAK,KACLA,EAAA,KAAO,OACPA,EAAA,IAAM,MAHIA,GAAL,CAAKA,GAAA,G,oDCiCAC,EAAL,CAAKA,IACVA,EAAAA,EAAA,KAAO,GAAP,OACAA,EAAAA,EAAA,IAAM,IAAN,MACAA,EAAAA,EAAA,OAAS,IAAT,SACAA,EAAAA,EAAA,KAAO,IAAP,OACAA,EAAAA,EAAA,OAAS,GAAT,SALUA,GAAL,CAAKA,GAAA,I,sBC1BZ,SAASC,EAAoBC,GAC3B,MAAO,CACLC,OAAQC,OAAOC,iBAKb,IAAIC,GAAO,EACX,MAAQA,GAAM,CAEZ,MAAMC,EAAU,IAAIC,SAA4BC,IAE9CP,EAAKQ,UAAaC,IACG,OAAfA,EAAMC,MACRN,GAAO,EACPG,EAAQ,OAERA,EAAQE,EAAMC,KAChB,CACD,IAGGC,QAAcN,EAEN,OAAVM,UACIA,EAEV,CACF,EAEJ,CAEA,MAAMC,EAGF,CACFC,UAAYC,GACVA,GAAOA,EAAIC,QAAsD,mBAArCD,EAAIC,OAAOb,OAAOC,eAChDa,UAAUF,GACR,QAAY,IAARA,EACF,MAAO,CAAC,KAAM,IAEhB,MAAM,OAAEC,KAAWE,GAASH,GACtB,MAAEI,EAAK,MAAEC,GAAU,IAAIC,eAY7B,OAXIL,GACF,WAEE,gBAAiBJ,KAASI,EACxBG,EAAMG,YAAYV,GAEpBO,EAAMG,YAAY,MAElBH,EAAMI,OACP,EARD,GAUK,CAAC,IAAKL,EAAMjB,KAAMmB,GAAS,CAACA,GACrC,EACAI,YAAYC,GACV,IAAKA,EACH,OAEF,MAAM,KAAExB,KAASiB,GAASO,EAE1B,MAAO,IACFP,EACHF,OAAQhB,EAAoBC,GAEhC,G,gDC9DuD,oBAAjByB,cAEgBC,EAAQC,IAAIC,OAGpE,SAASC,IACP,KAAiBC,IAAI,cAAelB,GACpC,KAAiBkB,IAAI,aAAc,CACjCjB,UAAYF,GACHA,aAAiBoB,EAAA,EAE1BR,YAAcZ,GACL,IAAIoB,EAAA,GAAqBC,IAC9B,MAAMC,EAAS,KACZC,IAAI,SACJX,YAAYZ,GAEfsB,EACGE,WACC,QAAM,CACJlE,KAAOA,GAAkB+D,EAAS/D,KAAKA,GACvCmE,MAAQA,GAAmBJ,EAASI,MAAMA,GAC1CC,SAAU,IAAML,EAASK,cAG5BC,MAAMC,GACLP,EAASQ,KAAI,KACXD,EAAaE,cACbR,EAAO,OAAe,KAEzB,IAGPjB,UAAYL,GACH,KAAiBuB,IAAI,SAAUlB,UAAU,CAC9CmB,UAAYH,GACVrB,EAAMwB,UAAU,CACdlE,KAAOA,GAAkB+D,EAAS/D,KAAKA,GAAMqE,OAC7CF,MAAQA,GAAmBJ,EAASI,MAAMA,GAAOE,OACjDD,SAAU,IAAML,EAASK,WAAWC,aAM9C,KAAiBR,IAAI,eAAgB,CACnCjB,UAAYF,GACHA,aAAiB+B,EAAA,GAE1BnB,YAAcZ,GACL,IAAI+B,EAAA,IAAa,KACtB,MAAMT,EAAS,KACZC,IAAI,SACJX,YAAYZ,GAEfsB,EAAOQ,cAAcH,MAAK,KACxBL,EAAO,OAAe,GACtB,IAGNjB,UAAYL,GACH,KAAiBuB,IAAI,SAAUlB,UAAU,CAC9CyB,YAAa,IAAM9B,EAAM8B,iBAIjC,CAWA,SAASE,EAAgBC,GACvB,MAAMC,EAAgB,CACpBC,IAAK,CAAEC,SAAUC,EAAQF,KACzBV,MAAO,CAAEW,SAAUC,EAAQZ,OAC3Ba,KAAM,CAAEF,SAAUC,EAAQC,OAEtBC,EAAqBC,IACzB,MAAM,SAAEJ,GAAaF,EAAcM,GAEnCN,EAAcM,GAAQJ,SAAWC,EAAQG,GAEzCH,EAAQG,GAAU,IAAIC,KACpBL,EAASM,MAAML,EAASI,GACxB,MAAME,EAAmBF,EAAKG,KAAKC,GAtBzC,SAAuB1C,GACrB,IACE,OAAO2C,KAAKC,UAAU5C,EACxB,CAAE,MAAOsB,GACP,OAAOuB,OAAO7C,EAChB,CACF,CAgBiD8C,CAAcJ,KAEzDZ,EAAOvB,YAAY,CAAEwC,KAAM,UAAWV,SAAQC,KAAME,GAAmB,CACxE,EAGHQ,OAAOC,KAAKlB,GAAemB,SAASb,GAClCD,EAAkBC,IAEtB,C,gDC/GO,SAASc,EAAWtD,GAGzB,OAAOuD,QAAQvD,EAAMwD,MAAM,wBAC7B,CCkBO,IAAKC,EAAL,CAAKA,IACVA,EAAAA,EAAA,aAAe,GAAf,eACAA,EAAAA,EAAA,SAAW,GAAX,WACAA,EAAAA,EAAA,KAAO,GAAP,OAHUA,GAAL,CAAKA,GAAA,IAiEAC,EAAL,CAAKA,IACVA,EAAAA,EAAA,QAAU,GAAV,UACAA,EAAAA,EAAA,UAAY,GAAZ,YACAA,EAAAA,EAAA,KAAO,GAAP,OACAA,EAAAA,EAAA,OAAS,GAAT,QAJUA,GAAL,CAAKA,GAAA,IAOAC,EAAL,CAAKA,IACVA,EAAAA,EAAA,SAAW,GAAX,WACAA,EAAAA,EAAA,UAAY,GAAZ,YAFUA,GAAL,CAAKA,GAAA,I,WC5BZ,MAAMC,EAA2B,CAC/BC,KAAM,CACJC,WAAW,EACX/D,KAAM,GACN0B,WAAO,GAETsC,MAAO,CAAC,EACRC,QAAS,CACPC,YAAa,CACXC,MAAO,EACPC,UAAW,EACXC,QAAS,KAKf,SAASC,EAAcC,GACjBA,EAAKC,YAAcd,EAAUe,MAAQF,EAAKG,KAAKC,KACjDJ,EAAKC,UAAYd,EAAUkB,UAG7B,MAAM,KAAEF,GAASH,EAEXM,EAAuB,CAC3BC,UAAW,IAAIC,KAAKL,EAAKI,WAAWE,cAGpCC,gBACEV,EAAKU,iBACLV,EAAKW,MACLX,EAAKG,KAAKS,kBACVZ,EAAKG,KAAKQ,MACVX,EAAKG,KAAKO,gBAEZG,KAAMb,EAAKa,MAAQV,EAAKU,KAExBC,YAAad,EAAKe,GAElBpB,YAAaK,EAAKL,aAAe,GAGnC,OAAQK,EAAKC,WACX,KAAKd,EAAUe,KACf,KAAKf,EAAU6B,aAAc,CAC3B,MAAMb,EAAOH,EAAKG,MACZ,KAAEvB,GAASuB,EAEjB,IAAIc,EAAOjB,EAAKkB,QAEhB,GAAa,gCAATtC,EAAwC,CAE1CqC,EADcd,EAAKzE,MACNyF,WACf,MAAO,GAAa,qCAATvC,EAA6C,CAGtDqC,EAFcd,EAAKzE,MAEN0F,OAAO,GAAGC,OACzB,CAEAxC,OAAOyC,OAAOhB,EAAW,CACvB1B,OACAqC,OACAd,KAAMH,EAAKG,KAAKzE,QAGlB,KACF,CAEA,KAAKyD,EAAUkB,SAAU,CACvB,MAAMF,EAAOH,EAAKG,KAElBtB,OAAOyC,OAAOhB,EAAW,CACvB1B,KAAM,mCACNqC,KAAMd,EAAKoB,OACXpB,KAAMA,EACNqB,SAAS,IAGX,KACF,CAEA,QAGE,MAAO,CAAC,EAGZ,OAAOlB,CACT,CAEA,MAAMmB,GAAe,QACnB,sBACAzG,MAAO0G,UACcA,EAAUC,WACjBrD,IAAIyB,KAId6B,GAAe,QACnB,sBACA5G,OAAS+F,KAAIW,eAGX,GAFiB1C,EAAW+B,GAEd,CAeZ,aAdoBW,EAAUG,SAASd,IACVzC,KAAK0B,IAChC,GAAuB,IAAnBA,EAAKO,UAIT,OAAOR,EAAc,IAChBC,EACHe,KACAd,UAAWd,EAAUkB,SACrBF,KAAMH,GACN,IAGkB8B,OAAO7C,QAC/B,CAaA,aAXmByC,EAAUK,eAAehB,IACjBzC,KAAK0B,IAC9B,MAAMC,EAAYD,EAAKI,GAAKjB,EAAUkB,SAAWlB,EAAUe,KAC3D,OAAOH,EAAc,IAChBC,EACHC,YACAc,KACAZ,KAAMH,GACN,GAGgB,IAIlBgC,GAAa,QACjB,oBACAhH,OAAS+F,KAAIW,cACJA,EAAUM,WAAWjB,KAI1BkB,EAAyB,CAC7BlB,GAAI,GACJvB,WAAW,EACX/D,KAAM,GACN0B,WAAO,EACPwC,YAAa,GAGf,SAASuC,EAAqBhC,EAAYiC,GAOxC,OANqBjC,EAAKzE,KAAK2G,OAAO,GAEDC,MAAMC,GAClCA,EAAI5B,kBAAoByB,EAAWzB,iBAI9C,CAEA,MAAM0B,GAAQ,QAAY,CACxBG,KAAM,QACNjD,eACAkD,SAAU,CAERC,gBAAiB,CACfC,QAAS,CAACC,EAAOC,KACFA,EAAOC,QAEf9D,SAAS+D,IACZ,MAAQhC,YAAaC,GAAO+B,EAEvBH,EAAMlD,MAAMsB,KACf4B,EAAMlD,MAAMsB,GAAM,IAAKkB,IAGzB,MAAM/B,EAAOyC,EAAMlD,MAAMsB,GAEzBlC,OAAOyC,OAAOpB,EAAM,CAClBa,KAEApB,YAAamD,EAAQnD,aAAe,IAGjCuC,EAAqBhC,EAAM4C,KAC9B5C,EAAKzE,KAAOyE,EAAKzE,KAAKsH,OAAOD,GAC/B,IAGFV,EAAMY,aAAaC,eAAeN,EAAM,EAE1CO,QAAUzH,IACD,CACLoH,QAASpH,EAAK6C,IAAIyB,MAKxBoD,aACER,EACAC,GAEA,MAAM,GAAE7B,EAAE,KAAEf,GAAS4C,EAAOC,QACfF,EAAMlD,MAAMsB,GAEpBtF,KAAK2H,KAAK,IACVpD,EACHG,KAAMH,EAAKG,KACXkD,OAAQ,YAGV,MAAMC,EAAUX,EAAMpD,KAAK9D,KAAKqG,QAAQ9B,GAASA,IAASe,IAC1DuC,EAAQC,QAAQxC,GAChB4B,EAAMpD,KAAK9D,KAAO6H,CACpB,EAEAE,gBACEb,EACAC,GAMA,MAAM,OAAEa,EAAM,OAAEC,EAAM,UAAEC,GAAcf,EAAOC,QAGvC7C,EAFO2C,EAAMlD,MAAMgE,GAEPhI,KAAKmI,MAAM5D,GAASA,EAAKU,kBAAoBgD,IAE3D1D,IACE2D,SACK3D,EAAKqD,OAEZrD,EAAKqD,OAAS,QAGpB,EACAJ,eAAeN,GACb,MAmBMkB,EAnBmBhF,OAAOC,KAAK6D,EAAMlD,OAAOqE,QAKhD,CAACC,EAAKhD,KACN,MAAMb,EAAOyC,EAAMlD,MAAMsB,GAGzB,IAAKb,EAAKzE,KAAKuI,OACb,OAAOD,EAGT,MAAME,EAAU/D,EAAKzE,KAAKyE,EAAKzE,KAAKuI,OAAS,GAG7C,OAFAD,EAAIX,KAAK,CAAErC,KAAIkD,YAERF,CAAG,GACT,IAE6BG,MAAK,CAACC,EAAGC,IAErC5D,KAAK6D,MAAMD,EAAEH,QAAQ1D,WAAaC,KAAK6D,MAAMF,EAAEF,QAAQ1D,aAI3DoC,EAAMpD,KAAK9D,KAAOoI,EAAOvF,KAAKgG,GAAMA,EAAEvD,IACxC,EACAwD,MAAK,IACIjF,GAIXkF,cAAgBC,IACdA,EAAQC,QAAQjD,EAAakD,SAAUhC,IACrCA,EAAMpD,KAAKC,WAAY,CAAI,IAG7BiF,EAAQC,QAAQjD,EAAamD,WAAW,CAACjC,EAAOC,KAC9CD,EAAMpD,KAAKC,WAAY,EAEvB,MAAM8D,EAAsC,GAE5CV,EAAOC,QAAQ9D,SAAS+D,IACtB,MAAQhC,YAAaC,GAAO+B,EAEvBH,EAAMlD,MAAMsB,KACf4B,EAAMlD,MAAMsB,GAAM,IAAKkB,IAGzB,MAAM/B,EAAOyC,EAAMlD,MAAMsB,GAEzBlC,OAAOyC,OAAOpB,EAAM,CAClBa,KAEApB,YAAamD,EAAQnD,aAAe,IAGjCuC,EAAqBhC,EAAM4C,KAC9B5C,EAAKzE,KAAOyE,EAAKzE,KAAKsH,OAAOD,IAG/BQ,EAAQF,KAAKrC,EAAG,IAGlB4B,EAAMpD,KAAK9D,KAAO6H,CAAO,IAE3BmB,EAAQC,QAAQjD,EAAaoD,UAAU,CAAClC,EAAOC,KAC7C,EAAQzF,MAAMyF,GAEdD,EAAMpD,KAAKC,WAAY,EACvBmD,EAAMpD,KAAKpC,MAAQyF,EAAOzF,MAAM2F,OAAO,IAGzC2B,EAAQC,QAAQ9C,EAAa+C,SAAS,CAAChC,EAAOC,KAC5C,MAAM,GAAE7B,GAAO6B,EAAOzC,KAAK5B,IAEtBoE,EAAMlD,MAAMsB,KACf4B,EAAMlD,MAAMsB,GAAM,IAAKkB,IAIzBU,EAAMlD,MAAMsB,GAAIvB,WAAY,CAAI,IAGlCiF,EAAQC,QAAQ9C,EAAagD,WAAW,CAACjC,EAAOC,KAC9C,MAAM,GAAE7B,GAAO6B,EAAOzC,KAAK5B,IACrB2B,EAAOyC,EAAMlD,MAAMsB,GACzBb,EAAKV,WAAY,EAEjBU,EAAKa,GAAKA,EAEVb,EAAKzE,KAAOmH,EAAOC,OAAO,IAE5B4B,EAAQC,QAAQ9C,EAAaiD,UAAU,CAAClC,EAAOC,KAC7C,EAAQzF,MAAMyF,GAEd,MAAM1C,EAAOyC,EAAMlD,MAAMmD,EAAOzC,KAAK5B,IAAIwC,IACzCb,EAAKV,WAAY,EACjBU,EAAK/C,MAAQyF,EAAOzF,MAAM2F,OAAO,IAKnC2B,EAAQC,QAAQ1C,EAAW4C,WAAW,CAACjC,EAAOC,KAC5C,MAAM,GAAE7B,GAAO6B,EAAOzC,KAAK5B,IACrB2B,EAAOyC,EAAMlD,MAAMsB,GAEnBV,EAAWrB,EAAW+B,IAEtB,YAAEpB,GAAgBO,EAExByC,EAAMjD,QAAQC,YAAYC,OAASD,EAC/BU,EACFsC,EAAMjD,QAAQC,YAAYE,WAAaF,EAEvCgD,EAAMjD,QAAQC,YAAYG,SAAWH,EAGvCO,EAAKP,YAAc,CAAC,GACpB,KA8BO,aAAEwD,EAAY,gBAAEK,EAAe,gBAAEf,EAAiB8B,MAAK,KA1BzC,SACxB5B,GAAqBA,EAAMmC,MAAMrF,QACjCA,IACC,IAAIsF,EAAsB,EACtBC,EAAoB,EAExBnG,OAAOoG,OAAOxF,GAAOV,SAAQ,EAAGgC,KAAIpB,kBACjBX,EAAW+B,GAG1BgE,GAAuBpF,EAEvBqF,GAAqBrF,CACvB,IAKF,MAAO,CACLC,MAHYmF,EAAsBC,EAIlCnF,UAAWkF,EACXjF,QAASkF,EACV,IAKH5C,EAAM8C,SAOO9C,EAAa,QC7crB,MAAM,EACH,CACN+C,OAAQ,SACRC,eAAgB,iBCmBpB,MAAM,EAA2B,CAC/BC,UAAW,CACTC,M,SAAO,GAAOC,uBAAuBC,OAEvCC,eAAe,EACfC,eAAgB,CACdnD,KAAM,KACNoD,QAAS,MAEXC,SAAU,MAUZ,SAASC,EAAmBlD,GAC1B,MAAM,eAAE+C,EAAc,SAAEE,GAAajD,EAErC+C,GACEtM,aAAa0M,QACX,EAAwBX,OACxB3G,KAAKC,UAAU,CACb,CAACiH,EAAenD,MAAOmD,EAAeC,WAG5CC,GACExM,aAAa0M,QACX,EAAwBV,eACxB5G,KAAKC,UAAUmH,GAErB,CAEA,MAAM,GAAQ,QAAY,CACxBrD,KAAM,SACNjD,aAAY,EACZkD,SAAU,CACRuD,kBAAmB,CACjBpD,GAEEE,SAAWN,OAAMoD,eAGnBhD,EAAM+C,eAAiB,CACrBnD,OACAoD,QAASA,GAAWhD,EAAMiD,WAAWrD,IAAS,MAGhDsD,EAAmBlD,EAAM,EAE3BqD,YAAa,CAACrD,GAASE,cACrBF,EAAMiD,SAAW/C,EAEjBgD,EAAmBlD,EAAM,EAE3BsD,eAAiBtD,IACfA,EAAM8C,eAAgB,CAAI,EAE5BS,uBAAwB,CAACvD,GAASE,cAChCF,EAAM0C,UAAUC,MAAQzC,CAAO,EAIjCsD,cAAe,CAACxD,GAASE,cACnBF,EAAMiD,UACR/G,OAAOC,KAAK6D,EAAMiD,UAAU7G,SAASqH,IACnCvH,OAAOC,KAAK6D,EAAMiD,SAASQ,IAAarH,SAASsH,IAC/C,GAAI1D,EAAMiD,SAASQ,GAAYC,GAAYC,SAAWzD,EAAS,CAO7D,UANOF,EAAMiD,SAASQ,GAAYC,GAEqB,IAAnDxH,OAAOC,KAAK6D,EAAMiD,SAASQ,IAAapC,eACnCrB,EAAMiD,SAASQ,GAGpBzD,EAAM+C,gBAAgBC,SAASY,OAAOD,SAAWzD,EAAS,CAC5D,MAEM2D,EAFU3H,OAAO4H,QAAQ9D,EAAMiD,UAEVhC,MACzB,EAAE,CAAElI,KAAWA,EAAM6K,OAAOD,SAI5B3D,EAAM+C,eADJc,EACqB,CACrBjE,KAAMiE,EAAW,GACjBb,QAASa,EAAW,IAGC,CACrBjE,KAAM,KACNoD,QAAS,KAGf,CAEAE,EAAmBlD,EACrB,IACA,GAEN,MAQO,kBACXoD,EAAiB,YACjBC,EAAW,uBACXE,EAAsB,cACtBC,GACE,EAAMjB,QAEK,EAAa,QAGrB,MC5IMwB,EAAoB,oBCgEjC,MApDA,MAGEC,cACEC,KAAKC,QAAU,IAAIC,iBDjBc,wBCkBnC,CAEOC,kBACLxE,EACAc,EACAP,GAEA8D,KAAKC,QAAQzK,YAAY,CACvBwC,KAAM,iBACNlD,MAAO,CAAE6G,OAAMc,SAAQP,YAE3B,CAEOkE,sBAAsBC,EAAsBtE,GAEjDiE,KAAKC,QAAQzK,YAAY,CAAEwC,KAAM,aAAclD,MAAO,CAAEuL,QAAOtE,UACjE,CAEOuE,wBAAwBD,EAAetE,GAE5CiE,KAAKC,QAAQzK,YAAY,CACvBwC,KAAM,gBACNlD,MAAO,CAAEuL,QAAOtE,UAEpB,CAEOwE,gBAAgBC,GAEjBA,EAAUpD,OAAS,GACrB4C,KAAKC,QAAQzK,YAAYqG,EAAgB2E,GAE7C,CAEOC,sBAAsB9E,EAAcoD,GACzCiB,KAAKC,QAAQzK,YACX2J,EAAkB,CAChBxD,OACAoD,YAGN,CAEA2B,KAAKhF,GACHsE,KAAKC,QAAQzK,YAAYkG,EAC3B,G,iECtDK,MAAMiF,EAAkB,CAC7BhF,EACAiF,KAGO,CACLC,WAAY,CACVpE,EACAP,EACA4E,KAGAF,EAAWR,sBAAsBzE,EAAM,CACrCc,SACAP,UACA4E,WACAvM,KAAM,CAAC,SAAU,QAAS,UAAUkH,MAAMsF,GAAMA,IAAMtE,KACtD,IClBRrI,eAAe4M,EACbC,EACAC,EACAC,EAAY,IAEZ,IAAIC,EAAQ,GAEZ,gBAAiBhI,KAAQ6H,EACvBG,EAAM5E,KAAKpD,GACPgI,EAAMhE,SAAW+D,UACbD,EAAaE,GACnBA,EAAQ,IAIRA,EAAMhE,OAAS,SACX8D,EAAaE,EAEvB,CA4BOhN,eAAgBiN,GACrBC,EACAC,GAEA,IAAIC,EAAS,EACb,OAAa,CAEX,MAAMP,QAAcK,EAAc,IAAKC,EAAQC,WAE/C,GAAqB,IAAjBP,EAAM7D,OACR,YAGI6D,EAENO,GAAUP,EAAM7D,MAClB,CACF,CC7DO,MAUDqE,GAVkC,MACtC,MAAMxB,EAAU,IAAIC,iBAAiBJ,GAErC,MAAO,CACL4B,QAAUhG,IACRuE,EAAQzK,YAAYkG,EAAI,EAE3B,EAGeiG,GAELC,GAAgCxN,MAAOyN,IAClD,MAAMC,QAAuBC,GAA4BF,GAczD,OAZIC,GACFL,GAAUC,QAAQ,CAChB1J,KAAM,OACNnD,KAAM,CACJsF,GAAI0H,EAAQG,IACZnN,KAAMiN,EACNG,QAASxJ,EAAiByJ,UAC1BC,SAAUlO,EAAcmO,YAKrBN,CAAc,E,gBCnClB,MAAMO,GAAY,iDAEZC,GAAa,iDCUbC,GAAyB,CAACF,GAAWC,I,gBCwBlD,MAaaP,GAA8B3N,MAAOyN,IAChD,MAAOW,EAAa3N,QAZIT,OAAOyN,IAC/B,MAAMW,EAAcX,GAAStI,MAAMiJ,aAAe,GAGlD,MAAoB,SAAhBA,EACK,CAACA,EAAaX,EAAQY,aAGxB,CAACD,OAAa,EAAU,EAIGE,CAAkBb,GAEpD,IAAIc,EAA8B,SAAhBH,KAA4B3N,EAM9C,OAJA8N,EACEA,KACE9N,EAAMyD,MAAM,SAAoBzD,EAAMyD,MAAM,QAEzCqK,EAAc9N,OAAO,GA0P9B,OAvPA,MA6BEkL,YAAY6C,GACV,GAjBF,KAAQC,UAAYlC,EAAgB,WAAY,IAAI,GAEpD,KAAQmC,YAAc,IAAIC,EAAA,EACxB,IAAIC,MAcCJ,EAAKK,uBACR,MAAM,IAAIC,MAAM,yCAGlBlD,KAAKiD,uBAAyBL,EAAKK,uBAEnCL,EAAKO,cAAc7M,WAAW8M,IAC5BpD,KAAKoD,aAAeA,EAEhBpD,KAAKqD,MAAMC,KAAO,GACpBtD,KAAK8C,YAAY1Q,KAAK4N,KAAKqD,MAC7B,IAGFT,EAAKW,YACFC,MACC,EAAAC,EAAA,IAAO3O,QAAoB,IAAVA,KAElBwB,WAAUlC,MAAOsP,IAChB1D,KAAK0D,GAAKA,QACJ1D,KAAK2D,eAAe,IAG9B3D,KAAK4D,gBAAiB,EAAAC,EAAA,GAAc,CAClCjB,EAAKW,YACLX,EAAKkB,gBACJN,MACD,EAAA9L,EAAA,IAAI,EAAEqM,EAAYC,OAAoBA,KAAkBD,IAE5D,CApDYE,eACV,QAASjE,KAAKoD,YAChB,CAUWC,YACT,OAAOrD,KAAK8C,YAAYoB,UAC1B,CAIWC,YACT,OAAOnE,KAAKoE,MACd,CAkCA,0BAAkCjK,EAAiBgI,GACjD,OAAOnC,KAAKiD,uBAAuB9I,EAAIgI,GACpC1L,MAAKrC,OAASqI,SAAQvH,cACS,cAAXuH,IACCvH,WAId0M,GAA8B1M,IAC7B,KAERmP,OAAM,KAAM,GACjB,CAEA,oBAA4BrC,EAAkBsC,GAC5C,IAGE,UAFsBtE,KAAK0D,GAAIa,eAAevC,GAEhC,CACZ,MAAMwC,QAAYxE,KAAKoD,aAAcqB,gBAAgBH,SAEhCtE,KAAK0D,GAAIgB,aAAa1C,EAAKwC,EAClD,CAEA,OAAO,CACT,CAAE,MAAOG,GAEP,OADA,GAAQpO,MAAM,wBAAwByL,OAASsC,KAASK,EAAIC,aACrD,CACT,CACF,CAEA,uBAA+BC,GAG7B,MAAM1D,EAAY0D,EAAazH,OAE/B4C,KAAK6C,UAAUhC,WACb,cACA,oBAAoBM,KAAaA,YAAoBnB,KAAKqD,MAAMC,mBAGlE,IAAI5F,EAAIyD,QACF1M,QAAQqQ,IACZD,EAAanN,KAAItD,MAAOgF,IACtB,MAAM,GAAEe,EAAE,QAAE8H,EAAO,KAAEpN,GAASuE,EAE9B,IAAI2L,EAAatQ,QAAQC,SAAQ,GASjC,OAPIuN,IAAYxJ,EAAiByJ,WAAarN,EAC5CkQ,EAAa/E,KAAKgF,cAAc7K,EAAItF,GAC3BoN,IAAYxJ,EAAiBgB,WACtCsL,EAAa/E,KAAKiF,oBAAoB9K,EAAIlG,EAAcmO,SAInD2C,EAAWtO,MAAKrC,MAAOc,IACxBA,QACI8K,KAAK0D,GAAIwB,gBAAgB,CAAE/K,KAAI8H,kBAE/BjC,KAAK0D,GAAIyB,gBAAgB,CAC7BhL,KACA8H,UACAxF,OAAQjE,EAAgBjC,QAI5B,MAAM8M,EAAQrD,KAAK8C,YAAYhO,MAC/BuO,EAAM+B,OAAOjL,GACbuD,IACAsC,KAAK8C,YAAY1Q,KAAKiR,GAEtBrD,KAAK6C,UAAUhC,WACb,cACA,oBAAoBM,EAAYzD,KAAKyD,YACnCnB,KAAKqD,MAAMC,kBAEd,GACD,IAGR,CAEA+B,QACE,MAAMC,EAAUtF,KAAK4D,eAAeJ,MAClC,EAAA+B,EAAA,IAAKC,GAAM,GAAQvO,IAAI,8BAA8BuO,QACrD,EAAAtK,EAAA,IAAQ2D,IAAoC,IAAlBA,KAC1B,EAAA4G,EAAA,IAAS,IAAMzF,KAAK8C,eAEpB,EAAA5H,EAAA,IAAQsK,GAAMA,EAAElC,KAAO,KACvB,EAAAmC,EAAA,IAAUpC,IACR,MAAM1K,EAAO,IAAI0K,EAAMhF,UAMjB8C,EAtLW,IAkLMxI,EAAKuC,QACzBwC,GAAMA,EAAEjB,SAAWjE,EAAgBkN,YACpCtI,OAIIuI,EAAiBjI,GACrBA,EAAEuE,UAAYxJ,EAAiBgB,UAC9BiE,EAAEuE,UAAYxJ,EAAiByJ,WAAalC,KAAKiE,SAEpD,GAAI9C,EAAY,EAAG,CACjB,MAAM0D,EAAelM,EAClBuC,QACEwC,GAAMA,EAAEjB,SAAWjE,EAAgBuF,SAAW4H,EAAcjI,KAE9DJ,MAAK,CAACC,EAAGC,IACDD,EAAE4E,SAAW3E,EAAE2E,WAEvB3G,MAAM,EAAG2F,GAEZ,GAAI0D,EAAazH,OAAS,EAWxB,OAVAyH,EAAa1M,SAASuF,IACpB2F,EAAMpN,IAAIyH,EAAEvD,GAAI,IACXuD,EACHjB,OAAQjE,EAAgBkN,WACxB,IAGJ1F,KAAK8C,YAAY1Q,KAAKiR,GAEtBrD,KAAK6C,UAAUhC,WAAW,cAAe,eAClCb,KAAK4F,iBAAiBf,EAEjC,CAEA,OAAO,GAAK,KAahB,OATA7E,KAAKoE,OAASkB,EAAQ9B,MAAK,EAAAqC,EAAA,MAE3B7F,KAAKoE,OAAO9N,UAAU,CACpBlE,KAAO8C,IACL8K,KAAK6C,UAAUhC,WAAW,SAAS,EAErCtK,MAAQoO,GAAQ3E,KAAK6C,UAAUhC,WAAW,QAAS8D,EAAIC,cAGlD5E,IACT,CAEA,mBACE8F,EACA7D,EACAE,GAEA,OAAOnB,EACL8E,GACCA,GACC9F,KAAK0B,QACHoE,EAAKpO,KAAKsK,IAAQ,CAChB7H,GAAI6H,EACJG,WACAF,gBD/QyB,ICoRnC,CAEA,cAAqBhB,GACnB,GAAqB,IAAjBA,EAAM7D,OACR,aAGmB4C,KAAK0D,GAAIqC,aAAa9E,GAA3C,MAEMoC,EAAQrD,KAAK8C,YAAYhO,MAE/BmM,EAAM9I,SAASiB,GACbiK,EAAMpN,IAAImD,EAAKe,GAAI,IAAKf,EAAMqD,OAAQjE,EAAgBuF,YAExDiC,KAAK8C,YAAY1Q,KAAKiR,EACxB,CAEA,sBACE,MAAMA,QAAcrD,KAAK0D,GAAIsC,aAAa,CACxCC,SAAU,CAACzN,EAAgBuF,WAC1BtH,MAAMwK,GAAU,IAAI+B,IAAI/B,EAAMvJ,KAAK0B,GAAS,CAACA,EAAKe,GAAIf,QAEzD4G,KAAK8C,YAAY1Q,KAAK,IAAI4Q,IAAI,IAAIK,KAAUrD,KAAKqD,QACnD,G,uEClTK,SAAS6C,GAAcC,EAAgB/I,EAAS,KACrD,OAAO+I,EAAO/I,OAASA,EAAS,GAAG+I,EAAO3K,MAAM,EAAG4B,QAAe+I,CACpE,CA0BA,MAAMC,GAAqB,uBClBpB,MA0BMC,GAAgC,CAC3C1L,EACA2L,KAEA,MAAM,iBACJtM,EAAgB,MAChBuM,EACAC,aAAa,KACXvM,EACAwM,OAAO,UAAE9M,EAAS,OAAE+M,GAAQ,QAC5BC,GACD,KACD3O,EAAI,MACJlD,GACEwR,EACJ,MAAO,CACLvM,KAAMC,EACNuM,QACAvO,OACA2B,WAAW,EAAAiN,GAAA,IAAgBjN,GAE3BM,OACAnF,QACA6R,UACAhM,SACAkM,YAAaH,EACd,EAoCUI,GAA0B,EACrCzM,OACAb,KACAmB,SACAhB,YACAK,uBACF,CACEK,OACAb,KACAmB,SACAhB,WAAW,EAAAiN,GAAA,IAAgBjN,GAC3BG,gBAAiBE,ICxDZ,SAAS+M,GACdC,EACAC,GAEA,OAAO7S,SAAUmD,KACf,GAAI0P,EAAOC,QACT,MAAM,IAAIC,aAAa,6BAA8B,cAEvD,OAAOH,KAAQzP,EAAK,CAExB,C,+BC6NO,IAy8LK6P,GAAL,CAAKA,IAEVA,EAAA,IAAM,MAENA,EAAA,cAAgB,kBAEhBA,EAAA,aAAe,iBAEfA,EAAA,KAAO,OAEPA,EAAA,eAAiB,mBAEjBA,EAAA,cAAgB,kBAZNA,GAAL,CAAKA,IAAA,IAo0QwB,KAAG;;;;;;;;;MAgCH,KAAG;;;;;;;;MAyCF,KAAG;;;;;;;;;;;;;;;MAiDV,KAAG;;;;;;;;;;;;;;MAkDK,KAAG;;;;;;;;MAyClC,MAAMC,GAA+B,KAAG;;;;;;;;;;MA+CxC,MAAMC,GAAkC,KAAG;;;;;;;;;;MA8CD,KAAG;;;;;;;;MA2C7C,MAAMC,GAAiC,KAAG;;;;;;;;;;;MA8C1C,MAAMC,GAAiC,KAAG;;;;;;;;;;;;;;;;;;;;;MA4D1C,MAAMC,GAAmC,KAAG;;;;;;;;;;;;;;;;;;;;;MAkDX,KAAG;;;;;;;;MAyCJ,KAAG;;;;;;MAwCD,KAAG;;;;;;;;;;;;;;;;;;;;;;;MA0DH,KAAG;;;;;;;;;;;;;;;;;;;;6CC7perC,MAAMC,GAA4B,8BAE5BC,GACX,mCCeWC,GAA+B,CAC1CjN,EACAzF,KAEA,MAAM,KAAEL,EAAI,OAAEgT,GAAW3S,EAEnB6E,EAAO8N,EAAO,WAAW,GACzBC,EAAkBD,EAAO,kBAAkB,GAAGrM,MAAM,GACpD7B,GAAY,WACZkN,EAAcgB,EAAO,aAAa,IAElC,KAAE5N,EAAO,YAAI8N,GAhCQ,CAAClT,IAC5B,MAAMK,EAAS,GAAA8S,GAAGC,QAAO,KAAAC,YAAWrT,IAC9BoF,EAAO/E,EAAOiT,MAAMlO,KACpB8N,EAAW7S,EAAOiT,MAAMJ,SAC3BrQ,KAAKwE,IACJ,MAAMkM,EAAUlM,EAAQmM,QAAQ7M,MAAM,GACtC,OAAI4M,IAAYV,GACP,GAAAY,QAAQL,OAAO/L,EAAQpH,OAG5BsT,IAAYT,GACP,GAAAY,aAAaN,OAAO/L,EAAQpH,YADrC,CAGO,IAERoG,QAAQgB,QAAwB,IAAZA,IAEvB,MAAO,CAAEjC,OAAM8N,WAAU,EAeOS,CAAc3T,EAAKC,MAAM2T,SAASnC,IAE5DlM,EAAiC,GAevC,OAdA2N,EAAU5P,SAAQ,CAAC+D,EAASqK,KAC1BnM,EAAaoC,KAAK,CAChBzC,OACAwM,QACAvO,KAAM8P,EACNnO,YACAgN,SAAS,EACT7R,MAAOoH,EACPjC,OACAU,SACAkM,eACA,IAGGzM,CAAY,E,8DCtDrB,MAAMsO,GAAqB,IAAI,MAC7B,SAAa,CACXC,IAAK,MACLC,YAAcC,IAA6B,EAC3CC,cAAe,GACfC,UAAW3U,MAAO4U,IAChBC,YAAW,IAAMxU,QAAQC,WAAWwU,KAAKC,IAAI,IAAO,GAAKH,EAAS,KAAO,KAqBlEI,GAAuBC,GAClC,IAAI,MAAc,MAAa,CAC7BpC,OAAQoC,ICnBZ,MAAMC,GAAkBlV,OACtBmV,cACAC,gBACAhI,SAAS,EACT6H,wBAOkBD,GAAoBC,GAAaI,QAGjDpC,GAA8B,CAC9BqC,MChC2B,IDiC3BlI,SACAmI,QAAS,CAAC,CAAEhQ,UAAWyN,GAASwC,MAChCC,MAAO,CACLC,IAAK,CACH,CAAEC,YAAa,CAAEC,IAAKT,IACtB,CAAEU,cAAe,CAAED,IAAKT,KAE1B5P,UAAW,CAAEuQ,KAAK,SAAgBV,QAI3BW,WAqBPC,GAA0BhW,OAC9BuG,SACA0P,gBACAb,gBACArI,YACAK,SAAS,EACT6H,kBASA,MAAMQ,EAAQ,CACZS,KAAM,CACJ,CACE3Q,UAAW,CACTuQ,KAAK,SAAgBV,KAGzB,CACE7O,OAAQ,CACNqP,IAAKrP,IAGT,CAAEsP,cAAe,CAAEM,IAAKF,MAkB5B,aAdkBjB,GAAoBC,GAAaI,QAGjDpC,GAA8B,CAC9BqC,MAAOvI,EACPK,SACAmI,QAAS,CACP,CACEhQ,UAAWyN,GAASwC,MAGxBC,WAGSM,UAAU,EAGVK,GAAkCpW,MAC7CuG,EACA0P,EACAb,EACArI,EACAkI,IAEAhI,GAAsB+I,GAAyB,CAC7CzP,SACA0P,gBACAb,gBACArI,YACAkI,gBE/GEoB,GAA+BC,GACnC,IACK,IAAIC,IAAI,IACND,EAAMhT,KAAKkT,GAASA,EAAKpR,QACzBkR,EAAMhT,KAAKkT,GAASA,EAAKvQ,UAKrBwQ,GAAqCzW,MAChD4N,EACA8I,EACAC,EACAC,EACA3B,KAEA,MAAM4B,EFkGwB,EAC9B1B,EACAC,EACAH,IAEAhI,GAAsBiI,GAAiB,CACrCC,cACAC,gBACAH,gBE1GyB6B,CACzBlJ,EACA8I,EACAzB,GAEIqB,EAAQ,GAEd,gBAAiBtJ,KAAS6J,EAAoB,CAC5CP,EAAMlO,QAAQ4E,GACd,MAAMnI,EAAYwR,GAA4BrJ,GAC1CnI,EAAUmE,OAAS,SACf4D,EACJ/H,GACC6M,GACCiF,EAAmBI,aACjBrF,EACArN,EAAiBgB,SACjBuR,IXvC2B,GW4CrC,CAEA,OAAON,CAAK,EC5BP,MAAMU,GAAgC,EAC3CzQ,SACA6O,gBACAhI,SAAS,EACT6J,QAAQ,GACRC,iBAAiB,OACjB5B,YACF,CACEjP,QAAS,IAAIE,KACb+O,QACA6B,gBAAgB,SAAgB/B,GAChChI,SACA6J,MAAO,IAAIA,EAAM3T,KAAK8T,GAAM,IAAIA,OAAMC,KAAK,SAC3CC,gBAAiBJ,IAGbK,GAAoBvX,OACxBuG,SACA6O,gBACAhI,SAAS,EACT6J,QAAQ,GACRC,iBAAiB,OACjB5B,QACAL,kBAEA,MAAMuC,QAAYxC,GAAoBC,GAAaI,QAIjDjC,GACA4D,GAA8B,CAC5BzQ,SACA6O,gBACAhI,SACA6J,QACAC,iBACA5B,QACAL,iBAIJ,OAAOuC,GAAKC,mBAAmB,ECrB3BC,GAAkB,CACtBjT,EACAkT,EACAP,EACAQ,EACAC,KAEA,MAAM3S,EAAOT,EAAMxC,IAAI0V,GACjB3R,EAAed,GAAMc,cAAgB,GAS3C,OAPAA,EAAaoC,KAAKgP,GAClB3S,EAAM5C,IAAI8V,EAAM,CACdG,YAAaH,EACbI,kBAAmBF,EAAWT,EAAE7R,UAAYL,GAAM6S,mBAAqB,EACvEC,KAAM,CAAEJ,SAAQ/R,KAAMuR,EAAEvR,KAAMoS,UAAWJ,EAAW,KAAO,QAC3D7R,iBAEKvB,CAAK,ECrDDyT,GAAclY,MACzBsP,EACA6I,EACA/C,EACAvC,EACAuF,GAAwB,KAExB,MAAMC,QAAkB/I,EAAGgJ,eAAe,CACxCpS,QAASiS,EACTlT,UAAWd,EAAUe,OAGjBqT,EAAe,IAAI3J,IAAIyJ,GAAW/U,KAAKgG,GAAM,CAACA,EAAEvD,GAAIuD,MAOpDkP,EDnByB,EAC/BL,EACAnS,KASA,GAAgC,KAN9BA,EAAcc,QACXsQ,GACCA,EAAExT,OAAS0P,IACX8D,EAAExT,OAAS2P,MACV,IAEcvK,OACnB,MAAO,GAET,MAAMvE,EAAQ,IAAImK,IAmBlB,OAlBA5I,EAAajC,SAASqT,IACpB,IAAIU,EAAc,GAClB,GAAIV,EAAExT,OAAS2P,GAAiC,CAC9C,MAAM,OAAEnN,EAAM,QAAEqS,GAAYrB,EAAE1W,MACxBmX,EAAWzR,EAAOwC,MAAMU,GAAMA,EAAEjD,UAAY8R,KAC7BN,EAAWY,EAAUrS,GAC7BrC,SAASuD,GACpBoQ,GAAgBjT,EAAO6C,EAAIjB,QAAS+Q,EAAG9P,EAAIoR,MAAOb,IAEtD,MAAO,GAAIT,EAAExT,OAAS0P,GAA2B,CAC/C,MAAM,YAAEnN,EAAW,UAAEwS,EAAS,OAAEf,GAC9BR,EAAE1W,MACEmX,EAAW1R,IAAgBgS,EACjCL,EAAcD,EAAWc,EAAYxS,EACrCuR,GAAgBjT,EAAOqT,EAAaV,EAAGQ,EAAQC,EACjD,KAGKpT,CAAK,ECdImU,CAAkBT,QALL7I,EAAGuJ,gBAAgBV,EAAW,CACzDW,MAAO,MACP1D,mBAKI2D,EAA2B,GAGjC,UAAW7T,KAAQsT,EAAQvO,SAAU,CACnC,MAAM+O,EAAWT,EAAatW,IAAIiD,EAAK4S,aACjCmB,EAAkB/T,EAAKc,aAAakT,IAAI,IAEtC3T,UAAW4T,EAAoB,KAAExT,EAAI,MAAEwM,GAAU8G,EACnDG,EAAiB,CACrBnU,UAAWd,EAAUe,KACrBgB,QAASiS,EACThT,KAAM,CACJO,gBAAiBC,EACjBwM,UAKJ,GAAK6G,EAmBE,CACL,MAAM,GACJjT,EAAE,cACFsT,EAAa,gBACb3C,EAAe,KACfvR,EACAR,YAAa2U,GACXN,EAEEO,EAAoBzE,KAAK0E,IAC7BH,EACAnU,EAAK6S,oBAED,uBAAE0B,EAAyB,EAAC,oBAAEC,EAAsB,GAAMvU,EAC1DwU,EAAsB7E,KAAK0E,IAC/BtU,EAAK6S,kBACL2B,GAEI/U,EACJ2U,EACApU,EAAKc,aAAac,QAAQsQ,GAAMA,EAAE7R,UAAYoU,IAC3C3Q,OAEL,GAAI0N,EAAkByC,EAAsB,CAE1C,MAAMS,EAAyBxB,EAC3Be,EACAO,EAEEG,EAAoB,IACrBT,EACHrT,KACApB,cACA0U,cAAeE,EAGf7C,gBAAiB5B,KAAK0E,IACpBL,EACAM,EACAG,GAGFzU,KAAM,IACDiU,EAAejU,KAClBuU,oBAAqBE,EACrBH,iCAKE9G,GACJrD,EAAGwK,iBAAiBC,KAAKzK,GACzBuD,EAFIF,CAGJkH,GAEFd,EAAQ3Q,KAAK,IACR4Q,KACAa,EACH1U,KAAM8T,GAEV,CACF,KAhFe,CACb,MAAMtU,EAAcO,EAAKc,aAAac,QACnCsQ,GAAMA,EAAE7R,UAAYL,EAAK6S,oBAC1B/O,OAEIgR,EAAU,IACXZ,EACHrT,GAAIb,EAAK4S,YACTnT,cAEA+R,gBAAiB0B,EAAwBe,EAAuB,EAChEE,cAAenU,EAAK6S,kBACpBkC,UAAU,SAINtH,GAAerD,EAAG4K,cAAcH,KAAKzK,GAAKuD,EAA1CF,CAAkDqH,GAExDjB,EAAQ3Q,KAAK,IAAK4R,EAAS7U,KAAM8T,GACnC,CA8DF,CACA,OAAOF,CAAO,E,wCCxHT,MAAMoB,GAqBXxO,YAAYyO,GApBZ,KAAQC,eAAkC,GAE1C,KAAQC,cAAgB,EAExB,KAAQC,kBAAoB,EAE5B,KAAQC,eAAiB,EAEzB,KAAQzN,UAAY,EAalBnB,KAAKwO,iBAAmBA,CAC1B,CAVW1N,eACT,MAAO,CACL+N,WAAY7O,KAAK0O,cACjBI,cAAe9O,KAAK2O,kBACpBC,cAAe5O,KAAK4O,cAExB,CAMOvJ,MAAMqJ,EAAuBvN,EAAY,GAO9C,OANAnB,KAAK0O,cAAgBA,EACrB1O,KAAKyO,eAAiB,GACtBzO,KAAK2O,kBAAoB,EACzB3O,KAAK4O,eAAiB,EACtB5O,KAAKmB,UAAYA,EAEVnB,KAAKc,QACd,CAEOnK,IAAIoY,GAGT,OAFA/O,KAAK0O,eAAiBK,EAEf/O,KAAKc,QACd,CAEOkO,cAAcC,GAOnB,GANAjP,KAAKkP,iBAAiBD,GAElBjP,KAAKyO,eAAerR,OAtDL,IAuDjB4C,KAAKyO,eAAeU,QAGlBnP,KAAKyO,eAAerR,OAAS,EAAG,CAClC,MAGMgS,EAHqBpP,KAAKqP,gCACNrP,KAAK0O,cAAgB1O,KAAK2O,mBACAM,GAIpDjP,KAAK2O,mBAAqBM,EAC1BjP,KAAK4O,cAAgB1F,KAAKoG,MAAMF,GAChCpP,KAAKwO,kBAAoBxO,KAAKwO,iBAAiBxO,KAAKc,SACtD,CAEA,OAAOd,KAAKc,QACd,CAEQoO,iBAAiBK,GACvBvP,KAAKyO,eAAejS,KAAK,CAAE7C,UAAWC,KAAK4V,MAAOD,aACpD,CAEQF,8BACN,IAAII,EAAY,EACZC,EAAa,EAEjB,QAAShS,EAAI,EAAGA,EAAIsC,KAAKyO,eAAerR,OAAQM,IAAK,CACnD,MAAMiS,EACJ3P,KAAKyO,eAAe/Q,GAAG/D,UAAYqG,KAAKyO,eAAe/Q,EAAI,GAAG/D,WAC1D,UAAE4V,GAAcvP,KAAKyO,eAAe/Q,GAE1C+R,GAAaE,EAAWJ,EACxBG,GAAcH,CAChB,CAEA,OAAsB,IAAfG,EAAmB,EAAID,EAAYC,CAC5C,ECyBF,OAjGA,MAuBE3P,YACEpE,EACAiH,EACAmI,GASA,GA5BF,KAAU6E,gBAAkB,IAAIrB,GAEhC,KAAU3N,WAAa,IAAI,EAM3B,KAAUW,OAA4B,CACpCgL,UAAW,MAYXvM,KAAKrE,KAAOA,EAEZqE,KAAK6P,gBAAkB,IAAIC,gBAE3B9P,KAAK6C,UAAYlC,EAAgBhF,EAAMqE,KAAKY,YAC5CZ,KAAK+K,kBAAoBA,EACzB/K,KAAK+P,UAAW,QAAoB,CAAEC,OAAQ,OAAQC,OAAQtU,KACzDiH,EAAKsN,QACR,MAAM,IAAIhN,MAAM,0BAGlBN,EAAKW,YAAYjN,WAAWoN,IAC1B1D,KAAK0D,GAAKA,CAAE,IAGd1D,KAAK+K,kBAAoBA,EAEzB/K,KAAK4D,eAAiB5D,KAAKmQ,4BAA4BvN,GAEvD5C,KAAK4D,eAAetN,WAAWuI,IAC7BmB,KAAK+P,SAASK,KACZ,OAAOpQ,KAAKrE,UAAUkD,EAAgB,cAAgB,cAExDmB,KAAK6C,UAAUhC,WAAWhC,EAAgB,cAAgB,WAAW,IAGvEmB,KAAK4D,eACFJ,MAAK,EAAA6M,GAAA,IAAU,IAAMzN,EAAKsN,WAC1B5Z,WAAWiL,IACVvB,KAAKuB,OAASA,EACdvB,KAAK+P,SAASK,KAAK,OAAOpQ,KAAKrE,wBAAyB,CACtD9G,KAAM0M,GACN,IAINvB,KAAK4D,eACFJ,MACC,EAAAtI,EAAA,IAAQ2D,KAAoBA,KAC5B,EAAAwR,GAAA,IAAU,IAAMrQ,KAAKsQ,sBAAsB1N,EAAKsN,YAEjD5Z,WAAU,KACT0J,KAAKuQ,SAAS,GAEpB,CAEUC,sBACRxQ,KAAK6P,gBAAkB,IAAIC,eAC7B,CAOUQ,sBAAsBJ,GAC9B,OAAOA,EAAQ1M,MACb,EAAA9L,EAAA,IAAK6J,GAAWA,EAAOgL,aACvB,EAAAkE,GAAA,IAAqB,CAACC,EAAYC,IAAcD,IAAeC,KAC/D,EAAAjZ,EAAA,IAAKkZ,KAAQA,KACb,EAAA1V,EAAA,IAAQ0V,KAAQA,IAEpB,GCrGK,MAAMC,GAAwB,CACnCjN,EACAkN,EACAC,IAEAnN,EAAeJ,MACb,EAAAiN,GAAA,MACA,EAAAlL,EAAA,IAAK1G,GAAkBkS,IAAWlS,MAClC,EAAA3D,EAAA,IAAQ8V,GAAgBA,KACxB,EAAAX,GAAA,IAAU,IAAMS,KAChB,EAAAjL,EAAA,M,gBC+DJ,OAzEA,cAAsC,GAKpC9F,YACEpE,EACAiH,EACAmI,GAEAkG,MAAMtV,EAAMiH,EAAMmI,GAPpB,KAAmBmG,eAAiB,IAAIC,EAAA,EAStC,MAAM7L,EAAUuL,GACd7Q,KAAK4D,eACL5D,KAAKkR,eAAe1N,MAClB,EAAA4N,GAAA,GAAU,OACV,EAAA7L,EAAA,IAAI,KAEFvF,KAAKwQ,qBAAqB,KAE5B,EAAAH,GAAA,IAAU,IACRrQ,KAAKqR,uBAAuB7N,MAC1B,EAAA6M,GAAA,IAAW7G,GACTxJ,KAAKsR,uBAAuB9H,GAAehG,MACzC,EAAA+B,EAAA,IAAI,IAAMvF,KAAK6C,UAAUhC,WAAW,aACpC,EAAAwP,GAAA,IAAWxb,IAAS,EAAAwF,GAAA,GAAK2F,KAAKuR,SAAS1c,EAAMmL,KAAKuB,mBAM3D1C,IACC,GAAQ5H,IAAI,OAAO0E,kBAAsBkD,GACzCmB,KAAK6C,UAAUhC,WAAWhC,EAAgB,cAAgB,WAAW,IAIzEyG,EAAQhP,UAAU,CAChBlE,KAAM,KACJ4N,KAAK6C,UAAUhC,WAAW,SAAS,EAErCtK,MAAQoO,IACN3E,KAAK6C,UAAUhC,WAAW,QAAS8D,EAAI,IAG3C3E,KAAKsF,QAAUA,CACjB,CAQOiL,UACLvQ,KAAK6P,iBAAiB2B,QACtBxR,KAAKkR,eAAe9e,OACpB,GAAQ6E,IAAI,OAAO+I,KAAKrE,sBAC1B,CAOO0J,QAIL,OAHArF,KAAKsF,QAAQhP,WAAU,SAGhB0J,IACT,GCsRF,OA9SA,cAAmC,GACvBmQ,4BAA4BvN,GAepC,OAduB,EAAAiB,EAAA,GAAc,CACnCjB,EAAKW,YACLX,EAAKsN,QAAS1M,MACZ,EAAA9L,EAAA,IAAK6J,GAAWA,EAAOgL,aACvB,EAAAkE,GAAA,MAEFzQ,KAAK+K,kBAAmBnH,iBACvBJ,MACD,EAAA9L,EAAA,IACE,EAAEqM,EAAYwI,EAAWkF,OACrB1N,KAAgB0N,KAA0BlF,IAKpD,CAGU+E,uBACR9H,GAEA,MAAM,UAAE+C,GAAcvM,KAAKuB,OAC3BvB,KAAK+P,SAASK,KACZ,OAAOpQ,KAAKrE,kBAAkB4Q,WAAkB,SAC9C/C,MAIJ,MAAMkI,EAAYtG,GAA8B,CAC9CzQ,OAAQ4R,EACR/C,gBACA6B,MAAO,GACPC,eAAgB,OAChB5B,MAAO,MAGHiI,EXrDH,SACLC,EACAF,GAEA,MAKMG,EALS,IAAIC,GAAA,EAAa,CAC9BlH,KAAMlC,GACNqJ,MAAO,IAAI,OAGmBzb,UAAU,CAAEsb,QAAOF,cACnD,OAAO,IAAIxb,EAAA,GAAY8b,IACrB,MAAMtb,EAAemb,EAAiBvb,UAAU,CAC9ClE,KAAK8C,GACH8c,EAAW5f,KAAK8C,EAAOL,KACzB,EACA0B,MAAMoO,GACJqN,EAAWzb,MAAMoO,EACnB,EACAnO,WACEwb,EAAWxb,UACb,IAIF,MAAO,IAAME,EAAaE,aAAa,GAE3C,CW4BMqb,CACExK,GACAiK,GACAlO,MACA,EAAA9L,EAAA,IAAKwa,IACI,CACLC,OAAQ,UACR/X,aAAc8X,EAASrG,oBAAoBnU,KAAKgG,GAC9C2I,GAA8BkG,EAAY7O,UAM9C0U,ECpGH,SACL3X,EACAmX,EACA3a,GAEA,OAAO,IAAIf,EAAA,GAAY8b,IACrB,MAAMK,EAAK,IAAIC,UAAU,OA8BzB,OA5BAD,EAAGE,OAAS,KACVtb,EAAI,wBAAwB,cAAsB2a,KAClDS,EAAGG,KACD5a,KAAKC,UAAU,CACb4a,QAAS,MACTnb,OAAQ,YACR6C,GAAI,IACJoH,OAAQ,CAAEqQ,WAEb,EAGHS,EAAG1d,UAAaC,IACd,MAAMsH,EAAUtE,KAAK6F,MAAM7I,EAAMC,MACjCoC,EAAI,WAAWwD,cAAqByB,GACpC8V,EAAW5f,KAAK8J,EAAQhH,OAAO,EAGjCmd,EAAGK,QAAW9d,IACZqC,EAAI,WAAWwD,UAAiB,CAAElE,MAAO3B,IACzCod,EAAWzb,MAAM3B,EAAM,EAGzByd,EAAGM,QAAU,KACX1b,EAAI,WAAWwD,YACfuX,EAAWxb,UAAU,EAGhB,KACL6b,EAAG5c,OAAO,CACX,GAEL,CD4D6Bmd,CACvBrG,GCzGoC9R,ED0GV8R,ECzG9B,yCAAyC9R,OD0GrC,CAACyB,EAAS2W,IAAQ7S,KAAK+P,SAASK,KAAKlU,EAAS,CAAE4W,KAAM,aAAcD,MACpErP,MACA,EAAAtI,EAAA,IAAQrG,KAAU,KAAAke,SAAQle,MAC1B,EAAA6C,EAAA,IAAK7C,IACI,CACLsd,OAAQ,OACR/X,aAAcwN,GAA6B2E,EAAY1X,QCjHxB,IAAC4F,EDsHtC,OAAO,EAAAuY,GAAA,GACLrB,EACAS,EAEJ,CAEUf,uBACR,OAAO,EAAA4B,GAAA,IAAM,KAAM,EAAA5Y,GAAA,GAAK2F,KAAKkT,aAE/B,CAEA,iBACE,MAAM,UAAE3G,GAAcvM,KAAKuB,QACrB,OAAE0F,GAAWjH,KAAK6P,gBAClBzC,QAAiBpN,KAAK0D,GAAIyP,cAAc5G,EAAYA,GAEpD6G,QAAiCpT,KAAKqT,iBAC1C9G,EACAA,EACAa,GAGFpN,KAAK6C,UAAUhC,WAAW,cAAe,iBACzC,MAAMyS,QAAwBhH,GAC5BtM,KAAK0D,GACL6I,EACAa,EAAStC,gBACT7D,GAMF,OAHAjH,KAAKY,WAAWL,gBAAgB+S,GAChCtT,KAAK6C,UAAUhC,WAAW,UAEnBuS,CACT,CAEA,gBACE,OAAEjB,EAAM,aAAE/X,GACVmH,GAEA,MAAM,UAAEgL,GAAchL,GAChB,OAAE0F,GAAWjH,KAAK6P,gBACxB,GAA4B,IAAxBzV,EAAagD,OAEf,YADA4C,KAAK+P,SAASK,KAAK,OAAOpQ,KAAKrE,QAAQ4Q,wBAGzC,MAAMa,QAAiBpN,KAAK0D,GAAIyP,cAAc5G,EAAYA,SAEpDvM,KAAKuT,yBACThH,EACAA,EACAnS,EACAgT,EACA+E,GAGFnS,KAAK6C,UAAUhC,WAAW,cAAe,iBACzC,MAAMyS,QAAwBhH,GAC5BtM,KAAK0D,GACL6I,EACAa,EAAStC,gBACT7D,EACW,SAAXkL,GAGFnS,KAAKY,WAAWL,gBAAgB+S,GAChCtT,KAAK6C,UAAUhC,WAAW,SAC5B,CAEA,+BACE0L,EACA9R,EACAL,GACA,cAAEqT,EAAa,YAAE1U,EAAW,gBAAE+R,GAC9BqH,GAEA,MAAM,OAAElL,GAAWjH,KAAK6P,gBAIlBrD,EAAmC,SAAX2F,EAE9BnS,KAAK+P,SAASK,KACZ,iCAAiC3V,KAAW0X,eAC1C/X,EAAagD,iBACJhD,EAAakT,GAAG,IAAI3T,kBAC7BS,EAAakT,IAAI,IAAI3T,mBAKnBoN,GAAe/G,KAAK0D,GAAI8P,gBAAiBvM,EAAzCF,CAAiD3M,GAGvD4F,KAAKyT,UAAUrZ,EAAc6M,GAE7B,MAAM,KACJlN,EAAI,MACJwM,EAAK,UAEL5M,GACES,EAAakT,IAAI,GAEfoG,EAAoB/Z,EAGpBga,EAAc,CAClBrZ,QAASiS,EACTlT,UAAWd,EAAU6B,aACrBD,GAAIM,EACJqQ,gBAAiB0B,EACbkH,EACA5I,EACJ/R,YAAaA,EAAeqB,EAAagD,OACzCqQ,cAAeA,GAAiB,EAChCY,UAAU,EACV9U,KAAM,CACJO,gBAAiBC,EACjBwM,UAMJ,aAFMQ,GAAe/G,KAAK0D,GAAI4K,cAAerH,EAAvCF,CAA+C4M,GAE9CD,CACT,CAEA,uBACEnH,EACA9R,EACA2S,GAEA,MAAM,YAAErU,EAAW,gBAAE+R,GAAoBsC,EACnC5D,EAAgBsB,EAAkB,EAExC9K,KAAK6C,UAAUhC,WAAW,cAE1B,MAAM+S,OP7LmCxf,OAC3CqG,EACA+O,EACAH,KAEA,MAAMuC,QAAYxC,GAAoBC,GAAaI,QAGjDlC,GAAgC,CAChC9M,QAAS,IAAIA,KACbd,WAAW,SAAgB6P,KAG7B,OAAOoC,GAAKiI,8BAA8BC,WAAWC,KAAK,EOgLxBC,CAC9BvZ,EACA+O,EACAxJ,KAAK6P,gBAAiB5I,QAOxB,GAJAjH,KAAK+P,SAASK,KACZ,gCAAgC3V,cAAoBmZ,YAA4BpK,KAGxD,IAAtBoK,EACF,OAAOpK,EAGTxJ,KAAK6C,UAAUhC,WACb,cACA,QAAQpG,OACRuF,KAAK4P,gBAAgBvK,MACnB6D,KAAK+K,KAAKL,ETtRe,OS0R7B,MAAMM,EPnM+B,GACvCvZ,SACA6O,gBACA6B,QACAC,iBACA5B,QACAL,iBAEAhI,GAAsBsK,GAAmB,CACvChR,SACA6O,gBACA6B,QACAC,iBACA5B,QACAL,gBOqLkC8K,CAA0B,CAC1DxZ,OAAQF,EACR+O,gBACA6B,MAAO,GACPC,eAAgB,MAChB5B,MT/R2B,ISgS3BL,YAAarJ,KAAK6P,iBAAiB5I,SAGrC,IAAImN,EAAmB,EACnBV,EAAoBlK,EAGxB,gBAAiBpI,KAAS8S,EAA2B,CACnDlU,KAAK6C,UAAUhC,WACb,cACA,QAAQpG,OACRuF,KAAK4P,gBAAgBZ,cAAc,IAGrCoF,GAAoBhT,EAAMhE,OAE1B,MAAMhD,EAAegH,EAAM1J,KAAKgG,GAC9B2I,GAA8B5L,EAASiD,KAGzCgW,QAA0B1T,KAAKuT,yBAC7BhH,EACA9R,EACAL,EACA,IACKgT,EACHrU,YAAaA,EAAcqb,GAE7B,UAEJ,CAEA,OAAOV,CACT,CAEA,gBAAwBtS,EAAyB6F,GAC/C,MAAM,OAAEoN,EAAM,eAAEC,EAAc,MAAE5J,GR1Q7B,SAAyCtJ,GAC9C,MAAM+I,EAAa/I,EAAMlG,QACtBqZ,GLvCsC,qCKuChCA,EAAEvc,OAELsc,EAAiB,IAAI3J,IACrBD,EAAmB,GAuBzB,MAAO,CACL2J,OAtB2ClK,EAAWjN,QAEtD,CAACC,GAAOrI,QAAOiF,OAAMJ,gBACpB7E,EAAyB4V,MAAMvS,SAASyS,IACvC0J,EAAe3d,IAAIiU,EAAKpR,IACxB8a,EAAe3d,IAAIiU,EAAKvQ,MACxB,MAAMma,EAAS,IACV5J,EACHjR,YACAgB,OAAS7F,EAAyB6F,OAClCb,gBAAiBC,GAEnB2Q,EAAMlO,KAAKgY,GAEP5J,EAAKvQ,OAASgI,KAChBlF,EAAIqX,EAAOhb,IAAMgb,EACnB,IAEKrX,IACN,CAAC,GAIFmX,eAAgB,IAAIA,GACpB5J,QAEJ,CQ0OM+J,CAAgCrT,GAC9BsJ,EAAMtN,OAAS,SACX4D,EACJ0J,GACCA,GAAU3D,GAAe/G,KAAK0D,GAAIgR,cAAezN,EAAvCF,CAA+C2D,InBlU7B,KmBuUjC,MAAMiK,EAAiB1c,OAAOC,KAAKmc,GAE7BO,EAAoBN,EAAepZ,QACtC8G,IAAS2S,EAAeE,SAAS7S,WAI9BhC,KAAK+K,kBAAmBI,aAC5BwJ,EACAlc,EAAiBgB,SACjBxF,EAAc6gB,MAIZF,EAAkBxX,OAAS,SACvB4C,KAAK+K,kBAAmBI,aAC5ByJ,EACAnc,EAAiBgB,SACjBxF,EAAc8gB,IAGpB,GEhWK,MAAMC,GAAgBC,GAC3BA,EAAIC,QAAQ,gBAAiBC,GAC3BA,EAAMC,cAAcF,QAAQ,IAAK,IAAIA,QAAQ,IAAK,MAQ/C,SAASG,GACdC,GAEA,IAAKA,GAAgC,iBAAbA,EACtB,OAAOA,EAET,MAAMC,EAA2B,CAAC,EAelC,OAdAtd,OAAOC,KAAKod,GAAUnd,SAASqd,IAC7B,GAAIvd,OAAOwd,UAAUC,eAAeC,KAAKL,EAAUE,GAAM,CACvD,MAAMI,EAAeZ,GAAaQ,GAClC,IAAI1gB,EAAQwgB,EAASE,GACjBK,MAAMC,QAAQR,EAASE,IACzB1gB,EAAQwgB,EAASE,GAAK9d,KAAK0B,GAASic,GAAYjc,KACd,iBAAlBkc,EAASE,GACzB1gB,EAAQugB,GAAYC,EAASE,IACK,iBAAlBF,EAASE,KACzB1gB,EAA0BA,EnBT7BogB,QAAQ,OAAQ,MAChBA,QAAQ,OAAQ,MAChBA,QAAQ,OAAQ,KAChBA,QAAQ,QAAS,KACjBA,QAAQ,QAAS,MACjBA,QAAQ,QAAS,MmBMhBK,EAAIK,GAAgB9gB,CACtB,KAEKygB,CACT,CCzBO,SAASQ,GACdrL,EACApQ,EACA0b,EAAoB,EACpBtI,EAAkB,GAElB,MAAMuI,EAAkBvL,EAAMxP,QAC3B0P,GAASA,EAAKjR,UAAYqc,IAEvBE,GAAkB,KAAAC,eACtBF,GACCrL,GAASA,EAAKjQ,SAAWL,IAGtBvB,EACJmd,EAAkB,EACdxI,EAAkBuI,EAAgB7Y,OAClC6Y,EAAgB7Y,OAAS8Y,EAAkB,EAKjD,MAAO,CACLzI,cAHAyI,EAAkB,EAAIF,EAAoBtL,EAAMwL,GAAiBvc,UAIjEZ,cAEJ,CAEO,SAASqd,GACdC,EACA3L,EACApQ,EACAkS,GAAwB,GAExB,MAAM,cAAEiB,EAAa,YAAE1U,GAAgBgd,GACrCrL,EACApQ,EACA+b,EAAW5I,cACX4I,EAAWtd,aAGPud,EAAWjB,GAAY3K,EAAMA,EAAMtN,OAAS,IAC5C0N,EAAkBwL,EAAS3c,UACjC,MAAO,IACF0c,EACH/b,UACAjB,UAAWd,EAAUkB,SACrB4U,UAAU,EACVtV,cACAQ,KAAM,IACD+c,EACH3c,UAAWmR,GAEb2C,gBACA3C,gBAAiB0B,EACb1B,EACAuL,EAAWvL,gBAEnB,CAEA,MCtEayL,GAAoBC,GAC/BA,aAAarP,cAA2B,eAAXqP,EAAE7a,K,4ECsFjC,OA3EA,cAAoC,GAKlCoE,YACEpE,EACA8a,EACA7T,EACAmI,GACA,SACE2L,GAGE,CAAEA,SAAU,IAEhBzF,MAAMtV,EAAMiH,EAAMmI,GAElB,MAAM,MAAE5G,EAAK,YAAEwS,GCPiB,EAClC/S,EACAkN,EACA8F,EAAiC,CAAC,KAElC,MAAM,WACJH,EAAU,SACVC,EAAW,EAAC,gBACZG,EAAe,QACfC,EAAO,aACPC,EAAe,EAAC,SAChBhG,GACE6F,EAEEI,EAAkB,IAAI7F,EAAA,EAEtB8F,EAAqBD,EAAgBxT,MACzC,EAAA4N,GAAA,GAAU,OACV,EAAAf,GAAA,IAAU,KAAM,EAAA6G,GAAA,GAAST,GAAYjT,MAAK,EAAA4N,GAAA,GAAU,IAAI,EAAA+F,GAAA,GAAMT,OAsBhE,MAAO,CACLvS,MApBc0M,GACdjN,EACAqT,EAAmBzT,MACjB,EAAA+B,EAAA,IAAI,IAAMsR,GAAmBA,OAC7B,EAAAO,GAAA,IAAW,IACTtG,EAAkBtN,MAChB,EAAA6T,GAAA,GAAM,CACJF,MAAQ5gB,IACN,GAAQU,IAAI,QAASV,GACrBugB,GAAWA,EAAQvgB,IACZ,EAAA2gB,GAAA,GAASH,WAMzBlY,GAAkBkS,IAAWlS,KAK9B8X,YAAa,KAGXK,EAAgB5kB,MAAM,EAEzB,EDxCgCklB,CAC7BtX,KAAK4D,gBAEL,EAAAqP,GAAA,IAAM,KAAM,EAAA5Y,GAAA,GAAK2F,KAAKuX,YACtB,CACEd,aACAC,WAEAI,QAAUvgB,IACRyJ,KAAK+P,SAASK,KAAK,OAAOzU,UAAcpF,EAAMqO,YAC9C5E,KAAK6C,UAAUhC,WAAW,QAAStK,EAAMqO,WAAW,EAEtDmM,SAAWlS,IACTmB,KAAK+P,SAASK,KAAK,OAAOzU,kBAAqBkD,KAC/CmB,KAAK6C,UAAUhC,WAAWhC,EAAgB,cAAgB,WAAW,IAK3EmB,KAAKmE,MAAQA,EACbnE,KAAK2W,YAAcA,CACrB,CAEOpG,UACLvQ,KAAK6P,iBAAiB2B,QACtBxR,KAAK2W,gBACL3W,KAAK+P,SAASK,KAAK,OAAOpQ,KAAKrE,oBACjC,CAEO0J,QAEL,OADArF,KAAKmE,MAAM7N,WAAU,IAAM0J,KAAK6C,UAAUhC,WAAW,YAC9Cb,IACT,CAEA,eACE,MAAMuB,GAAS,UAAMvB,KAAKuB,QAC1BvB,KAAKwQ,sBACL,UACQxQ,KAAKwX,KAAKjW,EAClB,CAAE,MAAOiV,GACP,MAAMiB,EAAYlB,GAAiBC,GAQnC,GAPAxW,KAAK+P,SAASK,KACZ,OAAOpQ,KAAKrE,QAAQ4F,EAAOgL,8BAA8BkL,MACzD,CACElhB,MAAOigB,KAINiB,EACH,MAAMjB,CAEV,CACF,GE2HF,OAlLA,cAAgC,GACpBrG,4BAA4BvN,GAmBpC,OAlBuB,EAAAiB,EAAA,GAAc,CACnCjB,EAAKW,YACLX,EAAKkB,cACLlB,EAAKsN,QAAS1M,MACZ,EAAA9L,EAAA,IAAK6J,GAAWA,EAAOgL,aACvB,EAAAkE,GAAA,MAEFzQ,KAAK+K,kBAAmBnH,iBACvBJ,MACD,EAAA9L,EAAA,IACE,EAAEqM,EAAYC,EAAcuI,EAAWmL,QACnC1T,GACAD,GACA2T,GACAnL,KAKV,CAEA,WAAqBhL,GACnB,MAAM,UAAEgL,GAAchL,GAChB,OAAE0F,GAAWjH,KAAK6P,gBACxB7P,KAAK6C,UAAUhC,WAAW,cAE1B,MAAM8W,QAA0B3X,KAAK0D,GAAIgJ,eAAe,CACtDpS,QAASiS,EACTlT,UAAWd,EAAUkB,WAGjBqR,EAAkB6M,EAAkBrK,GAAG,IAAIxC,iBAAmB,EAG9D8M,OjBfmBxjB,OAC3BqG,EACA4P,EACAb,EACAH,KAEA,MAAMuC,QAAYxC,GAAoBC,GAAaI,QAGjDnC,GAAiC,CACjC7M,UACAod,eAAgBxN,EAChB1Q,WAAW,SAAgB6P,KAG7B,OAAOoC,EAAIkM,qBAAqBhE,WAAWC,KAAK,EiBAnBgE,CACzBxL,EACA,CAAClK,IACDyI,EACA7D,GAaF,GAVAjH,KAAK+P,SAASK,KACZ,uBAAuB7D,WAAmBqL,KAE5C5X,KAAK4P,gBAAgBvK,MAAMuS,EAAeD,EAAkBva,QAC5D4C,KAAK6C,UAAUhC,WACb,cACA,eACAb,KAAK4P,gBAAgB9O,UAGnB8W,EAAe,EAAG,CAEpB,MAAMI,QAA6BhY,KAAKiY,eACtC1L,EACAzB,EACA7D,GAIF0Q,EAAkBnb,QAAQwb,EAC5B,OACMhY,KAAKkY,cAAc3L,EAAYoL,EAAmB1Q,EAC1D,CAEA,qBACEsF,EACAzB,EACA7D,GAEA,MAAMkR,QAA4B3N,GAChC+B,EACA,CAAClK,IACDyI,EhBrGyB,IgBuGzB9K,KAAK6P,iBAAiB5I,QAGlBmR,EAA6B,GAC7BC,QAA0BrY,KAAK0D,GAAIgJ,eAAe,CACtDpS,QAASiS,EACTlT,UAAWd,EAAUkB,WAEjB6e,EAAuB,IAAItV,IAC/BqV,EAAkB3gB,KAAKgG,GAAM,CAACA,EAAEvD,GAAIuD,MAGtC,gBAAiB6a,KAAeJ,EAAqB,CACnDnY,KAAK6C,UAAUhC,WACb,cACA,yBACAb,KAAK4P,gBAAgBZ,cAAc,IAErC,MAAMwJ,EAAqBD,EAAY7gB,IAAI2d,IAAa3d,KAAK0B,IAC3D,MAAM,UAAEO,EAAS,GAAEH,GAAOJ,EACpB0R,GAAkB,EAAAlE,GAAA,IAAgBjN,GAGlC8e,EAAoBH,EAAqBjiB,IAAImD,IAC/C,EAAAoN,GAAA,IAAgBjN,GAChB,EAGJ,MAAO,CACLW,QAASiS,EACTpS,GAAIX,EACJH,UAAWd,EAAUkB,SACrBqR,gBAAiB2N,EACjBhL,cAAe3C,EACf/R,YAAa,EACbsV,UAAU,EACV9U,KAAM,IAAKH,EAAMO,UAAWmR,GAC7B,IAGC0N,EAAmBpb,OAAS,UACxB2J,GACJ/G,KAAK0D,GAAI4K,cACTrH,EAFIF,CAGJyR,GACFJ,EAAU5b,QAAQgc,GAEtB,CAEA,OAAOJ,CACT,CAEA,oBACE7L,EACAE,EACAxF,GAEA,MAAMyR,EAAoC,GAG1C,UAAWtL,KAAYX,EAAW,CAChC,MAAM,GAAEtS,EAAE,gBAAE2Q,GAAoBsC,EAEhCpN,KAAK6C,UAAUhC,WACb,cACA,4BACAb,KAAK4P,gBAAgBZ,cAAc,IAGrC,MAAM2J,QAAqB9N,GACzB1Q,EACA2Q,EACA9K,KAAK+K,kBACL9W,EAAcmO,OACdpC,KAAK6P,iBAAiB5I,QAGxB,GAAI0R,EAAavb,OAAS,EAAG,CAC3B,MAAMsN,EAAQiO,EAAajhB,IAAIoP,UAIzB9F,EACJ0J,GACCA,GAAU3D,GAAe/G,KAAK0D,GAAIgR,cAAezN,EAAvCF,CAA+C2D,I1BrL/B,K0ByL7B,MAAM0D,EAAUgI,GAAyBhJ,EAAU1C,EAAO6B,GAE1DmM,EAAiBlc,KAAK4R,EACxB,CACF,CAEIsK,EAAiBtb,OAAS,SACtB2J,GAAe/G,KAAK0D,GAAI4K,cAAerH,EAAvCF,CAA+C2R,GAEvD1Y,KAAKY,WAAWL,gBAAgBmY,EAClC,GC0BF,OAlMA,cAAgC,GAG9B3Y,YACEpE,EACA8a,EACA7T,EACAmI,GACA,SAAE2L,GAAmC,CAAEA,SAAU,IAEjD,IAAK9T,EAAKgW,YACR,MAAM,IAAI1V,MAAM,2BAGlB+N,MAAMtV,EAAM8a,EAAY7T,EAAMmI,EAAmB,CAC/C2L,aAdJ,KAAUmC,WAA8B,EAgBxC,CAEU1I,4BAA4BvN,GACpC,MAAMkW,EAAyB,IAAI/V,EAAA,GAAyB,GAC5DH,EAAKsN,SACD1M,MACA,EAAA9L,EAAA,IAAK6J,GAAWA,EAAOgL,aACvB,EAAAkE,GAAA,MAEDna,WAAU,KACTwiB,EAAuB1mB,MAAK,EAAM,IAGtCwQ,EAAKgW,YAAatiB,WAAWuiB,IAC3B7Y,KAAK6Y,WAAaA,EAClBC,EAAuB1mB,MAAK,GAE5B4N,KAAKuQ,SAAS,IAkBhB,OAfuB,EAAA1M,EAAA,GAAc,CACnCjB,EAAKW,YACLX,EAAKsN,QACLlQ,KAAK+K,kBAAmBnH,eACxBkV,IACCtV,MACD,EAAA9L,EAAA,IACE,EAAEqM,EAAYxC,EAAQkQ,EAAsBsH,OACxChV,KACAxC,EAAOgL,aACPkF,GACFsH,IAKR,CAEA,WAAqBxX,GACnB,MAAM,OAAE0F,GAAWjH,KAAK6P,gBAExB7P,KAAK6C,UAAUhC,WAAW,cAAe,gBACzC,MAAM,UAAE0L,GAAchL,GAEhB,WAAEsX,GAAe7Y,KAEvBA,KAAK6C,UAAUhC,WAAW,cAE1Bb,KAAK+P,SAASK,KACZ,qBAAqB7D,WAAmBsM,EAAWzb,SACnD,CACE0V,KAAM,eACNje,KAAMgkB,IAIV7Y,KAAK4P,gBAAgBvK,MAAMwT,EAAWzb,QACtC4C,KAAK6C,UAAUhC,WACb,cACA,UACAb,KAAK4P,gBAAgB9O,UAIvB,UAAWiL,KAAQ8M,QAEX7Y,KAAKyT,UAAUlH,EAAYR,EAAM9E,EAE3C,CAEA,gBACEsF,EACA9R,EACAwM,GAEA,IAAI+R,EAAc,GAClB,IACEhZ,KAAK6C,UAAUhC,WACb,cACA,iBAAiBpG,OACjBuF,KAAK4P,gBAAgB9O,UAEvB,MAAM,cAAE2M,EAAa,YAAE1U,EAAW,KAAEQ,SAAeyG,KAAK0D,GAAIyP,cAC1D5G,EACA9R,IAGI,oBAAEqT,EAAsB,EAAC,uBAAED,EAAyB,GACxDtU,GAAQ,CAAC,EAELiQ,EAAgBqE,EAAyB,EAEzCoL,QAA2BzO,GAC/B/P,EACA8H,GACAiH,EjBjJuB,IiBmJvBvC,GAIF,gBAAiBiS,KAAcD,EAAoB,CACjDjZ,KAAK6C,UAAUhC,WACb,cACA,QAAQpG,OACRuF,KAAK4P,gBAAgBZ,cAAc,IAGrC,MAAMtE,EAAQwO,EAAWxhB,IAAIoP,KAErB2G,cAAe0L,EAAkBpgB,YAAaqgB,GACpDrD,GAAgBrL,EAAO6B,EAAWkB,EAAe1U,GAInD,GAAI2R,EAAMtN,OAAS,EAAG,CACpB,MAAMkZ,EAAWjB,GAAY3K,EAAM4C,IAAI,IACjC+L,EAA4B/C,EAAU3c,gBAEtCoN,GAAe/G,KAAK0D,GAAIgR,cAAezN,EAAvCF,CAA+C2D,GAErD,MAAMzR,EAAYyR,EAAMhT,KAAK8T,GAAMA,EAAEhS,WAC/BwG,KAAK+K,kBAAmBI,aAC5BlS,EACAR,EAAiBgB,SACjBxF,EAAc6gB,MAGhB,MAAMnB,EAAc,CAClBrZ,QAASiS,EACTlT,UAAWd,EAAUe,KACrBa,GAAIM,EACJqQ,gBAAiB5B,KAAK0E,IACpByL,EACAvL,GAEF/U,YAAaqgB,EACb3L,cAAe0L,EACf9K,UAAU,EACV9U,KAAM,IACD+c,EACHzI,uBAAwBwL,EACxBvL,8BAIE/G,GAAe/G,KAAK0D,GAAI4K,cAAerH,EAAvCF,CAA+C4M,GAErDqF,EAAYxc,KAAKmX,EACnB,CACF,CACF,CAAE,MAAOhP,GAIP,GAHA3E,KAAK+P,SAASxZ,MAAM,qBAAqBkE,UAAiB,CACxDlE,MAAOoO,IAEJ4R,GAAiB5R,GAIpB,MADAqU,EAAc,GACRrU,EAHN3E,KAAK6C,UAAUhC,WAAW,QAAS8D,EAAIC,WAK3C,CAAE,QAEA5E,KAAKY,WAAWL,gBAAgByY,EAClC,CACF,G,iCCvMF,MAAMjJ,IAAW,QAAoB,CACnCC,OAAQ,OACR8C,KAAM,6BAIKwG,GAA4B,CACvCC,EACA9e,EACA+e,EACAvS,IAEO,IAAI/Q,EAAA,GAAiC8b,IAC1CA,EAAW5f,KAAK,CAAE4J,OAAQ,QAASiF,MAAO,KAE1C,WACE,MAAMwY,QAAwBF,EAAMG,aAAajf,GAEjDuX,EAAW5f,KAAK,CAAE4J,OAAQ,MAAOiF,MAAOwY,IAExC,MAAME,EAAsB,IAAI3W,IAC9ByW,EAAgB/hB,KAAKkiB,GAAM,CAACA,EAAEngB,SAAUmgB,MAGpCC,EAAwB7X,GAC5B2X,EAAoBtjB,IAAI2L,IAAQ,CAC9B1H,QAASG,EACTkB,KAAM,GACNme,WAAW,EACXC,UAAU,GAGRC,OC5CmB5lB,OAC7BqG,EACAwM,KAEA,MAAMiL,QAAiB,QAAgB,CACrCrK,OAAQ,CACN,CACE2N,IAAK,mBACL1gB,MAAO2F,GAET,CACE+a,IAAK,yBACL1gB,MAAOwN,KAGX2X,WAAY,CACVvQ,MAAO,KAETwQ,OAAQ,CACNjT,YAIJ,OAAKiL,EAASiI,YAAY/c,OAInB8U,EAASiI,YAAYziB,KACzB0B,GAASA,GAAMkN,IAAI6B,KAAKJ,SAAS,GAAG2C,MAAM,GAAGlR,KAJvC,EAKR,EDe6B,CAAgBiB,EAASwM,GAC7CmT,OCbgBhmB,OAC1BqG,EACAwM,KAEA,MAAMoT,QAAoB,QAAY5f,GAEhCyX,QAAiB,QAAgB,CACrCrK,OAAQ,CACN,CACE2N,IAAK,yBACL1gB,MAAOwN,IAET,CACEkT,IAAK,uBACL1gB,MAAOulB,IAGXJ,WAAY,CACVvQ,MAAO,KAETwQ,OAAQ,CACNjT,YAIJ,OAAKiL,EAASiI,YAAY/c,OAInB8U,EAASiI,YAAYziB,KAAK0B,GAASA,GAAMkN,IAAI6B,KAAKJ,SAAS,GAAGpN,SAH5D,EAGmE,EDhBhD,CAAaF,EAASwM,GAExCqT,EAAkBN,EAAY9e,QACjC8G,IAASyX,EAAgBhe,MAAMiC,GAAMA,EAAEjE,WAAauI,GAAOtE,EAAEoc,cAG1DS,EAAsBH,EAAUlf,QACnC6Q,IAAU0N,EAAgBhe,MAAMiC,GAAMA,EAAE/C,SAAWoR,GAAQrO,EAAEqc,aAGhEhK,GAASK,KACP,uBAAuB3V,wBAA8Bgf,EAAgBrc,yBAAyBkd,EAAgBld,yBAAyBmd,EAAoBnd,UAG7J,MAAMod,QAA2B/lB,QAAQqQ,IACvCyV,EAAoB7iB,KAAItD,MAAOuG,IAC7B,MAAMqH,QAAY,QAAYrH,GAExB8f,EAAgB,IACjBZ,EAAqB7X,GACxBvI,SAAUuI,EACVrH,SACAof,UAAU,GAKZ,aAFMR,EAAMmB,aAAaD,GACzBd,EAAoB1jB,IAAI+L,EAAKyY,GACtBA,CAAa,KAIxBzI,EAAW5f,KAAK,CAAE4J,OAAQ,MAAOiF,MAAOuZ,UAElC/lB,QAAQqQ,IACZwV,EAAgB5iB,KAAItD,MAAO4N,IACzB,MAAMrH,SAAgB6e,EAAoBxX,EAAK/N,EAAc0mB,UACzDzlB,QAAQuN,YACZ,GAAI9H,GAAUA,EAAOrC,MAAM,OAAgB,CACzC,MAAMmiB,EAAgB,IACjBZ,EAAqB7X,GACxBrH,SACAlB,SAAUuI,EACV8X,WAAW,SAGPP,EAAMmB,aAAaD,GACzBd,EAAoB1jB,IAAI+L,EAAKyY,GAC7BzI,EAAW5f,KAAK,CAAE4J,OAAQ,MAAOiF,MAAO,CAACwZ,IAC3C,MAIJ1K,GAASK,KAAK,uBAAuB3V,WAMrCuX,EAAW5f,KAAK,CAAE4J,OAAQ,WAAYiF,MAAO,KAE7C+Q,EAAWxb,UACZ,EA/ED,GA+EK6N,OAAOM,IACVoL,GAASxZ,MAAM,uBAAuBkE,WAAkB,CAAElE,MAAOoO,IACjEqN,EAAWzb,MAAMoO,EAAI,GACrB,I,gBEhBN,OApFA,MAOE5E,YACEgL,EACAxH,GARF,KAAQtD,QAAU,IAAIC,iBAAiBJ,GAUrCE,KAAK+K,kBAAoBA,EACzBxH,EAAYjN,WAAWsa,IACrB5Q,KAAKuD,YAAYnR,KAAKwe,EAAE,IAE1B5Q,KAAKuD,YAAc,IAAIR,EAAA,OAA+B,GAEtD/C,KAAKC,QAAQtL,UAAaC,GAAUoL,KAAK4a,UAAUhmB,GAEnDoL,KAAKC,QAAQ4a,eAAkBjmB,GAC7B,GAAQ2B,MAAM,GAAGuJ,UAA2BlL,EAChD,CAEA,wBACE,OAAO,IAAIH,SAASC,IAClB,MAAM6kB,EAAQvZ,KAAKuD,YAAYW,WAC3BqV,GACF7kB,EAAQ6kB,GAGVvZ,KAAKuD,YACFC,MACC,EAAAC,EAAA,IAAO3O,QAAoB,IAAVA,KAElBwB,WAAWxB,IACVJ,EAAQI,EAAe,GACvB,GAER,CAEA,gBAAwB4V,GACtB,MAAM6O,QAAcvZ,KAAK8a,wBACPvB,EAAM7E,cAAchK,EAExC,CAEA,oBAA4B7I,GAC1B,IACE,MAAM0X,QAAcvZ,KAAK8a,kBACnBC,E3BrDuB,CAACthB,IAClC,MAAM,IAAEuI,EAAG,KAAEzI,EAAI,YAAEkJ,GAAgBhJ,GAC7B,KAAE6J,EAAI,KAAE0X,EAAI,KAAEhjB,EAAI,OAAEijB,EAAM,UAAEC,GAAc3hB,EDR3C,IAAuB4M,ECe5B,MAAO,CACLnE,MACAsB,KAAMA,GAAQ,EACd0X,KAAMA,GAAQ,UACdhjB,OACAsM,KATW7B,GDXe0D,EA8BvB,SAAkCgV,GAEvC,IAAI7W,EAAO6W,EAASjG,QAAQ,eAAgB,IAiC5C,OA9BA5Q,EAAOA,EAAK4Q,QAAQ,oBAAqB,MACzC5Q,EAAOA,EAAK4Q,QAAQ,iBAAkB,MACtC5Q,EAAOA,EAAK4Q,QAAQ,eAAgB,MAGpC5Q,EAAOA,EAAK4Q,QAAQ,yBAA0B,MAC9C5Q,EAAOA,EAAK4Q,QAAQ,kBAAmB,IAGvC5Q,EAAOA,EAAK4Q,QAAQ,iBAAkB,IAGtC5Q,EAAOA,EAAK4Q,QAAQ,oBAAqB,MAGzC5Q,EAAOA,EAAK4Q,QAAQ,qBAAsB,MAG1C5Q,EAAOA,EAAK4Q,QAAQ,YAAa,IAGjC5Q,EAAOA,EAAK4Q,QAAQ,iBAAkB,IAGtC5Q,EAAOA,EAAK4Q,QAAQ,iBAAkB,IAGtC5Q,EAAOA,EAAK4Q,QAAQ,UAAW,QAC/B5Q,EAAOA,EAAK4Q,QAAQ,aAAc,IAE3B5Q,CACT,CCtDoB8W,CAAyB3Y,GDXpC0D,EAAO+O,QAAQ,KAAM,MCYxB,GAQFmG,WAAYH,IAAc,EAC1BD,OAAQA,GAAU,EACnB,E2BoCkBK,CAAoBzZ,UACd0X,EAAMgC,aAAaR,IAC7BS,UACH5Z,GAA8BC,EAExC,CAAE,MAAO2U,GAOP,MANA,GAAQvf,IACN,oBACA4K,EACAA,EAAQY,YACR+T,EAAE5R,YAEE4R,CACR,CACF,CAEA,iBAAyB3hB,SAEjBmL,KAAK8a,kBAEX9a,KAAK+K,kBAAkBrJ,QAAQmU,MAAMC,QAAQjhB,GAAQA,EAAO,CAACA,GAC/D,CAEQ+lB,UAAUlf,GAChB,MAAM,KAAE1D,EAAI,KAAEnD,GAAS6G,EAAI7G,KACd,SAATmD,EACFgI,KAAKyb,UAAU5mB,GACG,aAATmD,EACTgI,KAAK0b,cAAc7mB,GACD,SAATmD,GACTgI,KAAK2b,WAAW9mB,EAEpB,GCvEF,MAAM,IAAW,QAAoB,CAAEmb,OAAQ,SAGxC,MAAM4L,GAOX7b,YAAY6C,GAJZ,KAAQhC,WAAa,IAAI,EAEzB,KAAQib,MAAsD,CAAC,EAG7D,MAAM,YAAEtY,EAAW,cAAEO,GAAkBlB,EAEjCmI,EAAoB,IAAI,GAAuBnI,GAAMyC,QAErC,IAAIyW,GACxB/Q,EACAxH,GAGFvD,KAAK4D,gBAAiB,EAAAC,EAAA,GAAc,CAACN,EAAaO,IAAgBN,MAChE,EAAA9L,EAAA,IAAI,EAAEqM,EAAYC,OAAoBD,KAAgBC,KAGxDhE,KAAK4D,eAAetN,UAAU,CAC5BlE,KAAO8C,GACEA,GAAU8K,KAAKY,WAAWT,kBAAkB,OAAQ,WAE7D5J,MAAQoO,GAAQ3E,KAAKY,WAAWT,kBAAkB,OAAQ,QAASwE,KAGrE,MAAMoX,ECpCK,SACbnZ,GAEA,MAAM,YAAEW,EAAW,cAAEO,EAAa,QAAEoM,GAAYtN,EAC1C3C,EAAU,IAAI,EAEpB,OAAO,EAAA4D,EAAA,GAAc,CACnBN,EACA2M,EAAS1M,MACP,EAAA9L,EAAA,IAAK6J,GAAWA,EAAOgL,aACvB,EAAAkE,GAAA,MAEF3M,IACCN,MACD,EAAAtI,EAAA,IACE,EAAE6I,EAAYwI,EAAWvI,OACrBD,KAAgBC,KAAkBuI,KAExC,EAAA8D,GAAA,IAAU,EAAEkJ,EAAOhN,EAAWvI,MAC5B,MAAM,uBAAEf,GAA2BL,EACnC,IAAIoZ,EAA4B,GAChC,OAAO,IAAI9lB,EAAA,GAA4BC,IACrCA,EAAS/D,KAAK,IAEdknB,GACEC,EACAhN,EACAtJ,GACA3M,WAAU,EAAG0F,SAAQiF,YACrBhB,EAAQS,KAAK,CAAE1I,KAAM,iBAAkBlD,MAAO,CAAEkH,SAAQiF,WAEzC,UAAXjF,EACFggB,EAAY,GACH,CAAC,MAAO,YAAYvgB,MAAMsF,GAAMA,IAAM/E,KAC/CggB,EAAUxf,QAAQyE,GAGL,aAAXjF,IACF7F,EAAS/D,KAAK4pB,GACd7lB,EAASK,WACX,GACA,GACF,IAGR,CDT2BylB,CAAqBrZ,GAC5CmZ,EAAezlB,WAAW0lB,IACxB,GAAS5L,KAAK,2BAAyB,CACrC0C,KAAM,YACNje,KAAMmnB,GACN,IAGJ,MAAMpD,EAAcmD,EAAevY,MACjC,EAAA9L,EAAA,IAAKkiB,GAAMA,EAAE1e,QAAQwC,GAAMA,EAAEoc,eAC7B,EAAApiB,EAAA,IAAKkiB,GAAMA,EAAEliB,KAAKgG,GAAMA,EAAE/C,YAK5B,IAAI,GAAqB,eAAgBiI,EAAMmI,GAAmB1F,QAElE,IAAI,GACF,Y/BtEoC,I+BwEpCzC,EACAmI,GACA1F,QAEF,IAAI,GACF,a/B5EkC,I+B8ElC,IAAKzC,EAAMgW,eACX7N,GAEA1F,OACJ,CAEOkL,QAAQ5U,GACbqE,KAAK6b,MAAMlgB,IAAO4U,SACpB,E,gHErEK,MAAM2L,GAAwB9nB,MACnC+nB,IAEA,IAAKA,EACH,MAAO,UAGT,MAAMC,QAAiB,SAAmBD,GAE1C,OAAOC,GAAUpB,MAAQ,cC5B3B,MAAMtX,GAAK,I,SAAI,IAAM,mBACrBA,GAAG2Y,QAAQ,GAAGC,OAAO,CACnBta,IAAK,MACL8X,UAAW,QAGb,UCkBA,OAHS,CAAEnjB,IApBGvC,MAAO4N,EAAama,KAG9B,UAFsB,GAAGI,MAAM,OAAOlmB,IAAI,CAAE2L,QAE9B,CACZ,MAAMwa,EAA6B,CACjCxa,MACAnN,KAAMsnB,GAER,GAAGI,MAAM,OAAO5lB,IAAI6lB,EACtB,GAWYnmB,IARFjC,MAAO4N,IAEjB,MAAMya,QAAgB,GAAGF,MAAM,OAAOlmB,IAAI,CAAE2L,QAG5C,OAAOya,GAAS5nB,MAAQ4nB,GAAS5a,OAAO,G,YClBrC,MAAM6a,GACX,iDAEWC,GAA2B,+CAA+CD,KAC1EE,GAA2B,mCAAmCF,KAI9DG,GAAoB,oCCajC,OAfqB,MACnB,MAAMC,EAAU,IAAI,MDDU,2BCY9B,MAAO,CAAEnmB,IATGvC,MACV2oB,IAEA,MAAMC,EACY,iBAATD,EAAoB,IAAIE,KAAK,CAACF,GAAO,YAAcA,EAC5D,OAAOD,EAAQnmB,IAAIqmB,EAAU,CAAEE,WAAY,EAAGC,WAAW,GAAQ,EAIrD1gB,OADCrI,MAAO4N,GAAgB8a,EAAQrgB,OAAOuF,GAC/B,EAGxB,G,kDCTA,SAASob,GAAgBC,EAAqBrlB,GAC5C,MAAMslB,EAAO,IAAIC,KAAK,CAACF,GAAU,CAAErlB,SACnC,OAAOwlB,IAAIJ,gBAAgBE,EAC7B,CAEA,SAASG,GAAcJ,EAAqBrlB,GAG1C,MADa,QAAQA,aADH,QAAwBqlB,EAAS,WAGrD,CAGO,MAAMK,GACX1C,IAEA,GAAIA,EAAM,CACR,GAAIA,EAAKnG,SAAS,SAChB,MAAO,QAGT,GAAImG,EAAKnG,SAAS,SAChB,MAAO,QAGT,GAAImG,EAAKnG,SAAS,QAChB,MAAO,MAEX,CACO,EAGH8I,GAAQ,+DAQP,MAMMC,GACX5C,IAEA,IAAKA,EACH,MAAO,QAGT,MAAM6C,EAAcH,GAAyB1C,GAC7C,OAAI6C,KAK8B,IAAhC7C,EAAK8C,QAAQ,gBACwB,IAArC9C,EAAK8C,QAAQ,mBAEN,QAEsB,IAA3B9C,EAAK8C,QAAQ,SACR,SAEgC,IAArC9C,EAAK8C,QAAQ,mBACR,MAEF,UAIIC,GAA0B3pB,MACrCyN,EACAG,EACAgc,KAGA,IAAKnc,IAAYA,GAAS3M,OACxB,MAAO,CACL+oB,SAAS,EACT3Z,KAAMtC,EAAI4C,WACV5C,OAIJ,MAAM,OAAE9M,EAAM,KAAEqE,GAASsI,GAEnB,KAAEmZ,EAAI,YAAExY,GAAgBjJ,EAE9B,IAAKyhB,EACH,MAAO,CACLhZ,MACAic,SAAS,EACT3Z,KAAM,yBAAyBtC,EAAI4C,cAGvC,MAEMsN,EAA+B,CACnCtH,KAAM,SAAS5I,IACfic,SAAS,EACTjc,IALiBH,EAAQG,IAMzBhK,KAAMwK,GAGR,GAAIkb,GAAyB1C,GAC3B,MAAO,IAAK9I,EAAU+L,SAAS,GAGjC,MAAMZ,EACc,iBAAXnoB,OL1BsBd,OAC/B8d,EACA8L,KAEA,IAAIE,EAAkB,EACtB,IACE,GAAIhM,aAAoBiM,WAEtB,OADAH,GAAcA,EAAW9L,EAASkM,YAC3BlM,EAET,MAAMmM,EAA4B,GAElC,GAAInM,aAAoBoM,eAAgB,CACtC,MAAMC,EAASrM,EAASsM,YAElBC,EAAarqB,OACjBG,OACAO,WAEIP,GACK,QAAiB8pB,IAG1BA,EAAO7hB,KAAK1H,GACZopB,GAAmBppB,EAAOspB,WAC1BJ,GAAcA,EAAWE,GAClBK,EAAOG,OAAOjoB,KAAKgoB,IAK5B,aAFoCF,EAAOG,OAAOjoB,KAAKgoB,EAGzD,CAEA,GAAIpqB,OAAOC,iBAAiB4d,EAAU,CACpC,MAAMqM,EAASrM,EAAS7d,OAAOC,iBAM/B,gBAAiBqqB,KAASJ,EACpBI,aAAiBR,aACnBE,EAAO7hB,KAAKmiB,GACZT,GAAmBS,EAAMP,WACzBJ,GAAcA,EAAWE,IAI7B,OADe,QAAiBG,EAElC,CACA,MACF,CAAE,MAAO9nB,GAQP,YAPA,GAAQA,MACN,gEACAA,EAMJ,GKlCYqoB,CAAkB1pB,EAAQ8oB,GAChC9oB,EAEA2pB,EAAkC,iBAAZxB,EAG5B,IAAKA,EACH,MAAO,IACFnL,EACH+L,SAAS,EACT3Z,KAAM,2BAA2BtC,EAAI4C,cAKzC,GAAsB,SAAlBsN,EAASla,KAAiB,CAE5B,IAAK6mB,GAAgB,KAAMC,GAAOzkB,KAAKgjB,IACrC,MAAO,IACFnL,EACHla,KAAM,QACN6J,QAAS4b,GAAcJ,EAAS,kBAIpC,MAAMpI,EAAM4J,EAAexB,GAAU,QAAwBA,GAE7D,OAAIpI,EAAI3c,MAAM,OACL,IACF4Z,EACHla,KAAM,MACN6J,QAASoT,GAGTA,EAAI3c,MAAM,OACL,IACF4Z,EACHla,KAAM,OACN6J,QAASoT,GAtHjB,SAAgB9O,GACd,MAAM4Y,EAAY5Y,EAAO6Y,OAAOxjB,MAAM,EAAG,KACzC,OAAOmiB,GAAMsB,KAAKF,EACpB,CAsHQG,CAAOjK,GACF,IACF/C,EACHla,KAAM,OACNimB,SAAS,EACTpc,QAASG,EAAI4C,YAMV,IACFsN,EACHtH,KAAMqK,EAAI7X,OAAS,GAAK,SAAS4E,IAAQ,WAAWiT,IACpDjd,KAAM,OACNsM,KAAM4B,GAAc+O,GACpBpT,QAASoT,EAEb,CAEA,IAAK4J,EAAc,CACjB,GAAsB,UAAlB3M,EAASla,KACX,MAAO,IAAKka,EAAUrQ,QAAS4b,GAAcJ,EAASrC,IAExD,GAAsB,QAAlB9I,EAASla,KACX,MAAO,IACFka,EACHrQ,QAASub,GAAgBC,EAASrC,GAClCiD,SAAS,EAGf,CAEA,OAAO/L,CAAQ,EAiBJiN,GAAoB,CAC/BC,EACA5c,EACA6c,EAAgB,OAEhB,GAAKD,EAGL,MAAqB,iBAAVA,EACFA,EAAM5jB,MAAM,EAAG6jB,GAEjB7c,GAA+B,SAAhBA,GAClB,QAAwB4c,GAAO5jB,MAAM,EAAG6jB,QACxC,G,gBC3MN,MAAMC,GAAwBlrB,MAC5B4N,IAKA,MAAMnN,QAAa,GAAYwB,IAAI2L,GACnC,GAAInN,GAAQA,EAAKuI,OAAQ,CAEvB,MAAM4d,QAAakB,GAAsBrnB,GACnC2N,EAAcob,GAAsB5C,GAEpCvY,EAAc0c,GAAkBtqB,EAAM2N,GAS5C,MAAO,CAAEtN,OAAQL,EAAMmN,MAAKzI,KAPE,CAC5BvB,KAAM,OACNsL,KAAMzO,EAAKuI,OACX8d,UAAWrmB,EAAKuI,OAChB4d,OACAxY,eAEgC2P,OAAQ,KAAM1P,cAClD,CAEO,EAGH8c,GAA4B,CAChCvnB,KAAM,OACNsL,UAAM,EACN4X,eAAW,EACXD,YAAQ,GAGJuE,GAAuBprB,MAC3B4N,EACAyd,EACAxY,KAEA,GAAIwY,EAAM,CAER,aADoBA,EAAKC,KAAK1d,EAAK,CAAEiF,UAEvC,CACA,OAAOsY,EAAU,EAGbI,GAA2BvrB,MAC/B4N,EACAyd,EACAG,KAEA,MAAMC,EAAmBD,GAAc,IAAI9P,iBACrC,OAAE7I,GAAW4Y,EACnB,IAAIC,EAEJ,GAAKL,EAAL,CAKKG,IACHE,EAAQ7W,YAAW,KACjB4W,EAAiBrO,OAAO,GACvB,MAIL,IAEE,MAAMuO,EAAYnmB,KAAK4V,MACjBwQ,QAAcR,GAAqBxd,EAAKyd,EAAMxY,GAC9C1N,EAAOymB,EACPC,EAAgBrmB,KAAK4V,MAC3BjW,EAAK2mB,UAAYD,EAAgBF,EACjC,MAAMI,IAAcH,EAAM1c,MAAO0c,EAAM1c,KH1FT,IG6F9B,GAFAwc,GAASM,aAAaN,GAGf,cADCE,EAAMhoB,KAGV,MAAO,CAAEgK,MAAKqe,mBAAmB,EAAMlO,OAAQ,OAAQ5Y,KAAMymB,GAEtD,CAEP,MAAQlrB,MAAOwrB,SAAqBb,EACjCc,IAAIve,EAAK,CAAEiF,SAAQ7J,OAAQ,KAAMoE,OAAQ,IACzCnN,OAAOC,iBACPlC,OAEHmH,EAAKyhB,WAAakB,GAAsBoE,GACxC/mB,EAAKiJ,YAAcob,GAAsBrkB,EAAKyhB,MAC9C,MAAMwF,EACJR,EAAM1c,MAAQ0c,EAAM1c,MAAQ,GAAKgd,EAAWljB,QAAU4iB,EAAM1c,KAExDb,EAAc0c,GAAkBmB,EAAY/mB,EAAKiJ,aAEnDge,SACI,GAAY7pB,IAAIqL,GAAK,QAAiB,CAACse,KAI/C,MAAMG,EAASD,EACXF,EACAH,EACAV,EAAKc,IAAIve,EAAK,CAAEiF,gBAChB,EAcJ,OAZA1N,EAAKmnB,QAAU9mB,KAAK4V,MAAQyQ,GAIvB1mB,EAAKonB,OAASR,GACjBV,EAAKmB,IAAI5e,GAETzI,EAAKsnB,QAAUjnB,KAAK4V,MAAQjW,EAAKmnB,SAEjCnnB,EAAKsnB,SAAW,EAGX,CACL3rB,OAAQurB,EACRhe,cACAT,MACAzI,OACA4Y,OAAQ,OAGZ,CAEJ,CAAE,MAAO5b,GAEP,OADA,GAAQuqB,MAAM,iCAAkCvqB,GACzC,CACLyL,MACAqe,mBAAmB,EACnBlO,OAAQ,OACR5Y,KAAM,IAAKgmB,IAEf,CA/EA,MAFE,GAAQtoB,IAAI,8DAiFd,EAGI8pB,GAA8B3sB,MAClC4N,EACAyd,EACAG,EACAoB,KAGA,MAAMC,EAAoC,aAAnBxB,GAAMyB,SAEvBlB,EAAQiB,QACJzB,GAAqBxd,EAAKyd,EAAMG,GAAY3Y,QAClDsY,GAEE4B,EAAa,GAAGtE,WAA0B7a,IAC1CkQ,QAAiBkP,MAAMD,EAAY,CACvC7pB,OAAQ,MACR2P,OAAQ2Y,GAAY3Y,OACpB+Z,YAEF,GAAI9O,GAAYA,EAAS/J,KAAM,CAe7B,MAAMkZ,EAAgBhD,GACnB4C,EAEGxsB,QAAQC,UADR,GAAYiC,IAAIqL,GAAK,QAAiBqc,KAGtC,KAAErD,EAAI,OAAE9lB,EAAM,WAAEorB,SN9KnBlsB,eACLqsB,EACAa,GAEA,MAAOC,EAAkBC,GAAcf,EAAOgB,MACxCpD,EAA4B,GAG5BqD,EAAcH,EAAiB/C,aAC/B,MAAE1pB,SAAgB4sB,EAAYhD,OAC9B1D,EAAOlmB,QAAconB,GAAsBpnB,QAAS,EAEpD6sB,EAAaH,EAAWhD,YAExBoD,EAA2C,CAC/CxtB,OAAQC,OAAOC,iBACb,OAAa,CACX,MAAM,KAAEC,EAAMO,MAAAA,SAAgB6sB,EAAWjD,OACzC,GAAInqB,EAEF,YADA+sB,GAASA,EAAMjD,EAAQrD,IAGzBsG,GAASjD,EAAO7hB,KAAK1H,SACfA,CACR,CACF,GAGF,MAAO,CAAEkmB,OAAM9lB,OAAQ0sB,EAAetB,WAAYxrB,EACpD,CMiJ+C+sB,CACzC3P,EAAS/J,KACTkZ,GAGI7e,EAAcob,GAAsB5C,GAEpCvY,EAAc0c,GAAkBmB,EAAY9d,GAClD,MAAO,CACLR,MACAS,cACAlJ,KAAM,IAAKymB,EAAOhF,OAAMxY,eACxBtN,SACAid,OAAQ,UACRgP,aAEJ,CAEO,EAiCT,MAAMW,GAAiB1tB,MACrB4N,EACAyd,EACAG,EACAmC,KAEA,MAAMC,QAAsB1C,GAAsBtd,GAClD,QAAsB,IAAlBggB,EACF,OAAOA,EAGT,GAAIvC,EAAM,CACRsC,GAAsBA,EAAmB,6BAIzC,aAF0BpC,GAAyB3d,EAAKyd,EAAMG,EAGhE,CAEAmC,GAAsBA,EAAmB,+BAQzC,aAN6BhB,GAC3B/e,EACAyd,EACAG,EAGmB,EAqDjBqC,GAAkB7tB,MACtBqrB,EACA5d,KAEA,IAAIG,EAQJ,OAPIyd,IACFzd,QAAYyd,EAAK9oB,IAAIkL,IAGvB,GAAalL,IAAIkL,GAEjBG,SAAc,GAAYrL,IAAIqL,OD7IG5N,OACjCyN,GAEO,IAAIsc,WACU,iBAAZtc,EACHid,GAAOzkB,KAAKwH,SACNA,EAAQqgB,eCuIqBC,CAAoBtgB,IACtDG,CAAG,E,gBC5VL,MAAMogB,GAKXriB,YAAYsiB,EAAyBnV,GACnClN,KAAKqiB,SAAWA,EAChBriB,KAAKkN,MAAQA,CACf,CAEAoV,cAAcnQ,GACZ,MAAM5L,EAAQvG,KAAKkN,MAAM4Q,QAAQ3L,GACjC,OAAO5L,EAAQvG,KAAKkN,MAAM9P,OAAS4C,KAAKkN,MAAM3G,EAAQ,QAAK,CAC7D,ECfK,MAAMgc,WAA8Brf,MACzCnD,YAAYyiB,GACVvR,MAAM,iBAAiBuR,KACvBvqB,OAAOwqB,eAAeziB,KAAMuiB,GAAsB9M,UACpD,ECHK,MAAMiN,GACC,eAGP,IAAKC,GAAL,CAAKA,IACVA,EAAA,aAAe,gBADLA,GAAL,CAAKA,IAAA,I,YC8CZ,SAASC,GAA0BxpB,GACjC,OAAQA,EAAK+I,UAAY,IAAM/I,EAAKypB,kBAAoB,EAC1D,CAEA,MAIMC,GAAa,CACjBC,SAAU,IAAIX,GACZ,CACE1e,GAAI,CAAEsf,QAAS,IAAMC,wBAAyB,KAC9CxD,KAAM,CAAEuD,QAAS,IAAWC,wBAAyB,IACrDhF,QAAS,CAAE+E,QAAS,IAAOC,wBAAyB,KAEtD,CAAC,KAAM,OAAQ,YAEjBC,SAAU,IAAId,GACZ,CACE1e,GAAI,CAAEsf,QAAS,IAAMC,wBAAyB,KAC9CxD,KAAM,CAAEuD,QAAS,IAAWC,wBAAyB,IACrDhF,QAAS,CAAE+E,QAAS,KAAOC,wBAAyB,KAEtD,CAAC,KAAM,UAAW,SAEpBE,MAAO,IAAIf,GACT,CACE1e,GAAI,CAAEsf,QAAS,IAAMC,wBAAyB,KAC9CxD,KAAM,CAAEuD,QAAS,IAAWC,wBAAyB,IACrDhF,QAAS,CAAE+E,QAAS,IAAOC,wBAAyB,KAEtD,CAAC,KAAM,OAAQ,aAganB,OA1ZA,MA+LEljB,YACE+D,GACA,SACEsf,EAAQ,gBACRC,IAlMJ,KAAQC,OAAS,IAAIvgB,EAAA,EAA0B,IAAIC,KAEnD,KAAQyc,UAAgC,EAMxC,KAAQ8D,iBAA2B3pB,KAAK4V,MAExC,KAAQvP,QAAU,IAAI,EAEtB,KAAQyF,UAAmD,CACzDhC,GAAI,IAAIiH,IACR8U,KAAM,IAAI9U,IACVsT,QAAS,IAAItT,KAyLb7G,EAAcxN,WAAWmpB,IACnBA,GACFzf,KAAKwjB,QAAQ/D,EACf,IAGFzf,KAAKojB,SAAWA,GAAYN,GAAWI,SACvCljB,KAAKqjB,gBAAkBA,GAxPD,IA4PtB,EAAAnM,GAAA,GA3P+B,MA4P5B1T,MACC,EAAAtI,EAAA,IACE,MACI8E,KAAKyf,QACL,IAAIzf,KAAKsjB,OAAOxuB,MAAMuJ,UAAUrB,MAAMU,GAAmB,SAAbA,EAAEyU,YAGrD7b,WAAU,KAMT0J,KAAKyf,KAAMgE,kBAAiB,EAAK,IAGrC,MAAM7f,GAAiB,EAAAC,EAAA,GAAc,CAACC,IAAgBN,MACpD,EAAA9L,EAAA,IAAI,EAAEsM,OAAoBA,GAAgBA,GAAc0f,aACxD,EAAAjT,GAAA,MACA,EAAA5K,EAAA,MAGFjC,EAAetN,WAAWuI,IACxBA,GAAiB,GAAQ5H,IAAI,6BAC7B+I,KAAKyf,MAAMgE,kBAAiB,EAAK,IAGnCzjB,KAAKsjB,OACF9f,MACC,EAAAmgB,GAAA,GAAe/f,IACf,EAAA1I,EAAA,IAAO,EAAE,CAAE2D,KAAmBA,KAC9B,EAAA+kB,GAAA,GAAa5jB,KAAKqjB,kBAClB,EAAA3rB,EAAA,IAAI,EAAE2L,KAAWrD,KAAK6jB,yBAAyBxgB,MAC/C,EAAAoC,EAAA,IAAUpC,IACR,MAAMygB,EAAY9jB,KAAK+jB,2BAA2B1gB,GAElD,OAAIygB,EAAU1mB,OAAS,GAErB4C,KAAKyf,MAAMgE,kBAAiB,IAErB,EAAAzQ,GAAA,MAAS8Q,EAAUpsB,KAAK0B,GAAS4G,KAAKgkB,WAAW5qB,OAEnD,GAAK,KAGf9C,WAAU,EAAG8C,OAAMqD,SAAQ0V,SAAQjd,aAClC,MAAM,IAAE8M,GAAQ5I,EACV6qB,EAAYjkB,KAAKsjB,OAAOxuB,MAAMuB,IAAI2L,IAAMiiB,WAAa,GAc3D,GAVAA,EAAUvsB,KAAKwsB,GAAaA,EAASliB,EAAKvF,EAAQ0V,EAAQjd,KAG3C,SAAXid,IACFnS,KAAKujB,iBAAmB3pB,KAAK4V,OAG/BxP,KAAK0F,UAAUyM,GAAQ/M,OAAOpD,GAGf,cAAXvF,GAAqC,cAAXA,EAE5BuD,KAAKmkB,cAAcniB,OACd,CAIL,MAAMoiB,EAAapkB,KAAKojB,SAASd,cAAcnQ,GAE3CiS,EACFpkB,KAAKqkB,oBAAoBjrB,EAAMgrB,IAE/BpkB,KAAKmkB,cAAcniB,GAEnBiiB,EAAUvsB,KAAKwsB,GACbA,EAASliB,EAAK,YAAamQ,EAAQjd,KAGzC,CAEA8K,KAAKskB,aAAa,GAExB,CArRQC,eAAenB,GACrBpjB,KAAKojB,SAAWA,CAClB,CAEA,cAAqB3D,EAAmB+E,GACtC,GAAQvtB,IACN,sBAAsB+I,KAAKyf,MAAMyB,UAAY,eAC3CzB,EAAKyB,YAGTlhB,KAAKyf,KAAOA,EACZzf,KAAKukB,eAAeC,GAAkB1B,GAAWrD,EAAKyB,UACxD,CAEQ6C,2BAA2B1gB,GACjC,MAAMwB,EAAe,IAAIxB,EAAMhF,UAAUnD,QACtCwC,GAAmB,YAAbA,EAAEjB,SAGLgoB,EAAkB,QAAW/mB,GAAMA,EAAEyU,QAAQtN,GAE7C6f,EAA8B,GAEpC,UAAYC,EAAa1jB,KAAUhJ,OAAO4H,QAAQ4kB,GAAkB,CAClE,MAEMG,EAFW5kB,KAAKojB,SAASf,SAASsC,GAG7B1B,wBACTjjB,KAAK0F,UAAUif,GAAkCrhB,KAC7CuhB,EAAkB5jB,EACrB3D,MACC,CAACC,EAAGC,IAAMolB,GAA0BplB,GAAKolB,GAA0BrlB,KAEpE/B,MAAM,EAAGopB,GAEZF,EAAeloB,QAAQqoB,EACzB,CAEA,OAAOH,CACT,CAEQJ,cACN,MAAMxrB,EAAU,WAAWkH,KAAKsjB,OAAOxuB,MAAMwO,gBAAgBtD,KAAK0F,UAAUhC,GAAGJ,eAAetD,KAAK0F,UAAU+Z,KAAKnc,kBAAkBtD,KAAK0F,UAAUuY,QAAQ3a,QAC3JtD,KAAKC,QAAQE,kBAAkB,OAAQ,UAAWrH,EACpD,CAEQkrB,WAAW5qB,GACjB,MAAM,IAAE4I,EAAG,OAAEmQ,EAAM,UAAE8R,EAAS,WAAErE,GAAexmB,EAEzCipB,EAAWriB,KAAKojB,SAASf,SAASlQ,GACxCnS,KAAK0F,UAAUyM,GAAQxb,IAAIqL,GAC3BhC,KAAKskB,cACL,MAAMQ,EAAY9kB,KAAKsjB,OAAOxuB,MAAMuB,IAAI2L,GAWxC,OATAhC,KAAKsjB,OAAOxuB,MAAMmB,IAAI+L,EAAK,IACtB8iB,EACHroB,OAAQ,YACRsoB,cAAenrB,KAAK4V,MACpBoQ,WAAY,IAAI9P,kBAGlBmU,EAAUvsB,KAAKwsB,GAAaA,EAASliB,EAAK,YAAamQ,MCjKpB6S,EDmKR5wB,SJgE/BA,eACE4N,EACAmQ,EACAyE,GAEA,MAAM,KAAE6I,EAAI,WAAEG,EAAU,QAAEoB,GAAYpK,EAEtC,IACE,OAAQzE,GACN,IAAK,KACH,OAAOmN,GAAsBtd,GAC/B,IAAK,OACH,OAAO2d,GAAyB3d,EAAKyd,EAAMG,GAC7C,IAAK,UACH,OAAOmB,GAA4B/e,EAAKyd,EAAMG,EAAYoB,GAC5D,QACE,OAEN,CAAE,MAAOxK,GAEP,YADA,GAAQvf,IAAI,6BAA8Buf,EAE5C,CACF,CIrFayO,CAAiBjjB,EAAKmQ,EAAQ,CACnCyN,aACAH,KAAMzf,KAAKyf,KACXuB,QAAS,CACP,CAAC0B,IAA2BC,GAAiBuC,gBAE9CzuB,MAAMoL,IAEHA,GAAsB,OAAXsQ,G7C5IY,CAACtQ,IAClCJ,GAAUC,QAAQ,CAChB1J,KAAM,WAENnD,KAAM,IAAKgN,EAAS3M,YAAQ,IAGnB,E6CsIHiwB,CAAoBtjB,GAGfA,KC/KN,IAAI3L,EAAA,GAAeC,IACxB6uB,IACGvuB,MAAMyb,IACL/b,EAAS/D,KAAK8f,GACd/b,EAASK,UAAU,IAEpB6N,OAAO9N,IACN,GAAQuqB,MAAM,gCAAiCvqB,GAC/CJ,EAASI,MAAMA,EAAM,GACrB,KDwKDiN,MACD,EAAAwf,GAAA,GAAQ,CACNoC,KAAM/C,EAASW,QACfqC,KAAM,KACJ,EAAAC,GAAA,IAAW,KACT1F,GAAYpO,MAAM,WAEX,IAAI+Q,GAAsBF,EAASW,eAGhD,EAAAtrB,EAAA,IAAKxC,IACI,CACLkE,OACAqD,OAAQvH,EAAS,YAAc,QAC/Bid,SACAjd,cAGJ,EAAAqwB,GAAA,IAAYhvB,GAENA,aAAiBgsB,IACZ,EAAAiD,GAAAA,IAAG,CACRpsB,OACAqD,OAAQ,UACR0V,WAIgB,eAAhB5b,GAAOoF,MACF,EAAA6pB,GAAAA,IAAG,CAAEpsB,OAAMqD,OAAQ,YAAa0V,YAElC,EAAAqT,GAAAA,IAAG,CAAEpsB,OAAMqD,OAAQ,QAAS0V,cCjNpC,IAAgC6S,CDoNrC,CAQQS,gBAAgBzjB,EAAa0jB,GACnC,MAAMriB,EAAQrD,KAAKsjB,OAAOxuB,MACpBsE,EAAOiK,EAAMhN,IAAI2L,GAKvB,OAJI5I,GACFiK,EAAMpN,IAAI+L,EAAK,IAAK5I,KAASssB,IAGxB1lB,KAAKsjB,OAAOlxB,KAAKiR,EAC1B,CAEQ8gB,cAAcniB,GACpB,MAAMqB,EAAQrD,KAAKsjB,OAAOxuB,MAC1BuO,EAAM+B,OAAOpD,GACbhC,KAAKsjB,OAAOlxB,KAAKiR,EACnB,CAGQghB,oBAAoBjrB,EAAiBgrB,GAC3ChrB,EAAK6qB,UAAUvsB,KAAKwsB,GAAaA,EAAS9qB,EAAK4I,IAAK,UAAWoiB,KAE/DpkB,KAAKylB,gBAAgBrsB,EAAK4I,IAAK,CAAEvF,OAAQ,UAAW0V,OAAQiS,GAC9D,CAEQP,yBAAyBxgB,GAmB/B,MAlBC,CAAC,OAAQ,WAAmClL,SAASga,IACpD0D,MAAMxb,KAAK2F,KAAK0F,UAAUyM,IAASha,SAAS6J,IAC1C,MAAM5I,EAAOiK,EAAMhN,IAAI2L,GACnB5I,GAAQwpB,GAA0BxpB,GAAQ,GAAKA,EAAKwmB,aAEtDxmB,EAAKwmB,WAAWpO,MAAM,aACtBpY,EAAK6qB,UAAUvsB,KAAKwsB,GAClBA,EAAS9qB,EAAK4I,IAAK,UAAW5I,EAAK+Y,UAGrC9O,EAAMpN,IAAI+L,EAAK,IAAK5I,EAAMqD,OAAQ,YAGlCuD,KAAK0F,UAAUyM,GAAQ/M,OAAOpD,GAChC,GACA,IAGGqB,CACT,CAEQsiB,iBAAiB3jB,GAEvB/J,OAAOC,KAAK8H,KAAK0F,WAAWvN,SAASqd,GACnCxV,KAAK0F,UAAU8P,GAA0BpQ,OAAOpD,IAEpD,CA6GON,QACLM,EACAkiB,EACAtN,EAA4B,CAAC,GAE7B,MAAMvT,EAAQrD,KAAKsjB,OAAOxuB,MACpB8wB,EAAeviB,EAAMhN,IAAI2L,GAK/B,GAAI4jB,EACF5lB,KAAKylB,gBAAgBzjB,EAAK,CACxBiiB,UAAW,IAAI2B,EAAa3B,UAAWC,SAEpC,CACL,MAAM/R,EAASyE,EAAQiP,eAAiB7lB,KAAKojB,SAASlW,MAAM,GACtD9T,EAAkB,CACtB4I,MACAiiB,UAAW,CAACC,GACZ/R,SACA1V,OAAQ,UACRqpB,gBAAgB,KACblP,GAGLsN,EAASliB,EAAK,UAAWmQ,GAEzB9O,EAAMpN,IAAI+L,EAAK5I,GACf4G,KAAKsjB,OAAOlxB,KAAKiR,EACnB,CACF,CAEO0iB,eACL/jB,EACA4U,EAA4B,CAAC,GAE7B,OAAO,IAAIniB,SAASC,IAOlBsL,KAAK0B,QAAQM,GANK,CAACA,EAAKvF,EAAQ0V,EAAQjd,KACvB,cAAXuH,GAAqC,cAAXA,GAC5B/H,EAAQ,CAAE+H,SAAQ0V,SAAQjd,UAC5B,GAG0B0hB,EAAQ,GAExC,CAEOoP,uBAAuBhkB,EAAa6gB,GACzC7iB,KAAKylB,gBAAgBzjB,EAAK,CAAE6gB,oBAC9B,CAEOoD,OAAOjkB,GACZ,MACM5I,EADQ4G,KAAKsjB,OAAOxuB,MACPuB,IAAI2L,GAEnB5I,IAGGA,EAAKwmB,WAGRxmB,EAAKwmB,WAAWpO,MAAM,aAFtBxR,KAAKmkB,cAAcniB,GAKzB,CAEOkkB,eAAeC,GACpB,MAAM9iB,EAAQrD,KAAKsjB,OAAOxuB,MAE1BuO,EAAMlL,SAAQ,CAACiB,EAAM4I,KACf5I,EAAK+sB,SAAWA,IAClBnmB,KAAK2lB,iBAAiB3jB,GACtB5I,EAAKwmB,YAAYpO,MAAM,aACvBnO,EAAM+B,OAAOpD,GACf,IAGFhC,KAAKsjB,OAAOlxB,KAAKiR,EACnB,CAEO+iB,QACL,MAAM/iB,EAAQrD,KAAKsjB,OAAOxuB,MAE1BuO,EAAMlL,SAAQ,CAACiB,EAAM4I,KACnBhC,KAAK2lB,iBAAiB3jB,GACtB5I,EAAKwmB,YAAYpO,MAAM,aACvBnO,EAAM+B,OAAOpD,EAAI,IAGnBhC,KAAKsjB,OAAOlxB,KAAK,IAAI4Q,IACvB,CAEOqjB,cACL,OAAOrmB,KAAKsjB,OAAOxuB,KACrB,CAEOwxB,eACL,OAAOzQ,MAAMxb,KAAK2F,KAAKsjB,OAAOxuB,MAAMuJ,SACtC,CAEOkoB,WAOL,OANW,OACT,OAAqB,OAAO,WAC5B,OACA,OAAM,OAAS,CAAC,SAAU,WAGrBC,CAAGxmB,KAAKsmB,eACjB,G,kCE7eK,MAAMG,GAAe1lB,GAAc,MAAItD,MAAMsD,GACvC2lB,GAAoB3lB,GAAc,SAASA,I,gBC+GxD,OAnGA,oBACE,KAASmgB,SAAyB,WAIlC,KAAQyF,QAA6B,CAAC,EAMtC,KAAQC,YAAsB,EAJ1B1M,aACF,OAAOla,KAAK2mB,OACd,CAIIjD,gBACF,OAAO1jB,KAAK4mB,UACd,CAEA,mBACE,MAAM1U,QAAiBlS,KAAKyf,KAAMvF,OAAO7jB,IAAI,qBAC7C,IAAK6b,EACH,MAAO,CAAE2U,WAAYhK,IAEvB,MAAMpiB,GAAU,SAAUyX,GAAoB4U,cAE9C,MAAO,CAAED,WAAY,UAAUpsB,EAAQA,WAAWA,EAAQtG,OAC5D,CAEAC,WAAWwiB,GACT5W,KAAKyf,MAAO,SAAiB7I,GAC7B5W,KAAK2mB,cAAgB3mB,KAAK+mB,aAEJ,oBAAXC,SACTA,OAAOvH,KAAOzf,KAAKyf,KACnBuH,OAAOC,MAAQR,IAEjB,GAAQxvB,IACN,2BACO+I,KAAKyf,KAAKyH,MAAMC,cAAczvB,KAAK6F,GAAMA,EAAEqH,cAEpD5E,KAAK4mB,YAAa,CACpB,CAEAxyB,WAAW4N,EAAa4U,EAAwB,CAAC,GAC/C,OAAO5W,KAAKyf,KAAM2H,MAAM1H,KAAKgH,GAAiB1kB,GAAM,IAC/C4U,EACHyQ,WAAW,EACX/jB,MAAM,IACL7M,MAAMvB,IACP,MAAM,KAAE8C,EAAI,KAAEsL,EAAI,UAAE4X,EAAS,MAAEyF,EAAK,OAAE1F,GAAW/lB,EACjD,MAAO,CACL8C,OACAsL,KAAMA,IAAS,EACf4X,UAAWA,IAAc,EACzBD,SACD,GAEL,CAEAsF,IAAIve,EAAa4U,EAAsB,CAAC,GACtC,OAAO5W,KAAKyf,KAAMc,IAAIkG,GAAYzkB,GAAM4U,EAC1C,CAEAxiB,UAAUyN,EAAwB+U,EAAwB,CAAC,GACzD,aAAc5W,KAAKyf,KAAM9oB,IAAIkL,EAAS+U,IAAU5U,IAAI4C,UACtD,CAEAxQ,UAAU4N,EAAa4U,EAAwB,CAAC,GAC9C,aAAc5W,KAAKyf,KAAMmB,IAAIjqB,IAAI8vB,GAAYzkB,GAAM4U,IAAUhS,UAC/D,CAEAxQ,iBACE,aAAc4L,KAAKyf,KAAMyH,MAAMI,SAAS5vB,KAAKkiB,GAAMA,EAAE2N,KAAK3iB,YAC5D,CAEAxQ,aAAc,CACdA,cAAe,CAEfA,kBAAkBqG,GAChB,MAAMsR,GAAO,SAAUtR,GAIvB,aAHMuF,KAAKyf,KAAM+H,UAAU7wB,IAAIoV,SAEzB/L,KAAKyf,KAAMyH,MAAMO,QAAQ1b,IACxB,CACT,CAEA2b,KACE,OAAO1nB,KAAKyf,KAAMmB,IAAI8G,IACxB,CAEAtzB,aACE,MAAM,SAAEuzB,SAAmB3nB,KAAKyf,KAAMO,MAAM4H,OAEtCC,QAAmB7nB,KAAKyf,KAAMtlB,MAC9B,aAAE2tB,EAAY,GAAE3tB,GAAO0tB,EAC7B,MAAO,CAAE1tB,GAAIA,EAAGyK,WAAYkjB,eAAcH,WAC5C,G,wKCvEF,MA+DMI,GAAoC,CACxC7K,WAAY,EACZC,WAAW,GAiLb,OA9KA,oBACE,KAAS+D,SAAyB,QAMlC,KAAQ0F,YAAa,EAJjB1M,aACF,MAAO,CAAE2M,WAAYhK,GACvB,CAII6G,gBACF,OAAO1jB,KAAK4mB,UACd,CAMAxyB,aACE,MAAM4zB,EAAa,IAAI,KAAc,kBAC/BA,EAAWC,OAEjB,MAAMC,EAAY,IAAI,KAAa,kBAC7BA,EAAUD,OAEhB,MAOME,OAnGY/zB,OACpB8zB,EACAE,EAA0B,WAEL,QAAa,CAChCF,YAOAG,WAAY,EACV,WACA,WACA,QAAO,CACLC,iBAAkB,CAChBC,WAAY,CACV,CACEC,KAAM,CACJ,+BACA,mCACA,yBACA,wCACA,6BAGJ,CACEC,WAAY,OACZC,SAAU,OACVF,KAAM,CAAC,yBAA0B,iCAKzC,WACA,QAAsB,CACpBG,eAAgB,KAGpBC,qBAAsB,EAAC,WACvBC,aAAc,EAAC,WACfC,gBAAiB,CACfC,kBAAmB,KACV,GAOXC,cAAe,EACb,QAAU,CACRrwB,KAAMyvB,KAGVa,SAAU,CACRC,UAAU,EAAAA,GAAA,SA0CSC,CAAcjB,EAPb,CACpB,kFACA,kFACA,kFACA,kFACA,+FAIFloB,KAAKyf,WAAa,QAAY,CAAEuI,aAAYE,YAAWC,WAEvDnoB,KAAKopB,IAAK,SAAOppB,KAAKyf,MAEA,oBAAXuH,SACTA,OAAOmB,OAASA,EAChBnB,OAAOvH,KAAOzf,KAAKyf,KACnBuH,OAAOoC,GAAKppB,KAAKopB,GACjBpC,OAAOC,MAAQR,IAIjB0B,EAAOkB,iBAAiB,gBAAiBC,IACvC,MAAMC,EAASD,EAAIE,OAAO5kB,WACpB6kB,EAAOtB,EAAOuB,eAAeH,IAAW,GACxCI,EAAmB1xB,OAAO2xB,YAC9BH,EAAK/xB,KAAKkiB,GAAM,CACdA,EAAEiQ,WAAWjlB,WACbgV,EAAEiQ,WAAWC,aAAapyB,KAAKkZ,IAAM,SAAUA,IAAIjV,WAGvD,GAAQmlB,MAAM,gBAAgByI,IAAUI,EAAiB,IAe3DxB,EAAOkB,iBAAiB,mBAAoBC,IAC1C,GAAQxI,MAAM,qBAAqBwI,EAAIE,OAAO5kB,aAAa,IAE7D,GAAQ3N,IACN,qBACAkxB,EAAO4B,gBAAgBryB,KAAK6F,GAAMA,EAAEqH,cAStC5E,KAAK4mB,YAAa,CACpB,CAEAxyB,WAAW4N,EAAa4U,EAAwB,CAAC,GAC/C,OAAO5W,KAAKopB,GAAI1J,KAAK+G,GAAYzkB,GAAM4U,GAASngB,MAAMvB,IACpD,MAAM,KAAE8C,EAAI,SAAEgyB,EAAQ,cAAEC,EAAa,OAAEhP,EAAM,QAAEiP,EAAO,MAAEC,GAAUj1B,EAClE,MAAO,CACL8C,OACAsL,KAAM0mB,IAAa,EACnB9O,UAAW+O,IAAkB,EAC7BhP,SACD,GAEL,CAEAsF,IAAIve,EAAa4U,EAAsB,CAAC,GACtC,OAAO5W,KAAKopB,GAAI7I,IAAIkG,GAAYzkB,GAAM4U,EACxC,CAEAxiB,UAAUyN,EAAwB+U,EAAwB,CAAC,GAEzD,MAAMwT,EAAY,IACbxT,KACAmR,IAGL,IAAI/lB,EAEJ,GAAIH,aAAmBob,KAAM,CAC3B,MAAMoN,EAAWxoB,EAAQlG,KACnBumB,QAAoBrgB,EAAQqgB,cAC5BrtB,EAAO,IAAIspB,WAAW+D,GAC5BlgB,QAAYhC,KAAKopB,GAAIkB,QACnB,CAAEC,KAAMF,EAAUxoB,QAAShN,GAC3Bu1B,EAEJ,KAAO,CACL,MAAMv1B,GAAO,IAAI21B,aAAcC,OAAO5oB,GACtCG,QAAYhC,KAAKopB,GAAIsB,SAAS71B,EAAMu1B,EACtC,CAGA,OADApqB,KAAK4gB,IAAI5e,EAAI4C,WAAYgS,GAClB5U,EAAI4C,UACb,CAEAxQ,UAAU4N,EAAa4U,EAAwB,CAAC,GAC9C,MAAM+T,EAAOlE,GAAYzkB,GAEzB,UADuBhC,KAAKyf,MAAMmL,KAAKC,SAASF,EAAM/T,IACvC,QAEL5W,KAAKyf,MAAMmL,KAAKj0B,IAAIg0B,EAAM/T,MAC/B5U,IAAI4C,UAET,CAGF,CAEAxQ,iBACE,OAAO4L,KAAKyf,KAAM0I,OAAQuB,iBAAiBhyB,KAAKkiB,GAC9CA,EAAEkR,WAAWlmB,YAEjB,CAEAxQ,mBACQ4L,KAAKyf,MAAMsL,OACnB,CAEA32B,oBACQ4L,KAAKyf,MAAMpa,QACnB,CAEAjR,kBAAkBqG,SACGuF,KAAKyf,KAAM0I,OAAQ6C,MAAK,SAAUvwB,IACrD,OAAO,CACT,CAEAitB,KACE,MAAMxyB,EA/OVd,gBACE62B,GAGA,gBAAiB7xB,KAAQ6xB,EAAU,CACjC,MAAM,IAAEjpB,EAAG,SAAEkpB,GAAa9xB,OACpB,CAAE4I,IAAKA,EAAImpB,OAAQD,WAAUlzB,KAAM,YAC3C,CACF,CAuOmBozB,CAAcprB,KAAKyf,KAAMmL,KAAKlD,MAC7C,OAAOxyB,CACT,CAEAd,aAIE,MAAO,CAAE+F,GAHE6F,KAAKyf,KAAM0I,OAAOoB,OAAO3kB,WAGvBkjB,aAFQ9nB,KAAKyf,KAAM0I,OAAQc,SAAUC,SAAUmC,KACzDvD,aACwBH,UAAW,EACxC,G,wBC1LF,OAtFmB,MACjBtiB,OAAO,EACPuiB,KAAM,qBACN0D,MAAO,CACLC,SAAS,EACTC,IAAK,CACHD,SAAS,IAGbE,QAAS,CACPF,SAAS,GAEXrR,OAAQ,CACNwR,IAAK,CACHC,YAAa,CACX,+BAAgC,CAAC,MAAO,QACxC,8BAA+B,CAC7B,wBACA,wBACA,wBACA,2BAINC,UAAW,CACTC,QAAS,0BACTC,MAAO,GAKPC,UAAW,IAMbC,UAAW,CACTC,KAAM,CACJC,SAAS,EACTC,SAAU,IAEZC,WAAY,CACVF,SAAS,IAGbG,UAAW,GAQXC,OAAQ,CACNJ,SAAS,GAEXJ,MAAO,CACLS,QAAS,CACPC,UAAW,IACXC,SAAU,IAEZC,mBAAmB,GAErBC,QAAS,CACPC,KAAM,cAGVzE,OAAQ,CACNE,WAAY,EAIV,QAAW,CACTntB,OAAQ2xB,GAAA,MAGZC,IAAK,CACHvB,SAAS,IAGbwB,aAAc,CACZC,YAAY,KCWhB,OAnFA,oBACE,KAAS9L,SAAyB,WAMlC,KAAQ0F,YAAa,EAJjB1M,aACF,MAAO,CAAE2M,WAAYhK,GACvB,CAII6G,gBACF,OAAO1jB,KAAK4mB,UACd,CAIAxyB,aACE4L,KAAKyf,WAAa,SAAmB,MACf,oBAAXuH,SACTA,OAAOvH,KAAOzf,KAAKyf,KACnBuH,OAAOC,MAAQR,IAGjBzmB,KAAK4mB,YAAa,CACpB,CAEAxyB,WAAW4N,EAAa4U,EAAwB,CAAC,GAC/C,OAAO5W,KAAKyf,KAAM2H,MAAM1H,KAAKgH,GAAiB1kB,GAAM,IAC/C4U,EACHyQ,WAAW,EACX/jB,MAAM,IACL7M,MAAMvB,IACP,MAAM,KAAE8C,EAAI,KAAEsL,EAAI,UAAE4X,EAAS,MAAEyF,EAAK,OAAE1F,GAAW/lB,EACjD,MAAO,CACL8C,OACAsL,KAAMA,IAAS,EACf4X,UAAWA,IAAc,EACzBD,SACD,GAEL,CAEAsF,IAAIve,EAAa4U,EAAsB,CAAC,GACtC,OAAO5W,KAAKyf,KAAMc,IAAIkG,GAAYzkB,GAAM4U,EAC1C,CAEAxiB,UAAUyN,EAAwB+U,EAAwB,CAAC,GACzD,aAAc5W,KAAKyf,KAAM9oB,IAAIkL,EAAS+U,IAAU5U,IAAI4C,UACtD,CAEAxQ,UAAU4N,EAAa4U,EAAwB,CAAC,GAC9C,aAAc5W,KAAKyf,KAAMmB,IAAIjqB,IAAI8vB,GAAYzkB,GAAM4U,IAAUhS,UAC/D,CAEAxQ,iBACE,aAAc4L,KAAKyf,KAAMyH,MAAMI,SAAS5vB,KAAKkiB,GAAMA,EAAE2N,KAAK3iB,YAC5D,CAEAxQ,aAAc,CACdA,cAAe,CAEfA,kBAAkBqG,GAChB,MAAMsR,GAAO,SAAUtR,GAIvB,aAHMuF,KAAKyf,KAAM+H,UAAU7wB,IAAIoV,SAEzB/L,KAAKyf,KAAMyH,MAAMO,QAAQ1b,IACxB,CACT,CAEA2b,KACE,OAAO1nB,KAAKyf,KAAMmB,IAAI8G,IACxB,CAEAtzB,aACE,MAAM8d,QAAiBlS,KAAKyf,KAAMO,MAAM4H,OAClCD,EAAWsF,OAAO/a,EAASyV,UAE3BE,QAAmB7nB,KAAKyf,KAAMtlB,MAC9B,aAAE2tB,EAAY,GAAE3tB,GAAO0tB,EAC7B,MAAO,CAAE1tB,GAAIA,EAAGyK,WAAYkjB,eAAcH,WAC5C,G,YCjFF,MAAMuF,GAAyD,CAC7D/J,MAAO,GACPD,SAAU,GACVH,SAAU,IAIL3uB,eAAe+4B,GACpBvW,GAEA,MAAM,aAAEwW,KAAiBC,GAAgBzW,EASnC0W,ECvBR,SACEC,EACA3W,GAEA,OAAO,cAA+B2W,EACpCn5B,uBACE4N,EACAwrB,EACA3d,GAEA,MAAMhO,QAAgBigB,GAAe9f,EAAKhC,KAAM6P,GAE1C4d,QAAgB1P,GAAwBlc,EAASG,GACvD,OAAQwrB,EAEJC,GAASz1B,OAASw1B,EAClBC,OACA,EAHAA,CAIN,CAEAr5B,iBAAiByN,GACf,OAAOogB,GAAgBjiB,KAAM6B,EAC/B,CAEAzN,2BAEE,eADoB6c,MAAMyc,YACX1wB,MAAMusB,GAAWA,IAAW3S,EAAQ+W,aACrD,CAEAv5B,uBAAuBw5B,GAAS,SACG5tB,KAAK6tB,uBACXD,GAMzB3c,MACG6c,YAAYlX,EAAQmX,kBACpBt3B,MAAK,KACJ,GAAQQ,IAAI,2BAA2B2f,EAAQmX,qBACxC,KAER1pB,OAAOM,IACN,GAAQ1N,IACN,0BAA0B2f,EAAQmX,qBAAqBppB,EAAIzI,YAEtD,IAGf,EAEJ,CD7BwB8xB,CAAgBd,GAAaE,GAAe,CAChEO,YARkBjR,GASlBqR,iBANiB,aAAjBX,EACIxQ,GACAD,KAOAsR,EAAW,IAAIX,EAQrB,aANMW,EAASC,KAAK,CAAEvlB,IAAK0kB,EAAYc,gBAKjCF,EAASxK,mBACRwK,CACT,C,wDE5BA,OAAIG,kBAAmB,EAMvB,MAAMC,GAA4C,CAChDC,iBAAkB,CAChB3yB,KAAM,qBACN4yB,MAAO,4BAiGEC,GAAc,CACzBjrB,EACAkrB,KAEA,MAAMC,EAAoB,IAAIvd,EAAA,EACxBhO,EA9CoB,EAC1BI,EACAmrB,KAEA,MAAMC,EAAgB,IAAIC,GAAA,EAAc,GAiCxC,OA/BA,EAAA/qB,EAAA,GAAc,CAACN,EAAamrB,IAAoBp4B,WAC9C,EAAEyN,EAAYuqB,MACZ,GAAIvqB,GAAcuqB,EAAkB,CAClC,MAAM7pB,EAAkBrQ,MAAOkQ,UACRgqB,EAAiBhqB,EAAM,CAC1CuqB,QAAS,OACTC,WAAW,KAGCj6B,KAGVk6B,EAAoB36B,MAAOkQ,EAAcyP,KAC7C,MAAMvP,QAAYC,EAAgBH,GAMlC,aAHmBP,EAAWgrB,kBAAkBvqB,EAAKuP,EAG1C,EAGPib,EAAM,CACVvqB,kBACAsqB,qBAEFJ,EAAcv8B,MAAK,QAAM48B,GAC3B,KAIGL,CAAa,EASEM,CAAoB1rB,EAAamrB,GAEjDQ,EAAuB96B,MAAO+6B,IAClC,MAAM,KAAExzB,EAAI,MAAE4yB,GAAUF,GAAWc,GAE7BC,OA/FW,EACnBzzB,EACA4yB,EACAvQ,KAEO,UAASriB,EAAM4yB,EAAO,CAC3Bc,kBAAoBC,IAClB,IACE,MAAM,OACJ7yB,EAAM,SACNqE,EAAQ,OAERyuB,EAAM,MACNv2B,GACEs2B,EAEEpzB,EAAUqzB,EAAS,GAAGhB,OAAWgB,KAAUv2B,UAAgBu1B,EAE3Dh6B,EAAO,CAAC,QAAS,SAASkH,MAAMsF,GAAMA,IAAMtE,IAE5C+yB,EAAmB,CACvB/yB,SACAP,UACA3H,OACAuM,SAAUA,EAAWoI,KAAKoG,MAAMxO,GAAY,GAI9Ckd,EAAWwR,EACb,CAAE,MAAOhZ,GACP,GAAQvf,IAAI,yBAA0Bs3B,EAAO/X,EAAE5R,WACjD,KAgEqB6qB,CAAa9zB,EAAM4yB,GAAQ15B,GAChD45B,EAAanuB,wBAAwB6uB,EAAOt6B,KAEjC,uBAAT8G,GACF+yB,EAAkBt8B,KAAKg9B,GAEzB,GAAQn4B,IAAI,GAAGk4B,aAAiB,EAG5BjB,EAAO95B,UACXq6B,EAAatuB,kBAAkB,KAAM,YACrC,GAAQuvB,KAAK,qBAENj7B,QAAQqQ,IAAI,CACjBoqB,EAAqB,sBAIpBz4B,MAAMvB,IACL+T,YAAW,IAAMwlB,EAAatuB,kBAAkB,KAAM,YAAY,GAClE,GAAQwvB,QAAQ,qBAETz6B,KAERmP,OAAOmS,GACNiY,EAAatuB,kBAAkB,KAAM,QAASqW,EAAE5R,eAMtD,OAFAspB,IAEO,CAAE/qB,gBAAe+qB,OAAM,E,8oBC/GhC,MAAM0B,GAAgB,CACpBC,OAAQ,IACRC,cAAc,EACdC,cAAc,EACdnZ,QAAS,IAGLoZ,GAAwC,CAC5CC,UAAU,EACVC,SAAS,EACTC,SAAU,OACVC,WAAY,CAAC,EACb7uB,OAAQ,CAAC,EACT8uB,M,mvIACAC,OAAQ,IA0QV,MAAMC,GA9PN,WACE,IAAIC,EAA0C,CAAC,EAC3CC,EAAyB,CAAElvB,OAAQ,CAAC,EAAGmvB,KAAM,CAAC,EAAGC,QAAS,CAAC,GAC/D,MAAM/sB,EAAiB,IAAIb,EAAA,GAAyB,GAC9C6tB,EAAe,IAAI7tB,EAAA,EAA4C,CAAC,GAEhE8tB,EAAkB,IAAI7tB,IAEtB8tB,EAAqB,IAAIlC,GAAA,EAAc,IAC7C,EAAA/qB,EAAA,GAAc,CAACD,EAAgBgtB,IAC5BptB,MACC,EAAA9L,EAAA,IACE,EAAEmH,EAAe2xB,QACZ3xB,IAAiB2xB,EAAY/2B,aAEpC,EAAAgX,GAAA,MAEDna,WAAWsa,IACVkgB,EAAmB1+B,KAAKwe,EAAE,IAG9BggB,EAAat6B,WAAWsa,IACtB4f,EAAc5f,CAAC,IAGjB,MAgCMmgB,EAAM38B,MACVk8B,EACAU,EACA9M,EACA+M,EAAkC,CAAC,KAEnC,MAAMC,GAAQ,UAAStsB,WAEvBsf,GAAY2M,EAAgB56B,IAAIi7B,EAAOhN,GACvC,MAIMiN,EAAiB,IAClBnB,MACAgB,EACHX,MAAOC,EACPW,QAAS,IAAKA,EAASG,QAAS,IAChC7vB,OATmB,CACnB8vB,IAAKZ,EACLS,UAUII,QAAmB,SAAQH,EAAgBvB,KAG3C,OAAE16B,EAAM,MAAEqB,GAAU+6B,EAE1B,IAGE,OAFAT,EAAgBzrB,OAAO8rB,GAEhB,IACF7b,GAAYic,GACf/6B,QACArB,OAAQA,EACO,OAAXA,EACE,GACA0C,KAAK6F,OnD9IiB0I,EmD8ISjR,EnD7IpCiR,EAAO+O,QAAQ9O,GAAoB,MmD8IhC,CAAEpK,OAAQ,QAASE,QAAS,aAEpC,CAAE,MAAOsa,GASP,OARAqa,EAAgBzrB,OAAO8rB,GAEvB,GAAQj6B,IACN,kBAAkBk6B,EAAehB,WACjC3Z,EACA8a,EACAH,GAEK,CACLI,kBAAmB,0BAA0B/a,OAC1C8a,EACHp8B,OAAQ,CAAE8G,OAAQ,QAASE,QAASsa,GAAG5R,YAAc,iBAEzD,CnD/JG,IAA6BuB,CmD+JhC,EAGIqrB,EAA4B,KAEhC,IAAKhB,EAAY/2B,SACf,MAAO,CAAC,QAAS,IAGnB,MAAM,OAAE62B,EAAM,QAAE/E,GAAYiF,EAAY/2B,SAExC,OAAK8xB,EAIE,CAAC,SAAU+E,GAHT,CAAC,OAAQ,GAGO,EA+G3B,MAAO,CACLpC,KAnNW95B,UACX,GAAQs7B,KAAK,6BACP,WAEN,GAAQC,QAAQ,uBAChB/rB,EAAexR,MAAK,EAAK,EA+MzB0+B,qBACAC,MAEAU,aAnDmBr9B,MACnB4N,EACAQ,EACAX,EACAqiB,KAEA,MAAOwN,EAAYpB,GAAUkB,IAC7B,GAAmB,UAAfE,EACF,MAAO,CACL11B,OAAQ,QACR21B,UAAW,CAAC,CAAC,CAAE35B,KAAM,OAAQsM,KAAM,6BAIvC,GAAmB,SAAfotB,EACF,MAAO,CAAE11B,OAAQ,OAAQ21B,UAAW,IAGtC,MAAMC,QAAeb,EACnBT,EACA,CACEH,SAAU,gBACVC,WAAY,CAACpuB,EAAKQ,EAAaX,IAEjCqiB,GAGF,MAA6B,UAAzB0N,EAAO18B,OAAO8G,QAChB,GAAQzF,MAAM,wBAAyBq7B,GAChC,CACL51B,OAAQ,QACR21B,UAAW,CAAC,CAAC,CAAE35B,KAAM,OAAQsM,KAAMstB,EAAOr7B,WAIvC,CAAEyF,OAAQ,SAAU21B,UAAWC,EAAO18B,OAAO2M,QAAS,EAiB7DgwB,kBAlHwBz9B,MACxBmN,IAEA,MAAOmwB,EAAYpB,GAAUkB,IAE7B,GAAmB,UAAfE,EACF,MAAO,CAAE11B,OAAQ,QAASE,QAAS,0BAGrC,GAAmB,WAAfw1B,EACF,MAAO,CAAE11B,OAAQ,QAGnB,MAAM,IAAEgG,EAAG,YAAEQ,EAAW,QAAEX,GAAYN,EAChCqwB,QAAeb,EAAIT,EAAQ,CAC/BH,SAAU,qBACVC,WAAY,CAACpuB,EAAKQ,EAAaX,MAE3B,OAAE7F,EAAQ6F,QAASiwB,GAAkBF,EAAO18B,OAUlD,MARe,UAAX8G,GACF,GAAQzF,MACN,kCAAkCgL,EAAOS,MACzCT,EACAqwB,GAIAE,EACK,IAAKF,EAAO18B,OAAQ2M,QAASiwB,GAG/BF,EAAO18B,MAAM,EAmFpB68B,eAjMsBC,IACtBxB,GAAc,WACX5f,IAAM,IAAMA,EAAG0f,QAAQ,SAAkB1f,EAAE0f,WAC5C0B,GAEFpB,EAAax+B,KAAKo+B,EAAY,EA6L9ByB,YAlNkB,CAClBt2B,EACA7G,KAGA27B,EAAQ90B,GAAQ7G,CAAK,EA8MrBo9B,WA3MkBC,IAClB,MAAMC,EAAa3B,EACnB0B,EAAMh6B,SAASwD,IACby2B,EAAWz2B,GAAQ,CAAC,CAAC,IAEvB80B,EAAU2B,CAAU,EAuMpBC,gBAnFsBj+B,MACtBk8B,EACAH,EACAC,KAEA,GAAQn5B,IAAI,4BAA6Bk5B,EAAUC,GAOnD,aANqBW,EAAIT,EAAQ,CAC/BH,WACAC,aACAH,UAAU,KAGE/6B,MAAM,EAwEpBo9B,gBAnBsBl+B,MAAO88B,EAAer8B,KAC5C,MAAMqvB,EAAW2M,EAAgBx6B,IAAI66B,GAEjChN,SACIA,EAASrvB,EACjB,EAeA09B,SAAU,KAAM,CACd9B,UACAD,gBAGN,CAEqBgC,GAIrB,U,YC1UO,MC2EDC,GA9D4B,MAChC,MAAMhE,EAAe,IAAI,EAEnBlrB,EAAc,IAAI4N,EAAA,EAIlBjB,EAAU,IAAInN,EAAA,EAAmC,CACrDwJ,UAAW,QAGP,cAAEpJ,GAAkBqrB,GAAYjrB,EAAakrB,IAE7C,aAAEiE,EAAY,KAAEC,GD1BK,EAC3BxvB,EACAI,EACAkrB,KAEA,MAAMiE,EAAgB9vB,GACpBgwB,GAAA,EAASF,aAAa9vB,GAyBxB,OAvBAO,EAAc7M,WAAW8M,IACvBsvB,EAAa,CAAEtvB,gBAAe,IAGhCG,EAAYjN,WAAWijB,IACrBmZ,EAAa,CAAEnZ,SAAQ,IAGzB,GAAKuX,mBAAmBx6B,WAAWxB,IACjCA,EACImU,YAAW,IAAMwlB,EAAatuB,kBAAkB,OAAQ,YAAY,GACpEsuB,EAAatuB,kBAAkB,OAAQ,WAAW,IAG3C/L,WACXq6B,EAAatuB,kBAAkB,OAAQ,kBAEjC,GAAK+tB,OACXwE,EAAa,CAAEC,KAAI,IAAG,EAGxBzE,GAEO,CAAEyE,KAAI,GAAED,eAAclhB,MAAOohB,GAAA,EAASphB,MAAO,ECLrBqhB,CAC7B1vB,EACAI,EACAkrB,IAGI,UACJqE,EAAS,cACThvB,EACAkrB,IAAK+D,GCzBoB,EAC3BJ,EACAlE,KAEA,MAAM3qB,EAAgB,IAAIf,EAAA,OAAyC,GAC7D+vB,EAAY,IAAI,GAAahvB,EAAe,CAChD6uB,SAsCI3D,EAAM,CACV3pB,MA3BgBjR,MAAO4+B,IACvB,IAEE,GADiBlvB,EAAcI,WAI7B,OADA+E,YAAW,IAAMwlB,EAAatuB,kBAAkB,OAAQ,YAAY,GAC7D1L,QAAQC,UAGjB+5B,EAAatuB,kBAAkB,OAAQ,YACvC,GAAQuvB,KAAK,uBAEb,MAAMuD,QAAoB9F,GAAa6F,GAKvC,OAJA,GAAQrD,QAAQ,uBAEhB7rB,EAAc1R,KAAK6gC,GACnBhqB,YAAW,IAAMwlB,EAAatuB,kBAAkB,OAAQ,YAAY,IAC7D,CACT,CAAE,MAAOwE,GACP,GAAQ1N,IAAI,4BAA6B0N,GACzC,MAAMjJ,EAAMiJ,aAAezB,MAAQyB,EAAIzI,QAAWyI,EAElD,MADA8pB,EAAatuB,kBAAkB,OAAQ,QAASzE,GAC1CwH,MAAMxH,EACd,GAKAqvB,KAtCe32B,UACf,MAAM8+B,EAAWpvB,EAAcI,WAE3BgvB,SACIA,EAASnI,OAEjBjnB,EAAc1R,UAAK,GACnBq8B,EAAatuB,kBAAkB,OAAQ,WAAW,EAgClD+Z,OAAQ9lB,SAAY0P,EAAcI,YAAYgW,OAC9C9J,KAAMhc,SAAY0P,EAAcI,YAAYkM,OAC5C+iB,iBAAkB/+B,MAChB4N,EACAwrB,EACA5N,KAEA,MAAMsT,EAAWpvB,EAAcI,WAC/B,IAAKgvB,EACH,MAAM,IAAIhwB,MAAM,6BAElB,OAAOgwB,EAASC,iBAAiBnxB,EAAKwrB,EAAS5N,EAAW,EAE5Dle,QAAStN,MACP4N,EACAkiB,EACAtN,IACGkc,EAAUpxB,QAAQM,EAAKkiB,EAAUtN,GACtCmP,eAAgB3xB,MAAO4N,EAAa4U,IAClCkc,EAAW/M,eAAe/jB,EAAK4U,GACjCwc,QAASh/B,MAAO4N,GAAgB8wB,EAAU7M,OAAOjkB,GACjDqxB,gBAAiBj/B,MAAO+xB,GAAmB2M,EAAU5M,eAAeC,GACpEmN,WAAYl/B,SAAY0+B,EAAU1M,QAClCmN,WAAYn/B,MAAOyN,GACjBiC,EAAcI,YAAYqvB,WAAW1xB,IAGzC,MAAO,CAAEiC,gBAAegvB,YAAW9D,MAAK,EDhDpCwE,CAAcb,EAAMlE,GAOlBgF,EAAc,CAClBxwB,uBAN6B,CAC7BjB,EACAG,EAA0BlO,EAAcmO,SACrC0wB,EAAU/M,eAAe/jB,EAAK,CAAE8jB,gBAAgB,EAAO3jB,aAI1DoB,cACAO,gBACAX,gBACA+M,WAIkB,IAAI0L,GAAY6X,GAKpC,OAFAf,EAAa,CAAEK,YAER,CACLW,SAxCgBhwB,GAAcH,EAAYnR,KAAKsR,GAyC/CiwB,kBAAmB,MAAQ7vB,EAAcI,WAEzC6uB,SAAS,QAAMA,GACfJ,MAAM,QAAMA,GACZxvB,gBAEA2vB,WAAW,QAAMA,GACjBc,YACEhxB,GACG8vB,EAAa9vB,GAElBixB,UAAYtyB,GACV2O,EAAQ9d,KAAK,IAAK8d,EAAQpb,SAAUyM,IACvC,EAGsBuyB,GnE4DlB,IAA4B/8B,GAAoBi4B,GAApBj4B,GmEvDnBxE,KnEuDuCy8B,GmEvDjCyD,GnEwDpBz8B,SACgC,IAArBe,GAAOg9B,UAChBh9B,GAAOg9B,UAAavd,IAClB,MAAMriB,EAAOqiB,EAAEwd,MAAM,GACrBl9B,EAAgB3C,IAEhB,OAAO66B,GAAK76B,EAAK,GAInB,OAAO66B,G,qCqE1GJ,SAASiF,EAAkB9Y,GAChC,MAAM,OAAEmV,EAAQnV,SAAU+Y,EAAE,QAAEC,GA3BzB,SAA4BhZ,GAEjC,MAAMiZ,EAAY,2BAElB,IAAI97B,EACA+7B,EAAa,GACbC,EAAmBnZ,EACnBgZ,GAAU,EAEd,KAA8C,QAAtC77B,EAAQ87B,EAAUG,KAAKpZ,KAC7BgZ,GAAU,EAEVE,GAAc/7B,EAAM,GAAK,KAGzBg8B,EAAmBA,EAAiBpf,QAAQ5c,EAAM,GAAI,IAIxD,MAAO,CACLg4B,OAAQ+D,EAAWrV,OACnB7D,SAAUmZ,EACVH,UAEJ,CAG4CK,CAAmBrZ,GAE7D,OAAOgZ,EAAU7D,EAAS4D,CAC5B,C,6LC9C2B,qBAAyC,GAoCpE,I,gDChCA,MAaM,EACJ,qEA6HIO,EAA6BrgC,MAAOsgC,EAAQ9iB,KAChD,IACE,MAAMM,QC9IH9d,eAA2Bwd,GAQhC,aAPuB,IAAMvb,IAC3B,GACE/C,EAAA,EAAgBC,QAAQb,qCACI,YAAmC,IAAAiiC,WAC/D,IAAAC,SAAQh9B,KAAKC,UAAU+Z,SAGX/c,KAAKA,IACvB,CDqI2BggC,CAAYjjB,GAKnC,OAAOM,CACT,CAAE,MAAO3b,GAEP,OADA,EAAQU,IAAI,QAASV,GACd,IACT,G,+CE9JK,MAAMu+B,UAAgC5xB,MAG3CnD,YAAYmS,GACV,IAAIhW,EAAU,GACV64B,GAAQ,EACR7iB,aAAoB2D,MACtB3Z,EAAUgW,EAASzG,KAAK,QACfyG,EAAS8iB,QAClB94B,EAAUgW,EAAS8iB,OAAOpwB,WAC1BmwB,EAAO7iB,EAAS6iB,MAEhB74B,EAAUA,GAAS3F,MAGrB0a,MAAM/U,GACN+4B,EAAO1+B,MAAM2F,EAAS,CAAE3F,MAAO2b,IAE/BlS,KAAK+0B,KAAOA,CACd,E,2BCLF,MAAMG,EAAa,CACjBlpB,OAAQ,GACRmpB,IAAK,KAAmBvwB,YAGbwwB,EAAgBhhC,MAC3BuG,EACAN,EACAb,GAEEsB,WACAu6B,iBAKFC,EAAcJ,KAEd,MACMhgC,EDX4B,CAClCgd,IAGA,GADwBA,aAAoB2D,OAA2B,IAAlB3D,EAAS6iB,KAE5D,MAAM,IAAID,EAAwB5iB,GAEpC,OAAOA,CAAQ,ECIA,OADQmjB,EAAeE,UAAU56B,EAAQN,EAAMb,EAAI87B,KAG5D,gBAAEx7B,GAAoB5E,EACtB0V,EAAO,CACXvQ,OACAb,KACAM,kBACAH,WAAW,UACXgB,UAOF,aAHMG,GAAU06B,aAAa5qB,UACvB9P,GAAU26B,kBAAkB7qB,IAE3B9Q,CAAe,E,eChBxB,MAwJM84B,EAxJiB,MACrB,MAAM8C,EAA0C,CAE9C3C,QAAS,IAAIhwB,EAAA,OAA0C,GACvD4vB,KAAM,IAAI5vB,EAAA,OAAuC,GACjD4yB,YAAa,IAAI5yB,EAAA,OAA8C,GAC/DK,aAAc,IAAIL,EAAA,OAAsC,GACxDsyB,cAAe,IAAItyB,EAAA,OACjB,GAEFjI,SAAU,IAAIiI,EAAA,OAA2C,GACzDtI,QAAS,IAAIsI,EAAA,OAA0C,GACvDwW,MAAO,IAAIxW,EAAA,OAAwC,IAGrD,IAAI8M,EAEJ,MAAM+lB,EACJj6B,GAEO,IAAIlH,SAASC,IAClB,MAAMmhC,EAAQH,EAAY/5B,GAGtBk6B,EAAM3xB,YACRxP,EAAQmhC,EAAM3xB,YAGhB2xB,EACGryB,MACC,EAAAC,EAAA,IAAO3O,KAAYA,KAGpBwB,WAAWxB,IACVJ,EAAQI,EAAM,GACd,IAIR,EAAAghC,YAAYrO,QAAQ,MAAShxB,MAAMi+B,IACjCgB,EAAYC,aAAavjC,KAAKsiC,EAAO,IAGvC,MAmBMqB,EAAoB3hC,MAAO4N,UACR4zB,EAAmB,YAC3BzC,iBAAiBnxB,EAAK,QA2CjCg0B,EAAS,CACbC,sBAV4B,KAC5BpmB,EAAkB,IAAIC,gBACfD,GASPqmB,YAzDkB9hC,MAAOwd,EAAeukB,EAAO,KAC/C,MAAMR,QAAqBC,EACzB,eAGIQ,QAAoB,QAAexkB,GAEzC,OAAO,QAAa+jB,EAAaS,EAAaD,EAAK,EAmDnDZ,UAAWnhC,MAAOiG,EAAcb,KAC9B,MAAMiB,EAAUi7B,EAAYj7B,QAAQyJ,WACpC,IAAKzJ,EACH,MAAM,IAAIyI,MAAM,6BAElB,MAAMpI,QAAkB86B,EAAmB,YACrCP,QAAuBO,EAC3B,iBAGF,OAAOR,EAAc36B,EAASJ,EAAMb,EAAI,CACtCsB,WACAu6B,iBACA,EAEJgB,sBAAuBjiC,MAAOkiC,UACFV,EAAmB,eAA7C,MACMW,OJsFkBniC,OAAOsgC,EAAQ4B,KAC3C,IACE,MAAM1kB,EAAQ,CACZ4kB,qBAAsB,CACpBF,aAIJ,aADuB7B,EAA2BC,EAAQ9iB,EAE5D,CAAE,MAAOrb,GAEP,OADA,EAAQU,IAAI,QAASV,GACd,IACT,GIlG2B8/B,CAAsBV,EAAaW,GAE1D,OAAOC,CAAQ,EAEjBE,iBAAkBriC,MAAOkQ,EAAcyP,EAAQ,MAC7C,MAAM3Q,QAAsBwyB,EAC1B,gBAIF,aAFMA,EAAmB,SAElBxyB,EAAa2rB,kBAAkBzqB,EAAMyP,EAAM,EAEpD2iB,mBAxEyBtiC,MACzB4N,EACAmuB,EACA5uB,EAAS,CAAC,KAEV,IACE,MAAMrM,QAAe6gC,EAAkB/zB,GACvC,QAAwB,IAApB9M,GAAQ2M,QACV,MAAO,CAAE7F,OAAQ,QAASE,QAAS,sBAIrC,MAAMy6B,GAAa,QAAkBzhC,EAAO2M,SAI5C,aAFoB+zB,EAAmB,SAE3BvD,gBAAgBsE,EAAYxG,EAAU5uB,EACpD,CAAE,MAAOiV,GACP,MAAO,CAAExa,OAAQ,QAASE,QAASsa,EAAE5R,WACvC,GAsDAmxB,oBACA9T,gBAAiB7tB,MAAOyN,UACC+zB,EAAmB,YAE3BrC,WAAW1xB,GAE5B+0B,sBAzD4BxiC,MAAO88B,EAAer8B,EAAO,CAAC,KAC1D,IAEE,aADoB+gC,EAAmB,SAC3BtD,gBAAgBpB,EAAOr8B,EACrC,CAAE,MAAO2hB,GACP,MAAO,CAAExa,OAAQ,QAASE,QAASsa,EAAE5R,WACvC,IAsDF,MAAO,CAAE8tB,aA1GamE,IACpB5+B,OAAOC,KAAK2+B,GACT37B,QAAQS,QAAuD,IAA9Ck7B,EAAal7B,KAC9BxD,SAASwD,IACR,MAAMvC,EAAOy9B,EAAal7B,GAC1B+5B,EAAY/5B,GAA6BvJ,KAAKgH,EAAK,GACnD,EAoGiB48B,SAAQxkB,MA9CjB,KACZ3B,GAAiB2B,OAAO,EA6CY,EAGvBslB,GAMjB,O,2VC9KA,MAAMC,EAA6C,CACjDxI,MAAO,iBAGIyI,EAAmB5iC,MAC9B2T,EACAkvB,EACA11B,EAAgC,CAAC,EACjC21B,EACArnB,KAEA,MAAM1H,EAAOvQ,KAAKC,UAAU,CAC1BkQ,cACGgvB,KACAx1B,IAGCyf,EAAU,CACd,eAAgB,mBAChBmW,cAAe,UAAUF,KAGrB/kB,QAAiBkP,MAAM,6CAA8C,CACzE9pB,OAAQ,OACR2P,OAAQ4I,GAAiB5I,OACzB+Z,UACA7Y,SAGF,IAAK5G,EAAOkf,OAAQ,CAGlB,aADmBvO,EAASklB,QAChBC,QAAQ,GAAGn7B,QAAQ2F,OACjC,CAEA,MAAM0c,EAASrM,EAAS/J,MAAMqW,YACxB8Y,EAAU,IAAIC,YACpB,IAAIriC,EAAS,GACTsiC,EAAS,GAEb,GAAIjZ,EAEF,OAAa,CAEX,MAAM,KAAEhqB,EAAI,MAAEO,SAAgBypB,EAAOG,OACrC,GAAInqB,GAAQsb,GAAiB5I,OAAOC,QAClC,MAGFswB,GAAUF,EAAQrvB,OAAOnT,EAAO,CAAE2rB,QAAQ,IAC1C,MAAMgX,EAAQD,EAAOE,MAAM,MAG3BF,EAASC,EAAME,OAAS,GAGxB,UAAWC,KAAQH,EAAO,CACxB,MAAMv7B,EAAU07B,EAAK1iB,QAAQ,UAAW,IACxC,GAAgB,WAAZhZ,EACF,OAAOhH,EAET,IACE,MAAM2iC,EAASjgC,KAAK6F,MAAMvB,GAC1B,GAAI27B,EAAOR,SAAWQ,EAAOR,QAAQj6B,OAAS,EAAG,CAC/C,MAAM,QAAEyE,GAAYg2B,EAAOR,QAAQ,GAAGS,MACtC5iC,GAAU2M,EACNA,SAEIq1B,EAAGr1B,EAEb,CACF,CAAE,MAAOtL,GACPY,EAAQZ,MAAM,gCAAiC2F,EAAS3F,EAC1D,CACF,CACF,CAGF,OAAOrB,CAAM,E,eCpFRd,eAAe2jC,EAAcnmB,GAClC,OAAOghB,EAAA,EAASoD,OAAOE,YAAYtkB,EACrC,CAEOxd,eAAe4jC,EAAYC,EAAShR,GACzC,OAAO2L,EAAA,EAASoD,OAAOT,UAAU0C,EAAShR,EAC5C,CAEO7yB,eAAe8jC,EAAwB5B,GAC5C,OAAO1D,EAAA,EAASoD,OAAOK,sBAAsBC,EAC/C,CAEOliC,eAAe+jC,EAAqBn2B,EAAKmuB,EAAU5uB,EAAS,CAAC,GAClE,OAAOqxB,EAAA,EAASoD,OAAOU,mBAAmB10B,EAAKmuB,EAAU5uB,EAC3D,CAEOnN,eAAegkC,EAAqBp2B,GACzC,OAAO4wB,EAAA,EAASmD,kBAAkB/zB,EACpC,CAEO5N,eAAeikC,EAAkBx2B,GACtC,OAAO+wB,EAAA,EAAS3Q,gBAAgBpgB,EAClC,CAEOzN,eAAekkC,EAAwBpH,EAAOr8B,GAEnD,OADA,EAAQoC,IAAI,qBAAsBi6B,GAC3B0B,EAAA,EAASoD,OAAOY,sBAAsB1F,EAAOr8B,EACtD,CAEOT,eAAemkC,EAAoBxwB,EAAUkvB,EAAQ11B,EAAQ2vB,GASlE,aAPqB8F,EACnBjvB,EACAkvB,EACA11B,GAJenN,MAAOS,GAASyjC,EAAwBpH,EAAOr8B,IAM9D+9B,EAAA,EAASoD,OAAOC,wBAGpB,CAEO7hC,eAAeokC,EAAoBl0B,EAAMyP,GAC9C,OAAO6e,EAAA,EAASoD,OAAOS,iBAAiBnyB,EAAMyP,EAChD,CAEO3f,eAAeqkC,EAAiBz2B,GAErC,aADqB,QAAYA,EAEnC,CAEO5N,eAAeskC,EAAe12B,GAEnC,aADqB,QAAUA,EAEjC,C,+FClDO5N,eAAe6Y,GAAgB,OACpCpF,EAAM,WACNoS,EAAa,CAAEvQ,MAAO,GAAIlI,OAAQ,GAAG,QACrCmI,EAAO,OACPuQ,IAEA,MAAM,OAAE1Y,EAAM,MAAEkI,GAAUuQ,EACpB/H,QAAiB,IAAM7b,IAC3B,GAAG,6BACH,CACEkL,OAAQ,CACN,oBAAqBC,EACrB,mBAAoBkI,EACpBC,UACA9B,OAAQA,EAAOnQ,KAAKihC,GAAQ,GAAGA,EAAInjB,QAAQmjB,EAAI7jC,YAEjD8jC,iBAAkB,CAChBC,QAAS,MAEX5xB,OAAQiT,GAAQjT,UAId,IAAE6xB,GAAQ5mB,EAASrd,KAKnB6E,EAAY,CAChBo/B,MACA7e,WAAY/H,EAASrd,KAAKolB,YAAc,CACtCjhB,MAAOkZ,EAASrd,KAAKmE,OAEvBmhB,YAAajI,EAASrd,KAAKkkC,cAO7B,OAJKr/B,EAAUugB,YAAYjhB,QACzBU,EAAUugB,WAAWjhB,MAAQU,EAAUygB,YAAY/c,QAG9C1D,CACT,C,qECxDO,IAAWs/B,EAAX,CAAWA,IAChBA,EAAA,QAAU,UACVA,EAAA,YAAc,cACdA,EAAA,IAAM,MACNA,EAAA,KAAO,OACPA,EAAA,MAAQ,QACRA,EAAA,OAAS,cANOA,GAAX,CAAWA,GAAA,G,sECAlB,MAiBMz6B,EAAS,CACbI,uBAAwB,CACtBs6B,WAAY,YACZC,OAAQ,SACRt6B,MAAO,S,8ICnBJ,MAAMu6B,EAAmBx/B,GAC9B,IAAW,IAAIC,KAAKD,GAAY,2BAA2B,GAEhDy/B,EAAmBC,GAC9Bz/B,KAAK6D,MAAM47B,EAAUC,SAAS,KAAOD,EAAY,GAAGA,MAEzCE,EAAkB,IAAM3/B,KAAK4V,K,yICCnC,MAKMgqB,EAAerzB,GAC1B,IAAI1R,SAAQ,CAACC,EAAS+kC,KACpB,MAEMjC,EAFa,IAAI,IAAJ,CAAW,OAAQ1Y,EAAOzkB,KAAK8L,IAExBuzB,UAC1B,EAAAC,QAAA,OAAenC,GAAQ,CAAC7yB,EAAKi1B,KACvBj1B,GACF80B,EAAO,IAAIv2B,MAAM,+BAGnB,WAAY02B,GAAS,CAACrjC,EAAOyL,KAC3BtN,EAAQsN,EAAI63B,sBAAsB,GAClC,GACF,G,qEC3BC,MAEMC,EAAgC,yB,iHCE7C,MAAMC,EAAqB,GAoJpB,MAAMC,EAAsB,CACjCC,EAAyC,CAAC,KAE1C,MAAMh6B,EAAU,IAAIC,iBAAiB,MAErC,SAASg6B,EACPC,EACAj+B,EACAu0B,GAEA,MAAM5d,EAAM,IAAKonB,KAAmBxJ,GAChCA,GAASl6B,QACXsc,EAAItc,MAAQqB,KAAKC,UAAU44B,EAAQl6B,QAErC0J,EAAQzK,YAAY,CAClBwC,KAAM,MACNlD,MAAO,CAAEqlC,QAAOj+B,UAASu0B,QAAS5d,IAEtC,CAkBA,MAAO,CAAEzC,KAhBT,SAAiBlU,EAAYu0B,GAC3B,OAAOyJ,EAAiB,OAAQh+B,EAASu0B,EAC3C,EAcel6B,MAZf,SAAkB2F,EAAYu0B,GAC5B,OAAOyJ,EAAiB,QAASh+B,EAASu0B,EAC5C,EAUsBr5B,KARtB,SAAiB8E,EAAYu0B,GAC3B,OAAOyJ,EAAiB,OAAQh+B,EAASu0B,EAC3C,EAM4B2J,MAJ5B,SAAkBl+B,EAAYu0B,GAC5B,OAAOyJ,EAAiB,OAAQh+B,EAASu0B,EAC3C,EAEmC,EAG/BwE,EAzLN,SAAyBgF,EAAyC,CAAC,GAQjE,IAAII,EAAmB,CAAC,EAyCxB,SAASpjC,EACPkjC,EACAj+B,EACAu0B,EAA2BwJ,GAE3B,IACE,MAAMK,EAAmB7J,GAAS8J,UAC9B9J,GAAS8J,UAAUr+B,GACnBA,GAxDR,SAAmBs+B,EAAkBC,GAAW,GAG9C,IAFAV,EAAQv9B,KAAKg+B,GAENC,GAAYV,EAAQ38B,OAAS,KAClC28B,EAAQ5qB,OAEZ,CA4DIurB,CARiB,CACf/gC,UAAW,IAAIC,KACfugC,QACAj+B,QAASo+B,EACTK,WAAYlK,GAASkK,WACrBlK,QAAS,SAAOA,EAAS,CAAC,YAAa,iBAKlBx4B,OAAOC,KAAKmiC,GAAkBn9B,QACnD,CAACC,EAAcqY,KACb,MAAMjU,EAAS84B,EAAiB7kB,GAC1BolB,EAAcnK,EAAQjb,GAC5B,OAAIjU,GAAUq5B,EAEVz9B,GACW,QAAXoE,GACkB,IAAlBA,EAAOnE,QACPmE,EAAO9F,MAAMo/B,GAAMA,IAAMD,IAGtBz9B,CAAG,IAEZ,IA/DN,SACEg9B,EACAj+B,EACAu0B,GAEA,MAAM5d,EAAM,SAAO4d,EAAS,CAC1B,YACA,SACA,SACA,OACA,UAEI,OAAEzgB,EAAS,UAAIC,EAAS,QAAI6C,EAAO,QAAIje,EAAO,IAAO47B,EACrDqK,GAAU,IAAA/nB,SAAQF,GAAO,GAAKA,EAEhCgD,MAAMC,QAAQ5Z,GAChB/E,EAAQgjC,MAAUj+B,EAAS4+B,GAIzBrK,GAAS8J,UACXpjC,EAAQgjC,GAAO1J,GAAS8J,UAAUr+B,GAAU4+B,GAI9C3jC,EAAQgjC,GAAO,IAAInqB,KAAUC,KAAU6C,MAAS5W,IAAWrH,EAAMimC,EACnE,CAyCMC,CAAWZ,EAAOj+B,EAASu0B,EAE/B,CAAE,MAAOl6B,GACPY,EAAQF,IAAI,eAAgBV,EAC9B,CACF,CAyCA,OA3HgB,IAAI2J,iBAAiB,MAE7BvL,UAAaC,IACK,WAApBA,EAAMC,KAAKmD,OACbqiC,EAAmB,IAAKA,KAAqBzlC,EAAMC,KAAKC,OAC1D,EAsHK,CACLmC,MACAmZ,KAzCF,SAAiBlU,EAAYu0B,GAC3B,OAAOx5B,EAAI,OAAQiF,EAASu0B,EAC9B,EAwCEl6B,MAtCF,SAAkB2F,EAAYu0B,GAC5B,OAAOx5B,EAAI,QAASiF,EAASu0B,EAC/B,EAqCEr5B,KAnCF,SAAiB8E,EAAYu0B,GAC3B,OAAOx5B,EAAI,OAAQiF,EAASu0B,EAC9B,EAkCE2J,MAhCF,SAAkBl+B,EAAYu0B,GAC5B,OAAOx5B,EAAI,OAAQiF,EAASu0B,EAC9B,EA+BEsJ,UACAiB,QAAS,IA7BFjB,EAAQriC,KAAK8iC,IAClB,MAAM,QAAE/J,KAAYr7B,GAASolC,GACvB,KACJ1nB,EAAO,UACP7C,EAAS,UACTD,EAAS,QACTnb,EAAO,GACP0B,MAAAA,EAAQ,cACRokC,EAAa,IACXlK,GAAW,CAAC,EAChB,MAAO,IACFr7B,EACH0d,OACA7C,SACAD,SACAnb,OACA0B,MAAAA,EACAokC,aACD,IAYHvU,MAAO,IAAM2T,EAAQkB,OAAO,EAAGlB,EAAQ38B,QACvC89B,oBA5H0B,IAAMb,EA8HpC,CAyCec,CAAa,CAAEnrB,OAAQ,SAMtC,K,uOCvLO,MA8ZDorB,EAAUhnC,MACd4N,EACAhK,EAAkB,IAAgBqC,MAChCmH,SAAQkI,QAAOwD,QAAQ,EAAAmuB,QAAQC,kBAEjC,IAiBE,aAhBuB,OAAgB,CACrCzzB,OAAQ,CACN,CACE2N,IAAK,sBACHxd,IAAS,IAAgBwB,GAAK,KAAO,QAEvC1E,MAAOkN,IAGXiY,WAAY,CACVvQ,QACAlI,UAEFmI,QAASuD,GAIb,CAAE,MAAOsJ,GAEP,OADArf,EAAQF,IAAIuf,GACL,IACT,GAGW+kB,EAAcnnC,MAAO4N,EAAKR,EAAQkI,IACtC0xB,EAAQp5B,EAAK,IAAgB3H,KAAM,CAAEmH,SAAQkI,UAGzC8xB,EAAYpnC,MAAO4N,EAAKR,EAAQkI,IACpC0xB,EAAQp5B,EAAK,IAAgBxI,GAAI,CAAEgI,SAAQkI,UA+F7C,MA4GM+xB,EAAiBrnC,MAAOwd,GACnCA,EAAMtZ,MAAM,MAAqBsZ,GAAQ,QAAY,QAAYA,IAEtD8pB,EAAetnC,MAC1BsgC,EACA36B,EACAo8B,KAEA,IAGE,aAFsBzB,EAAOiH,OAAO5hC,EAAMo8B,EAG5C,CAAE,MAAO5/B,GAGP,YADAY,EAAQZ,MAAMA,EAEhB,E,2KChYF,MAEMqlC,EAAet3B,GAASA,EAAK4Q,QAAQ,OAAQ,I,m/BC1S/C2mB,EAA2B,CAAC,EAGhC,SAASC,EAAoBC,GAE5B,IAAIC,EAAeH,EAAyBE,GAC5C,QAAqBE,IAAjBD,EACH,OAAOA,EAAaE,QAGrB,IAAIjsB,EAAS4rB,EAAyBE,GAAY,CACjD5hC,GAAI4hC,EACJxM,QAAQ,EACR2M,QAAS,CAAC,GAUX,OANAC,EAAoBJ,GAAUpmB,KAAK1F,EAAOisB,QAASjsB,EAAQA,EAAOisB,QAASJ,GAG3E7rB,EAAOsf,QAAS,EAGTtf,EAAOisB,OACf,CAGAJ,EAAoBM,EAAID,EAGxBL,EAAoBO,EAAI,WAGvB,IAAIC,EAAsBR,EAAoBS,OAAEN,EAAW,CAAC,IAAI,EAAE,IAAI,IAAI,IAAI,IAAI,IAAI,MAAM,WAAa,OAAOH,EAAoB,MAAQ,IAE5I,OADAQ,EAAsBR,EAAoBS,EAAED,EAE7C,ECrCAR,EAAoBU,KAAO,CAAC,EjGAxBvqC,EAAW,GACf6pC,EAAoBS,EAAI,SAASrnC,EAAQunC,EAAUjW,EAAIrkB,GACtD,IAAGs6B,EAAH,CAMA,IAAIC,EAAeC,IACnB,IAASj/B,EAAI,EAAGA,EAAIzL,EAASmL,OAAQM,IAAK,CACrC++B,EAAWxqC,EAASyL,GAAG,GACvB8oB,EAAKv0B,EAASyL,GAAG,GACjByE,EAAWlQ,EAASyL,GAAG,GAE3B,IAJA,IAGIM,GAAY,EACP4+B,EAAI,EAAGA,EAAIH,EAASr/B,OAAQw/B,MACpB,EAAXz6B,GAAsBu6B,GAAgBv6B,IAAalK,OAAOC,KAAK4jC,EAAoBS,GAAGM,OAAM,SAASrnB,GAAO,OAAOsmB,EAAoBS,EAAE/mB,GAAKinB,EAASG,GAAK,IAChKH,EAASxB,OAAO2B,IAAK,IAErB5+B,GAAY,EACTmE,EAAWu6B,IAAcA,EAAev6B,IAG7C,GAAGnE,EAAW,CACb/L,EAASgpC,OAAOv9B,IAAK,GACrB,IAAIo/B,EAAItW,SACEyV,IAANa,IAAiB5nC,EAAS4nC,EAC/B,CACD,CACA,OAAO5nC,CArBP,CAJCiN,EAAWA,GAAY,EACvB,IAAI,IAAIzE,EAAIzL,EAASmL,OAAQM,EAAI,GAAKzL,EAASyL,EAAI,GAAG,GAAKyE,EAAUzE,IAAKzL,EAASyL,GAAKzL,EAASyL,EAAI,GACrGzL,EAASyL,GAAK,CAAC++B,EAAUjW,EAAIrkB,EAwB/B,EkG5BA25B,EAAoBiB,EAAI,SAAS9sB,GAChC,IAAI+sB,EAAS/sB,GAAUA,EAAOgtB,WAC7B,WAAa,OAAOhtB,EAAgB,OAAG,EACvC,WAAa,OAAOA,CAAQ,EAE7B,OADA6rB,EAAoBoB,EAAEF,EAAQ,CAAEz/B,EAAGy/B,IAC5BA,CACR,EjGPI7qC,EAAW8F,OAAOklC,eAAiB,SAASloC,GAAO,OAAOgD,OAAOklC,eAAeloC,EAAM,EAAI,SAASA,GAAO,OAAOA,EAAImoC,SAAW,EAQpItB,EAAoBtwB,EAAI,SAAS1W,EAAOuoC,GAEvC,GADU,EAAPA,IAAUvoC,EAAQkL,KAAKlL,IAChB,EAAPuoC,EAAU,OAAOvoC,EACpB,GAAoB,iBAAVA,GAAsBA,EAAO,CACtC,GAAW,EAAPuoC,GAAavoC,EAAMmoC,WAAY,OAAOnoC,EAC1C,GAAW,GAAPuoC,GAAoC,mBAAfvoC,EAAM2B,KAAqB,OAAO3B,CAC5D,CACA,IAAIwoC,EAAKrlC,OAAOslC,OAAO,MACvBzB,EAAoBgB,EAAEQ,GACtB,IAAIE,EAAM,CAAC,EACXtrC,EAAiBA,GAAkB,CAAC,KAAMC,EAAS,CAAC,GAAIA,EAAS,IAAKA,EAASA,IAC/E,IAAI,IAAIsrC,EAAiB,EAAPJ,GAAYvoC,EAAyB,iBAAX2oC,KAAyBvrC,EAAe4rB,QAAQ2f,GAAUA,EAAUtrC,EAASsrC,GACxHxlC,OAAOylC,oBAAoBD,GAAStlC,SAAQ,SAASqd,GAAOgoB,EAAIhoB,GAAO,WAAa,OAAO1gB,EAAM0gB,EAAM,CAAG,IAI3G,OAFAgoB,EAAa,QAAI,WAAa,OAAO1oC,CAAO,EAC5CgnC,EAAoBoB,EAAEI,EAAIE,GACnBF,CACR,EkGxBAxB,EAAoBoB,EAAI,SAAShB,EAASyB,GACzC,IAAI,IAAInoB,KAAOmoB,EACX7B,EAAoB8B,EAAED,EAAYnoB,KAASsmB,EAAoB8B,EAAE1B,EAAS1mB,IAC5Evd,OAAO4lC,eAAe3B,EAAS1mB,EAAK,CAAEsoB,YAAY,EAAMznC,IAAKsnC,EAAWnoB,IAG3E,ECPAsmB,EAAoBiC,EAAI,CAAC,EAGzBjC,EAAoBtlB,EAAI,SAASwnB,GAChC,OAAOvpC,QAAQqQ,IAAI7M,OAAOC,KAAK4jC,EAAoBiC,GAAG7gC,QAAO,SAAS+gC,EAAUzoB,GAE/E,OADAsmB,EAAoBiC,EAAEvoB,GAAKwoB,EAASC,GAC7BA,CACR,GAAG,IACJ,ECPAnC,EAAoBoC,EAAI,SAASF,GAEhC,OAAgB,MAAZA,EAAwB,kBACZ,IAAZA,EAAsB,gBACV,MAAZA,EAAwB,kBACZ,MAAZA,EAAwB,kBAEhBA,EAAU,IAAM,CAAC,IAAM,WAAW,IAAM,WAAW,IAAM,WAAW,IAAM,WAAW,IAAM,YAAYA,GAAW,WAC/H,ECRAlC,EAAoBqC,SAAW,SAASH,GAGxC,ECJAlC,EAAoBsC,EAAI,WACvB,GAA0B,iBAAfC,WAAyB,OAAOA,WAC3C,IACC,OAAOr+B,MAAQ,IAAIs+B,SAAS,cAAb,EAChB,CAAE,MAAO9nB,GACR,GAAsB,iBAAXwQ,OAAqB,OAAOA,MACxC,CACA,CAPuB,GCAxB8U,EAAoByC,IAAM,SAAStuB,GASlC,OARAA,EAAShY,OAAOslC,OAAOttB,IACXuuB,WAAUvuB,EAAOuuB,SAAW,IACxCvmC,OAAO4lC,eAAe5tB,EAAQ,UAAW,CACxC6tB,YAAY,EACZ7nC,IAAK,WACJ,MAAM,IAAIiN,MAAM,0FAA4F+M,EAAO9V,GACpH,IAEM8V,CACR,ECVA6rB,EAAoB8B,EAAI,SAAS3oC,EAAKwpC,GAAQ,OAAOxmC,OAAOwd,UAAUC,eAAeC,KAAK1gB,EAAKwpC,EAAO,ECCtG3C,EAAoBgB,EAAI,SAASZ,GACX,oBAAX7nC,QAA0BA,OAAOqqC,aAC1CzmC,OAAO4lC,eAAe3B,EAAS7nC,OAAOqqC,YAAa,CAAE5pC,MAAO,WAE7DmD,OAAO4lC,eAAe3B,EAAS,aAAc,CAAEpnC,OAAO,GACvD,ECNAgnC,EAAoB6C,IAAM,SAAS1uB,GAGlC,OAFAA,EAAO2uB,MAAQ,GACV3uB,EAAOuuB,WAAUvuB,EAAOuuB,SAAW,IACjCvuB,CACR,ECJA6rB,EAAoBjB,EAAI,I,WCAxBiB,EAAoBt+B,EAAIjL,KAAKssC,SAAW,GAIxC,IAAIC,EAAkB,CACrB,IAAK,GAkBNhD,EAAoBiC,EAAErgC,EAAI,SAASsgC,EAASC,GAEvCa,EAAgBd,IAElBe,cAAcjD,EAAoBjB,EAAIiB,EAAoBoC,EAAEF,GAG/D,EAEA,IAAIgB,EAAqBzsC,KAAsB,gBAAIA,KAAsB,iBAAK,GAC1E0sC,EAA6BD,EAAmBxiC,KAAK2R,KAAK6wB,GAC9DA,EAAmBxiC,KAzBA,SAAS3H,GAC3B,IAAI4nC,EAAW5nC,EAAK,GAChBqqC,EAAcrqC,EAAK,GACnBu8B,EAAUv8B,EAAK,GACnB,IAAI,IAAIknC,KAAYmD,EAChBpD,EAAoB8B,EAAEsB,EAAanD,KACrCD,EAAoBM,EAAEL,GAAYmD,EAAYnD,IAIhD,IADG3K,GAASA,EAAQ0K,GACdW,EAASr/B,QACd0hC,EAAgBrC,EAAS9E,OAAS,EACnCsH,EAA2BpqC,EAC5B,C,I3GtBIzC,EAAO0pC,EAAoBO,EAC/BP,EAAoBO,EAAI,WACvB,OAAO5nC,QAAQqQ,IAAI,CAAC,IAAI,EAAE,IAAI,IAAI,IAAI,IAAI,IAAI,KAAKpN,IAAIokC,EAAoBtlB,EAAGslB,IAAsBrlC,KAAKrE,EAC1G,E4GF0B0pC,EAAoBO,G","sources":["webpack://cyb/webpack/runtime/chunk loaded","webpack://cyb/webpack/runtime/create fake namespace object","webpack://cyb/webpack/runtime/startup chunk dependencies","webpack://cyb/./src/constants/config.ts","webpack://cyb/./src/constants/defaultNetworks.ts","webpack://cyb/./src/constants/patterns.ts","webpack://cyb/./src/containers/Search/types.ts","webpack://cyb/./src/services/QueueManager/types.ts","webpack://cyb/./src/services/backend/workers/serializers.ts","webpack://cyb/./src/services/backend/workers/factoryMethods.ts","webpack://cyb/./src/features/particle/utils.tsx","webpack://cyb/./src/services/CozoDb/types/entities.ts","webpack://cyb/./src/features/sense/redux/sense.redux.ts","webpack://cyb/./src/constants/localStorageKeys.ts","webpack://cyb/./src/redux/features/pocket.ts","webpack://cyb/./src/services/backend/channels/consts.ts","webpack://cyb/./src/services/backend/channels/BroadcastChannelSender.ts","webpack://cyb/./src/services/backend/channels/broadcastStatus.ts","webpack://cyb/./src/utils/async/iterable.ts","webpack://cyb/./src/services/backend/channels/BackendQueueChannel/backendQueueSenders.ts","webpack://cyb/./src/constants/app.ts","webpack://cyb/./src/services/backend/services/sync/services/consts.ts","webpack://cyb/./src/services/backend/services/sync/services/ParticlesResolverQueue/ParticlesResolverQueue.ts","webpack://cyb/./src/utils/string.ts","webpack://cyb/./src/services/CozoDb/mapping.ts","webpack://cyb/./src/utils/async/promise.ts","webpack://cyb/./src/generated/graphql.ts","webpack://cyb/./src/services/backend/services/indexer/types.ts","webpack://cyb/./src/services/lcd/utils/mapping.ts","webpack://cyb/./src/services/backend/services/indexer/utils/graphqlClient.ts","webpack://cyb/./src/services/backend/services/indexer/cyberlinks.ts","webpack://cyb/./src/services/backend/services/indexer/consts.ts","webpack://cyb/./src/services/backend/services/sync/services/utils/links.ts","webpack://cyb/./src/services/backend/services/indexer/transactions.ts","webpack://cyb/./src/services/backend/services/sync/services/utils/sense.ts","webpack://cyb/./src/services/backend/services/sync/services/SyncTransactionsLoop/services/chat.ts","webpack://cyb/./src/services/backend/services/sync/services/ProgressTracker/ProgressTracker.ts","webpack://cyb/./src/services/backend/services/sync/services/BaseSyncLoop/BaseSync.ts","webpack://cyb/./src/services/backend/services/sync/services/utils/rxjs/withInitializer.ts","webpack://cyb/./src/services/backend/services/sync/services/BaseSyncLoop/BaseSyncClient.ts","webpack://cyb/./src/services/backend/services/sync/services/SyncTransactionsLoop/SyncTransactionsLoop.ts","webpack://cyb/./src/services/lcd/websocket.ts","webpack://cyb/./src/utils/dto.ts","webpack://cyb/./src/services/backend/services/sync/utils.ts","webpack://cyb/./src/utils/exceptions/helpers.ts","webpack://cyb/./src/services/backend/services/sync/services/BaseSyncLoop/BaseSyncLoop.ts","webpack://cyb/./src/services/backend/services/sync/services/utils/rxjs/loop.ts","webpack://cyb/./src/services/backend/services/sync/services/SyncParticlesLoop/SyncParticlesLoop.ts","webpack://cyb/./src/services/backend/services/sync/services/SyncMyFriendsLoop/SyncMyFriendsLoop.ts","webpack://cyb/./src/services/community/community.ts","webpack://cyb/./src/services/community/lcd.ts","webpack://cyb/./src/services/backend/channels/BackendQueueChannel/BackendQueueChannel.ts","webpack://cyb/./src/services/backend/services/sync/sync.ts","webpack://cyb/./src/services/backend/services/sync/services/CommunitySync/CommunitySync.ts","webpack://cyb/./src/services/ipfs/utils/stream.ts","webpack://cyb/./src/db.js","webpack://cyb/./src/services/ipfs/utils/ipfsCacheDb.ts","webpack://cyb/./src/services/ipfs/config.ts","webpack://cyb/./src/services/ipfs/utils/cluster.ts","webpack://cyb/./src/services/ipfs/utils/content.ts","webpack://cyb/./src/services/ipfs/utils/utils-ipfs.ts","webpack://cyb/./src/services/QueueManager/QueueStrategy.ts","webpack://cyb/./src/services/QueueManager/QueueItemTimeoutError.ts","webpack://cyb/./src/services/QueueManager/constants.ts","webpack://cyb/./src/services/QueueManager/QueueManager.ts","webpack://cyb/./src/utils/rxjs/helpers.ts","webpack://cyb/./src/services/ipfs/utils/cid.ts","webpack://cyb/./src/services/ipfs/node/impl/kubo.ts","webpack://cyb/./src/services/ipfs/node/impl/helia.ts","webpack://cyb/./src/services/ipfs/node/impl/configs/jsIpfsConfig.ts","webpack://cyb/./src/services/ipfs/node/impl/js-ipfs.ts","webpack://cyb/./src/services/ipfs/node/factory.ts","webpack://cyb/./src/services/ipfs/node/mixins/withCybFeatures.ts","webpack://cyb/./src/services/backend/workers/background/api/mlApi.ts","webpack://cyb/./src/services/scripting/engine.ts","webpack://cyb/./src/services/backend/workers/background/api/runeApi.ts","webpack://cyb/./src/services/backend/workers/background/worker.ts","webpack://cyb/./src/services/backend/workers/background/api/ipfsApi.ts","webpack://cyb/./src/services/scripting/helpers.ts","webpack://cyb/./src/contexts/queryClient.tsx","webpack://cyb/./src/containers/portal/utils.ts","webpack://cyb/./src/services/passports/lcd.ts","webpack://cyb/./src/services/neuron/errors.ts","webpack://cyb/./src/services/neuron/neuronApi.ts","webpack://cyb/./src/services/scripting/runeDeps.ts","webpack://cyb/./src/services/scripting/services/llmRequests/openai.ts","webpack://cyb/./src/services/scripting/wasmBindings.js","webpack://cyb/./src/services/transactions/lcd.tsx","webpack://cyb/./src/types/networks.ts","webpack://cyb/./src/utils/config.ts","webpack://cyb/./src/utils/date.ts","webpack://cyb/./src/utils/ipfs/helpers.ts","webpack://cyb/./src/utils/logging/constants.ts","webpack://cyb/./src/utils/logging/cyblog.ts","webpack://cyb/./src/utils/search/utils.ts","webpack://cyb/./src/utils/utils.ts","webpack://cyb/webpack/bootstrap","webpack://cyb/webpack/runtime/amd options","webpack://cyb/webpack/runtime/compat get default export","webpack://cyb/webpack/runtime/define property getters","webpack://cyb/webpack/runtime/ensure chunk","webpack://cyb/webpack/runtime/get javascript chunk filename","webpack://cyb/webpack/runtime/get mini-css chunk filename","webpack://cyb/webpack/runtime/global","webpack://cyb/webpack/runtime/harmony module decorator","webpack://cyb/webpack/runtime/hasOwnProperty shorthand","webpack://cyb/webpack/runtime/make namespace object","webpack://cyb/webpack/runtime/node module decorator","webpack://cyb/webpack/runtime/publicPath","webpack://cyb/webpack/runtime/importScripts chunk loading","webpack://cyb/webpack/startup"],"sourcesContent":["var deferred = [];\n__webpack_require__.O = function(result, chunkIds, fn, priority) {\n\tif(chunkIds) {\n\t\tpriority = priority || 0;\n\t\tfor(var i = deferred.length; i > 0 && deferred[i - 1][2] > priority; i--) deferred[i] = deferred[i - 1];\n\t\tdeferred[i] = [chunkIds, fn, priority];\n\t\treturn;\n\t}\n\tvar notFulfilled = Infinity;\n\tfor (var i = 0; i < deferred.length; i++) {\n\t\tvar chunkIds = deferred[i][0];\n\t\tvar fn = deferred[i][1];\n\t\tvar priority = deferred[i][2];\n\t\tvar fulfilled = true;\n\t\tfor (var j = 0; j < chunkIds.length; j++) {\n\t\t\tif ((priority & 1 === 0 || notFulfilled >= priority) && Object.keys(__webpack_require__.O).every(function(key) { return __webpack_require__.O[key](chunkIds[j]); })) {\n\t\t\t\tchunkIds.splice(j--, 1);\n\t\t\t} else {\n\t\t\t\tfulfilled = false;\n\t\t\t\tif(priority < notFulfilled) notFulfilled = priority;\n\t\t\t}\n\t\t}\n\t\tif(fulfilled) {\n\t\t\tdeferred.splice(i--, 1)\n\t\t\tvar r = fn();\n\t\t\tif (r !== undefined) result = r;\n\t\t}\n\t}\n\treturn result;\n};","var getProto = Object.getPrototypeOf ? function(obj) { return Object.getPrototypeOf(obj); } : function(obj) { return obj.__proto__; };\nvar leafPrototypes;\n// create a fake namespace object\n// mode & 1: value is a module id, require it\n// mode & 2: merge all properties of value into the ns\n// mode & 4: return value when already ns object\n// mode & 16: return value when it's Promise-like\n// mode & 8|1: behave like require\n__webpack_require__.t = function(value, mode) {\n\tif(mode & 1) value = this(value);\n\tif(mode & 8) return value;\n\tif(typeof value === 'object' && value) {\n\t\tif((mode & 4) && value.__esModule) return value;\n\t\tif((mode & 16) && typeof value.then === 'function') return value;\n\t}\n\tvar ns = Object.create(null);\n\t__webpack_require__.r(ns);\n\tvar def = {};\n\tleafPrototypes = leafPrototypes || [null, getProto({}), getProto([]), getProto(getProto)];\n\tfor(var current = mode & 2 && value; typeof current == 'object' && !~leafPrototypes.indexOf(current); current = getProto(current)) {\n\t\tObject.getOwnPropertyNames(current).forEach(function(key) { def[key] = function() { return value[key]; }; });\n\t}\n\tdef['default'] = function() { return value; };\n\t__webpack_require__.d(ns, def);\n\treturn ns;\n};","var next = __webpack_require__.x;\n__webpack_require__.x = function() {\n\treturn Promise.all([444,1,922,964,948,112,235,187].map(__webpack_require__.e, __webpack_require__)).then(next);\n};","import { Networks } from 'src/types/networks';\nimport defaultNetworks from './defaultNetworks';\n\n// FIXME: seems temp\nfunction isWorker() {\n  return (\n    typeof WorkerGlobalScope !== 'undefined' &&\n    self instanceof WorkerGlobalScope\n  );\n}\n\nconst LOCALSTORAGE_CHAIN_ID = !isWorker() && localStorage.getItem('chainId');\n\nconst DEFAULT_CHAIN_ID: Networks.BOSTROM | Networks.SPACE_PUSSY =\n  LOCALSTORAGE_CHAIN_ID || process.env.CHAIN_ID || Networks.BOSTROM;\n\nexport const CHAIN_ID = DEFAULT_CHAIN_ID;\n\nexport const LCD_URL =\n  process.env.LCD_URL || defaultNetworks[DEFAULT_CHAIN_ID].LCD_URL;\n\nexport const RPC_URL =\n  process.env.RPC_URL || defaultNetworks[DEFAULT_CHAIN_ID].RPC_URL;\n\nexport const WEBSOCKET_URL =\n  process.env.WEBSOCKET_URL || defaultNetworks[DEFAULT_CHAIN_ID].WEBSOCKET_URL;\n\nexport const INDEX_HTTPS =\n  process.env.INDEX_HTTPS || defaultNetworks[DEFAULT_CHAIN_ID].INDEX_HTTPS;\n\nexport const INDEX_WEBSOCKET =\n  process.env.INDEX_WEBSOCKET ||\n  defaultNetworks[DEFAULT_CHAIN_ID].INDEX_WEBSOCKET;\n\nexport const BECH32_PREFIX =\n  process.env.BECH32_PREFIX || defaultNetworks[DEFAULT_CHAIN_ID].BECH32_PREFIX;\n\nconst BECH32_PREFIX_VAL = `${BECH32_PREFIX}val`;\n\nexport const BECH32_PREFIX_VALOPER = `${BECH32_PREFIX_VAL}oper`;\n\nexport const BECH32_PREFIX_VAL_CONS = `${BECH32_PREFIX_VAL}cons`;\n\nexport const BASE_DENOM =\n  process.env.BASE_DENOM || defaultNetworks[DEFAULT_CHAIN_ID].BASE_DENOM;\n\nexport const DENOM_LIQUID =\n  process.env.DENOM_LIQUID || defaultNetworks[DEFAULT_CHAIN_ID].DENOM_LIQUID;\n\nexport const CYBER_GATEWAY =\n  process.env.CYBER_GATEWAY || 'https://gateway.ipfs.cybernode.ai';\n\nexport const DIVISOR_CYBER_G = 10 ** 9;\n\nexport const DEFAULT_GAS_LIMITS = 200000;\n\nexport const COIN_DECIMALS_RESOURCE = 3;\n\nexport const { MEMO_KEPLR } = defaultNetworks[DEFAULT_CHAIN_ID];\n","import { NetworkConfig, Networks } from 'src/types/networks';\n\ntype NetworksList = {\n  [key in Networks.BOSTROM | Networks.SPACE_PUSSY]: NetworkConfig;\n};\n\nconst defaultNetworks: NetworksList = {\n  bostrom: {\n    CHAIN_ID: Networks.BOSTROM,\n    BASE_DENOM: 'boot',\n    DENOM_LIQUID: 'hydrogen',\n    RPC_URL: 'https://rpc.bostrom.cybernode.ai',\n    LCD_URL: 'https://lcd.bostrom.cybernode.ai',\n    WEBSOCKET_URL: 'wss://rpc.bostrom.cybernode.ai/websocket',\n    INDEX_HTTPS: 'https://index.bostrom.cybernode.ai/v1/graphql',\n    INDEX_WEBSOCKET: 'wss://index.bostrom.cybernode.ai/v1/graphql',\n    BECH32_PREFIX: 'bostrom',\n    MEMO_KEPLR: '[bostrom] cyb.ai, using keplr',\n  },\n  // localbostrom: {\n  //   CHAIN_ID: 'localbostrom',\n  //   BASE_DENOM: 'boot',\n  //   DENOM_LIQUID: 'hydrogen',\n  //   RPC_URL: 'https://rpc.bostrom.cybernode.ai',\n  //   LCD_URL: 'https://lcd.bostrom.cybernode.ai',\n  //   WEBSOCKET_URL: 'wss://rpc.bostrom.cybernode.ai/websocket',\n  //   INDEX_HTTPS: 'https://index.bostrom.cybernode.ai/v1/graphql',\n  //   INDEX_WEBSOCKET: 'wss://index.bostrom.cybernode.ai/v1/graphql',\n  //   BECH32_PREFIX: 'bostrom',\n  //   MEMO_KEPLR: '[bostrom] cyb.ai, using keplr',\n  // },\n  //    RPC_URL=https://rpc.bostrom.moon.cybernode.ai\n  //  LCD_URL=https://lcd.bostrom.moon.cybernode.ai\n  //  WEBSOCKET_URL=wss://rpc.bostrom.moon.cybernode.ai/websocket\n  //  INDEX_HTTPS=https://index.bostrom.moon.cybernode.ai/v1/graphql\n  //  INDEX_WEBSOCKET = wss://index.bostrom.moon.cybernode.ai/v1/graphql\n  //  CHAIN_ID = localbostrom\n\n  'space-pussy': {\n    CHAIN_ID: Networks.SPACE_PUSSY,\n    BASE_DENOM: 'pussy',\n    DENOM_LIQUID: 'liquidpussy',\n    RPC_URL: 'https://rpc.space-pussy.cybernode.ai/',\n    LCD_URL: 'https://lcd.space-pussy.cybernode.ai',\n    WEBSOCKET_URL: 'wss://rpc.space-pussy.cybernode.ai/websocket',\n    INDEX_HTTPS: 'https://index.space-pussy.cybernode.ai/v1/graphql',\n    INDEX_WEBSOCKET: 'wss://index.space-pussy.cybernode.ai/v1/graphql',\n    BECH32_PREFIX: 'pussy',\n    MEMO_KEPLR: '[space-pussy] cyb.ai, using keplr',\n  },\n};\n\nexport default defaultNetworks;\n","import { BECH32_PREFIX, BECH32_PREFIX_VALOPER } from './config';\n\nexport const PATTERN_CYBER = new RegExp(\n  `^${BECH32_PREFIX}[a-zA-Z0-9]{39}$`,\n  'g'\n);\n\nexport const PATTERN_SPACE_PUSSY = /^pussy[a-zA-Z0-9]{39}$/g;\n\nexport const PATTERN_IPFS_HASH = /^Qm[a-zA-Z0-9]{44}$/g;\n\nexport const PATTERN_CYBER_CONTRACT = new RegExp(\n  `^${BECH32_PREFIX}[a-zA-Z0-9]{59}$`,\n  'g'\n);\n\nexport const PATTERN_CYBER_VALOPER = new RegExp(\n  `^${BECH32_PREFIX_VALOPER}valoper[a-zA-Z0-9]{39}$`,\n  'g'\n);\n\nexport const PATTERN_COSMOS = /^cosmos[a-zA-Z0-9]{39}$/g;\n\nexport const PATTERN_OSMOS = /^osmo[a-zA-Z0-9]{39}$/g;\n\nexport const PATTERN_TERRA = /^terra[a-zA-Z0-9]{39}$/g;\n\nexport const PATTERN_ETH = /^0x[a-fA-F0-9]{40}$/g;\n\nexport const PATTERN_TX = /[0-9a-fA-F]{64}$/g;\n\nexport const PATTERN_BLOCK = /^[0-9]+$/g;\n\nexport const PATTERN_HTTP = /^https:\\/\\/|^http:\\/\\//g;\n\nexport const PATTERN_HTML = /<\\/?[\\w\\d]+>/gi;\n","export enum LinksTypeFilter {\n  to = 'to',\n  from = 'from',\n  all = 'all',\n}\n\nexport type LinksType = Exclude<LinksTypeFilter, LinksTypeFilter.all>;\n\nexport type SearchItem = {\n  cid: string;\n  rank?: string;\n  grade?: string;\n  timestamp?: string;\n  type?: LinksTypeFilter;\n};\n\nexport enum SortBy {\n  rank = 'rank',\n  date = 'date',\n  // not ready\n  // popular = 'popular',\n  // mine = 'mine',\n}\n","import { Option } from 'src/types';\nimport { IPFSContent, IpfsContentSource } from '../ipfs/types';\n\n/* eslint-disable import/no-unused-modules */\nexport type QueueItemStatus =\n  | 'pending'\n  | 'executing'\n  | 'timeout'\n  | 'completed'\n  | 'cancelled'\n  | 'error'\n  | 'not_found';\n\nexport type QueueSourceSettings = {\n  timeout: number;\n  maxConcurrentExecutions: number;\n};\n\nexport type QueueSource = IpfsContentSource;\n\nexport type QueueSettings = Record<QueueSource, QueueSourceSettings>;\n\nexport interface IQueueStrategy {\n  settings: QueueSettings;\n  order: QueueSource[];\n  getNextSource(source: QueueSource): QueueSource | undefined;\n}\n\nexport type QueueStats = {\n  status: QueueItemStatus;\n  count: number;\n};\n\nexport enum QueuePriority {\n  ZERO = 0,\n  LOW = 0.1,\n  MEDIUM = 0.5,\n  HIGH = 0.9,\n  URGENT = 1,\n}\nexport type QueueItemOptions = {\n  parent?: string;\n  priority?: QueuePriority | number;\n  viewPortPriority?: number;\n  initialSource?: QueueSource;\n  postProcessing?: boolean;\n};\n\nexport type QueueItemCallback = (\n  cid: string,\n  status: QueueItemStatus,\n  source: QueueSource,\n  result?: Option<IPFSContent>\n) => void;\n\nexport type QueueItem = {\n  cid: string;\n  source: QueueSource;\n  status: QueueItemStatus;\n  callbacks: QueueItemCallback[];\n  controller?: AbortController;\n  executionTime?: number;\n} & Omit<QueueItemOptions, 'initialSource'>;\n\nexport type QueueItemResult = {\n  item: QueueItem;\n  status: QueueItemStatus;\n  source: QueueSource;\n  result?: Option<IPFSContent>;\n};\n\nexport type QueueItemAsyncResult = Omit<QueueItemResult, 'item'>;\n\nexport type QueueItemPostProcessor = (\n  content: Option<IPFSContent>\n) => Promise<Option<IPFSContent>>;\n\nexport type FetchParticleAsync = (\n  cid: string,\n  options?: QueueItemOptions\n) => Promise<QueueItemAsyncResult>;\n","import type { TransferHandler } from 'comlink';\nimport { IPFSContent } from 'src/services/ipfs/types';\n\nexport type IPFSContentTransferable = Omit<IPFSContent, 'result'> & {\n  port: MessagePort;\n};\n\nfunction createAsyncIterable(port: MessagePort): AsyncIterable<Uint8Array> {\n  return {\n    async *[Symbol.asyncIterator](): AsyncGenerator<\n      Uint8Array,\n      void,\n      undefined\n    > {\n      let done = false;\n      while (!done) {\n        // eslint-disable-next-line no-loop-func\n        const promise = new Promise<Uint8Array | null>((resolve) => {\n          // resolve = res;\n          port.onmessage = (event: MessageEvent) => {\n            if (event.data === null) {\n              done = true;\n              resolve(null);\n            } else {\n              resolve(event.data);\n            }\n          };\n        });\n        // eslint-disable-next-line no-await-in-loop\n        const value = await promise;\n        // eslint-disable-next-line no-await-in-loop\n        if (value !== null) {\n          yield value;\n        }\n      }\n    },\n  };\n}\n\nconst IPFSContentTransferHandler: TransferHandler<\n  IPFSContent | undefined,\n  IPFSContentTransferable | null\n> = {\n  canHandle: (obj: IPFSContent | undefined) =>\n    obj && obj.result && typeof obj.result[Symbol.asyncIterator] === 'function',\n  serialize(obj: IPFSContent) {\n    if (obj === undefined) {\n      return [null, []];\n    }\n    const { result, ...rest } = obj;\n    const { port1, port2 } = new MessageChannel();\n    if (result) {\n      (async () => {\n        // eslint-disable-next-line no-restricted-syntax\n        for await (const value of result) {\n          port1.postMessage(value);\n        }\n        port1.postMessage(null); // Send  \"end\" message\n\n        port1.close();\n      })();\n    }\n    return [{ ...rest, port: port2 }, [port2]];\n  },\n  deserialize(serializedObj: IPFSContentTransferable | null) {\n    if (!serializedObj) {\n      return undefined;\n    }\n    const { port, ...rest } = serializedObj;\n\n    return {\n      ...rest,\n      result: createAsyncIterable(port),\n    };\n  },\n};\n\nexport {\n  IPFSContentTransferHandler,\n  // serializeIPFSContent,\n  // deserializeIPFSContent,\n};\n","import {\n  wrap,\n  Remote,\n  proxy,\n  releaseProxy,\n  expose,\n  transferHandlers,\n} from 'comlink';\nimport { IPFSContentTransferHandler } from './serializers';\nimport { Observable, Observer, Subscribable, Subscription } from 'rxjs'; // v7.8.0\ntype WorkerType = SharedWorker | Worker;\n\nconst isSharedWorkersSupported = typeof SharedWorker !== 'undefined';\n\nconst isSharedWorkerUsed = isSharedWorkersSupported && !process.env.IS_DEV;\n\n// apply serializers for custom types\nfunction installTransferHandlers() {\n  transferHandlers.set('IPFSContent', IPFSContentTransferHandler);\n  transferHandlers.set('observable', {\n    canHandle: (value: unknown): value is Observable<unknown> => {\n      return value instanceof Observable;\n    },\n    deserialize: (value: MessagePort) => {\n      return new Observable<unknown>((observer) => {\n        const remote = transferHandlers\n          .get('proxy')!\n          .deserialize(value) as Remote<Subscribable<unknown>>;\n\n        remote\n          .subscribe(\n            proxy({\n              next: (next: unknown) => observer.next(next),\n              error: (error: unknown) => observer.error(error),\n              complete: () => observer.complete(),\n            })\n          )\n          .then((subscription) =>\n            observer.add(() => {\n              subscription.unsubscribe();\n              remote[releaseProxy]();\n            })\n          );\n      });\n    },\n    serialize: (value: Observable<unknown>) => {\n      return transferHandlers.get('proxy')!.serialize({\n        subscribe: (observer: Remote<Observer<unknown>>) =>\n          value.subscribe({\n            next: (next: unknown) => observer.next(next).then(),\n            error: (error: unknown) => observer.error(error).then(),\n            complete: () => observer.complete().then(),\n          }),\n      });\n    },\n  });\n\n  transferHandlers.set('subscription', {\n    canHandle: (value: unknown): value is Subscription => {\n      return value instanceof Subscription;\n    },\n    deserialize: (value: MessagePort) => {\n      return new Subscription(() => {\n        const remote = transferHandlers\n          .get('proxy')!\n          .deserialize(value) as Remote<Subscription>;\n\n        remote.unsubscribe().then(() => {\n          remote[releaseProxy]();\n        });\n      });\n    },\n    serialize: (value: Subscription) => {\n      return transferHandlers.get('proxy')!.serialize({\n        unsubscribe: () => value.unsubscribe(),\n      });\n    },\n  });\n}\n\nfunction safeStringify(obj: any): string {\n  try {\n    return JSON.stringify(obj);\n  } catch (error) {\n    return String(obj);\n  }\n}\n\n// Override console.log to send logs to main thread\nfunction overrideLogging(worker: Worker | MessagePort) {\n  const consoleLogMap = {\n    log: { original: console.log },\n    error: { original: console.error },\n    warn: { original: console.warn },\n  };\n  const replaceConsoleLog = (method: keyof typeof consoleLogMap) => {\n    const { original } = consoleLogMap[method];\n\n    consoleLogMap[method].original = console[method];\n\n    console[method] = (...args) => {\n      original.apply(console, args);\n      const serializableArgs = args.map((arg) => safeStringify(arg));\n\n      worker.postMessage({ type: 'console', method, args: serializableArgs });\n    };\n  };\n\n  Object.keys(consoleLogMap).forEach((method) =>\n    replaceConsoleLog(method as keyof typeof consoleLogMap)\n  );\n}\n\n// Install handlers for logging from worker\nfunction installLoggingHandler(worker: Worker | MessagePort, name: string) {\n  // Add event listener\n  worker.addEventListener('message', (event) => {\n    if (event.data.type === 'console') {\n      const { method, args } = event.data;\n\n      console[method](name, ...args);\n    }\n  });\n}\n\n// Create Shared Worker with fallback to usual Worker(in case of DEV too)\nexport function createWorkerApi<T>(\n  workerUrl: URL,\n  workerName: string\n): { worker: WorkerType; workerApiProxy: Remote<T> } {\n  installTransferHandlers();\n  //&& !process.env.IS_DEV\n  if (isSharedWorkerUsed) {\n    const worker = new SharedWorker(workerUrl, { name: workerName });\n    installLoggingHandler(worker.port, workerName);\n    return { worker, workerApiProxy: wrap<T>(worker.port) };\n  }\n\n  const worker = new Worker(workerUrl);\n  // installLoggingHandler(worker, workerName);\n  return { worker, workerApiProxy: wrap<T>(worker) };\n}\n\nexport function exposeWorkerApi<T>(worker: WorkerType, api: T) {\n  installTransferHandlers();\n  if (typeof worker.onconnect !== 'undefined') {\n    worker.onconnect = (e) => {\n      const port = e.ports[0];\n      overrideLogging(port);\n\n      expose(api, port);\n    };\n  } else {\n    // overrideLogging(worker);\n    expose(api);\n  }\n}\n","export function isParticle(value: string) {\n  // copied from src/utils/config.ts , to prevent crash in worker, need refactor\n  // import { PATTERN_IPFS_HASH } from 'src/utils/config';\n  return Boolean(value.match(/^Qm[a-zA-Z0-9]{44}$/g));\n}\n","import { PinType } from 'ipfs-core-types/src/pin';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { Transaction } from 'src/services/backend/services/indexer/types';\nimport {\n  SenseChatExtension,\n  SenseLinkMeta,\n  SenseListItemtMeta,\n  SenseTransactionMeta,\n} from 'src/services/backend/types/sense';\nimport { IpfsContentType } from 'src/services/ipfs/types';\nimport { NeuronAddress, ParticleCid, TransactionHash } from 'src/types/base';\nimport { DtoToEntity } from 'src/types/dto';\n\ntype PinEntryType = Exclude<PinType, 'all'>;\n// example of db optimization for classifiers\n\nexport const PinTypeMap: Record<PinEntryType, number> = {\n  indirect: -1,\n  direct: 0,\n  recursive: 1,\n};\n\nexport enum EntryType {\n  transactions = 1,\n  particle = 2,\n  chat = 3,\n}\n\n// Transaction if formed by frontend\n// Should be replaced after sync\n\nexport type PinDbEntity = {\n  cid: string;\n  type: keyof typeof PinTypeMap;\n};\n\nexport type TransactionDbEntity = {\n  hash: string;\n  index: number;\n  type: string;\n  timestamp: number;\n  block_height: number;\n  value: Transaction['value'];\n  success: boolean;\n  memo: string;\n  neuron: NeuronAddress;\n};\n\ntype SyncItemMeta = DtoToEntity<\n  (SenseLinkMeta | SenseTransactionMeta) & SenseChatExtension\n>;\n\nexport type SyncStatusDbEntity = {\n  entry_type: EntryType;\n  id: NeuronAddress | ParticleCid;\n  owner_id: NeuronAddress;\n  timestamp_update: number;\n  timestamp_read: number;\n  disabled: boolean;\n  unread_count: number;\n  meta: SyncItemMeta;\n};\n\nexport type ParticleDbEntity = {\n  id: ParticleCid;\n  size: number;\n  size_local: number;\n  blocks: number;\n  mime: string;\n  type: IpfsContentType;\n  text: string;\n};\n\nexport type LinkDbEntity = {\n  from: ParticleCid;\n  to: ParticleCid;\n  neuron: NeuronAddress;\n  timestamp: number;\n  transaction_hash: string;\n};\n\nexport type ConfigDbEntity = {\n  key: string;\n  group_key: string;\n  value: NonNullable<unknown>;\n};\n\nexport enum SyncQueueStatus {\n  pending = 0,\n  executing = 1,\n  done = 2,\n  error = -1,\n}\n\nexport enum SyncQueueJobType {\n  particle = 0,\n  embedding = 1,\n}\n\nexport type SyncQueueKey = {\n  id: string;\n  job_type: SyncQueueJobType;\n};\n\nexport type SyncQueueDbEntity = SyncQueueKey & {\n  data: string;\n  status: SyncQueueStatus;\n  priority: QueuePriority | number;\n};\n\nexport type CommunityDbEntity = {\n  ownerId: NeuronAddress;\n  particle: ParticleCid;\n  neuron: NeuronAddress;\n  name: string;\n  following: boolean;\n  follower: boolean;\n};\n\nexport type EmbeddinsDbEntity = {\n  cid: ParticleCid;\n  vec: number[];\n};\n\nexport type DbEntity =\n  | TransactionDbEntity\n  | ParticleDbEntity\n  | SyncStatusDbEntity\n  | ConfigDbEntity\n  | SyncQueueDbEntity\n  | EmbeddinsDbEntity;\n","import {\n  createAsyncThunk,\n  createSelector,\n  createSlice,\n  PayloadAction,\n} from '@reduxjs/toolkit';\nimport { SenseApi } from 'src/contexts/backend/services/senseApi';\nimport {\n  SenseItemLinkMeta,\n  SenseListItem,\n  SenseListItemTransactionMeta,\n  SenseUnread,\n} from 'src/services/backend/types/sense';\nimport { isParticle } from '../../particle/utils';\nimport { SenseItemId } from '../types/sense';\nimport { EntryType } from 'src/services/CozoDb/types/entities';\nimport {\n  MsgMultiSendValue,\n  MsgSendValue,\n} from 'src/services/backend/services/indexer/types';\nimport { RootState } from 'src/redux/store';\n\n// similar to blockchain/tx/message type\nexport type SenseItem = {\n  id: SenseItemId;\n  transactionHash: string;\n\n  // add normal type\n  type: string;\n\n  meta: SenseListItem['meta'];\n  timestamp: string;\n  memo: string | undefined;\n  from: string;\n\n  // for optimistic update\n  status?: 'pending' | 'error';\n  fromLog?: boolean;\n};\n\ntype Chat = {\n  id: SenseItemId;\n  isLoading: boolean;\n  error: string | undefined;\n  data: SenseItem[];\n  unreadCount: number;\n};\n\ntype SliceState = {\n  list: {\n    isLoading: boolean;\n    data: string[];\n    error: string | undefined;\n  };\n  chats: {\n    [key in SenseItemId]?: Chat;\n  };\n  summary: {\n    unreadCount: {\n      total: number;\n      particles: number;\n      neurons: number;\n    };\n  };\n};\n\nconst initialState: SliceState = {\n  list: {\n    isLoading: false,\n    data: [],\n    error: undefined,\n  },\n  chats: {},\n  summary: {\n    unreadCount: {\n      total: 0,\n      particles: 0,\n      neurons: 0,\n    },\n  },\n};\n\nfunction formatApiData(item: SenseListItem): SenseItem {\n  if (item.entryType === EntryType.chat && item.meta.to) {\n    item.entryType = EntryType.particle;\n  }\n\n  const { meta } = item;\n\n  const formatted: SenseItem = {\n    timestamp: new Date(meta.timestamp).toISOString(),\n\n    // lol\n    transactionHash:\n      item.transactionHash ||\n      item.hash ||\n      item.meta.transaction_hash ||\n      item.meta.hash ||\n      item.meta.transactionHash,\n\n    memo: item.memo || meta.memo,\n\n    senseChatId: item.id,\n    // not good\n    unreadCount: item.unreadCount || 0,\n  };\n\n  switch (item.entryType) {\n    case EntryType.chat:\n    case EntryType.transactions: {\n      const meta = item.meta as SenseListItemTransactionMeta;\n      const { type } = meta;\n\n      let from = item.ownerId;\n\n      if (type === 'cosmos.bank.v1beta1.MsgSend') {\n        const value = meta.value as MsgSendValue;\n        from = value.fromAddress;\n      } else if (type === 'cosmos.bank.v1beta1.MsgMultiSend') {\n        const value = meta.value as MsgMultiSendValue;\n\n        from = value.inputs[0].address;\n      }\n\n      Object.assign(formatted, {\n        type,\n        from,\n        meta: item.meta.value,\n      });\n\n      break;\n    }\n\n    case EntryType.particle: {\n      const meta = item.meta as SenseItemLinkMeta;\n\n      Object.assign(formatted, {\n        type: 'cyber.graph.v1beta1.MsgCyberlink',\n        from: meta.neuron,\n        meta: meta,\n        fromLog: true,\n      });\n\n      break;\n    }\n\n    default:\n      // sholdn't be\n      debugger;\n      return {};\n  }\n\n  return formatted;\n}\n\nconst getSenseList = createAsyncThunk(\n  'sense/getSenseList',\n  async (senseApi: SenseApi) => {\n    const data = await senseApi!.getList();\n    return data.map(formatApiData);\n  }\n);\n\nconst getSenseChat = createAsyncThunk(\n  'sense/getSenseChat',\n  async ({ id, senseApi }: { id: SenseItemId; senseApi: SenseApi }) => {\n    const particle = isParticle(id);\n\n    if (particle) {\n      const links = await senseApi!.getLinks(id);\n      const formattedLinks = links.map((item) => {\n        if (item.timestamp === 0) {\n          // FIXME:\n          return;\n        }\n        return formatApiData({\n          ...item,\n          id,\n          entryType: EntryType.particle,\n          meta: item,\n        });\n      });\n\n      return formattedLinks.filter(Boolean);\n    }\n\n    const data = await senseApi!.getFriendItems(id);\n    const formattedData = data.map((item) => {\n      const entryType = item.to ? EntryType.particle : EntryType.chat;\n      return formatApiData({\n        ...item,\n        entryType,\n        id,\n        meta: item,\n      });\n    });\n\n    return formattedData;\n  }\n);\n\nconst markAsRead = createAsyncThunk(\n  'sense/markAsRead',\n  async ({ id, senseApi }: { id: SenseItemId; senseApi: SenseApi }) => {\n    return senseApi!.markAsRead(id);\n  }\n);\n\nconst newChatStructure: Chat = {\n  id: '',\n  isLoading: false,\n  data: [],\n  error: undefined,\n  unreadCount: 0,\n};\n\nfunction checkIfMessageExists(chat: Chat, newMessage: SenseItem) {\n  const lastMessages = chat.data.slice(-5);\n\n  const isMessageExists = lastMessages.some((msg) => {\n    return msg.transactionHash === newMessage.transactionHash;\n  });\n\n  return isMessageExists;\n}\n\nconst slice = createSlice({\n  name: 'sense',\n  initialState,\n  reducers: {\n    // backend may push this action\n    updateSenseList: {\n      reducer: (state, action: PayloadAction<SenseItem[]>) => {\n        const data = action.payload;\n\n        data.forEach((message) => {\n          const { senseChatId: id } = message;\n\n          if (!state.chats[id]) {\n            state.chats[id] = { ...newChatStructure };\n          }\n\n          const chat = state.chats[id]!;\n\n          Object.assign(chat, {\n            id,\n            // fix ts\n            unreadCount: message.unreadCount || 0,\n          });\n\n          if (!checkIfMessageExists(chat, message)) {\n            chat.data = chat.data.concat(message);\n          }\n        });\n\n        slice.caseReducers.orderSenseList(state);\n      },\n      prepare: (data: SenseListItem[]) => {\n        return {\n          payload: data.map(formatApiData),\n        };\n      },\n    },\n    // optimistic update\n    addSenseItem(\n      state,\n      action: PayloadAction<{ id: SenseItemId; item: SenseItem }>\n    ) {\n      const { id, item } = action.payload;\n      const chat = state.chats[id]!;\n\n      chat.data.push({\n        ...item,\n        meta: item.meta,\n        status: 'pending',\n      });\n\n      const newList = state.list.data.filter((item) => item !== id);\n      newList.unshift(id);\n      state.list.data = newList;\n    },\n    // optimistic confirm/error\n    updateSenseItem(\n      state,\n      action: PayloadAction<{\n        chatId: SenseItemId;\n        txHash: string;\n        isSuccess: boolean;\n      }>\n    ) {\n      const { chatId, txHash, isSuccess } = action.payload;\n      const chat = state.chats[chatId]!;\n\n      const item = chat.data.find((item) => item.transactionHash === txHash);\n\n      if (item) {\n        if (isSuccess) {\n          delete item.status;\n        } else {\n          item.status = 'error';\n        }\n      }\n    },\n    orderSenseList(state) {\n      const chatsLastMessage = Object.keys(state.chats).reduce<\n        {\n          id: string;\n          lastMsg: SenseItem;\n        }[]\n      >((acc, id) => {\n        const chat = state.chats[id]!;\n\n        // may be loading this moment, no data\n        if (!chat.data.length) {\n          return acc;\n        }\n\n        const lastMsg = chat.data[chat.data.length - 1];\n        acc.push({ id, lastMsg });\n\n        return acc;\n      }, []);\n\n      const sorted = chatsLastMessage.sort((a, b) => {\n        return (\n          Date.parse(b.lastMsg.timestamp) - Date.parse(a.lastMsg.timestamp)\n        );\n      });\n\n      state.list.data = sorted.map((i) => i.id);\n    },\n    reset() {\n      return initialState;\n    },\n  },\n\n  extraReducers: (builder) => {\n    builder.addCase(getSenseList.pending, (state) => {\n      state.list.isLoading = true;\n    });\n\n    builder.addCase(getSenseList.fulfilled, (state, action) => {\n      state.list.isLoading = false;\n\n      const newList: SliceState['list']['data'] = [];\n\n      action.payload.forEach((message) => {\n        const { senseChatId: id } = message;\n\n        if (!state.chats[id]) {\n          state.chats[id] = { ...newChatStructure };\n        }\n\n        const chat = state.chats[id]!;\n\n        Object.assign(chat, {\n          id,\n          // fix\n          unreadCount: message.unreadCount || 0,\n        });\n\n        if (!checkIfMessageExists(chat, message)) {\n          chat.data = chat.data.concat(message);\n        }\n\n        newList.push(id);\n      });\n\n      state.list.data = newList;\n    });\n    builder.addCase(getSenseList.rejected, (state, action) => {\n      console.error(action);\n\n      state.list.isLoading = false;\n      state.list.error = action.error.message;\n    });\n\n    builder.addCase(getSenseChat.pending, (state, action) => {\n      const { id } = action.meta.arg;\n\n      if (!state.chats[id]) {\n        state.chats[id] = { ...newChatStructure };\n      }\n\n      // don't understand why ts warning\n      state.chats[id].isLoading = true;\n    });\n\n    builder.addCase(getSenseChat.fulfilled, (state, action) => {\n      const { id } = action.meta.arg;\n      const chat = state.chats[id]!;\n      chat.isLoading = false;\n\n      chat.id = id;\n\n      chat.data = action.payload;\n    });\n    builder.addCase(getSenseChat.rejected, (state, action) => {\n      console.error(action);\n\n      const chat = state.chats[action.meta.arg.id]!;\n      chat.isLoading = false;\n      chat.error = action.error.message;\n    });\n\n    // maybe add .pending, .rejected\n    // can be optimistic\n    builder.addCase(markAsRead.fulfilled, (state, action) => {\n      const { id } = action.meta.arg;\n      const chat = state.chats[id]!;\n\n      const particle = isParticle(id);\n\n      const { unreadCount } = chat;\n\n      state.summary.unreadCount.total -= unreadCount;\n      if (particle) {\n        state.summary.unreadCount.particles -= unreadCount;\n      } else {\n        state.summary.unreadCount.neurons -= unreadCount;\n      }\n\n      chat.unreadCount = 0;\n    });\n  },\n});\n\nconst selectUnreadCounts = createSelector(\n  (state: RootState) => state.sense.chats,\n  (chats) => {\n    let unreadCountParticle = 0;\n    let unreadCountNeuron = 0;\n\n    Object.values(chats).forEach(({ id, unreadCount }) => {\n      const particle = isParticle(id);\n\n      if (particle) {\n        unreadCountParticle += unreadCount;\n      } else {\n        unreadCountNeuron += unreadCount;\n      }\n    });\n\n    const total = unreadCountParticle + unreadCountNeuron;\n\n    return {\n      total,\n      particles: unreadCountParticle,\n      neurons: unreadCountNeuron,\n    };\n  }\n);\n\nexport const { addSenseItem, updateSenseItem, updateSenseList, reset } =\n  slice.actions;\n\nexport { getSenseList, getSenseChat, markAsRead };\n\n// selectors\nexport { selectUnreadCounts };\n\nexport default slice.reducer;\n","export const localStorageKeys = {\n  pocket: {\n    POCKET: 'pocket',\n    POCKET_ACCOUNT: 'pocketAccount',\n  },\n  MENU_SHOW: 'menuShow',\n  settings: {\n    adviserAudio: 'adviserAudio',\n    adviserVoice: 'adviserVoice',\n  },\n};\n","import { Dispatch } from 'redux';\nimport { localStorageKeys } from 'src/constants/localStorageKeys';\n\nimport {\n  Account,\n  AccountValue,\n  Accounts,\n  DefaultAccount,\n} from 'src/types/defaultAccount';\nimport { PayloadAction, createSlice } from '@reduxjs/toolkit';\nimport { POCKET } from '../../utils/config';\nimport { RootState } from '../store';\n\ntype SliceState = {\n  actionBar: {\n    tweet: string;\n  };\n  defaultAccount: DefaultAccount;\n  accounts: null | Accounts;\n  isInitialized: boolean;\n};\n\nconst initialState: SliceState = {\n  actionBar: {\n    tweet: POCKET.STAGE_TWEET_ACTION_BAR.TWEET, // stage for tweet ActionBar: 'addAvatar' 'follow' 'tweet'\n  },\n  isInitialized: false,\n  defaultAccount: {\n    name: null,\n    account: null,\n  },\n  accounts: null,\n};\n\nconst checkAddress = (obj, network, address) =>\n  Object.keys(obj).some((k) => {\n    if (obj[k][network]) {\n      return obj[k][network].bech32 === address;\n    }\n  });\n\nfunction saveToLocalStorage(state: SliceState) {\n  const { defaultAccount, accounts } = state;\n\n  defaultAccount &&\n    localStorage.setItem(\n      localStorageKeys.pocket.POCKET,\n      JSON.stringify({\n        [defaultAccount.name]: defaultAccount.account,\n      })\n    );\n  accounts &&\n    localStorage.setItem(\n      localStorageKeys.pocket.POCKET_ACCOUNT,\n      JSON.stringify(accounts)\n    );\n}\n\nconst slice = createSlice({\n  name: 'pocket',\n  initialState,\n  reducers: {\n    setDefaultAccount: (\n      state,\n      {\n        payload: { name, account },\n      }: PayloadAction<{ name: string; account?: Account }>\n    ) => {\n      state.defaultAccount = {\n        name,\n        account: account || state.accounts?.[name] || null,\n      };\n\n      saveToLocalStorage(state);\n    },\n    setAccounts: (state, { payload }: PayloadAction<Accounts>) => {\n      state.accounts = payload;\n\n      saveToLocalStorage(state);\n    },\n    setInitialized: (state) => {\n      state.isInitialized = true;\n    },\n    setStageTweetActionBar: (state, { payload }: PayloadAction<string>) => {\n      state.actionBar.tweet = payload;\n    },\n\n    // bullshit\n    deleteAddress: (state, { payload }: PayloadAction<string>) => {\n      if (state.accounts) {\n        Object.keys(state.accounts).forEach((accountKey) => {\n          Object.keys(state.accounts[accountKey]).forEach((networkKey) => {\n            if (state.accounts[accountKey][networkKey].bech32 === payload) {\n              delete state.accounts[accountKey][networkKey];\n\n              if (Object.keys(state.accounts[accountKey]).length === 0) {\n                delete state.accounts[accountKey];\n              }\n\n              if (state.defaultAccount?.account?.cyber?.bech32 === payload) {\n                const entries = Object.entries(state.accounts);\n\n                const entryCyber = entries.find(\n                  ([, value]) => value.cyber?.bech32\n                );\n\n                if (entryCyber) {\n                  state.defaultAccount = {\n                    name: entryCyber[0],\n                    account: entryCyber[1],\n                  };\n                } else {\n                  state.defaultAccount = {\n                    name: null,\n                    account: null,\n                  };\n                }\n              }\n\n              saveToLocalStorage(state);\n            }\n          });\n        });\n      }\n    },\n  },\n});\n\nexport const selectCurrentAddress = (store: RootState) =>\n  store.pocket.defaultAccount.account?.cyber?.bech32;\n\nexport const {\n  setDefaultAccount,\n  setAccounts,\n  setStageTweetActionBar,\n  deleteAddress,\n} = slice.actions;\n\nexport default slice.reducer;\n\n// refactor this\nexport const initPocket = () => (dispatch: Dispatch) => {\n  let defaultAccounts = null;\n  let defaultAccountsKeys = null;\n  let accountsTemp: Accounts | null = null;\n\n  const localStoragePocketAccount = localStorage.getItem(\n    localStorageKeys.pocket.POCKET_ACCOUNT\n  );\n  const localStoragePocket = localStorage.getItem(\n    localStorageKeys.pocket.POCKET\n  );\n  if (localStoragePocket !== null) {\n    const localStoragePocketData = JSON.parse(localStoragePocket);\n    const keyPocket = Object.keys(localStoragePocketData)[0];\n    const accountPocket = Object.values(localStoragePocketData)[0];\n    defaultAccounts = accountPocket;\n    defaultAccountsKeys = keyPocket;\n  }\n  if (localStoragePocketAccount !== null) {\n    const localStoragePocketAccountData = JSON.parse(localStoragePocketAccount);\n    if (localStoragePocket === null) {\n      const keys0 = Object.keys(localStoragePocketAccountData)[0];\n      localStorage.setItem(\n        localStorageKeys.pocket.POCKET,\n        JSON.stringify({ [keys0]: localStoragePocketAccountData[keys0] })\n      );\n      defaultAccounts = localStoragePocketAccountData[keys0];\n      defaultAccountsKeys = keys0;\n    } else if (defaultAccountsKeys !== null) {\n      accountsTemp = {\n        [defaultAccountsKeys]:\n          localStoragePocketAccountData[defaultAccountsKeys] || undefined,\n        ...localStoragePocketAccountData,\n      };\n    }\n  } else {\n    localStorage.removeItem(localStorageKeys.pocket.POCKET);\n    localStorage.removeItem(localStorageKeys.pocket.POCKET_ACCOUNT);\n  }\n\n  defaultAccountsKeys &&\n    defaultAccounts &&\n    dispatch(\n      setDefaultAccount({\n        name: defaultAccountsKeys,\n        account: defaultAccounts,\n      })\n    );\n\n  accountsTemp &&\n    Object.keys(accountsTemp).forEach((key) => {\n      if (!accountsTemp[key] || Object.keys(accountsTemp[key]).length === 0) {\n        delete accountsTemp[key];\n      }\n    });\n\n  accountsTemp && dispatch(setAccounts(accountsTemp));\n\n  dispatch(slice.actions.setInitialized());\n};\n\nconst defaultNameAccount = () => {\n  let key = 'Account 1';\n  let count = 1;\n\n  const localStorageCount = localStorage.getItem('count');\n\n  if (localStorageCount !== null) {\n    const dataCount = JSON.parse(localStorageCount);\n    count = parseFloat(dataCount);\n    key = `Account ${count}`;\n  }\n\n  localStorage.setItem('count', JSON.stringify(count + 1));\n\n  return key;\n};\n\nexport const addAddressPocket =\n  (accounts: AccountValue) => (dispatch: Dispatch) => {\n    const key = accounts.name || defaultNameAccount();\n\n    let dataPocketAccount = null;\n    let valueObj = {};\n    let pocketAccount: Accounts = {};\n\n    const localStorageStory = localStorage.getItem(\n      localStorageKeys.pocket.POCKET_ACCOUNT\n    );\n\n    if (localStorageStory !== null) {\n      dataPocketAccount = JSON.parse(localStorageStory);\n      valueObj = Object.values(dataPocketAccount);\n    }\n\n    const isAdded = !checkAddress(valueObj, 'cyber', accounts.bech32);\n\n    if (!isAdded) {\n      return;\n    }\n\n    const cyberAccounts: Account = {\n      cyber: accounts,\n    };\n\n    if (localStorageStory !== null) {\n      pocketAccount = { [key]: cyberAccounts, ...dataPocketAccount };\n    } else {\n      pocketAccount = { [key]: cyberAccounts };\n    }\n\n    if (Object.keys(pocketAccount).length > 0) {\n      dispatch(setAccounts(pocketAccount));\n      if (accounts.keys !== 'read-only') {\n        dispatch(setDefaultAccount({ name: key, account: cyberAccounts }));\n      }\n    }\n  };\n","export const CYB_BROADCAST_CHANNEL = 'cyb-broadcast-channel';\nexport const CYB_QUEUE_CHANNEL = 'cyb-queue-channel';\n","import { updateSenseList } from 'src/features/sense/redux/sense.redux';\nimport { setDefaultAccount } from 'src/redux/features/pocket';\nimport { Account } from 'src/types/defaultAccount';\nimport { SenseListItem } from '../types/sense';\nimport {\n  BroadcastChannelMessage,\n  ServiceName,\n  ServiceStatus,\n  SyncEntryName,\n  SyncProgress,\n} from '../types/services';\nimport { CYB_BROADCAST_CHANNEL } from './consts';\n\nclass BroadcastChannelSender {\n  private channel: BroadcastChannel;\n\n  constructor() {\n    this.channel = new BroadcastChannel(CYB_BROADCAST_CHANNEL);\n  }\n\n  public postServiceStatus(\n    name: ServiceName,\n    status: ServiceStatus,\n    message?: string\n  ) {\n    this.channel.postMessage({\n      type: 'service_status',\n      value: { name, status, message },\n    });\n  }\n\n  public postSyncEntryProgress(entry: SyncEntryName, state: SyncProgress) {\n    // console.log('postSyncEntryProgress', entry, state);\n    this.channel.postMessage({ type: 'sync_entry', value: { entry, state } });\n  }\n\n  public postMlSyncEntryProgress(entry: string, state: SyncProgress) {\n    // console.log('postMlSyncEntryProgress', entry, state);\n    this.channel.postMessage({\n      type: 'sync_ml_entry',\n      value: { entry, state },\n    });\n  }\n\n  public postSenseUpdate(senseList: SenseListItem[]) {\n    // console.log('postSenseUpdate', senseList);\n    if (senseList.length > 0) {\n      this.channel.postMessage(updateSenseList(senseList));\n    }\n  }\n\n  public postSetDefaultAccount(name: string, account?: Account) {\n    this.channel.postMessage(\n      setDefaultAccount({\n        name,\n        account,\n      })\n    );\n  }\n\n  post(msg: BroadcastChannelMessage) {\n    this.channel.postMessage(msg);\n  }\n}\n\nexport default BroadcastChannelSender;\n","import { createCyblogChannel } from 'src/utils/logging/cyblog';\nimport {\n  ProgressTracking,\n  SyncEntryName,\n  SyncProgress,\n} from '../types/services';\nimport BroadcastChannelSender from './BroadcastChannelSender';\n\nexport const broadcastStatus = (\n  name: SyncEntryName,\n  channelApi: BroadcastChannelSender\n) => {\n  // const cyblogCh = createCyblogChannel({ thread: 'bckd', module: name });\n  return {\n    sendStatus: (\n      status: SyncProgress['status'],\n      message?: string,\n      progress?: ProgressTracking\n    ) => {\n      // cyblogCh.info(`>>>$ sync ${name} status: ${status} message: ${message}`);\n      channelApi.postSyncEntryProgress(name, {\n        status,\n        message,\n        progress,\n        done: ['active', 'error', 'listen'].some((s) => s === status),\n      });\n    },\n  };\n};\n","async function* arrayToAsyncIterable<T>(array: T[]): AsyncIterable<T> {\n  // eslint-disable-next-line no-restricted-syntax\n  for (const item of array) {\n    yield item;\n  }\n}\n\nasync function asyncIterableBatchProcessor<T, K>(\n  items: AsyncIterable<T> | Iterable<T>,\n  batchProcess: (arg: T[]) => Promise<K>,\n  batchSize = 10\n): Promise<void> {\n  let batch = [];\n  // eslint-disable-next-line no-restricted-syntax\n  for await (const item of items) {\n    batch.push(item);\n    if (batch.length === batchSize) {\n      await batchProcess(batch);\n      batch = [];\n    }\n  }\n  // process the rest\n  if (batch.length > 0) {\n    await batchProcess(batch);\n  }\n}\n\nasync function asyncIterableToArray<T>(asyncIterable: AsyncIterable<T>) {\n  const resultArray = [];\n  // eslint-disable-next-line no-restricted-syntax\n  for await (const item of asyncIterable) {\n    resultArray.push(item);\n  }\n  return resultArray;\n}\n// Create a helper function to create AsyncIterable from a list and iterate one by one\nfunction createAsyncIterable<T>(data: T[]): AsyncIterable<T> {\n  let index = 0;\n  return {\n    [Symbol.asyncIterator]() {\n      return {\n        next(): Promise<IteratorResult<T>> {\n          if (index < data.length) {\n            return Promise.resolve({ done: false, value: data[index++] });\n          }\n          return Promise.resolve({ done: true, value: undefined as any });\n        },\n      };\n    },\n  };\n}\n\n// eslint-disable-next-line import/prefer-default-export\nexport async function* fetchIterableByOffset<T, P>(\n  fetchFunction: (params: P & { offset: number }) => Promise<T[]>,\n  params: P\n): AsyncGenerator<T[], void, undefined> {\n  let offset = 0;\n  while (true) {\n    // eslint-disable-next-line no-await-in-loop\n    const items = await fetchFunction({ ...params, offset });\n\n    if (items.length === 0) {\n      break;\n    }\n\n    yield items;\n\n    offset += items.length;\n  }\n}\n\nexport {\n  arrayToAsyncIterable,\n  asyncIterableBatchProcessor,\n  asyncIterableToArray,\n  createAsyncIterable,\n};\n","import { SyncQueueJobType } from 'src/services/CozoDb/types/entities';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { IPFSContent } from 'src/services/ipfs/types';\nimport { LinkDto } from 'src/services/CozoDb/types/dto';\n\nimport { getTextContentIfShouldEmbed } from '../../services/sync/services/ParticlesResolverQueue/ParticlesResolverQueue';\nimport { CYB_QUEUE_CHANNEL } from '../consts';\nimport { QueueChannelMessage } from './types';\n\nexport const createBackendQueueSender = () => {\n  const channel = new BroadcastChannel(CYB_QUEUE_CHANNEL);\n\n  return {\n    enqueue: (msg: QueueChannelMessage) => {\n      channel.postMessage(msg);\n    },\n  };\n};\n\nconst busSender = createBackendQueueSender();\n\nexport const enqueueParticleEmbeddingMaybe = async (content: IPFSContent) => {\n  const contentToEmbed = await getTextContentIfShouldEmbed(content);\n\n  if (contentToEmbed) {\n    busSender.enqueue({\n      type: 'sync',\n      data: {\n        id: content.cid,\n        data: contentToEmbed,\n        jobType: SyncQueueJobType.embedding,\n        priority: QueuePriority.MEDIUM,\n      },\n    });\n  }\n\n  return !!contentToEmbed;\n};\n\nexport const enqueueParticleSave = (content: IPFSContent) => {\n  busSender.enqueue({\n    type: 'particle',\n    // TODO: add AsyncIterator serializer\n    data: { ...content, result: undefined } as IPFSContent,\n  });\n\n  return true;\n};\n\nexport const enqueueLinksSave = (links: LinkDto[]) => {\n  busSender.enqueue({\n    type: 'link',\n    data: links,\n  });\n};\n","// export const CID_AVATAR = 'Qmf89bXkJH9jw4uaLkHmZkxQ51qGKfUPtAMxA8rTwBrmTs';\nexport const CID_TWEET = 'QmbdH2WBamyKLPE5zu4mJ9v49qvY8BFfoumoVPMR5V4Rvx';\n\nexport const CID_FOLLOW = 'QmPLSA5oPqYxgc8F7EwrM8WS9vKrr1zPoDniSRFh8HSrxx';\n\nexport const INFINITY = '';\n\nexport const WP =\n  'https://ipfs.io/ipfs/QmQ1Vong13MDNxixDyUdjniqqEj8sjuNEBYMyhQU4gQgq3';\n\nexport const CYBER_CONGRESS_ADDRESS =\n  'bostrom1xszmhkfjs3s00z2nvtn7evqxw3dtus6yr8e4pw';\n","import { CID_FOLLOW, CID_TWEET } from 'src/constants/app';\nimport { SyncEntryName } from 'src/services/backend/types/services';\n\nexport const MY_PARTICLES_SYNC_INTERVAL = 5 * 60 * 1000; // 60 sec\nexport const MY_FRIENDS_SYNC_INTERVAL = 5 * 60 * 1000; // 60 sec\nexport const IPFS_SYNC_INTERVAL = 15 * 60 * 1000; // 15 minutes\n\nexport const MAX_DATABASE_PUT_SIZE = 500;\n\nexport const MAX_LINKS_RESOLVE_BATCH = 20;\n\nexport const DAY_IN_MS = 24 * 60 * 60 * 1000;\n\nexport const SENSE_FRIEND_PARTICLES = [CID_TWEET, CID_FOLLOW];\n\nexport const SYNC_ENTRIES_TO_TRACK_PROGRESS = [\n  'my-friends',\n  'particles',\n  'transactions',\n] as SyncEntryName[];\n","import {\n  BehaviorSubject,\n  Observable,\n  filter,\n  mergeMap,\n  tap,\n  map,\n  combineLatest,\n  share,\n  EMPTY,\n  Subject,\n  first,\n} from 'rxjs';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport { broadcastStatus } from 'src/services/backend/channels/broadcastStatus';\nimport { ParticleCid } from 'src/types/base';\nimport {\n  SyncQueueJobType,\n  SyncQueueStatus,\n} from 'src/services/CozoDb/types/entities';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { asyncIterableBatchProcessor } from 'src/utils/async/iterable';\n\nimport { enqueueParticleEmbeddingMaybe } from 'src/services/backend/channels/BackendQueueChannel/backendQueueSenders';\n\nimport { PATTERN_COSMOS, PATTERN_CYBER } from 'src/constants/patterns';\nimport { EmbeddingApi } from 'src/services/backend/workers/background/api/mlApi';\nimport { Option } from 'src/types';\n\nimport { IPFSContent } from 'src/services/ipfs/types';\nimport { FetchIpfsFunc } from '../../types';\nimport { ServiceDeps } from '../types';\nimport { SyncQueueItem } from './types';\nimport { MAX_DATABASE_PUT_SIZE } from '../consts';\n\nimport DbApi from '../../../DbApi/DbApi';\n\nconst QUEUE_BATCH_SIZE = 100;\n\nconst getContentToEmbed = async (content: IPFSContent) => {\n  const contentType = content?.meta?.contentType || '';\n\n  // create embedding for allowed content\n  if (contentType === 'text') {\n    return [contentType, content.textPreview];\n  }\n\n  return [contentType, undefined];\n};\n\nexport const getTextContentIfShouldEmbed = async (content: IPFSContent) => {\n  const [contentType, data] = await getContentToEmbed(content);\n\n  let shouldEmbed = contentType === 'text' && !!data;\n\n  shouldEmbed =\n    shouldEmbed &&\n    (!data!.match(PATTERN_COSMOS) || !data!.match(PATTERN_CYBER));\n\n  return shouldEmbed ? data : undefined;\n};\n\nclass ParticlesResolverQueue {\n  public isInitialized$: Observable<boolean>;\n\n  private db: Option<DbApi>;\n\n  private embeddingApi: Option<EmbeddingApi>;\n\n  private get canEmbed() {\n    return !!this.embeddingApi;\n  }\n\n  private waitForParticleResolve: FetchIpfsFunc;\n\n  private statusApi = broadcastStatus('resolver', new BroadcastChannelSender());\n\n  private _syncQueue$ = new BehaviorSubject<Map<ParticleCid, SyncQueueItem>>(\n    new Map()\n  );\n\n  public get queue(): Map<ParticleCid, SyncQueueItem> {\n    return this._syncQueue$.getValue();\n  }\n\n  private _loop$: Observable<any> | undefined;\n\n  public get loop$(): Observable<any> | undefined {\n    return this._loop$;\n  }\n\n  constructor(deps: ServiceDeps) {\n    if (!deps.waitForParticleResolve) {\n      throw new Error('waitForParticleResolve is not defined');\n    }\n\n    this.waitForParticleResolve = deps.waitForParticleResolve;\n\n    deps.embeddingApi$.subscribe((embeddingApi) => {\n      this.embeddingApi = embeddingApi;\n      // if embedding function is provided, retriger the queue\n      if (this.queue.size > 0) {\n        this._syncQueue$.next(this.queue);\n      }\n    });\n\n    deps.dbInstance$\n      .pipe(\n        first((value) => value !== undefined) // Automatically unsubscribes after the first valid value\n      )\n      .subscribe(async (db) => {\n        this.db = db;\n        await this.loadSyncQueue();\n      });\n\n    this.isInitialized$ = combineLatest([\n      deps.dbInstance$,\n      deps.ipfsInstance$,\n    ]).pipe(\n      map(([dbInstance, ipfsInstance]) => !!ipfsInstance && !!dbInstance)\n    );\n  }\n\n  private async resolveIpfsParticle(id: ParticleCid, priority: QueuePriority) {\n    return this.waitForParticleResolve(id, priority)\n      .then(async ({ status, result }) => {\n        const isResolved = status !== 'not_found';\n        if (!isResolved || !result) {\n          return false;\n        }\n\n        await enqueueParticleEmbeddingMaybe(result);\n        return true;\n      })\n      .catch(() => false);\n  }\n\n  private async saveEmbedding(cid: ParticleCid, text: string) {\n    try {\n      const hasItem = await this.db!.existEmbedding(cid);\n\n      if (!hasItem) {\n        const vec = await this.embeddingApi!.createEmbedding(text);\n\n        const result = await this.db!.putEmbedding(cid, vec);\n      }\n\n      return true;\n    } catch (err) {\n      console.error(`saveEmbedding error: ${cid} - ${text} `, err.toString());\n      return false;\n    }\n  }\n\n  private async processSyncQueue(pendingItems: SyncQueueItem[]) {\n    // console.log('------processSyncQueue ', pendingItems);\n\n    const batchSize = pendingItems.length;\n\n    this.statusApi.sendStatus(\n      'in-progress',\n      `processing batch ${batchSize}/${batchSize} batch. ${this.queue.size} pending...`\n    );\n\n    let i = batchSize;\n    await Promise.all(\n      pendingItems.map(async (item) => {\n        const { id, jobType, data } = item;\n\n        let jobPromise = Promise.resolve(false);\n\n        if (jobType === SyncQueueJobType.embedding && data) {\n          jobPromise = this.saveEmbedding(id, data as string);\n        } else if (jobType === SyncQueueJobType.particle) {\n          jobPromise = this.resolveIpfsParticle(id, QueuePriority.MEDIUM);\n        }\n\n        // eslint-disable-next-line no-await-in-loop\n        return jobPromise.then(async (result) => {\n          if (result) {\n            await this.db!.removeSyncQueue({ id, jobType });\n          } else {\n            await this.db!.updateSyncQueue({\n              id,\n              jobType,\n              status: SyncQueueStatus.error,\n            });\n          }\n\n          const queue = this._syncQueue$.value;\n          queue.delete(id);\n          i--;\n          this._syncQueue$.next(queue);\n\n          this.statusApi.sendStatus(\n            'in-progress',\n            `processing batch ${batchSize - i}/${batchSize} batch. ${\n              this.queue.size\n            } pending...`\n          );\n        });\n      })\n    );\n  }\n\n  start() {\n    const source$ = this.isInitialized$.pipe(\n      tap((q) => console.log(`sync queue isInitialized - ${q}`)),\n      filter((isInitialized) => isInitialized === true),\n      mergeMap(() => this._syncQueue$), // Merge the queue$ stream here.\n      // tap((q) => console.log(`sync queue - ${q.size}`)),\n      filter((q) => q.size > 0),\n      mergeMap((queue) => {\n        const list = [...queue.values()];\n\n        const executingCount = list.filter(\n          (i) => i.status === SyncQueueStatus.executing\n        ).length;\n\n        const batchSize = QUEUE_BATCH_SIZE - executingCount;\n\n        const jobTypeFilter = (i: SyncQueueItem) =>\n          i.jobType === SyncQueueJobType.particle ||\n          (i.jobType === SyncQueueJobType.embedding && this.canEmbed);\n\n        if (batchSize > 0) {\n          const pendingItems = list\n            .filter(\n              (i) => i.status === SyncQueueStatus.pending && jobTypeFilter(i)\n            )\n            .sort((a, b) => {\n              return a.priority - b.priority;\n            })\n            .slice(0, batchSize);\n\n          if (pendingItems.length > 0) {\n            pendingItems.forEach((i) => {\n              queue.set(i.id, {\n                ...i,\n                status: SyncQueueStatus.executing,\n              });\n            });\n\n            this._syncQueue$.next(queue);\n\n            this.statusApi.sendStatus('in-progress', `starting...`);\n            return this.processSyncQueue(pendingItems);\n          }\n        }\n\n        return EMPTY;\n      })\n    );\n\n    this._loop$ = source$.pipe(share());\n\n    this._loop$.subscribe({\n      next: (result) => {\n        this.statusApi.sendStatus('active');\n      },\n      error: (err) => this.statusApi.sendStatus('error', err.toString()),\n    });\n\n    return this;\n  }\n\n  public async enqueueBatch(\n    cids: ParticleCid[],\n    jobType: SyncQueueJobType,\n    priority: QueuePriority\n  ) {\n    return asyncIterableBatchProcessor(\n      cids,\n      (cids) =>\n        this.enqueue(\n          cids.map((cid) => ({\n            id: cid /* from is tweet */,\n            priority,\n            jobType,\n          }))\n        ),\n      MAX_DATABASE_PUT_SIZE\n    );\n  }\n\n  public async enqueue(items: SyncQueueItem[]) {\n    if (items.length === 0) {\n      return;\n    }\n\n    const result = await this.db!.putSyncQueue(items);\n\n    const queue = this._syncQueue$.value;\n\n    items.forEach((item) =>\n      queue.set(item.id, { ...item, status: SyncQueueStatus.pending })\n    );\n    this._syncQueue$.next(queue);\n  }\n\n  private async loadSyncQueue() {\n    const queue = await this.db!.getSyncQueue({\n      statuses: [SyncQueueStatus.pending],\n    }).then((items) => new Map(items.map((item) => [item.id, item])));\n\n    this._syncQueue$.next(new Map([...queue, ...this.queue]));\n  }\n}\n\nexport default ParticlesResolverQueue;\n","export function shortenString(string: string, length = 300) {\n  return string.length > length ? `${string.slice(0, length)}...` : string;\n}\n\nexport function replaceQuotes(string: string) {\n  return string.replace(/\"/g, \"'\");\n}\n\nexport function serializeString(input: string): string {\n  return input\n    .replace(/\\\\/g, '\\\\\\\\') // Escape backslashes\n    .replace(/\"/g, \"\\\\''\") // Escape double quotes\n    .replace(/'/g, \"\\\\'\") // Escape single quotes\n    .replace(/\\n/g, '\\\\n') // Escape newlines\n    .replace(/\\r/g, '\\\\r') // Escape carriage returns\n    .replace(/#/g, '\\\\!!'); // Escape  - that's comment in cozo\n}\n\nexport function deserializeString(serialized: string): string {\n  return serialized\n    .replace(/\\\\r/g, '\\r') // Unescape carriage returns\n    .replace(/\\\\n/g, '\\n') // Unescape newlines\n    .replace(/\\\\'/g, \"'\") // Unescape single quotes\n    .replace(/\\\\''/g, '\"') // Unescape double quotes\n    .replace(/\\\\\\\\/g, '\\\\') // Unescape backslashes\n    .replace(/\\\\!!/g, '#'); // Unescape # cozo comment\n}\n\nconst specialCharsRegexe = /\\\\u\\{[a-fA-F0-9]+\\}/g;\n\nexport function removeBrokenUnicode(string: string): string {\n  return string.replace(specialCharsRegexe, '');\n}\n\nexport function removeMarkdownFormatting(markdown: string): string {\n  // Remove headers\n  let text = markdown.replace(/^#{1,6}\\s+/gm, '');\n\n  // Remove emphasis (bold, italic, strikethrough)\n  text = text.replace(/(\\*\\*|__)(.*?)\\1/g, '$2');\n  text = text.replace(/(\\*|_)(.*?)\\1/g, '$2');\n  text = text.replace(/(~~)(.*?)\\1/g, '$2');\n\n  // Remove inline code and code blocks\n  text = text.replace(/`{1,3}[^`](.*?)`{1,3}/g, '$1');\n  text = text.replace(/```[\\s\\S]*?```/g, '');\n\n  // Remove blockquotes\n  text = text.replace(/^\\s{0,3}>\\s?/gm, '');\n\n  // Remove links\n  text = text.replace(/\\[(.*?)\\]\\(.*?\\)/g, '$1');\n\n  // Remove images\n  text = text.replace(/!\\[(.*?)\\]\\(.*?\\)/g, '$1');\n\n  // Remove horizontal rules\n  text = text.replace(/^-{3,}$/gm, '');\n\n  // Remove unordered lists\n  text = text.replace(/^\\s*[-+*]\\s+/gm, '');\n\n  // Remove ordered lists\n  text = text.replace(/^\\s*\\d+\\.\\s+/gm, '');\n\n  // Remove extra spaces and new lines\n  text = text.replace(/\\n{2,}/g, '\\n\\n');\n  text = text.replace(/^\\s+|\\s+$/g, '');\n\n  return text;\n}\n","import { LsResult } from 'ipfs-core-types/src/pin';\nimport { dateToUtcNumber } from 'src/utils/date';\nimport { NeuronAddress, ParticleCid, TransactionHash } from 'src/types/base';\nimport { IPFSContent } from '../ipfs/types';\nimport { LinkDbEntity, PinTypeMap } from './types/entities';\nimport { Transaction } from '../backend/services/indexer/types';\nimport { LinkDto, ParticleDto, PinDto, TransactionDto } from './types/dto';\nimport { CyberlinksByParticleQuery } from 'src/generated/graphql';\nimport { removeMarkdownFormatting, replaceQuotes } from 'src/utils/string';\n\nexport const mapParticleToEntity = (particle: IPFSContent): ParticleDto => {\n  const { cid, meta, textPreview } = particle;\n  const { size, mime, type, blocks, sizeLocal } = meta;\n\n  // hack to fix string command\n  const text = textPreview\n    ? replaceQuotes(removeMarkdownFormatting(textPreview))\n    : '';\n\n  return {\n    cid,\n    size: size || 0,\n    mime: mime || 'unknown',\n    type,\n    text,\n    size_local: sizeLocal || -1,\n    blocks: blocks || 0,\n  };\n};\n\n//TODO: REFACTOR\nexport const mapPinToEntity = (pin: LsResult): PinDto => ({\n  cid: pin.cid.toString(),\n  type: PinTypeMap[pin.type],\n});\n\nexport const mapIndexerTransactionToEntity = (\n  neuron: string,\n  tx: Transaction\n): TransactionDto => {\n  const {\n    transaction_hash,\n    index,\n    transaction: {\n      memo,\n      block: { timestamp, height },\n      success,\n    },\n    type,\n    value,\n  } = tx;\n  return {\n    hash: transaction_hash,\n    index,\n    type,\n    timestamp: dateToUtcNumber(timestamp),\n    // value: JSON.stringify(value),\n    memo,\n    value,\n    success,\n    neuron,\n    blockHeight: height,\n  };\n};\n\n// export const mapSyncStatusToEntity = (\n//   id: NeuronAddress | ParticleCid,\n//   entryType: EntryType,\n//   unreadCount: number,\n//   timestampUpdate: number,\n//   lastId: TransactionHash | ParticleCid = '',\n//   timestampRead: number = timestampUpdate,\n//   meta: Object = {}\n// ): SyncStatusDbEntity => {\n//   return {\n//     entry_type: entryType,\n//     id,\n//     timestamp_update: timestampUpdate,\n//     timestamp_read: timestampRead,\n//     unread_count: unreadCount,\n//     disabled: false,\n//     last_id: lastId,\n//     meta,\n//   };\n// };\n\nexport const mapLinkToLinkDto = (\n  from: ParticleCid,\n  to: ParticleCid,\n  neuron: NeuronAddress = '',\n  timestamp: number = 0\n): LinkDto => ({\n  from,\n  to,\n  neuron,\n  timestamp,\n});\n\nexport const mapLinkFromIndexerToDto = ({\n  from,\n  to,\n  neuron,\n  timestamp,\n  transaction_hash,\n}: CyberlinksByParticleQuery['cyberlinks'][0]): LinkDto => ({\n  from,\n  to,\n  neuron,\n  timestamp: dateToUtcNumber(timestamp),\n  transactionHash: transaction_hash,\n});\n","export async function waitUntil(cond: () => boolean, timeoutDuration = 60000) {\n  if (cond()) {\n    return true;\n  }\n\n  const waitPromise = new Promise((resolve) => {\n    const interval = setInterval(() => {\n      if (cond()) {\n        clearInterval(interval);\n        resolve(true);\n      }\n    }, 10);\n  });\n\n  const timeoutPromise = new Promise((_, reject) => {\n    setTimeout(() => {\n      reject(new Error('waitUntil timed out!'));\n    }, timeoutDuration);\n  });\n\n  return Promise.race([waitPromise, timeoutPromise]);\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport function makeCancellable<T extends (...args: any[]) => Promise<any>>(\n  func: T,\n  signal: AbortSignal\n): (...funcArgs: Parameters<T>) => Promise<ReturnType<T>> {\n  return async (...args: Parameters<T>): Promise<ReturnType<T>> => {\n    // Promise that listens for the abort signal\n    const abortPromise = new Promise<ReturnType<T>>((_, reject) => {\n      const abortHandler = () => {\n        signal.removeEventListener('abort', abortHandler); // Clean up the event listener\n        reject(new DOMException('The operation was aborted.', 'AbortError'));\n      };\n      signal.addEventListener('abort', abortHandler, { once: true });\n    });\n\n    // Wrapping the original function in a promise\n    const taskPromise = new Promise<ReturnType<T>>(async (resolve, reject) => {\n      try {\n        const result = await func(...args);\n        resolve(result);\n      } catch (error) {\n        reject(error);\n      }\n    });\n\n    // Using Promise.race to handle cancellation\n    return Promise.race([taskPromise, abortPromise]);\n  };\n}\n\nexport function throwIfAborted<T extends (...args: any[]) => Promise<any>>(\n  func: T,\n  signal: AbortSignal\n): (...funcArgs: Parameters<T>) => Promise<ReturnType<T>> {\n  return async (...args: Parameters<T>): Promise<ReturnType<T>> => {\n    if (signal.aborted) {\n      throw new DOMException('The operation was aborted.', 'AbortError');\n    }\n    return func(...args);\n  };\n}\n\n/**\n * Promise will be rejected after timeout.\n *\n * @param promise\n * @param timeout ms\n * @param abortController trigger abort\n * @returns\n */\n// eslint-disable-next-line import/no-unused-modules\nexport async function withTimeout<T>(\n  promise: Promise<T>,\n  timeout: number,\n  abortController?: AbortController\n): Promise<T> {\n  return Promise.race([\n    promise,\n    new Promise<T>((_, reject) => {\n      const timer = setTimeout(() => {\n        abortController?.abort('timeout');\n        clearTimeout(timer);\n        reject(new DOMException('timeout', 'AbortError'));\n      }, timeout);\n    }),\n  ]);\n}\n","import { gql } from '@apollo/client';\nimport * as Apollo from '@apollo/client';\nexport type Maybe<T> = T | null;\nexport type InputMaybe<T> = Maybe<T>;\nexport type Exact<T extends { [key: string]: unknown }> = { [K in keyof T]: T[K] };\nexport type MakeOptional<T, K extends keyof T> = Omit<T, K> & { [SubKey in K]?: Maybe<T[SubKey]> };\nexport type MakeMaybe<T, K extends keyof T> = Omit<T, K> & { [SubKey in K]: Maybe<T[SubKey]> };\nexport type MakeEmpty<T extends { [key: string]: unknown }, K extends keyof T> = { [_ in K]?: never };\nexport type Incremental<T> = T | { [P in keyof T]?: P extends ' $fragmentName' | '__typename' ? T[P] : never };\nconst defaultOptions = {} as const;\n/** All built-in and custom scalars, mapped to their actual values */\nexport type Scalars = {\n  ID: { input: string; output: string; }\n  String: { input: string; output: string; }\n  Boolean: { input: boolean; output: boolean; }\n  Int: { input: number; output: number; }\n  Float: { input: number; output: number; }\n  _text: { input: any; output: any; }\n  bigint: { input: any; output: any; }\n  coin: { input: any; output: any; }\n  coin_scalar: { input: any; output: any; }\n  date: { input: any; output: any; }\n  float8: { input: any; output: any; }\n  json: { input: any; output: any; }\n  jsonb: { input: any; output: any; }\n  numeric: { input: any; output: any; }\n  timestamp: { input: any; output: any; }\n};\n\n/** Boolean expression to compare columns of type \"Boolean\". All fields are combined with logical 'AND'. */\nexport type Boolean_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['Boolean']['input']>;\n  _gt?: InputMaybe<Scalars['Boolean']['input']>;\n  _gte?: InputMaybe<Scalars['Boolean']['input']>;\n  _in?: InputMaybe<Array<Scalars['Boolean']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['Boolean']['input']>;\n  _lte?: InputMaybe<Scalars['Boolean']['input']>;\n  _neq?: InputMaybe<Scalars['Boolean']['input']>;\n  _nin?: InputMaybe<Array<Scalars['Boolean']['input']>>;\n};\n\n/** Boolean expression to compare columns of type \"Int\". All fields are combined with logical 'AND'. */\nexport type Int_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['Int']['input']>;\n  _gt?: InputMaybe<Scalars['Int']['input']>;\n  _gte?: InputMaybe<Scalars['Int']['input']>;\n  _in?: InputMaybe<Array<Scalars['Int']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['Int']['input']>;\n  _lte?: InputMaybe<Scalars['Int']['input']>;\n  _neq?: InputMaybe<Scalars['Int']['input']>;\n  _nin?: InputMaybe<Array<Scalars['Int']['input']>>;\n};\n\n/** Boolean expression to compare columns of type \"String\". All fields are combined with logical 'AND'. */\nexport type String_Array_Comparison_Exp = {\n  /** is the array contained in the given array value */\n  _contained_in?: InputMaybe<Array<Scalars['String']['input']>>;\n  /** does the array contain the given value */\n  _contains?: InputMaybe<Array<Scalars['String']['input']>>;\n  _eq?: InputMaybe<Array<Scalars['String']['input']>>;\n  _gt?: InputMaybe<Array<Scalars['String']['input']>>;\n  _gte?: InputMaybe<Array<Scalars['String']['input']>>;\n  _in?: InputMaybe<Array<Array<Scalars['String']['input']>>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Array<Scalars['String']['input']>>;\n  _lte?: InputMaybe<Array<Scalars['String']['input']>>;\n  _neq?: InputMaybe<Array<Scalars['String']['input']>>;\n  _nin?: InputMaybe<Array<Array<Scalars['String']['input']>>>;\n};\n\n/** Boolean expression to compare columns of type \"String\". All fields are combined with logical 'AND'. */\nexport type String_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['String']['input']>;\n  _gt?: InputMaybe<Scalars['String']['input']>;\n  _gte?: InputMaybe<Scalars['String']['input']>;\n  /** does the column match the given case-insensitive pattern */\n  _ilike?: InputMaybe<Scalars['String']['input']>;\n  _in?: InputMaybe<Array<Scalars['String']['input']>>;\n  /** does the column match the given POSIX regular expression, case insensitive */\n  _iregex?: InputMaybe<Scalars['String']['input']>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  /** does the column match the given pattern */\n  _like?: InputMaybe<Scalars['String']['input']>;\n  _lt?: InputMaybe<Scalars['String']['input']>;\n  _lte?: InputMaybe<Scalars['String']['input']>;\n  _neq?: InputMaybe<Scalars['String']['input']>;\n  /** does the column NOT match the given case-insensitive pattern */\n  _nilike?: InputMaybe<Scalars['String']['input']>;\n  _nin?: InputMaybe<Array<Scalars['String']['input']>>;\n  /** does the column NOT match the given POSIX regular expression, case insensitive */\n  _niregex?: InputMaybe<Scalars['String']['input']>;\n  /** does the column NOT match the given pattern */\n  _nlike?: InputMaybe<Scalars['String']['input']>;\n  /** does the column NOT match the given POSIX regular expression, case sensitive */\n  _nregex?: InputMaybe<Scalars['String']['input']>;\n  /** does the column NOT match the given SQL regular expression */\n  _nsimilar?: InputMaybe<Scalars['String']['input']>;\n  /** does the column match the given POSIX regular expression, case sensitive */\n  _regex?: InputMaybe<Scalars['String']['input']>;\n  /** does the column match the given SQL regular expression */\n  _similar?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** columns and relationships of \"_transaction\" */\nexport type _Transaction = {\n  fee?: Maybe<Scalars['jsonb']['output']>;\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  logs?: Maybe<Scalars['jsonb']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  messages?: Maybe<Scalars['json']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n  signer_infos?: Maybe<Scalars['jsonb']['output']>;\n  subject1?: Maybe<Scalars['String']['output']>;\n  subject2?: Maybe<Scalars['String']['output']>;\n  success?: Maybe<Scalars['Boolean']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n  value?: Maybe<Scalars['json']['output']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionFeeArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionLogsArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionMessagesArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionSigner_InfosArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionValueArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregated selection of \"_transaction\" */\nexport type _Transaction_Aggregate = {\n  aggregate?: Maybe<_Transaction_Aggregate_Fields>;\n  nodes: Array<_Transaction>;\n};\n\n/** aggregate fields of \"_transaction\" */\nexport type _Transaction_Aggregate_Fields = {\n  avg?: Maybe<_Transaction_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<_Transaction_Max_Fields>;\n  min?: Maybe<_Transaction_Min_Fields>;\n  stddev?: Maybe<_Transaction_Stddev_Fields>;\n  stddev_pop?: Maybe<_Transaction_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<_Transaction_Stddev_Samp_Fields>;\n  sum?: Maybe<_Transaction_Sum_Fields>;\n  var_pop?: Maybe<_Transaction_Var_Pop_Fields>;\n  var_samp?: Maybe<_Transaction_Var_Samp_Fields>;\n  variance?: Maybe<_Transaction_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"_transaction\" */\nexport type _Transaction_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<_Transaction_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type _Transaction_Avg_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"_transaction\". All fields are combined with a logical 'AND'. */\nexport type _Transaction_Bool_Exp = {\n  _and?: InputMaybe<Array<_Transaction_Bool_Exp>>;\n  _not?: InputMaybe<_Transaction_Bool_Exp>;\n  _or?: InputMaybe<Array<_Transaction_Bool_Exp>>;\n  fee?: InputMaybe<Jsonb_Comparison_Exp>;\n  gas_used?: InputMaybe<Bigint_Comparison_Exp>;\n  gas_wanted?: InputMaybe<Bigint_Comparison_Exp>;\n  hash?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  index?: InputMaybe<Bigint_Comparison_Exp>;\n  involved_accounts_addresses?: InputMaybe<String_Array_Comparison_Exp>;\n  logs?: InputMaybe<Jsonb_Comparison_Exp>;\n  memo?: InputMaybe<String_Comparison_Exp>;\n  messages?: InputMaybe<Json_Comparison_Exp>;\n  partition_id?: InputMaybe<Bigint_Comparison_Exp>;\n  raw_log?: InputMaybe<String_Comparison_Exp>;\n  signatures?: InputMaybe<String_Array_Comparison_Exp>;\n  signer_infos?: InputMaybe<Jsonb_Comparison_Exp>;\n  subject1?: InputMaybe<String_Comparison_Exp>;\n  subject2?: InputMaybe<String_Comparison_Exp>;\n  success?: InputMaybe<Boolean_Comparison_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  value?: InputMaybe<Json_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type _Transaction_Max_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  memo?: Maybe<Scalars['String']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n  subject1?: Maybe<Scalars['String']['output']>;\n  subject2?: Maybe<Scalars['String']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type _Transaction_Min_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  memo?: Maybe<Scalars['String']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n  subject1?: Maybe<Scalars['String']['output']>;\n  subject2?: Maybe<Scalars['String']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"_transaction\". */\nexport type _Transaction_Order_By = {\n  fee?: InputMaybe<Order_By>;\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  logs?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  messages?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n  signer_infos?: InputMaybe<Order_By>;\n  subject1?: InputMaybe<Order_By>;\n  subject2?: InputMaybe<Order_By>;\n  success?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"_transaction\" */\nexport enum _Transaction_Select_Column {\n  /** column name */\n  Fee = 'fee',\n  /** column name */\n  GasUsed = 'gas_used',\n  /** column name */\n  GasWanted = 'gas_wanted',\n  /** column name */\n  Hash = 'hash',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Index = 'index',\n  /** column name */\n  InvolvedAccountsAddresses = 'involved_accounts_addresses',\n  /** column name */\n  Logs = 'logs',\n  /** column name */\n  Memo = 'memo',\n  /** column name */\n  Messages = 'messages',\n  /** column name */\n  PartitionId = 'partition_id',\n  /** column name */\n  RawLog = 'raw_log',\n  /** column name */\n  Signatures = 'signatures',\n  /** column name */\n  SignerInfos = 'signer_infos',\n  /** column name */\n  Subject1 = 'subject1',\n  /** column name */\n  Subject2 = 'subject2',\n  /** column name */\n  Success = 'success',\n  /** column name */\n  TransactionHash = 'transaction_hash',\n  /** column name */\n  Type = 'type',\n  /** column name */\n  Value = 'value'\n}\n\n/** aggregate stddev on columns */\nexport type _Transaction_Stddev_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type _Transaction_Stddev_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type _Transaction_Stddev_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"_transaction\" */\nexport type _Transaction_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: _Transaction_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type _Transaction_Stream_Cursor_Value_Input = {\n  fee?: InputMaybe<Scalars['jsonb']['input']>;\n  gas_used?: InputMaybe<Scalars['bigint']['input']>;\n  gas_wanted?: InputMaybe<Scalars['bigint']['input']>;\n  hash?: InputMaybe<Scalars['String']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  index?: InputMaybe<Scalars['bigint']['input']>;\n  involved_accounts_addresses?: InputMaybe<Array<Scalars['String']['input']>>;\n  logs?: InputMaybe<Scalars['jsonb']['input']>;\n  memo?: InputMaybe<Scalars['String']['input']>;\n  messages?: InputMaybe<Scalars['json']['input']>;\n  partition_id?: InputMaybe<Scalars['bigint']['input']>;\n  raw_log?: InputMaybe<Scalars['String']['input']>;\n  signatures?: InputMaybe<Array<Scalars['String']['input']>>;\n  signer_infos?: InputMaybe<Scalars['jsonb']['input']>;\n  subject1?: InputMaybe<Scalars['String']['input']>;\n  subject2?: InputMaybe<Scalars['String']['input']>;\n  success?: InputMaybe<Scalars['Boolean']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n  value?: InputMaybe<Scalars['json']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type _Transaction_Sum_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type _Transaction_Var_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type _Transaction_Var_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type _Transaction_Variance_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"_uptime_temp\" */\nexport type _Uptime_Temp = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"_uptime_temp\" */\nexport type _Uptime_Temp_Aggregate = {\n  aggregate?: Maybe<_Uptime_Temp_Aggregate_Fields>;\n  nodes: Array<_Uptime_Temp>;\n};\n\n/** aggregate fields of \"_uptime_temp\" */\nexport type _Uptime_Temp_Aggregate_Fields = {\n  avg?: Maybe<_Uptime_Temp_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<_Uptime_Temp_Max_Fields>;\n  min?: Maybe<_Uptime_Temp_Min_Fields>;\n  stddev?: Maybe<_Uptime_Temp_Stddev_Fields>;\n  stddev_pop?: Maybe<_Uptime_Temp_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<_Uptime_Temp_Stddev_Samp_Fields>;\n  sum?: Maybe<_Uptime_Temp_Sum_Fields>;\n  var_pop?: Maybe<_Uptime_Temp_Var_Pop_Fields>;\n  var_samp?: Maybe<_Uptime_Temp_Var_Samp_Fields>;\n  variance?: Maybe<_Uptime_Temp_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"_uptime_temp\" */\nexport type _Uptime_Temp_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type _Uptime_Temp_Avg_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"_uptime_temp\". All fields are combined with a logical 'AND'. */\nexport type _Uptime_Temp_Bool_Exp = {\n  _and?: InputMaybe<Array<_Uptime_Temp_Bool_Exp>>;\n  _not?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n  _or?: InputMaybe<Array<_Uptime_Temp_Bool_Exp>>;\n  pre_commits?: InputMaybe<Bigint_Comparison_Exp>;\n  validator_address?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type _Uptime_Temp_Max_Fields = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type _Uptime_Temp_Min_Fields = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"_uptime_temp\". */\nexport type _Uptime_Temp_Order_By = {\n  pre_commits?: InputMaybe<Order_By>;\n  validator_address?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"_uptime_temp\" */\nexport enum _Uptime_Temp_Select_Column {\n  /** column name */\n  PreCommits = 'pre_commits',\n  /** column name */\n  ValidatorAddress = 'validator_address'\n}\n\n/** aggregate stddev on columns */\nexport type _Uptime_Temp_Stddev_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type _Uptime_Temp_Stddev_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type _Uptime_Temp_Stddev_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"_uptime_temp\" */\nexport type _Uptime_Temp_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: _Uptime_Temp_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type _Uptime_Temp_Stream_Cursor_Value_Input = {\n  pre_commits?: InputMaybe<Scalars['bigint']['input']>;\n  validator_address?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type _Uptime_Temp_Sum_Fields = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type _Uptime_Temp_Var_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type _Uptime_Temp_Var_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type _Uptime_Temp_Variance_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"account\" */\nexport type Account = {\n  /** An object relationship */\n  account_balance?: Maybe<Account_Balance>;\n  address: Scalars['String']['output'];\n  /** An array relationship */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An array relationship */\n  routesBySource: Array<Routes>;\n  /** An aggregate relationship */\n  routesBySource_aggregate: Routes_Aggregate;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n  /** An array relationship */\n  vesting_accounts: Array<Vesting_Account>;\n  /** An aggregate relationship */\n  vesting_accounts_aggregate: Vesting_Account_Aggregate;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountRoutesBySourceArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountRoutesBySource_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountVesting_AccountsArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Account_Order_By>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountVesting_Accounts_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Account_Order_By>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n/** aggregated selection of \"account\" */\nexport type Account_Aggregate = {\n  aggregate?: Maybe<Account_Aggregate_Fields>;\n  nodes: Array<Account>;\n};\n\n/** aggregate fields of \"account\" */\nexport type Account_Aggregate_Fields = {\n  count: Scalars['Int']['output'];\n  max?: Maybe<Account_Max_Fields>;\n  min?: Maybe<Account_Min_Fields>;\n};\n\n\n/** aggregate fields of \"account\" */\nexport type Account_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Account_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** columns and relationships of \"account_balance\" */\nexport type Account_Balance = {\n  /** An object relationship */\n  account: Account;\n  address: Scalars['String']['output'];\n  coins: Array<Scalars['coin']['output']>;\n  height: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"account_balance\" */\nexport type Account_Balance_Aggregate = {\n  aggregate?: Maybe<Account_Balance_Aggregate_Fields>;\n  nodes: Array<Account_Balance>;\n};\n\n/** aggregate fields of \"account_balance\" */\nexport type Account_Balance_Aggregate_Fields = {\n  avg?: Maybe<Account_Balance_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Account_Balance_Max_Fields>;\n  min?: Maybe<Account_Balance_Min_Fields>;\n  stddev?: Maybe<Account_Balance_Stddev_Fields>;\n  stddev_pop?: Maybe<Account_Balance_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Account_Balance_Stddev_Samp_Fields>;\n  sum?: Maybe<Account_Balance_Sum_Fields>;\n  var_pop?: Maybe<Account_Balance_Var_Pop_Fields>;\n  var_samp?: Maybe<Account_Balance_Var_Samp_Fields>;\n  variance?: Maybe<Account_Balance_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"account_balance\" */\nexport type Account_Balance_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Account_Balance_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"account_balance\". All fields are combined with a logical 'AND'. */\nexport type Account_Balance_Bool_Exp = {\n  _and?: InputMaybe<Array<Account_Balance_Bool_Exp>>;\n  _not?: InputMaybe<Account_Balance_Bool_Exp>;\n  _or?: InputMaybe<Array<Account_Balance_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  coins?: InputMaybe<Coin_Array_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Account_Balance_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  coins?: Maybe<Array<Scalars['coin']['output']>>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Account_Balance_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  coins?: Maybe<Array<Scalars['coin']['output']>>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"account_balance\". */\nexport type Account_Balance_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  address?: InputMaybe<Order_By>;\n  coins?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"account_balance\" */\nexport enum Account_Balance_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Coins = 'coins',\n  /** column name */\n  Height = 'height'\n}\n\n/** aggregate stddev on columns */\nexport type Account_Balance_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Account_Balance_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Account_Balance_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"account_balance\" */\nexport type Account_Balance_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Account_Balance_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Account_Balance_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n  coins?: InputMaybe<Array<Scalars['coin']['input']>>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Account_Balance_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Account_Balance_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Account_Balance_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Account_Balance_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"account\". All fields are combined with a logical 'AND'. */\nexport type Account_Bool_Exp = {\n  _and?: InputMaybe<Array<Account_Bool_Exp>>;\n  _not?: InputMaybe<Account_Bool_Exp>;\n  _or?: InputMaybe<Array<Account_Bool_Exp>>;\n  account_balance?: InputMaybe<Account_Balance_Bool_Exp>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  cyberlinks?: InputMaybe<Cyberlinks_Bool_Exp>;\n  cyberlinks_aggregate?: InputMaybe<Cyberlinks_Aggregate_Bool_Exp>;\n  investmints?: InputMaybe<Investmints_Bool_Exp>;\n  investmints_aggregate?: InputMaybe<Investmints_Aggregate_Bool_Exp>;\n  particles?: InputMaybe<Particles_Bool_Exp>;\n  particles_aggregate?: InputMaybe<Particles_Aggregate_Bool_Exp>;\n  routes?: InputMaybe<Routes_Bool_Exp>;\n  routesBySource?: InputMaybe<Routes_Bool_Exp>;\n  routesBySource_aggregate?: InputMaybe<Routes_Aggregate_Bool_Exp>;\n  routes_aggregate?: InputMaybe<Routes_Aggregate_Bool_Exp>;\n  vesting_accounts?: InputMaybe<Vesting_Account_Bool_Exp>;\n  vesting_accounts_aggregate?: InputMaybe<Vesting_Account_Aggregate_Bool_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Account_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Account_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"account\". */\nexport type Account_Order_By = {\n  account_balance?: InputMaybe<Account_Balance_Order_By>;\n  address?: InputMaybe<Order_By>;\n  cyberlinks_aggregate?: InputMaybe<Cyberlinks_Aggregate_Order_By>;\n  investmints_aggregate?: InputMaybe<Investmints_Aggregate_Order_By>;\n  particles_aggregate?: InputMaybe<Particles_Aggregate_Order_By>;\n  routesBySource_aggregate?: InputMaybe<Routes_Aggregate_Order_By>;\n  routes_aggregate?: InputMaybe<Routes_Aggregate_Order_By>;\n  vesting_accounts_aggregate?: InputMaybe<Vesting_Account_Aggregate_Order_By>;\n};\n\n/** select columns of table \"account\" */\nexport enum Account_Select_Column {\n  /** column name */\n  Address = 'address'\n}\n\n/** Streaming cursor of the table \"account\" */\nexport type Account_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Account_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Account_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** columns and relationships of \"average_block_time_from_genesis\" */\nexport type Average_Block_Time_From_Genesis = {\n  average_time: Scalars['numeric']['output'];\n  height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n};\n\n/** aggregated selection of \"average_block_time_from_genesis\" */\nexport type Average_Block_Time_From_Genesis_Aggregate = {\n  aggregate?: Maybe<Average_Block_Time_From_Genesis_Aggregate_Fields>;\n  nodes: Array<Average_Block_Time_From_Genesis>;\n};\n\n/** aggregate fields of \"average_block_time_from_genesis\" */\nexport type Average_Block_Time_From_Genesis_Aggregate_Fields = {\n  avg?: Maybe<Average_Block_Time_From_Genesis_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Average_Block_Time_From_Genesis_Max_Fields>;\n  min?: Maybe<Average_Block_Time_From_Genesis_Min_Fields>;\n  stddev?: Maybe<Average_Block_Time_From_Genesis_Stddev_Fields>;\n  stddev_pop?: Maybe<Average_Block_Time_From_Genesis_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Average_Block_Time_From_Genesis_Stddev_Samp_Fields>;\n  sum?: Maybe<Average_Block_Time_From_Genesis_Sum_Fields>;\n  var_pop?: Maybe<Average_Block_Time_From_Genesis_Var_Pop_Fields>;\n  var_samp?: Maybe<Average_Block_Time_From_Genesis_Var_Samp_Fields>;\n  variance?: Maybe<Average_Block_Time_From_Genesis_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"average_block_time_from_genesis\" */\nexport type Average_Block_Time_From_Genesis_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Average_Block_Time_From_Genesis_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Average_Block_Time_From_Genesis_Avg_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"average_block_time_from_genesis\". All fields are combined with a logical 'AND'. */\nexport type Average_Block_Time_From_Genesis_Bool_Exp = {\n  _and?: InputMaybe<Array<Average_Block_Time_From_Genesis_Bool_Exp>>;\n  _not?: InputMaybe<Average_Block_Time_From_Genesis_Bool_Exp>;\n  _or?: InputMaybe<Array<Average_Block_Time_From_Genesis_Bool_Exp>>;\n  average_time?: InputMaybe<Numeric_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Average_Block_Time_From_Genesis_Max_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Average_Block_Time_From_Genesis_Min_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"average_block_time_from_genesis\". */\nexport type Average_Block_Time_From_Genesis_Order_By = {\n  average_time?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"average_block_time_from_genesis\" */\nexport enum Average_Block_Time_From_Genesis_Select_Column {\n  /** column name */\n  AverageTime = 'average_time',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  OneRowId = 'one_row_id'\n}\n\n/** aggregate stddev on columns */\nexport type Average_Block_Time_From_Genesis_Stddev_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Average_Block_Time_From_Genesis_Stddev_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Average_Block_Time_From_Genesis_Stddev_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"average_block_time_from_genesis\" */\nexport type Average_Block_Time_From_Genesis_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Average_Block_Time_From_Genesis_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Average_Block_Time_From_Genesis_Stream_Cursor_Value_Input = {\n  average_time?: InputMaybe<Scalars['numeric']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  one_row_id?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Average_Block_Time_From_Genesis_Sum_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Average_Block_Time_From_Genesis_Var_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Average_Block_Time_From_Genesis_Var_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Average_Block_Time_From_Genesis_Variance_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"average_block_time_per_day\" */\nexport type Average_Block_Time_Per_Day = {\n  average_time: Scalars['numeric']['output'];\n  height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n};\n\n/** aggregated selection of \"average_block_time_per_day\" */\nexport type Average_Block_Time_Per_Day_Aggregate = {\n  aggregate?: Maybe<Average_Block_Time_Per_Day_Aggregate_Fields>;\n  nodes: Array<Average_Block_Time_Per_Day>;\n};\n\n/** aggregate fields of \"average_block_time_per_day\" */\nexport type Average_Block_Time_Per_Day_Aggregate_Fields = {\n  avg?: Maybe<Average_Block_Time_Per_Day_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Average_Block_Time_Per_Day_Max_Fields>;\n  min?: Maybe<Average_Block_Time_Per_Day_Min_Fields>;\n  stddev?: Maybe<Average_Block_Time_Per_Day_Stddev_Fields>;\n  stddev_pop?: Maybe<Average_Block_Time_Per_Day_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Average_Block_Time_Per_Day_Stddev_Samp_Fields>;\n  sum?: Maybe<Average_Block_Time_Per_Day_Sum_Fields>;\n  var_pop?: Maybe<Average_Block_Time_Per_Day_Var_Pop_Fields>;\n  var_samp?: Maybe<Average_Block_Time_Per_Day_Var_Samp_Fields>;\n  variance?: Maybe<Average_Block_Time_Per_Day_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"average_block_time_per_day\" */\nexport type Average_Block_Time_Per_Day_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Average_Block_Time_Per_Day_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Average_Block_Time_Per_Day_Avg_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"average_block_time_per_day\". All fields are combined with a logical 'AND'. */\nexport type Average_Block_Time_Per_Day_Bool_Exp = {\n  _and?: InputMaybe<Array<Average_Block_Time_Per_Day_Bool_Exp>>;\n  _not?: InputMaybe<Average_Block_Time_Per_Day_Bool_Exp>;\n  _or?: InputMaybe<Array<Average_Block_Time_Per_Day_Bool_Exp>>;\n  average_time?: InputMaybe<Numeric_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Average_Block_Time_Per_Day_Max_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Average_Block_Time_Per_Day_Min_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"average_block_time_per_day\". */\nexport type Average_Block_Time_Per_Day_Order_By = {\n  average_time?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"average_block_time_per_day\" */\nexport enum Average_Block_Time_Per_Day_Select_Column {\n  /** column name */\n  AverageTime = 'average_time',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  OneRowId = 'one_row_id'\n}\n\n/** aggregate stddev on columns */\nexport type Average_Block_Time_Per_Day_Stddev_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Average_Block_Time_Per_Day_Stddev_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Average_Block_Time_Per_Day_Stddev_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"average_block_time_per_day\" */\nexport type Average_Block_Time_Per_Day_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Average_Block_Time_Per_Day_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Average_Block_Time_Per_Day_Stream_Cursor_Value_Input = {\n  average_time?: InputMaybe<Scalars['numeric']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  one_row_id?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Average_Block_Time_Per_Day_Sum_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Average_Block_Time_Per_Day_Var_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Average_Block_Time_Per_Day_Var_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Average_Block_Time_Per_Day_Variance_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"average_block_time_per_hour\" */\nexport type Average_Block_Time_Per_Hour = {\n  average_time: Scalars['numeric']['output'];\n  height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n};\n\n/** aggregated selection of \"average_block_time_per_hour\" */\nexport type Average_Block_Time_Per_Hour_Aggregate = {\n  aggregate?: Maybe<Average_Block_Time_Per_Hour_Aggregate_Fields>;\n  nodes: Array<Average_Block_Time_Per_Hour>;\n};\n\n/** aggregate fields of \"average_block_time_per_hour\" */\nexport type Average_Block_Time_Per_Hour_Aggregate_Fields = {\n  avg?: Maybe<Average_Block_Time_Per_Hour_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Average_Block_Time_Per_Hour_Max_Fields>;\n  min?: Maybe<Average_Block_Time_Per_Hour_Min_Fields>;\n  stddev?: Maybe<Average_Block_Time_Per_Hour_Stddev_Fields>;\n  stddev_pop?: Maybe<Average_Block_Time_Per_Hour_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Average_Block_Time_Per_Hour_Stddev_Samp_Fields>;\n  sum?: Maybe<Average_Block_Time_Per_Hour_Sum_Fields>;\n  var_pop?: Maybe<Average_Block_Time_Per_Hour_Var_Pop_Fields>;\n  var_samp?: Maybe<Average_Block_Time_Per_Hour_Var_Samp_Fields>;\n  variance?: Maybe<Average_Block_Time_Per_Hour_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"average_block_time_per_hour\" */\nexport type Average_Block_Time_Per_Hour_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Average_Block_Time_Per_Hour_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Average_Block_Time_Per_Hour_Avg_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"average_block_time_per_hour\". All fields are combined with a logical 'AND'. */\nexport type Average_Block_Time_Per_Hour_Bool_Exp = {\n  _and?: InputMaybe<Array<Average_Block_Time_Per_Hour_Bool_Exp>>;\n  _not?: InputMaybe<Average_Block_Time_Per_Hour_Bool_Exp>;\n  _or?: InputMaybe<Array<Average_Block_Time_Per_Hour_Bool_Exp>>;\n  average_time?: InputMaybe<Numeric_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Average_Block_Time_Per_Hour_Max_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Average_Block_Time_Per_Hour_Min_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"average_block_time_per_hour\". */\nexport type Average_Block_Time_Per_Hour_Order_By = {\n  average_time?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"average_block_time_per_hour\" */\nexport enum Average_Block_Time_Per_Hour_Select_Column {\n  /** column name */\n  AverageTime = 'average_time',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  OneRowId = 'one_row_id'\n}\n\n/** aggregate stddev on columns */\nexport type Average_Block_Time_Per_Hour_Stddev_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Average_Block_Time_Per_Hour_Stddev_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Average_Block_Time_Per_Hour_Stddev_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"average_block_time_per_hour\" */\nexport type Average_Block_Time_Per_Hour_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Average_Block_Time_Per_Hour_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Average_Block_Time_Per_Hour_Stream_Cursor_Value_Input = {\n  average_time?: InputMaybe<Scalars['numeric']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  one_row_id?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Average_Block_Time_Per_Hour_Sum_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Average_Block_Time_Per_Hour_Var_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Average_Block_Time_Per_Hour_Var_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Average_Block_Time_Per_Hour_Variance_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"average_block_time_per_minute\" */\nexport type Average_Block_Time_Per_Minute = {\n  average_time: Scalars['numeric']['output'];\n  height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n};\n\n/** aggregated selection of \"average_block_time_per_minute\" */\nexport type Average_Block_Time_Per_Minute_Aggregate = {\n  aggregate?: Maybe<Average_Block_Time_Per_Minute_Aggregate_Fields>;\n  nodes: Array<Average_Block_Time_Per_Minute>;\n};\n\n/** aggregate fields of \"average_block_time_per_minute\" */\nexport type Average_Block_Time_Per_Minute_Aggregate_Fields = {\n  avg?: Maybe<Average_Block_Time_Per_Minute_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Average_Block_Time_Per_Minute_Max_Fields>;\n  min?: Maybe<Average_Block_Time_Per_Minute_Min_Fields>;\n  stddev?: Maybe<Average_Block_Time_Per_Minute_Stddev_Fields>;\n  stddev_pop?: Maybe<Average_Block_Time_Per_Minute_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Average_Block_Time_Per_Minute_Stddev_Samp_Fields>;\n  sum?: Maybe<Average_Block_Time_Per_Minute_Sum_Fields>;\n  var_pop?: Maybe<Average_Block_Time_Per_Minute_Var_Pop_Fields>;\n  var_samp?: Maybe<Average_Block_Time_Per_Minute_Var_Samp_Fields>;\n  variance?: Maybe<Average_Block_Time_Per_Minute_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"average_block_time_per_minute\" */\nexport type Average_Block_Time_Per_Minute_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Average_Block_Time_Per_Minute_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Average_Block_Time_Per_Minute_Avg_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"average_block_time_per_minute\". All fields are combined with a logical 'AND'. */\nexport type Average_Block_Time_Per_Minute_Bool_Exp = {\n  _and?: InputMaybe<Array<Average_Block_Time_Per_Minute_Bool_Exp>>;\n  _not?: InputMaybe<Average_Block_Time_Per_Minute_Bool_Exp>;\n  _or?: InputMaybe<Array<Average_Block_Time_Per_Minute_Bool_Exp>>;\n  average_time?: InputMaybe<Numeric_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Average_Block_Time_Per_Minute_Max_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Average_Block_Time_Per_Minute_Min_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"average_block_time_per_minute\". */\nexport type Average_Block_Time_Per_Minute_Order_By = {\n  average_time?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"average_block_time_per_minute\" */\nexport enum Average_Block_Time_Per_Minute_Select_Column {\n  /** column name */\n  AverageTime = 'average_time',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  OneRowId = 'one_row_id'\n}\n\n/** aggregate stddev on columns */\nexport type Average_Block_Time_Per_Minute_Stddev_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Average_Block_Time_Per_Minute_Stddev_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Average_Block_Time_Per_Minute_Stddev_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"average_block_time_per_minute\" */\nexport type Average_Block_Time_Per_Minute_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Average_Block_Time_Per_Minute_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Average_Block_Time_Per_Minute_Stream_Cursor_Value_Input = {\n  average_time?: InputMaybe<Scalars['numeric']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  one_row_id?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Average_Block_Time_Per_Minute_Sum_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Average_Block_Time_Per_Minute_Var_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Average_Block_Time_Per_Minute_Var_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Average_Block_Time_Per_Minute_Variance_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to compare columns of type \"bigint\". All fields are combined with logical 'AND'. */\nexport type Bigint_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['bigint']['input']>;\n  _gt?: InputMaybe<Scalars['bigint']['input']>;\n  _gte?: InputMaybe<Scalars['bigint']['input']>;\n  _in?: InputMaybe<Array<Scalars['bigint']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['bigint']['input']>;\n  _lte?: InputMaybe<Scalars['bigint']['input']>;\n  _neq?: InputMaybe<Scalars['bigint']['input']>;\n  _nin?: InputMaybe<Array<Scalars['bigint']['input']>>;\n};\n\n/** columns and relationships of \"block\" */\nexport type Block = {\n  /** An array relationship */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  hash: Scalars['String']['output'];\n  height: Scalars['bigint']['output'];\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  num_txs?: Maybe<Scalars['Int']['output']>;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  proposer_address?: Maybe<Scalars['String']['output']>;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n  /** An array relationship */\n  swaps: Array<Swaps>;\n  /** An aggregate relationship */\n  swaps_aggregate: Swaps_Aggregate;\n  timestamp: Scalars['timestamp']['output'];\n  total_gas?: Maybe<Scalars['bigint']['output']>;\n  /** An array relationship */\n  transaction_155s: Array<Transaction_155>;\n  /** An aggregate relationship */\n  transaction_155s_aggregate: Transaction_155_Aggregate;\n  /** An array relationship */\n  transactions: Array<Transaction>;\n  /** An aggregate relationship */\n  transactions_aggregate: Transaction_Aggregate;\n  /** An object relationship */\n  validator?: Maybe<Validator>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockSwapsArgs = {\n  distinct_on?: InputMaybe<Array<Swaps_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Swaps_Order_By>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockSwaps_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Swaps_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Swaps_Order_By>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockTransaction_155sArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_155_Order_By>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockTransaction_155s_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_155_Order_By>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockTransactionsArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockTransactions_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n/** aggregated selection of \"block\" */\nexport type Block_Aggregate = {\n  aggregate?: Maybe<Block_Aggregate_Fields>;\n  nodes: Array<Block>;\n};\n\nexport type Block_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Block_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Block_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Block_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Block_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"block\" */\nexport type Block_Aggregate_Fields = {\n  avg?: Maybe<Block_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Block_Max_Fields>;\n  min?: Maybe<Block_Min_Fields>;\n  stddev?: Maybe<Block_Stddev_Fields>;\n  stddev_pop?: Maybe<Block_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Block_Stddev_Samp_Fields>;\n  sum?: Maybe<Block_Sum_Fields>;\n  var_pop?: Maybe<Block_Var_Pop_Fields>;\n  var_samp?: Maybe<Block_Var_Samp_Fields>;\n  variance?: Maybe<Block_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"block\" */\nexport type Block_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Block_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"block\" */\nexport type Block_Aggregate_Order_By = {\n  avg?: InputMaybe<Block_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Block_Max_Order_By>;\n  min?: InputMaybe<Block_Min_Order_By>;\n  stddev?: InputMaybe<Block_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Block_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Block_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Block_Sum_Order_By>;\n  var_pop?: InputMaybe<Block_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Block_Var_Samp_Order_By>;\n  variance?: InputMaybe<Block_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Block_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"block\" */\nexport type Block_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"block\". All fields are combined with a logical 'AND'. */\nexport type Block_Bool_Exp = {\n  _and?: InputMaybe<Array<Block_Bool_Exp>>;\n  _not?: InputMaybe<Block_Bool_Exp>;\n  _or?: InputMaybe<Array<Block_Bool_Exp>>;\n  cyberlinks?: InputMaybe<Cyberlinks_Bool_Exp>;\n  cyberlinks_aggregate?: InputMaybe<Cyberlinks_Aggregate_Bool_Exp>;\n  hash?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  investmints?: InputMaybe<Investmints_Bool_Exp>;\n  investmints_aggregate?: InputMaybe<Investmints_Aggregate_Bool_Exp>;\n  num_txs?: InputMaybe<Int_Comparison_Exp>;\n  particles?: InputMaybe<Particles_Bool_Exp>;\n  particles_aggregate?: InputMaybe<Particles_Aggregate_Bool_Exp>;\n  proposer_address?: InputMaybe<String_Comparison_Exp>;\n  routes?: InputMaybe<Routes_Bool_Exp>;\n  routes_aggregate?: InputMaybe<Routes_Aggregate_Bool_Exp>;\n  swaps?: InputMaybe<Swaps_Bool_Exp>;\n  swaps_aggregate?: InputMaybe<Swaps_Aggregate_Bool_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  total_gas?: InputMaybe<Bigint_Comparison_Exp>;\n  transaction_155s?: InputMaybe<Transaction_155_Bool_Exp>;\n  transaction_155s_aggregate?: InputMaybe<Transaction_155_Aggregate_Bool_Exp>;\n  transactions?: InputMaybe<Transaction_Bool_Exp>;\n  transactions_aggregate?: InputMaybe<Transaction_Aggregate_Bool_Exp>;\n  validator?: InputMaybe<Validator_Bool_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Block_Max_Fields = {\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  num_txs?: Maybe<Scalars['Int']['output']>;\n  proposer_address?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  total_gas?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by max() on columns of table \"block\" */\nexport type Block_Max_Order_By = {\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  proposer_address?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Block_Min_Fields = {\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  num_txs?: Maybe<Scalars['Int']['output']>;\n  proposer_address?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  total_gas?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by min() on columns of table \"block\" */\nexport type Block_Min_Order_By = {\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  proposer_address?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"block\". */\nexport type Block_Order_By = {\n  cyberlinks_aggregate?: InputMaybe<Cyberlinks_Aggregate_Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  investmints_aggregate?: InputMaybe<Investmints_Aggregate_Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  particles_aggregate?: InputMaybe<Particles_Aggregate_Order_By>;\n  proposer_address?: InputMaybe<Order_By>;\n  routes_aggregate?: InputMaybe<Routes_Aggregate_Order_By>;\n  swaps_aggregate?: InputMaybe<Swaps_Aggregate_Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n  transaction_155s_aggregate?: InputMaybe<Transaction_155_Aggregate_Order_By>;\n  transactions_aggregate?: InputMaybe<Transaction_Aggregate_Order_By>;\n  validator?: InputMaybe<Validator_Order_By>;\n};\n\n/** select columns of table \"block\" */\nexport enum Block_Select_Column {\n  /** column name */\n  Hash = 'hash',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  NumTxs = 'num_txs',\n  /** column name */\n  ProposerAddress = 'proposer_address',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TotalGas = 'total_gas'\n}\n\n/** aggregate stddev on columns */\nexport type Block_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"block\" */\nexport type Block_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Block_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"block\" */\nexport type Block_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Block_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"block\" */\nexport type Block_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"block\" */\nexport type Block_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Block_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Block_Stream_Cursor_Value_Input = {\n  hash?: InputMaybe<Scalars['String']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  num_txs?: InputMaybe<Scalars['Int']['input']>;\n  proposer_address?: InputMaybe<Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  total_gas?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Block_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  num_txs?: Maybe<Scalars['Int']['output']>;\n  total_gas?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"block\" */\nexport type Block_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Block_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"block\" */\nexport type Block_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Block_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"block\" */\nexport type Block_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Block_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"block\" */\nexport type Block_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to compare columns of type \"coin\". All fields are combined with logical 'AND'. */\nexport type Coin_Array_Comparison_Exp = {\n  /** is the array contained in the given array value */\n  _contained_in?: InputMaybe<Array<Scalars['coin']['input']>>;\n  /** does the array contain the given value */\n  _contains?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _eq?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _gt?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _gte?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _in?: InputMaybe<Array<Array<Scalars['coin']['input']>>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _lte?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _neq?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _nin?: InputMaybe<Array<Array<Scalars['coin']['input']>>>;\n};\n\n/** Boolean expression to compare columns of type \"coin_scalar\". All fields are combined with logical 'AND'. */\nexport type Coin_Scalar_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['coin_scalar']['input']>;\n  _gt?: InputMaybe<Scalars['coin_scalar']['input']>;\n  _gte?: InputMaybe<Scalars['coin_scalar']['input']>;\n  _in?: InputMaybe<Array<Scalars['coin_scalar']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['coin_scalar']['input']>;\n  _lte?: InputMaybe<Scalars['coin_scalar']['input']>;\n  _neq?: InputMaybe<Scalars['coin_scalar']['input']>;\n  _nin?: InputMaybe<Array<Scalars['coin_scalar']['input']>>;\n};\n\n/** columns and relationships of \"contracts\" */\nexport type Contracts = {\n  address: Scalars['String']['output'];\n  admin: Scalars['String']['output'];\n  code_id: Scalars['bigint']['output'];\n  creation_time: Scalars['String']['output'];\n  creator: Scalars['String']['output'];\n  fees: Scalars['bigint']['output'];\n  gas: Scalars['bigint']['output'];\n  height: Scalars['bigint']['output'];\n  label: Scalars['String']['output'];\n  tx: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"contracts\" */\nexport type Contracts_Aggregate = {\n  aggregate?: Maybe<Contracts_Aggregate_Fields>;\n  nodes: Array<Contracts>;\n};\n\n/** aggregate fields of \"contracts\" */\nexport type Contracts_Aggregate_Fields = {\n  avg?: Maybe<Contracts_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Contracts_Max_Fields>;\n  min?: Maybe<Contracts_Min_Fields>;\n  stddev?: Maybe<Contracts_Stddev_Fields>;\n  stddev_pop?: Maybe<Contracts_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Contracts_Stddev_Samp_Fields>;\n  sum?: Maybe<Contracts_Sum_Fields>;\n  var_pop?: Maybe<Contracts_Var_Pop_Fields>;\n  var_samp?: Maybe<Contracts_Var_Samp_Fields>;\n  variance?: Maybe<Contracts_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"contracts\" */\nexport type Contracts_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Contracts_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Contracts_Avg_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"contracts\". All fields are combined with a logical 'AND'. */\nexport type Contracts_Bool_Exp = {\n  _and?: InputMaybe<Array<Contracts_Bool_Exp>>;\n  _not?: InputMaybe<Contracts_Bool_Exp>;\n  _or?: InputMaybe<Array<Contracts_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  admin?: InputMaybe<String_Comparison_Exp>;\n  code_id?: InputMaybe<Bigint_Comparison_Exp>;\n  creation_time?: InputMaybe<String_Comparison_Exp>;\n  creator?: InputMaybe<String_Comparison_Exp>;\n  fees?: InputMaybe<Bigint_Comparison_Exp>;\n  gas?: InputMaybe<Bigint_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  label?: InputMaybe<String_Comparison_Exp>;\n  tx?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Contracts_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  admin?: Maybe<Scalars['String']['output']>;\n  code_id?: Maybe<Scalars['bigint']['output']>;\n  creation_time?: Maybe<Scalars['String']['output']>;\n  creator?: Maybe<Scalars['String']['output']>;\n  fees?: Maybe<Scalars['bigint']['output']>;\n  gas?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  label?: Maybe<Scalars['String']['output']>;\n  tx?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Contracts_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  admin?: Maybe<Scalars['String']['output']>;\n  code_id?: Maybe<Scalars['bigint']['output']>;\n  creation_time?: Maybe<Scalars['String']['output']>;\n  creator?: Maybe<Scalars['String']['output']>;\n  fees?: Maybe<Scalars['bigint']['output']>;\n  gas?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  label?: Maybe<Scalars['String']['output']>;\n  tx?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"contracts\". */\nexport type Contracts_Order_By = {\n  address?: InputMaybe<Order_By>;\n  admin?: InputMaybe<Order_By>;\n  code_id?: InputMaybe<Order_By>;\n  creation_time?: InputMaybe<Order_By>;\n  creator?: InputMaybe<Order_By>;\n  fees?: InputMaybe<Order_By>;\n  gas?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  label?: InputMaybe<Order_By>;\n  tx?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"contracts\" */\nexport enum Contracts_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Admin = 'admin',\n  /** column name */\n  CodeId = 'code_id',\n  /** column name */\n  CreationTime = 'creation_time',\n  /** column name */\n  Creator = 'creator',\n  /** column name */\n  Fees = 'fees',\n  /** column name */\n  Gas = 'gas',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Label = 'label',\n  /** column name */\n  Tx = 'tx'\n}\n\n/** aggregate stddev on columns */\nexport type Contracts_Stddev_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Contracts_Stddev_Pop_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Contracts_Stddev_Samp_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"contracts\" */\nexport type Contracts_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Contracts_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Contracts_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n  admin?: InputMaybe<Scalars['String']['input']>;\n  code_id?: InputMaybe<Scalars['bigint']['input']>;\n  creation_time?: InputMaybe<Scalars['String']['input']>;\n  creator?: InputMaybe<Scalars['String']['input']>;\n  fees?: InputMaybe<Scalars['bigint']['input']>;\n  gas?: InputMaybe<Scalars['bigint']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  label?: InputMaybe<Scalars['String']['input']>;\n  tx?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Contracts_Sum_Fields = {\n  code_id?: Maybe<Scalars['bigint']['output']>;\n  fees?: Maybe<Scalars['bigint']['output']>;\n  gas?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  tx?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Contracts_Var_Pop_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Contracts_Var_Samp_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Contracts_Variance_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** ordering argument of a cursor */\nexport enum Cursor_Ordering {\n  /** ascending ordering of the cursor */\n  Asc = 'ASC',\n  /** descending ordering of the cursor */\n  Desc = 'DESC'\n}\n\n/** columns and relationships of \"cyb_cohort\" */\nexport type Cyb_Cohort = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neurons_activated?: Maybe<Scalars['bigint']['output']>;\n  redelegation_percent?: Maybe<Scalars['float8']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  undelegation_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"cyb_cohort\" */\nexport type Cyb_Cohort_Aggregate = {\n  aggregate?: Maybe<Cyb_Cohort_Aggregate_Fields>;\n  nodes: Array<Cyb_Cohort>;\n};\n\n/** aggregate fields of \"cyb_cohort\" */\nexport type Cyb_Cohort_Aggregate_Fields = {\n  avg?: Maybe<Cyb_Cohort_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyb_Cohort_Max_Fields>;\n  min?: Maybe<Cyb_Cohort_Min_Fields>;\n  stddev?: Maybe<Cyb_Cohort_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyb_Cohort_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyb_Cohort_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyb_Cohort_Sum_Fields>;\n  var_pop?: Maybe<Cyb_Cohort_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyb_Cohort_Var_Samp_Fields>;\n  variance?: Maybe<Cyb_Cohort_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyb_cohort\" */\nexport type Cyb_Cohort_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Cyb_Cohort_Avg_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"cyb_cohort\". All fields are combined with a logical 'AND'. */\nexport type Cyb_Cohort_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyb_Cohort_Bool_Exp>>;\n  _not?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyb_Cohort_Bool_Exp>>;\n  cyberlink_10_percent?: InputMaybe<Float8_Comparison_Exp>;\n  cyberlink_100_percent?: InputMaybe<Float8_Comparison_Exp>;\n  cyberlink_percent?: InputMaybe<Float8_Comparison_Exp>;\n  hero_hired_percent?: InputMaybe<Float8_Comparison_Exp>;\n  investmint_percent?: InputMaybe<Float8_Comparison_Exp>;\n  neurons_activated?: InputMaybe<Bigint_Comparison_Exp>;\n  redelegation_percent?: InputMaybe<Float8_Comparison_Exp>;\n  swap_percent?: InputMaybe<Float8_Comparison_Exp>;\n  undelegation_percent?: InputMaybe<Float8_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyb_Cohort_Max_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neurons_activated?: Maybe<Scalars['bigint']['output']>;\n  redelegation_percent?: Maybe<Scalars['float8']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  undelegation_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Cyb_Cohort_Min_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neurons_activated?: Maybe<Scalars['bigint']['output']>;\n  redelegation_percent?: Maybe<Scalars['float8']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  undelegation_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"cyb_cohort\". */\nexport type Cyb_Cohort_Order_By = {\n  cyberlink_10_percent?: InputMaybe<Order_By>;\n  cyberlink_100_percent?: InputMaybe<Order_By>;\n  cyberlink_percent?: InputMaybe<Order_By>;\n  hero_hired_percent?: InputMaybe<Order_By>;\n  investmint_percent?: InputMaybe<Order_By>;\n  neurons_activated?: InputMaybe<Order_By>;\n  redelegation_percent?: InputMaybe<Order_By>;\n  swap_percent?: InputMaybe<Order_By>;\n  undelegation_percent?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyb_cohort\" */\nexport enum Cyb_Cohort_Select_Column {\n  /** column name */\n  Cyberlink_10Percent = 'cyberlink_10_percent',\n  /** column name */\n  Cyberlink_100Percent = 'cyberlink_100_percent',\n  /** column name */\n  CyberlinkPercent = 'cyberlink_percent',\n  /** column name */\n  HeroHiredPercent = 'hero_hired_percent',\n  /** column name */\n  InvestmintPercent = 'investmint_percent',\n  /** column name */\n  NeuronsActivated = 'neurons_activated',\n  /** column name */\n  RedelegationPercent = 'redelegation_percent',\n  /** column name */\n  SwapPercent = 'swap_percent',\n  /** column name */\n  UndelegationPercent = 'undelegation_percent',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Cyb_Cohort_Stddev_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyb_Cohort_Stddev_Pop_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyb_Cohort_Stddev_Samp_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"cyb_cohort\" */\nexport type Cyb_Cohort_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Cyb_Cohort_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Cyb_Cohort_Stream_Cursor_Value_Input = {\n  cyberlink_10_percent?: InputMaybe<Scalars['float8']['input']>;\n  cyberlink_100_percent?: InputMaybe<Scalars['float8']['input']>;\n  cyberlink_percent?: InputMaybe<Scalars['float8']['input']>;\n  hero_hired_percent?: InputMaybe<Scalars['float8']['input']>;\n  investmint_percent?: InputMaybe<Scalars['float8']['input']>;\n  neurons_activated?: InputMaybe<Scalars['bigint']['input']>;\n  redelegation_percent?: InputMaybe<Scalars['float8']['input']>;\n  swap_percent?: InputMaybe<Scalars['float8']['input']>;\n  undelegation_percent?: InputMaybe<Scalars['float8']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Cyb_Cohort_Sum_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neurons_activated?: Maybe<Scalars['bigint']['output']>;\n  redelegation_percent?: Maybe<Scalars['float8']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  undelegation_percent?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyb_Cohort_Var_Pop_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyb_Cohort_Var_Samp_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Cyb_Cohort_Variance_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs = {\n  address: Scalars['String']['output'];\n  amount: Scalars['bigint']['output'];\n  details: Array<Scalars['jsonb']['output']>;\n  proof: Array<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs_Aggregate = {\n  aggregate?: Maybe<Cyber_Gift_Proofs_Aggregate_Fields>;\n  nodes: Array<Cyber_Gift_Proofs>;\n};\n\n/** aggregate fields of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs_Aggregate_Fields = {\n  avg?: Maybe<Cyber_Gift_Proofs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyber_Gift_Proofs_Max_Fields>;\n  min?: Maybe<Cyber_Gift_Proofs_Min_Fields>;\n  stddev?: Maybe<Cyber_Gift_Proofs_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyber_Gift_Proofs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyber_Gift_Proofs_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyber_Gift_Proofs_Sum_Fields>;\n  var_pop?: Maybe<Cyber_Gift_Proofs_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyber_Gift_Proofs_Var_Samp_Fields>;\n  variance?: Maybe<Cyber_Gift_Proofs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Cyber_Gift_Proofs_Avg_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"cyber_gift_proofs\". All fields are combined with a logical 'AND'. */\nexport type Cyber_Gift_Proofs_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyber_Gift_Proofs_Bool_Exp>>;\n  _not?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyber_Gift_Proofs_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  amount?: InputMaybe<Bigint_Comparison_Exp>;\n  details?: InputMaybe<Jsonb_Array_Comparison_Exp>;\n  proof?: InputMaybe<String_Array_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyber_Gift_Proofs_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  details?: Maybe<Array<Scalars['jsonb']['output']>>;\n  proof?: Maybe<Array<Scalars['String']['output']>>;\n};\n\n/** aggregate min on columns */\nexport type Cyber_Gift_Proofs_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  details?: Maybe<Array<Scalars['jsonb']['output']>>;\n  proof?: Maybe<Array<Scalars['String']['output']>>;\n};\n\n/** Ordering options when selecting data from \"cyber_gift_proofs\". */\nexport type Cyber_Gift_Proofs_Order_By = {\n  address?: InputMaybe<Order_By>;\n  amount?: InputMaybe<Order_By>;\n  details?: InputMaybe<Order_By>;\n  proof?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyber_gift_proofs\" */\nexport enum Cyber_Gift_Proofs_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Amount = 'amount',\n  /** column name */\n  Details = 'details',\n  /** column name */\n  Proof = 'proof'\n}\n\n/** aggregate stddev on columns */\nexport type Cyber_Gift_Proofs_Stddev_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyber_Gift_Proofs_Stddev_Pop_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyber_Gift_Proofs_Stddev_Samp_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Cyber_Gift_Proofs_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Cyber_Gift_Proofs_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n  amount?: InputMaybe<Scalars['bigint']['input']>;\n  details?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  proof?: InputMaybe<Array<Scalars['String']['input']>>;\n};\n\n/** aggregate sum on columns */\nexport type Cyber_Gift_Proofs_Sum_Fields = {\n  amount?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyber_Gift_Proofs_Var_Pop_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyber_Gift_Proofs_Var_Samp_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Cyber_Gift_Proofs_Variance_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"cyberlinks\" */\nexport type Cyberlinks = {\n  /** An object relationship */\n  account: Account;\n  /** An object relationship */\n  block: Block;\n  height: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n  neuron: Scalars['String']['output'];\n  particle_from: Scalars['String']['output'];\n  particle_to: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  transaction_hash: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"cyberlinks\" */\nexport type Cyberlinks_Aggregate = {\n  aggregate?: Maybe<Cyberlinks_Aggregate_Fields>;\n  nodes: Array<Cyberlinks>;\n};\n\nexport type Cyberlinks_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Cyberlinks_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Cyberlinks_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Cyberlinks_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"cyberlinks\" */\nexport type Cyberlinks_Aggregate_Fields = {\n  avg?: Maybe<Cyberlinks_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyberlinks_Max_Fields>;\n  min?: Maybe<Cyberlinks_Min_Fields>;\n  stddev?: Maybe<Cyberlinks_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyberlinks_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyberlinks_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyberlinks_Sum_Fields>;\n  var_pop?: Maybe<Cyberlinks_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyberlinks_Var_Samp_Fields>;\n  variance?: Maybe<Cyberlinks_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyberlinks\" */\nexport type Cyberlinks_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"cyberlinks\" */\nexport type Cyberlinks_Aggregate_Order_By = {\n  avg?: InputMaybe<Cyberlinks_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Cyberlinks_Max_Order_By>;\n  min?: InputMaybe<Cyberlinks_Min_Order_By>;\n  stddev?: InputMaybe<Cyberlinks_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Cyberlinks_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Cyberlinks_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Cyberlinks_Sum_Order_By>;\n  var_pop?: InputMaybe<Cyberlinks_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Cyberlinks_Var_Samp_Order_By>;\n  variance?: InputMaybe<Cyberlinks_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Cyberlinks_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"cyberlinks\". All fields are combined with a logical 'AND'. */\nexport type Cyberlinks_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyberlinks_Bool_Exp>>;\n  _not?: InputMaybe<Cyberlinks_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyberlinks_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n  particle_from?: InputMaybe<String_Comparison_Exp>;\n  particle_to?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyberlinks_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  particle_from?: Maybe<Scalars['String']['output']>;\n  particle_to?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle_from?: InputMaybe<Order_By>;\n  particle_to?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Cyberlinks_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  particle_from?: Maybe<Scalars['String']['output']>;\n  particle_to?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle_from?: InputMaybe<Order_By>;\n  particle_to?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"cyberlinks\". */\nexport type Cyberlinks_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle_from?: InputMaybe<Order_By>;\n  particle_to?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyberlinks\" */\nexport enum Cyberlinks_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  Neuron = 'neuron',\n  /** column name */\n  ParticleFrom = 'particle_from',\n  /** column name */\n  ParticleTo = 'particle_to',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TransactionHash = 'transaction_hash'\n}\n\n/** columns and relationships of \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats = {\n  cyberlinks?: Maybe<Scalars['numeric']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats_Aggregate = {\n  aggregate?: Maybe<Cyberlinks_Stats_Aggregate_Fields>;\n  nodes: Array<Cyberlinks_Stats>;\n};\n\n/** aggregate fields of \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats_Aggregate_Fields = {\n  avg?: Maybe<Cyberlinks_Stats_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyberlinks_Stats_Max_Fields>;\n  min?: Maybe<Cyberlinks_Stats_Min_Fields>;\n  stddev?: Maybe<Cyberlinks_Stats_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyberlinks_Stats_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyberlinks_Stats_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyberlinks_Stats_Sum_Fields>;\n  var_pop?: Maybe<Cyberlinks_Stats_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyberlinks_Stats_Var_Samp_Fields>;\n  variance?: Maybe<Cyberlinks_Stats_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Cyberlinks_Stats_Avg_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"cyberlinks_stats\". All fields are combined with a logical 'AND'. */\nexport type Cyberlinks_Stats_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyberlinks_Stats_Bool_Exp>>;\n  _not?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyberlinks_Stats_Bool_Exp>>;\n  cyberlinks?: InputMaybe<Numeric_Comparison_Exp>;\n  cyberlinks_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyberlinks_Stats_Max_Fields = {\n  cyberlinks?: Maybe<Scalars['numeric']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Cyberlinks_Stats_Min_Fields = {\n  cyberlinks?: Maybe<Scalars['numeric']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"cyberlinks_stats\". */\nexport type Cyberlinks_Stats_Order_By = {\n  cyberlinks?: InputMaybe<Order_By>;\n  cyberlinks_per_day?: InputMaybe<Order_By>;\n  date?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyberlinks_stats\" */\nexport enum Cyberlinks_Stats_Select_Column {\n  /** column name */\n  Cyberlinks = 'cyberlinks',\n  /** column name */\n  CyberlinksPerDay = 'cyberlinks_per_day',\n  /** column name */\n  Date = 'date'\n}\n\n/** aggregate stddev on columns */\nexport type Cyberlinks_Stats_Stddev_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyberlinks_Stats_Stddev_Pop_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyberlinks_Stats_Stddev_Samp_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Cyberlinks_Stats_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Cyberlinks_Stats_Stream_Cursor_Value_Input = {\n  cyberlinks?: InputMaybe<Scalars['numeric']['input']>;\n  cyberlinks_per_day?: InputMaybe<Scalars['bigint']['input']>;\n  date?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Cyberlinks_Stats_Sum_Fields = {\n  cyberlinks?: Maybe<Scalars['numeric']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyberlinks_Stats_Var_Pop_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyberlinks_Stats_Var_Samp_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Cyberlinks_Stats_Variance_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev on columns */\nexport type Cyberlinks_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyberlinks_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyberlinks_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"cyberlinks\" */\nexport type Cyberlinks_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Cyberlinks_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Cyberlinks_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  id?: InputMaybe<Scalars['Int']['input']>;\n  neuron?: InputMaybe<Scalars['String']['input']>;\n  particle_from?: InputMaybe<Scalars['String']['input']>;\n  particle_to?: InputMaybe<Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Cyberlinks_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** order by sum() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyberlinks_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyberlinks_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Cyberlinks_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons_Aggregate = {\n  aggregate?: Maybe<Daily_Amount_Of_Active_Neurons_Aggregate_Fields>;\n  nodes: Array<Daily_Amount_Of_Active_Neurons>;\n};\n\n/** aggregate fields of \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons_Aggregate_Fields = {\n  avg?: Maybe<Daily_Amount_Of_Active_Neurons_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Daily_Amount_Of_Active_Neurons_Max_Fields>;\n  min?: Maybe<Daily_Amount_Of_Active_Neurons_Min_Fields>;\n  stddev?: Maybe<Daily_Amount_Of_Active_Neurons_Stddev_Fields>;\n  stddev_pop?: Maybe<Daily_Amount_Of_Active_Neurons_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Daily_Amount_Of_Active_Neurons_Stddev_Samp_Fields>;\n  sum?: Maybe<Daily_Amount_Of_Active_Neurons_Sum_Fields>;\n  var_pop?: Maybe<Daily_Amount_Of_Active_Neurons_Var_Pop_Fields>;\n  var_samp?: Maybe<Daily_Amount_Of_Active_Neurons_Var_Samp_Fields>;\n  variance?: Maybe<Daily_Amount_Of_Active_Neurons_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Daily_Amount_Of_Active_Neurons_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"daily_amount_of_active_neurons\". All fields are combined with a logical 'AND'. */\nexport type Daily_Amount_Of_Active_Neurons_Bool_Exp = {\n  _and?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Bool_Exp>>;\n  _not?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n  _or?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Daily_Amount_Of_Active_Neurons_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Daily_Amount_Of_Active_Neurons_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"daily_amount_of_active_neurons\". */\nexport type Daily_Amount_Of_Active_Neurons_Order_By = {\n  count?: InputMaybe<Order_By>;\n  date?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"daily_amount_of_active_neurons\" */\nexport enum Daily_Amount_Of_Active_Neurons_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Date = 'date'\n}\n\n/** aggregate stddev on columns */\nexport type Daily_Amount_Of_Active_Neurons_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Daily_Amount_Of_Active_Neurons_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Daily_Amount_Of_Active_Neurons_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Daily_Amount_Of_Active_Neurons_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Daily_Amount_Of_Active_Neurons_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['bigint']['input']>;\n  date?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Daily_Amount_Of_Active_Neurons_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Daily_Amount_Of_Active_Neurons_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Daily_Amount_Of_Active_Neurons_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Daily_Amount_Of_Active_Neurons_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas = {\n  daily_gas?: Maybe<Scalars['numeric']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n  gas_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas_Aggregate = {\n  aggregate?: Maybe<Daily_Amount_Of_Used_Gas_Aggregate_Fields>;\n  nodes: Array<Daily_Amount_Of_Used_Gas>;\n};\n\n/** aggregate fields of \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas_Aggregate_Fields = {\n  avg?: Maybe<Daily_Amount_Of_Used_Gas_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Daily_Amount_Of_Used_Gas_Max_Fields>;\n  min?: Maybe<Daily_Amount_Of_Used_Gas_Min_Fields>;\n  stddev?: Maybe<Daily_Amount_Of_Used_Gas_Stddev_Fields>;\n  stddev_pop?: Maybe<Daily_Amount_Of_Used_Gas_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Daily_Amount_Of_Used_Gas_Stddev_Samp_Fields>;\n  sum?: Maybe<Daily_Amount_Of_Used_Gas_Sum_Fields>;\n  var_pop?: Maybe<Daily_Amount_Of_Used_Gas_Var_Pop_Fields>;\n  var_samp?: Maybe<Daily_Amount_Of_Used_Gas_Var_Samp_Fields>;\n  variance?: Maybe<Daily_Amount_Of_Used_Gas_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Daily_Amount_Of_Used_Gas_Avg_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"daily_amount_of_used_gas\". All fields are combined with a logical 'AND'. */\nexport type Daily_Amount_Of_Used_Gas_Bool_Exp = {\n  _and?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Bool_Exp>>;\n  _not?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n  _or?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Bool_Exp>>;\n  daily_gas?: InputMaybe<Numeric_Comparison_Exp>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  gas_total?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Daily_Amount_Of_Used_Gas_Max_Fields = {\n  daily_gas?: Maybe<Scalars['numeric']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n  gas_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Daily_Amount_Of_Used_Gas_Min_Fields = {\n  daily_gas?: Maybe<Scalars['numeric']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n  gas_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"daily_amount_of_used_gas\". */\nexport type Daily_Amount_Of_Used_Gas_Order_By = {\n  daily_gas?: InputMaybe<Order_By>;\n  date?: InputMaybe<Order_By>;\n  gas_total?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"daily_amount_of_used_gas\" */\nexport enum Daily_Amount_Of_Used_Gas_Select_Column {\n  /** column name */\n  DailyGas = 'daily_gas',\n  /** column name */\n  Date = 'date',\n  /** column name */\n  GasTotal = 'gas_total'\n}\n\n/** aggregate stddev on columns */\nexport type Daily_Amount_Of_Used_Gas_Stddev_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Daily_Amount_Of_Used_Gas_Stddev_Pop_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Daily_Amount_Of_Used_Gas_Stddev_Samp_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Daily_Amount_Of_Used_Gas_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Daily_Amount_Of_Used_Gas_Stream_Cursor_Value_Input = {\n  daily_gas?: InputMaybe<Scalars['numeric']['input']>;\n  date?: InputMaybe<Scalars['date']['input']>;\n  gas_total?: InputMaybe<Scalars['numeric']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Daily_Amount_Of_Used_Gas_Sum_Fields = {\n  daily_gas?: Maybe<Scalars['numeric']['output']>;\n  gas_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Daily_Amount_Of_Used_Gas_Var_Pop_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Daily_Amount_Of_Used_Gas_Var_Samp_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Daily_Amount_Of_Used_Gas_Variance_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions = {\n  date?: Maybe<Scalars['date']['output']>;\n  txs_per_day?: Maybe<Scalars['bigint']['output']>;\n  txs_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions_Aggregate = {\n  aggregate?: Maybe<Daily_Number_Of_Transactions_Aggregate_Fields>;\n  nodes: Array<Daily_Number_Of_Transactions>;\n};\n\n/** aggregate fields of \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions_Aggregate_Fields = {\n  avg?: Maybe<Daily_Number_Of_Transactions_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Daily_Number_Of_Transactions_Max_Fields>;\n  min?: Maybe<Daily_Number_Of_Transactions_Min_Fields>;\n  stddev?: Maybe<Daily_Number_Of_Transactions_Stddev_Fields>;\n  stddev_pop?: Maybe<Daily_Number_Of_Transactions_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Daily_Number_Of_Transactions_Stddev_Samp_Fields>;\n  sum?: Maybe<Daily_Number_Of_Transactions_Sum_Fields>;\n  var_pop?: Maybe<Daily_Number_Of_Transactions_Var_Pop_Fields>;\n  var_samp?: Maybe<Daily_Number_Of_Transactions_Var_Samp_Fields>;\n  variance?: Maybe<Daily_Number_Of_Transactions_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Daily_Number_Of_Transactions_Avg_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"daily_number_of_transactions\". All fields are combined with a logical 'AND'. */\nexport type Daily_Number_Of_Transactions_Bool_Exp = {\n  _and?: InputMaybe<Array<Daily_Number_Of_Transactions_Bool_Exp>>;\n  _not?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n  _or?: InputMaybe<Array<Daily_Number_Of_Transactions_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  txs_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n  txs_total?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Daily_Number_Of_Transactions_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  txs_per_day?: Maybe<Scalars['bigint']['output']>;\n  txs_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Daily_Number_Of_Transactions_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  txs_per_day?: Maybe<Scalars['bigint']['output']>;\n  txs_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"daily_number_of_transactions\". */\nexport type Daily_Number_Of_Transactions_Order_By = {\n  date?: InputMaybe<Order_By>;\n  txs_per_day?: InputMaybe<Order_By>;\n  txs_total?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"daily_number_of_transactions\" */\nexport enum Daily_Number_Of_Transactions_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  TxsPerDay = 'txs_per_day',\n  /** column name */\n  TxsTotal = 'txs_total'\n}\n\n/** aggregate stddev on columns */\nexport type Daily_Number_Of_Transactions_Stddev_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Daily_Number_Of_Transactions_Stddev_Pop_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Daily_Number_Of_Transactions_Stddev_Samp_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Daily_Number_Of_Transactions_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Daily_Number_Of_Transactions_Stream_Cursor_Value_Input = {\n  date?: InputMaybe<Scalars['date']['input']>;\n  txs_per_day?: InputMaybe<Scalars['bigint']['input']>;\n  txs_total?: InputMaybe<Scalars['numeric']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Daily_Number_Of_Transactions_Sum_Fields = {\n  txs_per_day?: Maybe<Scalars['bigint']['output']>;\n  txs_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Daily_Number_Of_Transactions_Var_Pop_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Daily_Number_Of_Transactions_Var_Samp_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Daily_Number_Of_Transactions_Variance_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to compare columns of type \"date\". All fields are combined with logical 'AND'. */\nexport type Date_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['date']['input']>;\n  _gt?: InputMaybe<Scalars['date']['input']>;\n  _gte?: InputMaybe<Scalars['date']['input']>;\n  _in?: InputMaybe<Array<Scalars['date']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['date']['input']>;\n  _lte?: InputMaybe<Scalars['date']['input']>;\n  _neq?: InputMaybe<Scalars['date']['input']>;\n  _nin?: InputMaybe<Array<Scalars['date']['input']>>;\n};\n\n/** columns and relationships of \"first_10_cyberlink\" */\nexport type First_10_Cyberlink = {\n  cyberlink_10?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_10_cyberlink\" */\nexport type First_10_Cyberlink_Aggregate = {\n  aggregate?: Maybe<First_10_Cyberlink_Aggregate_Fields>;\n  nodes: Array<First_10_Cyberlink>;\n};\n\n/** aggregate fields of \"first_10_cyberlink\" */\nexport type First_10_Cyberlink_Aggregate_Fields = {\n  avg?: Maybe<First_10_Cyberlink_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_10_Cyberlink_Max_Fields>;\n  min?: Maybe<First_10_Cyberlink_Min_Fields>;\n  stddev?: Maybe<First_10_Cyberlink_Stddev_Fields>;\n  stddev_pop?: Maybe<First_10_Cyberlink_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_10_Cyberlink_Stddev_Samp_Fields>;\n  sum?: Maybe<First_10_Cyberlink_Sum_Fields>;\n  var_pop?: Maybe<First_10_Cyberlink_Var_Pop_Fields>;\n  var_samp?: Maybe<First_10_Cyberlink_Var_Samp_Fields>;\n  variance?: Maybe<First_10_Cyberlink_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_10_cyberlink\" */\nexport type First_10_Cyberlink_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_10_Cyberlink_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_10_Cyberlink_Avg_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_10_cyberlink\". All fields are combined with a logical 'AND'. */\nexport type First_10_Cyberlink_Bool_Exp = {\n  _and?: InputMaybe<Array<First_10_Cyberlink_Bool_Exp>>;\n  _not?: InputMaybe<First_10_Cyberlink_Bool_Exp>;\n  _or?: InputMaybe<Array<First_10_Cyberlink_Bool_Exp>>;\n  cyberlink_10?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_10_Cyberlink_Max_Fields = {\n  cyberlink_10?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_10_Cyberlink_Min_Fields = {\n  cyberlink_10?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_10_cyberlink\". */\nexport type First_10_Cyberlink_Order_By = {\n  cyberlink_10?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_10_cyberlink\" */\nexport enum First_10_Cyberlink_Select_Column {\n  /** column name */\n  Cyberlink_10 = 'cyberlink_10',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_10_Cyberlink_Stddev_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_10_Cyberlink_Stddev_Pop_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_10_Cyberlink_Stddev_Samp_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_10_cyberlink\" */\nexport type First_10_Cyberlink_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_10_Cyberlink_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_10_Cyberlink_Stream_Cursor_Value_Input = {\n  cyberlink_10?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_10_Cyberlink_Sum_Fields = {\n  cyberlink_10?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_10_Cyberlink_Var_Pop_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_10_Cyberlink_Var_Samp_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_10_Cyberlink_Variance_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"first_100_cyberlink\" */\nexport type First_100_Cyberlink = {\n  cyberlink_100?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_100_cyberlink\" */\nexport type First_100_Cyberlink_Aggregate = {\n  aggregate?: Maybe<First_100_Cyberlink_Aggregate_Fields>;\n  nodes: Array<First_100_Cyberlink>;\n};\n\n/** aggregate fields of \"first_100_cyberlink\" */\nexport type First_100_Cyberlink_Aggregate_Fields = {\n  avg?: Maybe<First_100_Cyberlink_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_100_Cyberlink_Max_Fields>;\n  min?: Maybe<First_100_Cyberlink_Min_Fields>;\n  stddev?: Maybe<First_100_Cyberlink_Stddev_Fields>;\n  stddev_pop?: Maybe<First_100_Cyberlink_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_100_Cyberlink_Stddev_Samp_Fields>;\n  sum?: Maybe<First_100_Cyberlink_Sum_Fields>;\n  var_pop?: Maybe<First_100_Cyberlink_Var_Pop_Fields>;\n  var_samp?: Maybe<First_100_Cyberlink_Var_Samp_Fields>;\n  variance?: Maybe<First_100_Cyberlink_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_100_cyberlink\" */\nexport type First_100_Cyberlink_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_100_Cyberlink_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_100_Cyberlink_Avg_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_100_cyberlink\". All fields are combined with a logical 'AND'. */\nexport type First_100_Cyberlink_Bool_Exp = {\n  _and?: InputMaybe<Array<First_100_Cyberlink_Bool_Exp>>;\n  _not?: InputMaybe<First_100_Cyberlink_Bool_Exp>;\n  _or?: InputMaybe<Array<First_100_Cyberlink_Bool_Exp>>;\n  cyberlink_100?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_100_Cyberlink_Max_Fields = {\n  cyberlink_100?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_100_Cyberlink_Min_Fields = {\n  cyberlink_100?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_100_cyberlink\". */\nexport type First_100_Cyberlink_Order_By = {\n  cyberlink_100?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_100_cyberlink\" */\nexport enum First_100_Cyberlink_Select_Column {\n  /** column name */\n  Cyberlink_100 = 'cyberlink_100',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_100_Cyberlink_Stddev_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_100_Cyberlink_Stddev_Pop_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_100_Cyberlink_Stddev_Samp_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_100_cyberlink\" */\nexport type First_100_Cyberlink_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_100_Cyberlink_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_100_Cyberlink_Stream_Cursor_Value_Input = {\n  cyberlink_100?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_100_Cyberlink_Sum_Fields = {\n  cyberlink_100?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_100_Cyberlink_Var_Pop_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_100_Cyberlink_Var_Samp_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_100_Cyberlink_Variance_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"first_cyberlink\" */\nexport type First_Cyberlink = {\n  cyberlink?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_cyberlink\" */\nexport type First_Cyberlink_Aggregate = {\n  aggregate?: Maybe<First_Cyberlink_Aggregate_Fields>;\n  nodes: Array<First_Cyberlink>;\n};\n\n/** aggregate fields of \"first_cyberlink\" */\nexport type First_Cyberlink_Aggregate_Fields = {\n  avg?: Maybe<First_Cyberlink_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_Cyberlink_Max_Fields>;\n  min?: Maybe<First_Cyberlink_Min_Fields>;\n  stddev?: Maybe<First_Cyberlink_Stddev_Fields>;\n  stddev_pop?: Maybe<First_Cyberlink_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_Cyberlink_Stddev_Samp_Fields>;\n  sum?: Maybe<First_Cyberlink_Sum_Fields>;\n  var_pop?: Maybe<First_Cyberlink_Var_Pop_Fields>;\n  var_samp?: Maybe<First_Cyberlink_Var_Samp_Fields>;\n  variance?: Maybe<First_Cyberlink_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_cyberlink\" */\nexport type First_Cyberlink_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_Cyberlink_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_Cyberlink_Avg_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_cyberlink\". All fields are combined with a logical 'AND'. */\nexport type First_Cyberlink_Bool_Exp = {\n  _and?: InputMaybe<Array<First_Cyberlink_Bool_Exp>>;\n  _not?: InputMaybe<First_Cyberlink_Bool_Exp>;\n  _or?: InputMaybe<Array<First_Cyberlink_Bool_Exp>>;\n  cyberlink?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_Cyberlink_Max_Fields = {\n  cyberlink?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_Cyberlink_Min_Fields = {\n  cyberlink?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_cyberlink\". */\nexport type First_Cyberlink_Order_By = {\n  cyberlink?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_cyberlink\" */\nexport enum First_Cyberlink_Select_Column {\n  /** column name */\n  Cyberlink = 'cyberlink',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_Cyberlink_Stddev_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_Cyberlink_Stddev_Pop_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_Cyberlink_Stddev_Samp_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_cyberlink\" */\nexport type First_Cyberlink_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_Cyberlink_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_Cyberlink_Stream_Cursor_Value_Input = {\n  cyberlink?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_Cyberlink_Sum_Fields = {\n  cyberlink?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_Cyberlink_Var_Pop_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_Cyberlink_Var_Samp_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_Cyberlink_Variance_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"first_hero_hired\" */\nexport type First_Hero_Hired = {\n  hero_hired?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_hero_hired\" */\nexport type First_Hero_Hired_Aggregate = {\n  aggregate?: Maybe<First_Hero_Hired_Aggregate_Fields>;\n  nodes: Array<First_Hero_Hired>;\n};\n\n/** aggregate fields of \"first_hero_hired\" */\nexport type First_Hero_Hired_Aggregate_Fields = {\n  avg?: Maybe<First_Hero_Hired_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_Hero_Hired_Max_Fields>;\n  min?: Maybe<First_Hero_Hired_Min_Fields>;\n  stddev?: Maybe<First_Hero_Hired_Stddev_Fields>;\n  stddev_pop?: Maybe<First_Hero_Hired_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_Hero_Hired_Stddev_Samp_Fields>;\n  sum?: Maybe<First_Hero_Hired_Sum_Fields>;\n  var_pop?: Maybe<First_Hero_Hired_Var_Pop_Fields>;\n  var_samp?: Maybe<First_Hero_Hired_Var_Samp_Fields>;\n  variance?: Maybe<First_Hero_Hired_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_hero_hired\" */\nexport type First_Hero_Hired_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_Hero_Hired_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_Hero_Hired_Avg_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_hero_hired\". All fields are combined with a logical 'AND'. */\nexport type First_Hero_Hired_Bool_Exp = {\n  _and?: InputMaybe<Array<First_Hero_Hired_Bool_Exp>>;\n  _not?: InputMaybe<First_Hero_Hired_Bool_Exp>;\n  _or?: InputMaybe<Array<First_Hero_Hired_Bool_Exp>>;\n  hero_hired?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_Hero_Hired_Max_Fields = {\n  hero_hired?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_Hero_Hired_Min_Fields = {\n  hero_hired?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_hero_hired\". */\nexport type First_Hero_Hired_Order_By = {\n  hero_hired?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_hero_hired\" */\nexport enum First_Hero_Hired_Select_Column {\n  /** column name */\n  HeroHired = 'hero_hired',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_Hero_Hired_Stddev_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_Hero_Hired_Stddev_Pop_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_Hero_Hired_Stddev_Samp_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_hero_hired\" */\nexport type First_Hero_Hired_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_Hero_Hired_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_Hero_Hired_Stream_Cursor_Value_Input = {\n  hero_hired?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_Hero_Hired_Sum_Fields = {\n  hero_hired?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_Hero_Hired_Var_Pop_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_Hero_Hired_Var_Samp_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_Hero_Hired_Variance_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"first_investmint\" */\nexport type First_Investmint = {\n  investmint?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_investmint\" */\nexport type First_Investmint_Aggregate = {\n  aggregate?: Maybe<First_Investmint_Aggregate_Fields>;\n  nodes: Array<First_Investmint>;\n};\n\n/** aggregate fields of \"first_investmint\" */\nexport type First_Investmint_Aggregate_Fields = {\n  avg?: Maybe<First_Investmint_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_Investmint_Max_Fields>;\n  min?: Maybe<First_Investmint_Min_Fields>;\n  stddev?: Maybe<First_Investmint_Stddev_Fields>;\n  stddev_pop?: Maybe<First_Investmint_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_Investmint_Stddev_Samp_Fields>;\n  sum?: Maybe<First_Investmint_Sum_Fields>;\n  var_pop?: Maybe<First_Investmint_Var_Pop_Fields>;\n  var_samp?: Maybe<First_Investmint_Var_Samp_Fields>;\n  variance?: Maybe<First_Investmint_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_investmint\" */\nexport type First_Investmint_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_Investmint_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_Investmint_Avg_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_investmint\". All fields are combined with a logical 'AND'. */\nexport type First_Investmint_Bool_Exp = {\n  _and?: InputMaybe<Array<First_Investmint_Bool_Exp>>;\n  _not?: InputMaybe<First_Investmint_Bool_Exp>;\n  _or?: InputMaybe<Array<First_Investmint_Bool_Exp>>;\n  investmint?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_Investmint_Max_Fields = {\n  investmint?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_Investmint_Min_Fields = {\n  investmint?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_investmint\". */\nexport type First_Investmint_Order_By = {\n  investmint?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_investmint\" */\nexport enum First_Investmint_Select_Column {\n  /** column name */\n  Investmint = 'investmint',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_Investmint_Stddev_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_Investmint_Stddev_Pop_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_Investmint_Stddev_Samp_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_investmint\" */\nexport type First_Investmint_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_Investmint_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_Investmint_Stream_Cursor_Value_Input = {\n  investmint?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_Investmint_Sum_Fields = {\n  investmint?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_Investmint_Var_Pop_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_Investmint_Var_Samp_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_Investmint_Variance_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"first_neuron_activation\" */\nexport type First_Neuron_Activation = {\n  neuron_activation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_neuron_activation\" */\nexport type First_Neuron_Activation_Aggregate = {\n  aggregate?: Maybe<First_Neuron_Activation_Aggregate_Fields>;\n  nodes: Array<First_Neuron_Activation>;\n};\n\n/** aggregate fields of \"first_neuron_activation\" */\nexport type First_Neuron_Activation_Aggregate_Fields = {\n  avg?: Maybe<First_Neuron_Activation_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_Neuron_Activation_Max_Fields>;\n  min?: Maybe<First_Neuron_Activation_Min_Fields>;\n  stddev?: Maybe<First_Neuron_Activation_Stddev_Fields>;\n  stddev_pop?: Maybe<First_Neuron_Activation_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_Neuron_Activation_Stddev_Samp_Fields>;\n  sum?: Maybe<First_Neuron_Activation_Sum_Fields>;\n  var_pop?: Maybe<First_Neuron_Activation_Var_Pop_Fields>;\n  var_samp?: Maybe<First_Neuron_Activation_Var_Samp_Fields>;\n  variance?: Maybe<First_Neuron_Activation_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_neuron_activation\" */\nexport type First_Neuron_Activation_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_Neuron_Activation_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_Neuron_Activation_Avg_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_neuron_activation\". All fields are combined with a logical 'AND'. */\nexport type First_Neuron_Activation_Bool_Exp = {\n  _and?: InputMaybe<Array<First_Neuron_Activation_Bool_Exp>>;\n  _not?: InputMaybe<First_Neuron_Activation_Bool_Exp>;\n  _or?: InputMaybe<Array<First_Neuron_Activation_Bool_Exp>>;\n  neuron_activation?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_Neuron_Activation_Max_Fields = {\n  neuron_activation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_Neuron_Activation_Min_Fields = {\n  neuron_activation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_neuron_activation\". */\nexport type First_Neuron_Activation_Order_By = {\n  neuron_activation?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_neuron_activation\" */\nexport enum First_Neuron_Activation_Select_Column {\n  /** column name */\n  NeuronActivation = 'neuron_activation',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_Neuron_Activation_Stddev_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_Neuron_Activation_Stddev_Pop_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_Neuron_Activation_Stddev_Samp_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_neuron_activation\" */\nexport type First_Neuron_Activation_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_Neuron_Activation_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_Neuron_Activation_Stream_Cursor_Value_Input = {\n  neuron_activation?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_Neuron_Activation_Sum_Fields = {\n  neuron_activation?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_Neuron_Activation_Var_Pop_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_Neuron_Activation_Var_Samp_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_Neuron_Activation_Variance_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"first_swap\" */\nexport type First_Swap = {\n  swap?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_swap\" */\nexport type First_Swap_Aggregate = {\n  aggregate?: Maybe<First_Swap_Aggregate_Fields>;\n  nodes: Array<First_Swap>;\n};\n\n/** aggregate fields of \"first_swap\" */\nexport type First_Swap_Aggregate_Fields = {\n  avg?: Maybe<First_Swap_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_Swap_Max_Fields>;\n  min?: Maybe<First_Swap_Min_Fields>;\n  stddev?: Maybe<First_Swap_Stddev_Fields>;\n  stddev_pop?: Maybe<First_Swap_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_Swap_Stddev_Samp_Fields>;\n  sum?: Maybe<First_Swap_Sum_Fields>;\n  var_pop?: Maybe<First_Swap_Var_Pop_Fields>;\n  var_samp?: Maybe<First_Swap_Var_Samp_Fields>;\n  variance?: Maybe<First_Swap_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_swap\" */\nexport type First_Swap_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_Swap_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_Swap_Avg_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_swap\". All fields are combined with a logical 'AND'. */\nexport type First_Swap_Bool_Exp = {\n  _and?: InputMaybe<Array<First_Swap_Bool_Exp>>;\n  _not?: InputMaybe<First_Swap_Bool_Exp>;\n  _or?: InputMaybe<Array<First_Swap_Bool_Exp>>;\n  swap?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_Swap_Max_Fields = {\n  swap?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_Swap_Min_Fields = {\n  swap?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_swap\". */\nexport type First_Swap_Order_By = {\n  swap?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_swap\" */\nexport enum First_Swap_Select_Column {\n  /** column name */\n  Swap = 'swap',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_Swap_Stddev_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_Swap_Stddev_Pop_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_Swap_Stddev_Samp_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_swap\" */\nexport type First_Swap_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_Swap_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_Swap_Stream_Cursor_Value_Input = {\n  swap?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_Swap_Sum_Fields = {\n  swap?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_Swap_Var_Pop_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_Swap_Var_Samp_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_Swap_Variance_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to compare columns of type \"float8\". All fields are combined with logical 'AND'. */\nexport type Float8_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['float8']['input']>;\n  _gt?: InputMaybe<Scalars['float8']['input']>;\n  _gte?: InputMaybe<Scalars['float8']['input']>;\n  _in?: InputMaybe<Array<Scalars['float8']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['float8']['input']>;\n  _lte?: InputMaybe<Scalars['float8']['input']>;\n  _neq?: InputMaybe<Scalars['float8']['input']>;\n  _nin?: InputMaybe<Array<Scalars['float8']['input']>>;\n};\n\n/** columns and relationships of \"follow_stats\" */\nexport type Follow_Stats = {\n  date?: Maybe<Scalars['date']['output']>;\n  follow_total?: Maybe<Scalars['numeric']['output']>;\n  follows_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregated selection of \"follow_stats\" */\nexport type Follow_Stats_Aggregate = {\n  aggregate?: Maybe<Follow_Stats_Aggregate_Fields>;\n  nodes: Array<Follow_Stats>;\n};\n\n/** aggregate fields of \"follow_stats\" */\nexport type Follow_Stats_Aggregate_Fields = {\n  avg?: Maybe<Follow_Stats_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Follow_Stats_Max_Fields>;\n  min?: Maybe<Follow_Stats_Min_Fields>;\n  stddev?: Maybe<Follow_Stats_Stddev_Fields>;\n  stddev_pop?: Maybe<Follow_Stats_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Follow_Stats_Stddev_Samp_Fields>;\n  sum?: Maybe<Follow_Stats_Sum_Fields>;\n  var_pop?: Maybe<Follow_Stats_Var_Pop_Fields>;\n  var_samp?: Maybe<Follow_Stats_Var_Samp_Fields>;\n  variance?: Maybe<Follow_Stats_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"follow_stats\" */\nexport type Follow_Stats_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Follow_Stats_Avg_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"follow_stats\". All fields are combined with a logical 'AND'. */\nexport type Follow_Stats_Bool_Exp = {\n  _and?: InputMaybe<Array<Follow_Stats_Bool_Exp>>;\n  _not?: InputMaybe<Follow_Stats_Bool_Exp>;\n  _or?: InputMaybe<Array<Follow_Stats_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  follow_total?: InputMaybe<Numeric_Comparison_Exp>;\n  follows_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Follow_Stats_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  follow_total?: Maybe<Scalars['numeric']['output']>;\n  follows_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Follow_Stats_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  follow_total?: Maybe<Scalars['numeric']['output']>;\n  follows_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"follow_stats\". */\nexport type Follow_Stats_Order_By = {\n  date?: InputMaybe<Order_By>;\n  follow_total?: InputMaybe<Order_By>;\n  follows_per_day?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"follow_stats\" */\nexport enum Follow_Stats_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  FollowTotal = 'follow_total',\n  /** column name */\n  FollowsPerDay = 'follows_per_day'\n}\n\n/** aggregate stddev on columns */\nexport type Follow_Stats_Stddev_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Follow_Stats_Stddev_Pop_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Follow_Stats_Stddev_Samp_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"follow_stats\" */\nexport type Follow_Stats_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Follow_Stats_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Follow_Stats_Stream_Cursor_Value_Input = {\n  date?: InputMaybe<Scalars['date']['input']>;\n  follow_total?: InputMaybe<Scalars['numeric']['input']>;\n  follows_per_day?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Follow_Stats_Sum_Fields = {\n  follow_total?: Maybe<Scalars['numeric']['output']>;\n  follows_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Follow_Stats_Var_Pop_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Follow_Stats_Var_Samp_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Follow_Stats_Variance_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"genesis\" */\nexport type Genesis = {\n  chain_id: Scalars['String']['output'];\n  initial_height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n  time: Scalars['timestamp']['output'];\n};\n\n/** columns and relationships of \"genesis_accounts\" */\nexport type Genesis_Accounts = {\n  address: Scalars['String']['output'];\n  balance: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n};\n\n/** aggregated selection of \"genesis_accounts\" */\nexport type Genesis_Accounts_Aggregate = {\n  aggregate?: Maybe<Genesis_Accounts_Aggregate_Fields>;\n  nodes: Array<Genesis_Accounts>;\n};\n\n/** aggregate fields of \"genesis_accounts\" */\nexport type Genesis_Accounts_Aggregate_Fields = {\n  avg?: Maybe<Genesis_Accounts_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Genesis_Accounts_Max_Fields>;\n  min?: Maybe<Genesis_Accounts_Min_Fields>;\n  stddev?: Maybe<Genesis_Accounts_Stddev_Fields>;\n  stddev_pop?: Maybe<Genesis_Accounts_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Genesis_Accounts_Stddev_Samp_Fields>;\n  sum?: Maybe<Genesis_Accounts_Sum_Fields>;\n  var_pop?: Maybe<Genesis_Accounts_Var_Pop_Fields>;\n  var_samp?: Maybe<Genesis_Accounts_Var_Samp_Fields>;\n  variance?: Maybe<Genesis_Accounts_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"genesis_accounts\" */\nexport type Genesis_Accounts_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Genesis_Accounts_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Genesis_Accounts_Avg_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"genesis_accounts\". All fields are combined with a logical 'AND'. */\nexport type Genesis_Accounts_Bool_Exp = {\n  _and?: InputMaybe<Array<Genesis_Accounts_Bool_Exp>>;\n  _not?: InputMaybe<Genesis_Accounts_Bool_Exp>;\n  _or?: InputMaybe<Array<Genesis_Accounts_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  balance?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Genesis_Accounts_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  balance?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Genesis_Accounts_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  balance?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** Ordering options when selecting data from \"genesis_accounts\". */\nexport type Genesis_Accounts_Order_By = {\n  address?: InputMaybe<Order_By>;\n  balance?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"genesis_accounts\" */\nexport enum Genesis_Accounts_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Balance = 'balance',\n  /** column name */\n  Id = 'id'\n}\n\n/** aggregate stddev on columns */\nexport type Genesis_Accounts_Stddev_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Genesis_Accounts_Stddev_Pop_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Genesis_Accounts_Stddev_Samp_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"genesis_accounts\" */\nexport type Genesis_Accounts_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Genesis_Accounts_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Genesis_Accounts_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n  balance?: InputMaybe<Scalars['bigint']['input']>;\n  id?: InputMaybe<Scalars['Int']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Genesis_Accounts_Sum_Fields = {\n  balance?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Genesis_Accounts_Var_Pop_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Genesis_Accounts_Var_Samp_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Genesis_Accounts_Variance_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregated selection of \"genesis\" */\nexport type Genesis_Aggregate = {\n  aggregate?: Maybe<Genesis_Aggregate_Fields>;\n  nodes: Array<Genesis>;\n};\n\n/** aggregate fields of \"genesis\" */\nexport type Genesis_Aggregate_Fields = {\n  avg?: Maybe<Genesis_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Genesis_Max_Fields>;\n  min?: Maybe<Genesis_Min_Fields>;\n  stddev?: Maybe<Genesis_Stddev_Fields>;\n  stddev_pop?: Maybe<Genesis_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Genesis_Stddev_Samp_Fields>;\n  sum?: Maybe<Genesis_Sum_Fields>;\n  var_pop?: Maybe<Genesis_Var_Pop_Fields>;\n  var_samp?: Maybe<Genesis_Var_Samp_Fields>;\n  variance?: Maybe<Genesis_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"genesis\" */\nexport type Genesis_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Genesis_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Genesis_Avg_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"genesis\". All fields are combined with a logical 'AND'. */\nexport type Genesis_Bool_Exp = {\n  _and?: InputMaybe<Array<Genesis_Bool_Exp>>;\n  _not?: InputMaybe<Genesis_Bool_Exp>;\n  _or?: InputMaybe<Array<Genesis_Bool_Exp>>;\n  chain_id?: InputMaybe<String_Comparison_Exp>;\n  initial_height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n  time?: InputMaybe<Timestamp_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Genesis_Max_Fields = {\n  chain_id?: Maybe<Scalars['String']['output']>;\n  initial_height?: Maybe<Scalars['bigint']['output']>;\n  time?: Maybe<Scalars['timestamp']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Genesis_Min_Fields = {\n  chain_id?: Maybe<Scalars['String']['output']>;\n  initial_height?: Maybe<Scalars['bigint']['output']>;\n  time?: Maybe<Scalars['timestamp']['output']>;\n};\n\n/** columns and relationships of \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation = {\n  count?: Maybe<Scalars['float8']['output']>;\n  neurons?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation_Aggregate = {\n  aggregate?: Maybe<Genesis_Neurons_Activation_Aggregate_Fields>;\n  nodes: Array<Genesis_Neurons_Activation>;\n};\n\n/** aggregate fields of \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation_Aggregate_Fields = {\n  avg?: Maybe<Genesis_Neurons_Activation_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Genesis_Neurons_Activation_Max_Fields>;\n  min?: Maybe<Genesis_Neurons_Activation_Min_Fields>;\n  stddev?: Maybe<Genesis_Neurons_Activation_Stddev_Fields>;\n  stddev_pop?: Maybe<Genesis_Neurons_Activation_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Genesis_Neurons_Activation_Stddev_Samp_Fields>;\n  sum?: Maybe<Genesis_Neurons_Activation_Sum_Fields>;\n  var_pop?: Maybe<Genesis_Neurons_Activation_Var_Pop_Fields>;\n  var_samp?: Maybe<Genesis_Neurons_Activation_Var_Samp_Fields>;\n  variance?: Maybe<Genesis_Neurons_Activation_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Genesis_Neurons_Activation_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"genesis_neurons_activation\". All fields are combined with a logical 'AND'. */\nexport type Genesis_Neurons_Activation_Bool_Exp = {\n  _and?: InputMaybe<Array<Genesis_Neurons_Activation_Bool_Exp>>;\n  _not?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n  _or?: InputMaybe<Array<Genesis_Neurons_Activation_Bool_Exp>>;\n  count?: InputMaybe<Float8_Comparison_Exp>;\n  neurons?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Genesis_Neurons_Activation_Max_Fields = {\n  count?: Maybe<Scalars['float8']['output']>;\n  neurons?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Genesis_Neurons_Activation_Min_Fields = {\n  count?: Maybe<Scalars['float8']['output']>;\n  neurons?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"genesis_neurons_activation\". */\nexport type Genesis_Neurons_Activation_Order_By = {\n  count?: InputMaybe<Order_By>;\n  neurons?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"genesis_neurons_activation\" */\nexport enum Genesis_Neurons_Activation_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Neurons = 'neurons'\n}\n\n/** aggregate stddev on columns */\nexport type Genesis_Neurons_Activation_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Genesis_Neurons_Activation_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Genesis_Neurons_Activation_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Genesis_Neurons_Activation_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Genesis_Neurons_Activation_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['float8']['input']>;\n  neurons?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Genesis_Neurons_Activation_Sum_Fields = {\n  count?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Genesis_Neurons_Activation_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Genesis_Neurons_Activation_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Genesis_Neurons_Activation_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Ordering options when selecting data from \"genesis\". */\nexport type Genesis_Order_By = {\n  chain_id?: InputMaybe<Order_By>;\n  initial_height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n  time?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"genesis\" */\nexport enum Genesis_Select_Column {\n  /** column name */\n  ChainId = 'chain_id',\n  /** column name */\n  InitialHeight = 'initial_height',\n  /** column name */\n  OneRowId = 'one_row_id',\n  /** column name */\n  Time = 'time'\n}\n\n/** aggregate stddev on columns */\nexport type Genesis_Stddev_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Genesis_Stddev_Pop_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Genesis_Stddev_Samp_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"genesis\" */\nexport type Genesis_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Genesis_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Genesis_Stream_Cursor_Value_Input = {\n  chain_id?: InputMaybe<Scalars['String']['input']>;\n  initial_height?: InputMaybe<Scalars['bigint']['input']>;\n  one_row_id?: InputMaybe<Scalars['Boolean']['input']>;\n  time?: InputMaybe<Scalars['timestamp']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Genesis_Sum_Fields = {\n  initial_height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Genesis_Var_Pop_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Genesis_Var_Samp_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Genesis_Variance_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"investmints\" */\nexport type Investmints = {\n  /** An object relationship */\n  account: Account;\n  amount: Scalars['coin_scalar']['output'];\n  /** An object relationship */\n  block: Block;\n  height: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n  length: Scalars['bigint']['output'];\n  neuron: Scalars['String']['output'];\n  resource: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  transaction_hash: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"investmints\" */\nexport type Investmints_Aggregate = {\n  aggregate?: Maybe<Investmints_Aggregate_Fields>;\n  nodes: Array<Investmints>;\n};\n\nexport type Investmints_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Investmints_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Investmints_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Investmints_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Investmints_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"investmints\" */\nexport type Investmints_Aggregate_Fields = {\n  avg?: Maybe<Investmints_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Investmints_Max_Fields>;\n  min?: Maybe<Investmints_Min_Fields>;\n  stddev?: Maybe<Investmints_Stddev_Fields>;\n  stddev_pop?: Maybe<Investmints_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Investmints_Stddev_Samp_Fields>;\n  sum?: Maybe<Investmints_Sum_Fields>;\n  var_pop?: Maybe<Investmints_Var_Pop_Fields>;\n  var_samp?: Maybe<Investmints_Var_Samp_Fields>;\n  variance?: Maybe<Investmints_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"investmints\" */\nexport type Investmints_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Investmints_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"investmints\" */\nexport type Investmints_Aggregate_Order_By = {\n  avg?: InputMaybe<Investmints_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Investmints_Max_Order_By>;\n  min?: InputMaybe<Investmints_Min_Order_By>;\n  stddev?: InputMaybe<Investmints_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Investmints_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Investmints_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Investmints_Sum_Order_By>;\n  var_pop?: InputMaybe<Investmints_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Investmints_Var_Samp_Order_By>;\n  variance?: InputMaybe<Investmints_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Investmints_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"investmints\" */\nexport type Investmints_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"investmints\". All fields are combined with a logical 'AND'. */\nexport type Investmints_Bool_Exp = {\n  _and?: InputMaybe<Array<Investmints_Bool_Exp>>;\n  _not?: InputMaybe<Investmints_Bool_Exp>;\n  _or?: InputMaybe<Array<Investmints_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  amount?: InputMaybe<Coin_Scalar_Comparison_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  length?: InputMaybe<Bigint_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n  resource?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Investmints_Max_Fields = {\n  amount?: Maybe<Scalars['coin_scalar']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  length?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  resource?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"investmints\" */\nexport type Investmints_Max_Order_By = {\n  amount?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  resource?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Investmints_Min_Fields = {\n  amount?: Maybe<Scalars['coin_scalar']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  length?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  resource?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"investmints\" */\nexport type Investmints_Min_Order_By = {\n  amount?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  resource?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"investmints\". */\nexport type Investmints_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  amount?: InputMaybe<Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  resource?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"investmints\" */\nexport enum Investmints_Select_Column {\n  /** column name */\n  Amount = 'amount',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  Length = 'length',\n  /** column name */\n  Neuron = 'neuron',\n  /** column name */\n  Resource = 'resource',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TransactionHash = 'transaction_hash'\n}\n\n/** aggregate stddev on columns */\nexport type Investmints_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"investmints\" */\nexport type Investmints_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Investmints_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"investmints\" */\nexport type Investmints_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Investmints_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"investmints\" */\nexport type Investmints_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"investmints\" */\nexport type Investmints_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Investmints_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Investmints_Stream_Cursor_Value_Input = {\n  amount?: InputMaybe<Scalars['coin_scalar']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  id?: InputMaybe<Scalars['Int']['input']>;\n  length?: InputMaybe<Scalars['bigint']['input']>;\n  neuron?: InputMaybe<Scalars['String']['input']>;\n  resource?: InputMaybe<Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Investmints_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  length?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"investmints\" */\nexport type Investmints_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Investmints_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"investmints\" */\nexport type Investmints_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Investmints_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"investmints\" */\nexport type Investmints_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Investmints_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"investmints\" */\nexport type Investmints_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to compare columns of type \"json\". All fields are combined with logical 'AND'. */\nexport type Json_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['json']['input']>;\n  _gt?: InputMaybe<Scalars['json']['input']>;\n  _gte?: InputMaybe<Scalars['json']['input']>;\n  _in?: InputMaybe<Array<Scalars['json']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['json']['input']>;\n  _lte?: InputMaybe<Scalars['json']['input']>;\n  _neq?: InputMaybe<Scalars['json']['input']>;\n  _nin?: InputMaybe<Array<Scalars['json']['input']>>;\n};\n\n/** Boolean expression to compare columns of type \"jsonb\". All fields are combined with logical 'AND'. */\nexport type Jsonb_Array_Comparison_Exp = {\n  /** is the array contained in the given array value */\n  _contained_in?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  /** does the array contain the given value */\n  _contains?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _eq?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _gt?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _gte?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _in?: InputMaybe<Array<Array<Scalars['jsonb']['input']>>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _lte?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _neq?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _nin?: InputMaybe<Array<Array<Scalars['jsonb']['input']>>>;\n};\n\nexport type Jsonb_Cast_Exp = {\n  String?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** Boolean expression to compare columns of type \"jsonb\". All fields are combined with logical 'AND'. */\nexport type Jsonb_Comparison_Exp = {\n  _cast?: InputMaybe<Jsonb_Cast_Exp>;\n  /** is the column contained in the given json value */\n  _contained_in?: InputMaybe<Scalars['jsonb']['input']>;\n  /** does the column contain the given json value at the top level */\n  _contains?: InputMaybe<Scalars['jsonb']['input']>;\n  _eq?: InputMaybe<Scalars['jsonb']['input']>;\n  _gt?: InputMaybe<Scalars['jsonb']['input']>;\n  _gte?: InputMaybe<Scalars['jsonb']['input']>;\n  /** does the string exist as a top-level key in the column */\n  _has_key?: InputMaybe<Scalars['String']['input']>;\n  /** do all of these strings exist as top-level keys in the column */\n  _has_keys_all?: InputMaybe<Array<Scalars['String']['input']>>;\n  /** do any of these strings exist as top-level keys in the column */\n  _has_keys_any?: InputMaybe<Array<Scalars['String']['input']>>;\n  _in?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['jsonb']['input']>;\n  _lte?: InputMaybe<Scalars['jsonb']['input']>;\n  _neq?: InputMaybe<Scalars['jsonb']['input']>;\n  _nin?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n};\n\n/** columns and relationships of \"message\" */\nexport type Message = {\n  height: Scalars['bigint']['output'];\n  index: Scalars['bigint']['output'];\n  involved_accounts_addresses: Array<Scalars['String']['output']>;\n  /** An object relationship */\n  message_type: Message_Type;\n  partition_id: Scalars['bigint']['output'];\n  /** An object relationship */\n  transaction?: Maybe<Transaction>;\n  /** An object relationship */\n  transaction_155?: Maybe<Transaction_155>;\n  transaction_hash: Scalars['String']['output'];\n  type: Scalars['String']['output'];\n  value: Scalars['json']['output'];\n};\n\n\n/** columns and relationships of \"message\" */\nexport type MessageValueArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** columns and relationships of \"message_155\" */\nexport type Message_155 = {\n  height: Scalars['bigint']['output'];\n  index: Scalars['bigint']['output'];\n  involved_accounts_addresses: Array<Scalars['String']['output']>;\n  /** An object relationship */\n  message_type: Message_Type;\n  partition_id: Scalars['bigint']['output'];\n  /** An object relationship */\n  transaction?: Maybe<Transaction>;\n  transaction_hash: Scalars['String']['output'];\n  type: Scalars['String']['output'];\n  value: Scalars['json']['output'];\n};\n\n\n/** columns and relationships of \"message_155\" */\nexport type Message_155ValueArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregated selection of \"message_155\" */\nexport type Message_155_Aggregate = {\n  aggregate?: Maybe<Message_155_Aggregate_Fields>;\n  nodes: Array<Message_155>;\n};\n\nexport type Message_155_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Message_155_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Message_155_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Message_155_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Message_155_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"message_155\" */\nexport type Message_155_Aggregate_Fields = {\n  avg?: Maybe<Message_155_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Message_155_Max_Fields>;\n  min?: Maybe<Message_155_Min_Fields>;\n  stddev?: Maybe<Message_155_Stddev_Fields>;\n  stddev_pop?: Maybe<Message_155_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Message_155_Stddev_Samp_Fields>;\n  sum?: Maybe<Message_155_Sum_Fields>;\n  var_pop?: Maybe<Message_155_Var_Pop_Fields>;\n  var_samp?: Maybe<Message_155_Var_Samp_Fields>;\n  variance?: Maybe<Message_155_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"message_155\" */\nexport type Message_155_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Message_155_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"message_155\" */\nexport type Message_155_Aggregate_Order_By = {\n  avg?: InputMaybe<Message_155_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Message_155_Max_Order_By>;\n  min?: InputMaybe<Message_155_Min_Order_By>;\n  stddev?: InputMaybe<Message_155_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Message_155_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Message_155_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Message_155_Sum_Order_By>;\n  var_pop?: InputMaybe<Message_155_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Message_155_Var_Samp_Order_By>;\n  variance?: InputMaybe<Message_155_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Message_155_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"message_155\" */\nexport type Message_155_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"message_155\". All fields are combined with a logical 'AND'. */\nexport type Message_155_Bool_Exp = {\n  _and?: InputMaybe<Array<Message_155_Bool_Exp>>;\n  _not?: InputMaybe<Message_155_Bool_Exp>;\n  _or?: InputMaybe<Array<Message_155_Bool_Exp>>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  index?: InputMaybe<Bigint_Comparison_Exp>;\n  involved_accounts_addresses?: InputMaybe<String_Array_Comparison_Exp>;\n  message_type?: InputMaybe<Message_Type_Bool_Exp>;\n  partition_id?: InputMaybe<Bigint_Comparison_Exp>;\n  transaction?: InputMaybe<Transaction_Bool_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  value?: InputMaybe<Json_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Message_155_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"message_155\" */\nexport type Message_155_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Message_155_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"message_155\" */\nexport type Message_155_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"message_155\". */\nexport type Message_155_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  message_type?: InputMaybe<Message_Type_Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  transaction?: InputMaybe<Transaction_Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"message_155\" */\nexport enum Message_155_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Index = 'index',\n  /** column name */\n  InvolvedAccountsAddresses = 'involved_accounts_addresses',\n  /** column name */\n  PartitionId = 'partition_id',\n  /** column name */\n  TransactionHash = 'transaction_hash',\n  /** column name */\n  Type = 'type',\n  /** column name */\n  Value = 'value'\n}\n\n/** aggregate stddev on columns */\nexport type Message_155_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"message_155\" */\nexport type Message_155_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Message_155_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"message_155\" */\nexport type Message_155_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Message_155_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"message_155\" */\nexport type Message_155_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"message_155\" */\nexport type Message_155_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Message_155_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Message_155_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  index?: InputMaybe<Scalars['bigint']['input']>;\n  involved_accounts_addresses?: InputMaybe<Array<Scalars['String']['input']>>;\n  partition_id?: InputMaybe<Scalars['bigint']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n  value?: InputMaybe<Scalars['json']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Message_155_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"message_155\" */\nexport type Message_155_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Message_155_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"message_155\" */\nexport type Message_155_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Message_155_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"message_155\" */\nexport type Message_155_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Message_155_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"message_155\" */\nexport type Message_155_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregated selection of \"message\" */\nexport type Message_Aggregate = {\n  aggregate?: Maybe<Message_Aggregate_Fields>;\n  nodes: Array<Message>;\n};\n\nexport type Message_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Message_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Message_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Message_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Message_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"message\" */\nexport type Message_Aggregate_Fields = {\n  avg?: Maybe<Message_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Message_Max_Fields>;\n  min?: Maybe<Message_Min_Fields>;\n  stddev?: Maybe<Message_Stddev_Fields>;\n  stddev_pop?: Maybe<Message_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Message_Stddev_Samp_Fields>;\n  sum?: Maybe<Message_Sum_Fields>;\n  var_pop?: Maybe<Message_Var_Pop_Fields>;\n  var_samp?: Maybe<Message_Var_Samp_Fields>;\n  variance?: Maybe<Message_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"message\" */\nexport type Message_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Message_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"message\" */\nexport type Message_Aggregate_Order_By = {\n  avg?: InputMaybe<Message_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Message_Max_Order_By>;\n  min?: InputMaybe<Message_Min_Order_By>;\n  stddev?: InputMaybe<Message_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Message_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Message_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Message_Sum_Order_By>;\n  var_pop?: InputMaybe<Message_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Message_Var_Samp_Order_By>;\n  variance?: InputMaybe<Message_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Message_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"message\" */\nexport type Message_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"message\". All fields are combined with a logical 'AND'. */\nexport type Message_Bool_Exp = {\n  _and?: InputMaybe<Array<Message_Bool_Exp>>;\n  _not?: InputMaybe<Message_Bool_Exp>;\n  _or?: InputMaybe<Array<Message_Bool_Exp>>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  index?: InputMaybe<Bigint_Comparison_Exp>;\n  involved_accounts_addresses?: InputMaybe<String_Array_Comparison_Exp>;\n  message_type?: InputMaybe<Message_Type_Bool_Exp>;\n  partition_id?: InputMaybe<Bigint_Comparison_Exp>;\n  transaction?: InputMaybe<Transaction_Bool_Exp>;\n  transaction_155?: InputMaybe<Transaction_155_Bool_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  value?: InputMaybe<Json_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Message_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"message\" */\nexport type Message_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Message_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"message\" */\nexport type Message_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"message\". */\nexport type Message_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  message_type?: InputMaybe<Message_Type_Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  transaction?: InputMaybe<Transaction_Order_By>;\n  transaction_155?: InputMaybe<Transaction_155_Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"message\" */\nexport enum Message_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Index = 'index',\n  /** column name */\n  InvolvedAccountsAddresses = 'involved_accounts_addresses',\n  /** column name */\n  PartitionId = 'partition_id',\n  /** column name */\n  TransactionHash = 'transaction_hash',\n  /** column name */\n  Type = 'type',\n  /** column name */\n  Value = 'value'\n}\n\n/** aggregate stddev on columns */\nexport type Message_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"message\" */\nexport type Message_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Message_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"message\" */\nexport type Message_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Message_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"message\" */\nexport type Message_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"message\" */\nexport type Message_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Message_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Message_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  index?: InputMaybe<Scalars['bigint']['input']>;\n  involved_accounts_addresses?: InputMaybe<Array<Scalars['String']['input']>>;\n  partition_id?: InputMaybe<Scalars['bigint']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n  value?: InputMaybe<Scalars['json']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Message_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"message\" */\nexport type Message_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"message_type\" */\nexport type Message_Type = {\n  height: Scalars['bigint']['output'];\n  label: Scalars['String']['output'];\n  /** An array relationship */\n  message_155s: Array<Message_155>;\n  /** An aggregate relationship */\n  message_155s_aggregate: Message_155_Aggregate;\n  /** An array relationship */\n  messages: Array<Message>;\n  /** An aggregate relationship */\n  messages_aggregate: Message_Aggregate;\n  module: Scalars['String']['output'];\n  type: Scalars['String']['output'];\n};\n\n\n/** columns and relationships of \"message_type\" */\nexport type Message_TypeMessage_155sArgs = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"message_type\" */\nexport type Message_TypeMessage_155s_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"message_type\" */\nexport type Message_TypeMessagesArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"message_type\" */\nexport type Message_TypeMessages_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n/** aggregated selection of \"message_type\" */\nexport type Message_Type_Aggregate = {\n  aggregate?: Maybe<Message_Type_Aggregate_Fields>;\n  nodes: Array<Message_Type>;\n};\n\n/** aggregate fields of \"message_type\" */\nexport type Message_Type_Aggregate_Fields = {\n  avg?: Maybe<Message_Type_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Message_Type_Max_Fields>;\n  min?: Maybe<Message_Type_Min_Fields>;\n  stddev?: Maybe<Message_Type_Stddev_Fields>;\n  stddev_pop?: Maybe<Message_Type_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Message_Type_Stddev_Samp_Fields>;\n  sum?: Maybe<Message_Type_Sum_Fields>;\n  var_pop?: Maybe<Message_Type_Var_Pop_Fields>;\n  var_samp?: Maybe<Message_Type_Var_Samp_Fields>;\n  variance?: Maybe<Message_Type_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"message_type\" */\nexport type Message_Type_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Message_Type_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Message_Type_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"message_type\". All fields are combined with a logical 'AND'. */\nexport type Message_Type_Bool_Exp = {\n  _and?: InputMaybe<Array<Message_Type_Bool_Exp>>;\n  _not?: InputMaybe<Message_Type_Bool_Exp>;\n  _or?: InputMaybe<Array<Message_Type_Bool_Exp>>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  label?: InputMaybe<String_Comparison_Exp>;\n  message_155s?: InputMaybe<Message_155_Bool_Exp>;\n  message_155s_aggregate?: InputMaybe<Message_155_Aggregate_Bool_Exp>;\n  messages?: InputMaybe<Message_Bool_Exp>;\n  messages_aggregate?: InputMaybe<Message_Aggregate_Bool_Exp>;\n  module?: InputMaybe<String_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Message_Type_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  label?: Maybe<Scalars['String']['output']>;\n  module?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Message_Type_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  label?: Maybe<Scalars['String']['output']>;\n  module?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"message_type\". */\nexport type Message_Type_Order_By = {\n  height?: InputMaybe<Order_By>;\n  label?: InputMaybe<Order_By>;\n  message_155s_aggregate?: InputMaybe<Message_155_Aggregate_Order_By>;\n  messages_aggregate?: InputMaybe<Message_Aggregate_Order_By>;\n  module?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"message_type\" */\nexport enum Message_Type_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Label = 'label',\n  /** column name */\n  Module = 'module',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Message_Type_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Message_Type_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Message_Type_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"message_type\" */\nexport type Message_Type_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Message_Type_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Message_Type_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  label?: InputMaybe<Scalars['String']['input']>;\n  module?: InputMaybe<Scalars['String']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Message_Type_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Message_Type_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Message_Type_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Message_Type_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Message_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"message\" */\nexport type Message_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Message_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"message\" */\nexport type Message_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Message_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"message\" */\nexport type Message_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\nexport type Messages_By_Address_Args = {\n  addresses?: InputMaybe<Scalars['_text']['input']>;\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n};\n\nexport type Messages_By_Type_Args = {\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n};\n\n/** columns and relationships of \"modules\" */\nexport type Modules = {\n  module_name: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"modules\" */\nexport type Modules_Aggregate = {\n  aggregate?: Maybe<Modules_Aggregate_Fields>;\n  nodes: Array<Modules>;\n};\n\n/** aggregate fields of \"modules\" */\nexport type Modules_Aggregate_Fields = {\n  count: Scalars['Int']['output'];\n  max?: Maybe<Modules_Max_Fields>;\n  min?: Maybe<Modules_Min_Fields>;\n};\n\n\n/** aggregate fields of \"modules\" */\nexport type Modules_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Modules_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** Boolean expression to filter rows from the table \"modules\". All fields are combined with a logical 'AND'. */\nexport type Modules_Bool_Exp = {\n  _and?: InputMaybe<Array<Modules_Bool_Exp>>;\n  _not?: InputMaybe<Modules_Bool_Exp>;\n  _or?: InputMaybe<Array<Modules_Bool_Exp>>;\n  module_name?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Modules_Max_Fields = {\n  module_name?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Modules_Min_Fields = {\n  module_name?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"modules\". */\nexport type Modules_Order_By = {\n  module_name?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"modules\" */\nexport enum Modules_Select_Column {\n  /** column name */\n  ModuleName = 'module_name'\n}\n\n/** Streaming cursor of the table \"modules\" */\nexport type Modules_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Modules_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Modules_Stream_Cursor_Value_Input = {\n  module_name?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** columns and relationships of \"neuron_activation_source\" */\nexport type Neuron_Activation_Source = {\n  genesis_percent?: Maybe<Scalars['float8']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activated?: Maybe<Scalars['bigint']['output']>;\n  recieve_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"neuron_activation_source\" */\nexport type Neuron_Activation_Source_Aggregate = {\n  aggregate?: Maybe<Neuron_Activation_Source_Aggregate_Fields>;\n  nodes: Array<Neuron_Activation_Source>;\n};\n\n/** aggregate fields of \"neuron_activation_source\" */\nexport type Neuron_Activation_Source_Aggregate_Fields = {\n  avg?: Maybe<Neuron_Activation_Source_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Neuron_Activation_Source_Max_Fields>;\n  min?: Maybe<Neuron_Activation_Source_Min_Fields>;\n  stddev?: Maybe<Neuron_Activation_Source_Stddev_Fields>;\n  stddev_pop?: Maybe<Neuron_Activation_Source_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Neuron_Activation_Source_Stddev_Samp_Fields>;\n  sum?: Maybe<Neuron_Activation_Source_Sum_Fields>;\n  var_pop?: Maybe<Neuron_Activation_Source_Var_Pop_Fields>;\n  var_samp?: Maybe<Neuron_Activation_Source_Var_Samp_Fields>;\n  variance?: Maybe<Neuron_Activation_Source_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"neuron_activation_source\" */\nexport type Neuron_Activation_Source_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Neuron_Activation_Source_Avg_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"neuron_activation_source\". All fields are combined with a logical 'AND'. */\nexport type Neuron_Activation_Source_Bool_Exp = {\n  _and?: InputMaybe<Array<Neuron_Activation_Source_Bool_Exp>>;\n  _not?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n  _or?: InputMaybe<Array<Neuron_Activation_Source_Bool_Exp>>;\n  genesis_percent?: InputMaybe<Float8_Comparison_Exp>;\n  ibc_receive_percent?: InputMaybe<Float8_Comparison_Exp>;\n  neuron_activated?: InputMaybe<Bigint_Comparison_Exp>;\n  recieve_percent?: InputMaybe<Float8_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Neuron_Activation_Source_Max_Fields = {\n  genesis_percent?: Maybe<Scalars['float8']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activated?: Maybe<Scalars['bigint']['output']>;\n  recieve_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Neuron_Activation_Source_Min_Fields = {\n  genesis_percent?: Maybe<Scalars['float8']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activated?: Maybe<Scalars['bigint']['output']>;\n  recieve_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"neuron_activation_source\". */\nexport type Neuron_Activation_Source_Order_By = {\n  genesis_percent?: InputMaybe<Order_By>;\n  ibc_receive_percent?: InputMaybe<Order_By>;\n  neuron_activated?: InputMaybe<Order_By>;\n  recieve_percent?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"neuron_activation_source\" */\nexport enum Neuron_Activation_Source_Select_Column {\n  /** column name */\n  GenesisPercent = 'genesis_percent',\n  /** column name */\n  IbcReceivePercent = 'ibc_receive_percent',\n  /** column name */\n  NeuronActivated = 'neuron_activated',\n  /** column name */\n  RecievePercent = 'recieve_percent',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Neuron_Activation_Source_Stddev_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Neuron_Activation_Source_Stddev_Pop_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Neuron_Activation_Source_Stddev_Samp_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"neuron_activation_source\" */\nexport type Neuron_Activation_Source_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Neuron_Activation_Source_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Neuron_Activation_Source_Stream_Cursor_Value_Input = {\n  genesis_percent?: InputMaybe<Scalars['float8']['input']>;\n  ibc_receive_percent?: InputMaybe<Scalars['float8']['input']>;\n  neuron_activated?: InputMaybe<Scalars['bigint']['input']>;\n  recieve_percent?: InputMaybe<Scalars['float8']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Neuron_Activation_Source_Sum_Fields = {\n  genesis_percent?: Maybe<Scalars['float8']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activated?: Maybe<Scalars['bigint']['output']>;\n  recieve_percent?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Neuron_Activation_Source_Var_Pop_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Neuron_Activation_Source_Var_Samp_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Neuron_Activation_Source_Variance_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons = {\n  date?: Maybe<Scalars['date']['output']>;\n  new_neurons_daily?: Maybe<Scalars['bigint']['output']>;\n  new_neurons_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons_Aggregate = {\n  aggregate?: Maybe<Number_Of_New_Neurons_Aggregate_Fields>;\n  nodes: Array<Number_Of_New_Neurons>;\n};\n\n/** aggregate fields of \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons_Aggregate_Fields = {\n  avg?: Maybe<Number_Of_New_Neurons_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Number_Of_New_Neurons_Max_Fields>;\n  min?: Maybe<Number_Of_New_Neurons_Min_Fields>;\n  stddev?: Maybe<Number_Of_New_Neurons_Stddev_Fields>;\n  stddev_pop?: Maybe<Number_Of_New_Neurons_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Number_Of_New_Neurons_Stddev_Samp_Fields>;\n  sum?: Maybe<Number_Of_New_Neurons_Sum_Fields>;\n  var_pop?: Maybe<Number_Of_New_Neurons_Var_Pop_Fields>;\n  var_samp?: Maybe<Number_Of_New_Neurons_Var_Samp_Fields>;\n  variance?: Maybe<Number_Of_New_Neurons_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Number_Of_New_Neurons_Avg_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"number_of_new_neurons\". All fields are combined with a logical 'AND'. */\nexport type Number_Of_New_Neurons_Bool_Exp = {\n  _and?: InputMaybe<Array<Number_Of_New_Neurons_Bool_Exp>>;\n  _not?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n  _or?: InputMaybe<Array<Number_Of_New_Neurons_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  new_neurons_daily?: InputMaybe<Bigint_Comparison_Exp>;\n  new_neurons_total?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Number_Of_New_Neurons_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  new_neurons_daily?: Maybe<Scalars['bigint']['output']>;\n  new_neurons_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Number_Of_New_Neurons_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  new_neurons_daily?: Maybe<Scalars['bigint']['output']>;\n  new_neurons_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"number_of_new_neurons\". */\nexport type Number_Of_New_Neurons_Order_By = {\n  date?: InputMaybe<Order_By>;\n  new_neurons_daily?: InputMaybe<Order_By>;\n  new_neurons_total?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"number_of_new_neurons\" */\nexport enum Number_Of_New_Neurons_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  NewNeuronsDaily = 'new_neurons_daily',\n  /** column name */\n  NewNeuronsTotal = 'new_neurons_total'\n}\n\n/** aggregate stddev on columns */\nexport type Number_Of_New_Neurons_Stddev_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Number_Of_New_Neurons_Stddev_Pop_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Number_Of_New_Neurons_Stddev_Samp_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Number_Of_New_Neurons_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Number_Of_New_Neurons_Stream_Cursor_Value_Input = {\n  date?: InputMaybe<Scalars['date']['input']>;\n  new_neurons_daily?: InputMaybe<Scalars['bigint']['input']>;\n  new_neurons_total?: InputMaybe<Scalars['numeric']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Number_Of_New_Neurons_Sum_Fields = {\n  new_neurons_daily?: Maybe<Scalars['bigint']['output']>;\n  new_neurons_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Number_Of_New_Neurons_Var_Pop_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Number_Of_New_Neurons_Var_Samp_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Number_Of_New_Neurons_Variance_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to compare columns of type \"numeric\". All fields are combined with logical 'AND'. */\nexport type Numeric_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['numeric']['input']>;\n  _gt?: InputMaybe<Scalars['numeric']['input']>;\n  _gte?: InputMaybe<Scalars['numeric']['input']>;\n  _in?: InputMaybe<Array<Scalars['numeric']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['numeric']['input']>;\n  _lte?: InputMaybe<Scalars['numeric']['input']>;\n  _neq?: InputMaybe<Scalars['numeric']['input']>;\n  _nin?: InputMaybe<Array<Scalars['numeric']['input']>>;\n};\n\n/** column ordering options */\nexport enum Order_By {\n  /** in ascending order, nulls last */\n  Asc = 'asc',\n  /** in ascending order, nulls first */\n  AscNullsFirst = 'asc_nulls_first',\n  /** in ascending order, nulls last */\n  AscNullsLast = 'asc_nulls_last',\n  /** in descending order, nulls first */\n  Desc = 'desc',\n  /** in descending order, nulls first */\n  DescNullsFirst = 'desc_nulls_first',\n  /** in descending order, nulls last */\n  DescNullsLast = 'desc_nulls_last'\n}\n\n/** columns and relationships of \"particles\" */\nexport type Particles = {\n  /** An object relationship */\n  account: Account;\n  /** An object relationship */\n  block: Block;\n  height: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n  neuron: Scalars['String']['output'];\n  particle: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  transaction_hash: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"particles\" */\nexport type Particles_Aggregate = {\n  aggregate?: Maybe<Particles_Aggregate_Fields>;\n  nodes: Array<Particles>;\n};\n\nexport type Particles_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Particles_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Particles_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Particles_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Particles_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"particles\" */\nexport type Particles_Aggregate_Fields = {\n  avg?: Maybe<Particles_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Particles_Max_Fields>;\n  min?: Maybe<Particles_Min_Fields>;\n  stddev?: Maybe<Particles_Stddev_Fields>;\n  stddev_pop?: Maybe<Particles_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Particles_Stddev_Samp_Fields>;\n  sum?: Maybe<Particles_Sum_Fields>;\n  var_pop?: Maybe<Particles_Var_Pop_Fields>;\n  var_samp?: Maybe<Particles_Var_Samp_Fields>;\n  variance?: Maybe<Particles_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"particles\" */\nexport type Particles_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Particles_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"particles\" */\nexport type Particles_Aggregate_Order_By = {\n  avg?: InputMaybe<Particles_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Particles_Max_Order_By>;\n  min?: InputMaybe<Particles_Min_Order_By>;\n  stddev?: InputMaybe<Particles_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Particles_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Particles_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Particles_Sum_Order_By>;\n  var_pop?: InputMaybe<Particles_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Particles_Var_Samp_Order_By>;\n  variance?: InputMaybe<Particles_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Particles_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"particles\" */\nexport type Particles_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"particles\". All fields are combined with a logical 'AND'. */\nexport type Particles_Bool_Exp = {\n  _and?: InputMaybe<Array<Particles_Bool_Exp>>;\n  _not?: InputMaybe<Particles_Bool_Exp>;\n  _or?: InputMaybe<Array<Particles_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n  particle?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Particles_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  particle?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"particles\" */\nexport type Particles_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Particles_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  particle?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"particles\" */\nexport type Particles_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"particles\". */\nexport type Particles_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"particles\" */\nexport enum Particles_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  Neuron = 'neuron',\n  /** column name */\n  Particle = 'particle',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TransactionHash = 'transaction_hash'\n}\n\n/** aggregate stddev on columns */\nexport type Particles_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"particles\" */\nexport type Particles_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Particles_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"particles\" */\nexport type Particles_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Particles_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"particles\" */\nexport type Particles_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"particles\" */\nexport type Particles_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Particles_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Particles_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  id?: InputMaybe<Scalars['Int']['input']>;\n  neuron?: InputMaybe<Scalars['String']['input']>;\n  particle?: InputMaybe<Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Particles_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** order by sum() on columns of table \"particles\" */\nexport type Particles_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Particles_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"particles\" */\nexport type Particles_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Particles_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"particles\" */\nexport type Particles_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Particles_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"particles\" */\nexport type Particles_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"pools\" */\nexport type Pools = {\n  a_denom: Scalars['String']['output'];\n  address: Scalars['String']['output'];\n  b_denom: Scalars['String']['output'];\n  pool_denom: Scalars['String']['output'];\n  pool_id: Scalars['bigint']['output'];\n  pool_name: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"pools\" */\nexport type Pools_Aggregate = {\n  aggregate?: Maybe<Pools_Aggregate_Fields>;\n  nodes: Array<Pools>;\n};\n\n/** aggregate fields of \"pools\" */\nexport type Pools_Aggregate_Fields = {\n  avg?: Maybe<Pools_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pools_Max_Fields>;\n  min?: Maybe<Pools_Min_Fields>;\n  stddev?: Maybe<Pools_Stddev_Fields>;\n  stddev_pop?: Maybe<Pools_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pools_Stddev_Samp_Fields>;\n  sum?: Maybe<Pools_Sum_Fields>;\n  var_pop?: Maybe<Pools_Var_Pop_Fields>;\n  var_samp?: Maybe<Pools_Var_Samp_Fields>;\n  variance?: Maybe<Pools_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pools\" */\nexport type Pools_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pools_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pools_Avg_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pools\". All fields are combined with a logical 'AND'. */\nexport type Pools_Bool_Exp = {\n  _and?: InputMaybe<Array<Pools_Bool_Exp>>;\n  _not?: InputMaybe<Pools_Bool_Exp>;\n  _or?: InputMaybe<Array<Pools_Bool_Exp>>;\n  a_denom?: InputMaybe<String_Comparison_Exp>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  b_denom?: InputMaybe<String_Comparison_Exp>;\n  pool_denom?: InputMaybe<String_Comparison_Exp>;\n  pool_id?: InputMaybe<Bigint_Comparison_Exp>;\n  pool_name?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** columns and relationships of \"pools_liquidity\" */\nexport type Pools_Liquidity = {\n  liquidity_a: Scalars['bigint']['output'];\n  liquidity_b: Scalars['bigint']['output'];\n  pool_id: Scalars['bigint']['output'];\n  timestamp: Scalars['timestamp']['output'];\n};\n\n/** aggregated selection of \"pools_liquidity\" */\nexport type Pools_Liquidity_Aggregate = {\n  aggregate?: Maybe<Pools_Liquidity_Aggregate_Fields>;\n  nodes: Array<Pools_Liquidity>;\n};\n\n/** aggregate fields of \"pools_liquidity\" */\nexport type Pools_Liquidity_Aggregate_Fields = {\n  avg?: Maybe<Pools_Liquidity_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pools_Liquidity_Max_Fields>;\n  min?: Maybe<Pools_Liquidity_Min_Fields>;\n  stddev?: Maybe<Pools_Liquidity_Stddev_Fields>;\n  stddev_pop?: Maybe<Pools_Liquidity_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pools_Liquidity_Stddev_Samp_Fields>;\n  sum?: Maybe<Pools_Liquidity_Sum_Fields>;\n  var_pop?: Maybe<Pools_Liquidity_Var_Pop_Fields>;\n  var_samp?: Maybe<Pools_Liquidity_Var_Samp_Fields>;\n  variance?: Maybe<Pools_Liquidity_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pools_liquidity\" */\nexport type Pools_Liquidity_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pools_Liquidity_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pools_Liquidity_Avg_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pools_liquidity\". All fields are combined with a logical 'AND'. */\nexport type Pools_Liquidity_Bool_Exp = {\n  _and?: InputMaybe<Array<Pools_Liquidity_Bool_Exp>>;\n  _not?: InputMaybe<Pools_Liquidity_Bool_Exp>;\n  _or?: InputMaybe<Array<Pools_Liquidity_Bool_Exp>>;\n  liquidity_a?: InputMaybe<Bigint_Comparison_Exp>;\n  liquidity_b?: InputMaybe<Bigint_Comparison_Exp>;\n  pool_id?: InputMaybe<Bigint_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pools_Liquidity_Max_Fields = {\n  liquidity_a?: Maybe<Scalars['bigint']['output']>;\n  liquidity_b?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pools_Liquidity_Min_Fields = {\n  liquidity_a?: Maybe<Scalars['bigint']['output']>;\n  liquidity_b?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n};\n\n/** Ordering options when selecting data from \"pools_liquidity\". */\nexport type Pools_Liquidity_Order_By = {\n  liquidity_a?: InputMaybe<Order_By>;\n  liquidity_b?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pools_liquidity\" */\nexport enum Pools_Liquidity_Select_Column {\n  /** column name */\n  LiquidityA = 'liquidity_a',\n  /** column name */\n  LiquidityB = 'liquidity_b',\n  /** column name */\n  PoolId = 'pool_id',\n  /** column name */\n  Timestamp = 'timestamp'\n}\n\n/** aggregate stddev on columns */\nexport type Pools_Liquidity_Stddev_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pools_Liquidity_Stddev_Pop_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pools_Liquidity_Stddev_Samp_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"pools_liquidity\" */\nexport type Pools_Liquidity_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Pools_Liquidity_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Pools_Liquidity_Stream_Cursor_Value_Input = {\n  liquidity_a?: InputMaybe<Scalars['bigint']['input']>;\n  liquidity_b?: InputMaybe<Scalars['bigint']['input']>;\n  pool_id?: InputMaybe<Scalars['bigint']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Pools_Liquidity_Sum_Fields = {\n  liquidity_a?: Maybe<Scalars['bigint']['output']>;\n  liquidity_b?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pools_Liquidity_Var_Pop_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pools_Liquidity_Var_Samp_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pools_Liquidity_Variance_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate max on columns */\nexport type Pools_Max_Fields = {\n  a_denom?: Maybe<Scalars['String']['output']>;\n  address?: Maybe<Scalars['String']['output']>;\n  b_denom?: Maybe<Scalars['String']['output']>;\n  pool_denom?: Maybe<Scalars['String']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  pool_name?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pools_Min_Fields = {\n  a_denom?: Maybe<Scalars['String']['output']>;\n  address?: Maybe<Scalars['String']['output']>;\n  b_denom?: Maybe<Scalars['String']['output']>;\n  pool_denom?: Maybe<Scalars['String']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  pool_name?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"pools\". */\nexport type Pools_Order_By = {\n  a_denom?: InputMaybe<Order_By>;\n  address?: InputMaybe<Order_By>;\n  b_denom?: InputMaybe<Order_By>;\n  pool_denom?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n  pool_name?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"pools_rates\" */\nexport type Pools_Rates = {\n  pool_id: Scalars['bigint']['output'];\n  rate: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n};\n\n/** aggregated selection of \"pools_rates\" */\nexport type Pools_Rates_Aggregate = {\n  aggregate?: Maybe<Pools_Rates_Aggregate_Fields>;\n  nodes: Array<Pools_Rates>;\n};\n\n/** aggregate fields of \"pools_rates\" */\nexport type Pools_Rates_Aggregate_Fields = {\n  avg?: Maybe<Pools_Rates_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pools_Rates_Max_Fields>;\n  min?: Maybe<Pools_Rates_Min_Fields>;\n  stddev?: Maybe<Pools_Rates_Stddev_Fields>;\n  stddev_pop?: Maybe<Pools_Rates_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pools_Rates_Stddev_Samp_Fields>;\n  sum?: Maybe<Pools_Rates_Sum_Fields>;\n  var_pop?: Maybe<Pools_Rates_Var_Pop_Fields>;\n  var_samp?: Maybe<Pools_Rates_Var_Samp_Fields>;\n  variance?: Maybe<Pools_Rates_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pools_rates\" */\nexport type Pools_Rates_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pools_Rates_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pools_Rates_Avg_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pools_rates\". All fields are combined with a logical 'AND'. */\nexport type Pools_Rates_Bool_Exp = {\n  _and?: InputMaybe<Array<Pools_Rates_Bool_Exp>>;\n  _not?: InputMaybe<Pools_Rates_Bool_Exp>;\n  _or?: InputMaybe<Array<Pools_Rates_Bool_Exp>>;\n  pool_id?: InputMaybe<Bigint_Comparison_Exp>;\n  rate?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pools_Rates_Max_Fields = {\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  rate?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pools_Rates_Min_Fields = {\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  rate?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n};\n\n/** Ordering options when selecting data from \"pools_rates\". */\nexport type Pools_Rates_Order_By = {\n  pool_id?: InputMaybe<Order_By>;\n  rate?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pools_rates\" */\nexport enum Pools_Rates_Select_Column {\n  /** column name */\n  PoolId = 'pool_id',\n  /** column name */\n  Rate = 'rate',\n  /** column name */\n  Timestamp = 'timestamp'\n}\n\n/** aggregate stddev on columns */\nexport type Pools_Rates_Stddev_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pools_Rates_Stddev_Pop_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pools_Rates_Stddev_Samp_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"pools_rates\" */\nexport type Pools_Rates_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Pools_Rates_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Pools_Rates_Stream_Cursor_Value_Input = {\n  pool_id?: InputMaybe<Scalars['bigint']['input']>;\n  rate?: InputMaybe<Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Pools_Rates_Sum_Fields = {\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pools_Rates_Var_Pop_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pools_Rates_Var_Samp_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pools_Rates_Variance_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** select columns of table \"pools\" */\nexport enum Pools_Select_Column {\n  /** column name */\n  ADenom = 'a_denom',\n  /** column name */\n  Address = 'address',\n  /** column name */\n  BDenom = 'b_denom',\n  /** column name */\n  PoolDenom = 'pool_denom',\n  /** column name */\n  PoolId = 'pool_id',\n  /** column name */\n  PoolName = 'pool_name'\n}\n\n/** aggregate stddev on columns */\nexport type Pools_Stddev_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pools_Stddev_Pop_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pools_Stddev_Samp_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"pools\" */\nexport type Pools_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Pools_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Pools_Stream_Cursor_Value_Input = {\n  a_denom?: InputMaybe<Scalars['String']['input']>;\n  address?: InputMaybe<Scalars['String']['input']>;\n  b_denom?: InputMaybe<Scalars['String']['input']>;\n  pool_denom?: InputMaybe<Scalars['String']['input']>;\n  pool_id?: InputMaybe<Scalars['bigint']['input']>;\n  pool_name?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Pools_Sum_Fields = {\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pools_Var_Pop_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pools_Var_Samp_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pools_Variance_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"pools_volumes\" */\nexport type Pools_Volumes = {\n  fee_a: Scalars['bigint']['output'];\n  fee_b: Scalars['bigint']['output'];\n  pool_id: Scalars['bigint']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  volume_a: Scalars['bigint']['output'];\n  volume_b: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"pools_volumes\" */\nexport type Pools_Volumes_Aggregate = {\n  aggregate?: Maybe<Pools_Volumes_Aggregate_Fields>;\n  nodes: Array<Pools_Volumes>;\n};\n\n/** aggregate fields of \"pools_volumes\" */\nexport type Pools_Volumes_Aggregate_Fields = {\n  avg?: Maybe<Pools_Volumes_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pools_Volumes_Max_Fields>;\n  min?: Maybe<Pools_Volumes_Min_Fields>;\n  stddev?: Maybe<Pools_Volumes_Stddev_Fields>;\n  stddev_pop?: Maybe<Pools_Volumes_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pools_Volumes_Stddev_Samp_Fields>;\n  sum?: Maybe<Pools_Volumes_Sum_Fields>;\n  var_pop?: Maybe<Pools_Volumes_Var_Pop_Fields>;\n  var_samp?: Maybe<Pools_Volumes_Var_Samp_Fields>;\n  variance?: Maybe<Pools_Volumes_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pools_volumes\" */\nexport type Pools_Volumes_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pools_Volumes_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pools_Volumes_Avg_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pools_volumes\". All fields are combined with a logical 'AND'. */\nexport type Pools_Volumes_Bool_Exp = {\n  _and?: InputMaybe<Array<Pools_Volumes_Bool_Exp>>;\n  _not?: InputMaybe<Pools_Volumes_Bool_Exp>;\n  _or?: InputMaybe<Array<Pools_Volumes_Bool_Exp>>;\n  fee_a?: InputMaybe<Bigint_Comparison_Exp>;\n  fee_b?: InputMaybe<Bigint_Comparison_Exp>;\n  pool_id?: InputMaybe<Bigint_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  volume_a?: InputMaybe<Bigint_Comparison_Exp>;\n  volume_b?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pools_Volumes_Max_Fields = {\n  fee_a?: Maybe<Scalars['bigint']['output']>;\n  fee_b?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  volume_a?: Maybe<Scalars['bigint']['output']>;\n  volume_b?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pools_Volumes_Min_Fields = {\n  fee_a?: Maybe<Scalars['bigint']['output']>;\n  fee_b?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  volume_a?: Maybe<Scalars['bigint']['output']>;\n  volume_b?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"pools_volumes\". */\nexport type Pools_Volumes_Order_By = {\n  fee_a?: InputMaybe<Order_By>;\n  fee_b?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  volume_a?: InputMaybe<Order_By>;\n  volume_b?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pools_volumes\" */\nexport enum Pools_Volumes_Select_Column {\n  /** column name */\n  FeeA = 'fee_a',\n  /** column name */\n  FeeB = 'fee_b',\n  /** column name */\n  PoolId = 'pool_id',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  VolumeA = 'volume_a',\n  /** column name */\n  VolumeB = 'volume_b'\n}\n\n/** aggregate stddev on columns */\nexport type Pools_Volumes_Stddev_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pools_Volumes_Stddev_Pop_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pools_Volumes_Stddev_Samp_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"pools_volumes\" */\nexport type Pools_Volumes_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Pools_Volumes_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Pools_Volumes_Stream_Cursor_Value_Input = {\n  fee_a?: InputMaybe<Scalars['bigint']['input']>;\n  fee_b?: InputMaybe<Scalars['bigint']['input']>;\n  pool_id?: InputMaybe<Scalars['bigint']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  volume_a?: InputMaybe<Scalars['bigint']['input']>;\n  volume_b?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Pools_Volumes_Sum_Fields = {\n  fee_a?: Maybe<Scalars['bigint']['output']>;\n  fee_b?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  volume_a?: Maybe<Scalars['bigint']['output']>;\n  volume_b?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pools_Volumes_Var_Pop_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pools_Volumes_Var_Samp_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pools_Volumes_Variance_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"pre_commit\" */\nexport type Pre_Commit = {\n  height: Scalars['bigint']['output'];\n  proposer_priority: Scalars['bigint']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  /** An object relationship */\n  validator: Validator;\n  validator_address: Scalars['String']['output'];\n  voting_power: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"pre_commit\" */\nexport type Pre_Commit_Aggregate = {\n  aggregate?: Maybe<Pre_Commit_Aggregate_Fields>;\n  nodes: Array<Pre_Commit>;\n};\n\nexport type Pre_Commit_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Pre_Commit_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Pre_Commit_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Pre_Commit_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"pre_commit\" */\nexport type Pre_Commit_Aggregate_Fields = {\n  avg?: Maybe<Pre_Commit_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pre_Commit_Max_Fields>;\n  min?: Maybe<Pre_Commit_Min_Fields>;\n  stddev?: Maybe<Pre_Commit_Stddev_Fields>;\n  stddev_pop?: Maybe<Pre_Commit_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pre_Commit_Stddev_Samp_Fields>;\n  sum?: Maybe<Pre_Commit_Sum_Fields>;\n  var_pop?: Maybe<Pre_Commit_Var_Pop_Fields>;\n  var_samp?: Maybe<Pre_Commit_Var_Samp_Fields>;\n  variance?: Maybe<Pre_Commit_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pre_commit\" */\nexport type Pre_Commit_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"pre_commit\" */\nexport type Pre_Commit_Aggregate_Order_By = {\n  avg?: InputMaybe<Pre_Commit_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Pre_Commit_Max_Order_By>;\n  min?: InputMaybe<Pre_Commit_Min_Order_By>;\n  stddev?: InputMaybe<Pre_Commit_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Pre_Commit_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Pre_Commit_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Pre_Commit_Sum_Order_By>;\n  var_pop?: InputMaybe<Pre_Commit_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Pre_Commit_Var_Samp_Order_By>;\n  variance?: InputMaybe<Pre_Commit_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Pre_Commit_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"pre_commit\". All fields are combined with a logical 'AND'. */\nexport type Pre_Commit_Bool_Exp = {\n  _and?: InputMaybe<Array<Pre_Commit_Bool_Exp>>;\n  _not?: InputMaybe<Pre_Commit_Bool_Exp>;\n  _or?: InputMaybe<Array<Pre_Commit_Bool_Exp>>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  proposer_priority?: InputMaybe<Bigint_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  validator?: InputMaybe<Validator_Bool_Exp>;\n  validator_address?: InputMaybe<String_Comparison_Exp>;\n  voting_power?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pre_Commit_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  proposer_priority?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n  voting_power?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by max() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  validator_address?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Pre_Commit_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  proposer_priority?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n  voting_power?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by min() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  validator_address?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"pre_commit\". */\nexport type Pre_Commit_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  validator?: InputMaybe<Validator_Order_By>;\n  validator_address?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pre_commit\" */\nexport enum Pre_Commit_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  ProposerPriority = 'proposer_priority',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  ValidatorAddress = 'validator_address',\n  /** column name */\n  VotingPower = 'voting_power'\n}\n\n/** aggregate stddev on columns */\nexport type Pre_Commit_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pre_Commit_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pre_Commit_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"pre_commit\" */\nexport type Pre_Commit_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Pre_Commit_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Pre_Commit_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  proposer_priority?: InputMaybe<Scalars['bigint']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  validator_address?: InputMaybe<Scalars['String']['input']>;\n  voting_power?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Pre_Commit_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  proposer_priority?: Maybe<Scalars['bigint']['output']>;\n  voting_power?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pre_Commit_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pre_Commit_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Pre_Commit_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"pruning\" */\nexport type Pruning = {\n  last_pruned_height: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"pruning\" */\nexport type Pruning_Aggregate = {\n  aggregate?: Maybe<Pruning_Aggregate_Fields>;\n  nodes: Array<Pruning>;\n};\n\n/** aggregate fields of \"pruning\" */\nexport type Pruning_Aggregate_Fields = {\n  avg?: Maybe<Pruning_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pruning_Max_Fields>;\n  min?: Maybe<Pruning_Min_Fields>;\n  stddev?: Maybe<Pruning_Stddev_Fields>;\n  stddev_pop?: Maybe<Pruning_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pruning_Stddev_Samp_Fields>;\n  sum?: Maybe<Pruning_Sum_Fields>;\n  var_pop?: Maybe<Pruning_Var_Pop_Fields>;\n  var_samp?: Maybe<Pruning_Var_Samp_Fields>;\n  variance?: Maybe<Pruning_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pruning\" */\nexport type Pruning_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pruning_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pruning_Avg_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pruning\". All fields are combined with a logical 'AND'. */\nexport type Pruning_Bool_Exp = {\n  _and?: InputMaybe<Array<Pruning_Bool_Exp>>;\n  _not?: InputMaybe<Pruning_Bool_Exp>;\n  _or?: InputMaybe<Array<Pruning_Bool_Exp>>;\n  last_pruned_height?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pruning_Max_Fields = {\n  last_pruned_height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pruning_Min_Fields = {\n  last_pruned_height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"pruning\". */\nexport type Pruning_Order_By = {\n  last_pruned_height?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pruning\" */\nexport enum Pruning_Select_Column {\n  /** column name */\n  LastPrunedHeight = 'last_pruned_height'\n}\n\n/** aggregate stddev on columns */\nexport type Pruning_Stddev_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pruning_Stddev_Pop_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pruning_Stddev_Samp_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"pruning\" */\nexport type Pruning_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Pruning_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Pruning_Stream_Cursor_Value_Input = {\n  last_pruned_height?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Pruning_Sum_Fields = {\n  last_pruned_height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pruning_Var_Pop_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pruning_Var_Samp_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pruning_Variance_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\nexport type Query_Root = {\n  /** fetch data from the table: \"_transaction\" */\n  _transaction: Array<_Transaction>;\n  /** fetch aggregated fields from the table: \"_transaction\" */\n  _transaction_aggregate: _Transaction_Aggregate;\n  /** fetch data from the table: \"_uptime_temp\" */\n  _uptime_temp: Array<_Uptime_Temp>;\n  /** fetch aggregated fields from the table: \"_uptime_temp\" */\n  _uptime_temp_aggregate: _Uptime_Temp_Aggregate;\n  /** fetch data from the table: \"account\" */\n  account: Array<Account>;\n  /** fetch aggregated fields from the table: \"account\" */\n  account_aggregate: Account_Aggregate;\n  /** fetch data from the table: \"account_balance\" */\n  account_balance: Array<Account_Balance>;\n  /** fetch aggregated fields from the table: \"account_balance\" */\n  account_balance_aggregate: Account_Balance_Aggregate;\n  /** fetch data from the table: \"account_balance\" using primary key columns */\n  account_balance_by_pk?: Maybe<Account_Balance>;\n  /** fetch data from the table: \"account\" using primary key columns */\n  account_by_pk?: Maybe<Account>;\n  /** fetch data from the table: \"average_block_time_from_genesis\" */\n  average_block_time_from_genesis: Array<Average_Block_Time_From_Genesis>;\n  /** fetch aggregated fields from the table: \"average_block_time_from_genesis\" */\n  average_block_time_from_genesis_aggregate: Average_Block_Time_From_Genesis_Aggregate;\n  /** fetch data from the table: \"average_block_time_from_genesis\" using primary key columns */\n  average_block_time_from_genesis_by_pk?: Maybe<Average_Block_Time_From_Genesis>;\n  /** fetch data from the table: \"average_block_time_per_day\" */\n  average_block_time_per_day: Array<Average_Block_Time_Per_Day>;\n  /** fetch aggregated fields from the table: \"average_block_time_per_day\" */\n  average_block_time_per_day_aggregate: Average_Block_Time_Per_Day_Aggregate;\n  /** fetch data from the table: \"average_block_time_per_day\" using primary key columns */\n  average_block_time_per_day_by_pk?: Maybe<Average_Block_Time_Per_Day>;\n  /** fetch data from the table: \"average_block_time_per_hour\" */\n  average_block_time_per_hour: Array<Average_Block_Time_Per_Hour>;\n  /** fetch aggregated fields from the table: \"average_block_time_per_hour\" */\n  average_block_time_per_hour_aggregate: Average_Block_Time_Per_Hour_Aggregate;\n  /** fetch data from the table: \"average_block_time_per_hour\" using primary key columns */\n  average_block_time_per_hour_by_pk?: Maybe<Average_Block_Time_Per_Hour>;\n  /** fetch data from the table: \"average_block_time_per_minute\" */\n  average_block_time_per_minute: Array<Average_Block_Time_Per_Minute>;\n  /** fetch aggregated fields from the table: \"average_block_time_per_minute\" */\n  average_block_time_per_minute_aggregate: Average_Block_Time_Per_Minute_Aggregate;\n  /** fetch data from the table: \"average_block_time_per_minute\" using primary key columns */\n  average_block_time_per_minute_by_pk?: Maybe<Average_Block_Time_Per_Minute>;\n  /** fetch data from the table: \"block\" */\n  block: Array<Block>;\n  /** fetch aggregated fields from the table: \"block\" */\n  block_aggregate: Block_Aggregate;\n  /** fetch data from the table: \"block\" using primary key columns */\n  block_by_pk?: Maybe<Block>;\n  /** fetch data from the table: \"contracts\" */\n  contracts: Array<Contracts>;\n  /** fetch aggregated fields from the table: \"contracts\" */\n  contracts_aggregate: Contracts_Aggregate;\n  /** fetch data from the table: \"contracts\" using primary key columns */\n  contracts_by_pk?: Maybe<Contracts>;\n  /** fetch data from the table: \"cyb_cohort\" */\n  cyb_cohort: Array<Cyb_Cohort>;\n  /** fetch aggregated fields from the table: \"cyb_cohort\" */\n  cyb_cohort_aggregate: Cyb_Cohort_Aggregate;\n  /** fetch data from the table: \"cyber_gift_proofs\" */\n  cyber_gift_proofs: Array<Cyber_Gift_Proofs>;\n  /** fetch aggregated fields from the table: \"cyber_gift_proofs\" */\n  cyber_gift_proofs_aggregate: Cyber_Gift_Proofs_Aggregate;\n  /** An array relationship */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  /** fetch data from the table: \"cyberlinks\" using primary key columns */\n  cyberlinks_by_pk?: Maybe<Cyberlinks>;\n  /** fetch data from the table: \"cyberlinks_stats\" */\n  cyberlinks_stats: Array<Cyberlinks_Stats>;\n  /** fetch aggregated fields from the table: \"cyberlinks_stats\" */\n  cyberlinks_stats_aggregate: Cyberlinks_Stats_Aggregate;\n  /** fetch data from the table: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons: Array<Daily_Amount_Of_Active_Neurons>;\n  /** fetch aggregated fields from the table: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons_aggregate: Daily_Amount_Of_Active_Neurons_Aggregate;\n  /** fetch data from the table: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas: Array<Daily_Amount_Of_Used_Gas>;\n  /** fetch aggregated fields from the table: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas_aggregate: Daily_Amount_Of_Used_Gas_Aggregate;\n  /** fetch data from the table: \"daily_number_of_transactions\" */\n  daily_number_of_transactions: Array<Daily_Number_Of_Transactions>;\n  /** fetch aggregated fields from the table: \"daily_number_of_transactions\" */\n  daily_number_of_transactions_aggregate: Daily_Number_Of_Transactions_Aggregate;\n  /** fetch data from the table: \"first_10_cyberlink\" */\n  first_10_cyberlink: Array<First_10_Cyberlink>;\n  /** fetch aggregated fields from the table: \"first_10_cyberlink\" */\n  first_10_cyberlink_aggregate: First_10_Cyberlink_Aggregate;\n  /** fetch data from the table: \"first_100_cyberlink\" */\n  first_100_cyberlink: Array<First_100_Cyberlink>;\n  /** fetch aggregated fields from the table: \"first_100_cyberlink\" */\n  first_100_cyberlink_aggregate: First_100_Cyberlink_Aggregate;\n  /** fetch data from the table: \"first_cyberlink\" */\n  first_cyberlink: Array<First_Cyberlink>;\n  /** fetch aggregated fields from the table: \"first_cyberlink\" */\n  first_cyberlink_aggregate: First_Cyberlink_Aggregate;\n  /** fetch data from the table: \"first_hero_hired\" */\n  first_hero_hired: Array<First_Hero_Hired>;\n  /** fetch aggregated fields from the table: \"first_hero_hired\" */\n  first_hero_hired_aggregate: First_Hero_Hired_Aggregate;\n  /** fetch data from the table: \"first_investmint\" */\n  first_investmint: Array<First_Investmint>;\n  /** fetch aggregated fields from the table: \"first_investmint\" */\n  first_investmint_aggregate: First_Investmint_Aggregate;\n  /** fetch data from the table: \"first_neuron_activation\" */\n  first_neuron_activation: Array<First_Neuron_Activation>;\n  /** fetch aggregated fields from the table: \"first_neuron_activation\" */\n  first_neuron_activation_aggregate: First_Neuron_Activation_Aggregate;\n  /** fetch data from the table: \"first_swap\" */\n  first_swap: Array<First_Swap>;\n  /** fetch aggregated fields from the table: \"first_swap\" */\n  first_swap_aggregate: First_Swap_Aggregate;\n  /** fetch data from the table: \"follow_stats\" */\n  follow_stats: Array<Follow_Stats>;\n  /** fetch aggregated fields from the table: \"follow_stats\" */\n  follow_stats_aggregate: Follow_Stats_Aggregate;\n  /** fetch data from the table: \"genesis\" */\n  genesis: Array<Genesis>;\n  /** fetch data from the table: \"genesis_accounts\" */\n  genesis_accounts: Array<Genesis_Accounts>;\n  /** fetch aggregated fields from the table: \"genesis_accounts\" */\n  genesis_accounts_aggregate: Genesis_Accounts_Aggregate;\n  /** fetch data from the table: \"genesis_accounts\" using primary key columns */\n  genesis_accounts_by_pk?: Maybe<Genesis_Accounts>;\n  /** fetch aggregated fields from the table: \"genesis\" */\n  genesis_aggregate: Genesis_Aggregate;\n  /** fetch data from the table: \"genesis\" using primary key columns */\n  genesis_by_pk?: Maybe<Genesis>;\n  /** fetch data from the table: \"genesis_neurons_activation\" */\n  genesis_neurons_activation: Array<Genesis_Neurons_Activation>;\n  /** fetch aggregated fields from the table: \"genesis_neurons_activation\" */\n  genesis_neurons_activation_aggregate: Genesis_Neurons_Activation_Aggregate;\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  /** fetch data from the table: \"investmints\" using primary key columns */\n  investmints_by_pk?: Maybe<Investmints>;\n  /** fetch data from the table: \"message\" */\n  message: Array<Message>;\n  /** fetch data from the table: \"message_155\" */\n  message_155: Array<Message_155>;\n  /** fetch aggregated fields from the table: \"message_155\" */\n  message_155_aggregate: Message_155_Aggregate;\n  /** fetch aggregated fields from the table: \"message\" */\n  message_aggregate: Message_Aggregate;\n  /** fetch data from the table: \"message_type\" */\n  message_type: Array<Message_Type>;\n  /** fetch aggregated fields from the table: \"message_type\" */\n  message_type_aggregate: Message_Type_Aggregate;\n  /** execute function \"messages_by_address\" which returns \"message\" */\n  messages_by_address: Array<Message>;\n  /** execute function \"messages_by_address\" and query aggregates on result of table type \"message\" */\n  messages_by_address_aggregate: Message_Aggregate;\n  /** execute function \"messages_by_type\" which returns \"message\" */\n  messages_by_type: Array<Message>;\n  /** execute function \"messages_by_type\" and query aggregates on result of table type \"message\" */\n  messages_by_type_aggregate: Message_Aggregate;\n  /** fetch data from the table: \"modules\" */\n  modules: Array<Modules>;\n  /** fetch aggregated fields from the table: \"modules\" */\n  modules_aggregate: Modules_Aggregate;\n  /** fetch data from the table: \"modules\" using primary key columns */\n  modules_by_pk?: Maybe<Modules>;\n  /** fetch data from the table: \"neuron_activation_source\" */\n  neuron_activation_source: Array<Neuron_Activation_Source>;\n  /** fetch aggregated fields from the table: \"neuron_activation_source\" */\n  neuron_activation_source_aggregate: Neuron_Activation_Source_Aggregate;\n  /** fetch data from the table: \"number_of_new_neurons\" */\n  number_of_new_neurons: Array<Number_Of_New_Neurons>;\n  /** fetch aggregated fields from the table: \"number_of_new_neurons\" */\n  number_of_new_neurons_aggregate: Number_Of_New_Neurons_Aggregate;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  /** fetch data from the table: \"particles\" using primary key columns */\n  particles_by_pk?: Maybe<Particles>;\n  /** fetch data from the table: \"pools\" */\n  pools: Array<Pools>;\n  /** fetch aggregated fields from the table: \"pools\" */\n  pools_aggregate: Pools_Aggregate;\n  /** fetch data from the table: \"pools\" using primary key columns */\n  pools_by_pk?: Maybe<Pools>;\n  /** fetch data from the table: \"pools_liquidity\" */\n  pools_liquidity: Array<Pools_Liquidity>;\n  /** fetch aggregated fields from the table: \"pools_liquidity\" */\n  pools_liquidity_aggregate: Pools_Liquidity_Aggregate;\n  /** fetch data from the table: \"pools_rates\" */\n  pools_rates: Array<Pools_Rates>;\n  /** fetch aggregated fields from the table: \"pools_rates\" */\n  pools_rates_aggregate: Pools_Rates_Aggregate;\n  /** fetch data from the table: \"pools_volumes\" */\n  pools_volumes: Array<Pools_Volumes>;\n  /** fetch aggregated fields from the table: \"pools_volumes\" */\n  pools_volumes_aggregate: Pools_Volumes_Aggregate;\n  /** fetch data from the table: \"pre_commit\" */\n  pre_commit: Array<Pre_Commit>;\n  /** fetch aggregated fields from the table: \"pre_commit\" */\n  pre_commit_aggregate: Pre_Commit_Aggregate;\n  /** fetch data from the table: \"pruning\" */\n  pruning: Array<Pruning>;\n  /** fetch aggregated fields from the table: \"pruning\" */\n  pruning_aggregate: Pruning_Aggregate;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n  /** fetch data from the table: \"routes\" using primary key columns */\n  routes_by_pk?: Maybe<Routes>;\n  /** fetch data from the table: \"supply\" */\n  supply: Array<Supply>;\n  /** fetch aggregated fields from the table: \"supply\" */\n  supply_aggregate: Supply_Aggregate;\n  /** fetch data from the table: \"supply\" using primary key columns */\n  supply_by_pk?: Maybe<Supply>;\n  /** An array relationship */\n  swaps: Array<Swaps>;\n  /** An aggregate relationship */\n  swaps_aggregate: Swaps_Aggregate;\n  /** fetch data from the table: \"swaps\" using primary key columns */\n  swaps_by_pk?: Maybe<Swaps>;\n  /** fetch data from the table: \"today_top_txs\" */\n  today_top_txs: Array<Today_Top_Txs>;\n  /** fetch aggregated fields from the table: \"today_top_txs\" */\n  today_top_txs_aggregate: Today_Top_Txs_Aggregate;\n  /** fetch data from the table: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week: Array<Top_10_Of_Active_Neurons_Week>;\n  /** fetch aggregated fields from the table: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week_aggregate: Top_10_Of_Active_Neurons_Week_Aggregate;\n  /** fetch data from the table: \"top_first_txs\" */\n  top_first_txs: Array<Top_First_Txs>;\n  /** fetch aggregated fields from the table: \"top_first_txs\" */\n  top_first_txs_aggregate: Top_First_Txs_Aggregate;\n  /** fetch data from the table: \"top_leaders\" */\n  top_leaders: Array<Top_Leaders>;\n  /** fetch aggregated fields from the table: \"top_leaders\" */\n  top_leaders_aggregate: Top_Leaders_Aggregate;\n  /** fetch data from the table: \"top_txs\" */\n  top_txs: Array<Top_Txs>;\n  /** fetch aggregated fields from the table: \"top_txs\" */\n  top_txs_aggregate: Top_Txs_Aggregate;\n  /** fetch data from the table: \"transaction\" */\n  transaction: Array<Transaction>;\n  /** fetch data from the table: \"transaction_155\" */\n  transaction_155: Array<Transaction_155>;\n  /** fetch aggregated fields from the table: \"transaction_155\" */\n  transaction_155_aggregate: Transaction_155_Aggregate;\n  /** fetch aggregated fields from the table: \"transaction\" */\n  transaction_aggregate: Transaction_Aggregate;\n  /** fetch data from the table: \"tweets_stats\" */\n  tweets_stats: Array<Tweets_Stats>;\n  /** fetch aggregated fields from the table: \"tweets_stats\" */\n  tweets_stats_aggregate: Tweets_Stats_Aggregate;\n  /** fetch data from the table: \"txs_ranked\" */\n  txs_ranked: Array<Txs_Ranked>;\n  /** fetch aggregated fields from the table: \"txs_ranked\" */\n  txs_ranked_aggregate: Txs_Ranked_Aggregate;\n  /** fetch data from the table: \"uptime\" */\n  uptime: Array<Uptime>;\n  /** fetch aggregated fields from the table: \"uptime\" */\n  uptime_aggregate: Uptime_Aggregate;\n  /** fetch data from the table: \"validator\" */\n  validator: Array<Validator>;\n  /** fetch aggregated fields from the table: \"validator\" */\n  validator_aggregate: Validator_Aggregate;\n  /** fetch data from the table: \"validator\" using primary key columns */\n  validator_by_pk?: Maybe<Validator>;\n  /** fetch data from the table: \"vesting_account\" */\n  vesting_account: Array<Vesting_Account>;\n  /** fetch aggregated fields from the table: \"vesting_account\" */\n  vesting_account_aggregate: Vesting_Account_Aggregate;\n  /** fetch data from the table: \"vesting_account\" using primary key columns */\n  vesting_account_by_pk?: Maybe<Vesting_Account>;\n  /** fetch data from the table: \"vesting_period\" */\n  vesting_period: Array<Vesting_Period>;\n  /** fetch aggregated fields from the table: \"vesting_period\" */\n  vesting_period_aggregate: Vesting_Period_Aggregate;\n  /** fetch data from the table: \"week_redelegation\" */\n  week_redelegation: Array<Week_Redelegation>;\n  /** fetch aggregated fields from the table: \"week_redelegation\" */\n  week_redelegation_aggregate: Week_Redelegation_Aggregate;\n  /** fetch data from the table: \"week_undelegation\" */\n  week_undelegation: Array<Week_Undelegation>;\n  /** fetch aggregated fields from the table: \"week_undelegation\" */\n  week_undelegation_aggregate: Week_Undelegation_Aggregate;\n};\n\n\nexport type Query_Root_TransactionArgs = {\n  distinct_on?: InputMaybe<Array<_Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Transaction_Order_By>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Query_Root_Transaction_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<_Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Transaction_Order_By>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Query_Root_Uptime_TempArgs = {\n  distinct_on?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Uptime_Temp_Order_By>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Query_Root_Uptime_Temp_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Uptime_Temp_Order_By>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Query_RootAccountArgs = {\n  distinct_on?: InputMaybe<Array<Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Order_By>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Query_RootAccount_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Order_By>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Query_RootAccount_BalanceArgs = {\n  distinct_on?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Balance_Order_By>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Query_RootAccount_Balance_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Balance_Order_By>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Query_RootAccount_Balance_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootAccount_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootAverage_Block_Time_From_GenesisArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_From_Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_From_Genesis_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_From_Genesis_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_From_Genesis_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_From_Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_From_Genesis_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_From_Genesis_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_From_Genesis_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_DayArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Day_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Day_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Day_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_Day_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Day_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Day_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Day_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_Day_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_HourArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Hour_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Hour_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Hour_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_Hour_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Hour_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Hour_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Hour_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_Hour_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_MinuteArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Minute_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Minute_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Minute_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_Minute_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Minute_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Minute_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Minute_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_Minute_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootBlockArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Query_RootBlock_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Query_RootBlock_By_PkArgs = {\n  height: Scalars['bigint']['input'];\n};\n\n\nexport type Query_RootContractsArgs = {\n  distinct_on?: InputMaybe<Array<Contracts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Contracts_Order_By>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Query_RootContracts_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Contracts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Contracts_Order_By>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Query_RootContracts_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootCyb_CohortArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Query_RootCyb_Cohort_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Query_RootCyber_Gift_ProofsArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Query_RootCyber_Gift_Proofs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Query_RootCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Query_RootCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Query_RootCyberlinks_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootCyberlinks_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Stats_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootCyberlinks_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Stats_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Amount_Of_Active_NeuronsArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Amount_Of_Active_Neurons_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Amount_Of_Used_GasArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Amount_Of_Used_Gas_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Number_Of_TransactionsArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Number_Of_Transactions_Order_By>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Number_Of_Transactions_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Number_Of_Transactions_Order_By>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_10_CyberlinkArgs = {\n  distinct_on?: InputMaybe<Array<First_10_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_10_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_10_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_10_Cyberlink_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_10_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_10_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_10_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_100_CyberlinkArgs = {\n  distinct_on?: InputMaybe<Array<First_100_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_100_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_100_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_100_Cyberlink_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_100_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_100_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_100_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_CyberlinkArgs = {\n  distinct_on?: InputMaybe<Array<First_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Cyberlink_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Hero_HiredArgs = {\n  distinct_on?: InputMaybe<Array<First_Hero_Hired_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Hero_Hired_Order_By>>;\n  where?: InputMaybe<First_Hero_Hired_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Hero_Hired_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Hero_Hired_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Hero_Hired_Order_By>>;\n  where?: InputMaybe<First_Hero_Hired_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_InvestmintArgs = {\n  distinct_on?: InputMaybe<Array<First_Investmint_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Investmint_Order_By>>;\n  where?: InputMaybe<First_Investmint_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Investmint_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Investmint_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Investmint_Order_By>>;\n  where?: InputMaybe<First_Investmint_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Neuron_ActivationArgs = {\n  distinct_on?: InputMaybe<Array<First_Neuron_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Neuron_Activation_Order_By>>;\n  where?: InputMaybe<First_Neuron_Activation_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Neuron_Activation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Neuron_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Neuron_Activation_Order_By>>;\n  where?: InputMaybe<First_Neuron_Activation_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_SwapArgs = {\n  distinct_on?: InputMaybe<Array<First_Swap_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Swap_Order_By>>;\n  where?: InputMaybe<First_Swap_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Swap_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Swap_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Swap_Order_By>>;\n  where?: InputMaybe<First_Swap_Bool_Exp>;\n};\n\n\nexport type Query_RootFollow_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Follow_Stats_Order_By>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootFollow_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Follow_Stats_Order_By>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesisArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Order_By>>;\n  where?: InputMaybe<Genesis_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesis_AccountsArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Accounts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Accounts_Order_By>>;\n  where?: InputMaybe<Genesis_Accounts_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesis_Accounts_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Accounts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Accounts_Order_By>>;\n  where?: InputMaybe<Genesis_Accounts_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesis_Accounts_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootGenesis_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Order_By>>;\n  where?: InputMaybe<Genesis_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesis_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootGenesis_Neurons_ActivationArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Neurons_Activation_Order_By>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesis_Neurons_Activation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Neurons_Activation_Order_By>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Query_RootInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Query_RootInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Query_RootInvestmints_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootMessageArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessage_155Args = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\nexport type Query_RootMessage_155_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\nexport type Query_RootMessage_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessage_TypeArgs = {\n  distinct_on?: InputMaybe<Array<Message_Type_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Type_Order_By>>;\n  where?: InputMaybe<Message_Type_Bool_Exp>;\n};\n\n\nexport type Query_RootMessage_Type_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Type_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Type_Order_By>>;\n  where?: InputMaybe<Message_Type_Bool_Exp>;\n};\n\n\nexport type Query_RootMessages_By_AddressArgs = {\n  args: Messages_By_Address_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessages_By_Address_AggregateArgs = {\n  args: Messages_By_Address_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessages_By_TypeArgs = {\n  args: Messages_By_Type_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessages_By_Type_AggregateArgs = {\n  args: Messages_By_Type_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootModulesArgs = {\n  distinct_on?: InputMaybe<Array<Modules_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Modules_Order_By>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Query_RootModules_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Modules_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Modules_Order_By>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Query_RootModules_By_PkArgs = {\n  module_name: Scalars['String']['input'];\n};\n\n\nexport type Query_RootNeuron_Activation_SourceArgs = {\n  distinct_on?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Neuron_Activation_Source_Order_By>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Query_RootNeuron_Activation_Source_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Neuron_Activation_Source_Order_By>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Query_RootNumber_Of_New_NeuronsArgs = {\n  distinct_on?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Number_Of_New_Neurons_Order_By>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Query_RootNumber_Of_New_Neurons_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Number_Of_New_Neurons_Order_By>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Query_RootParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Query_RootParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Query_RootParticles_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootPoolsArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Order_By>>;\n  where?: InputMaybe<Pools_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Order_By>>;\n  where?: InputMaybe<Pools_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_By_PkArgs = {\n  pool_id: Scalars['bigint']['input'];\n};\n\n\nexport type Query_RootPools_LiquidityArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Liquidity_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Liquidity_Order_By>>;\n  where?: InputMaybe<Pools_Liquidity_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_Liquidity_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Liquidity_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Liquidity_Order_By>>;\n  where?: InputMaybe<Pools_Liquidity_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_RatesArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Rates_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Rates_Order_By>>;\n  where?: InputMaybe<Pools_Rates_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_Rates_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Rates_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Rates_Order_By>>;\n  where?: InputMaybe<Pools_Rates_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_VolumesArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Volumes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Volumes_Order_By>>;\n  where?: InputMaybe<Pools_Volumes_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_Volumes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Volumes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Volumes_Order_By>>;\n  where?: InputMaybe<Pools_Volumes_Bool_Exp>;\n};\n\n\nexport type Query_RootPre_CommitArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Query_RootPre_Commit_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Query_RootPruningArgs = {\n  distinct_on?: InputMaybe<Array<Pruning_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pruning_Order_By>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Query_RootPruning_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pruning_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pruning_Order_By>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Query_RootRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Query_RootRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Query_RootRoutes_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootSupplyArgs = {\n  distinct_on?: InputMaybe<Array<Supply_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Supply_Order_By>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Query_RootSupply_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Supply_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Supply_Order_By>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Query_RootSupply_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootSwapsArgs = {\n  distinct_on?: InputMaybe<Array<Swaps_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Swaps_Order_By>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\nexport type Query_RootSwaps_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Swaps_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Swaps_Order_By>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\nexport type Query_RootSwaps_By_PkArgs = {\n  pool_id: Scalars['bigint']['input'];\n};\n\n\nexport type Query_RootToday_Top_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Today_Top_Txs_Order_By>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootToday_Top_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Today_Top_Txs_Order_By>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_10_Of_Active_Neurons_WeekArgs = {\n  distinct_on?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Order_By>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_10_Of_Active_Neurons_Week_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Order_By>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_First_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_First_Txs_Order_By>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_First_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_First_Txs_Order_By>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_LeadersArgs = {\n  distinct_on?: InputMaybe<Array<Top_Leaders_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Leaders_Order_By>>;\n  where?: InputMaybe<Top_Leaders_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_Leaders_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_Leaders_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Leaders_Order_By>>;\n  where?: InputMaybe<Top_Leaders_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Txs_Order_By>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Txs_Order_By>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTransactionArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Query_RootTransaction_155Args = {\n  distinct_on?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_155_Order_By>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\nexport type Query_RootTransaction_155_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_155_Order_By>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\nexport type Query_RootTransaction_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Query_RootTweets_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Tweets_Stats_Order_By>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootTweets_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Tweets_Stats_Order_By>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootTxs_RankedArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Ranked_Order_By>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Query_RootTxs_Ranked_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Ranked_Order_By>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Query_RootUptimeArgs = {\n  distinct_on?: InputMaybe<Array<Uptime_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Uptime_Order_By>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Query_RootUptime_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Uptime_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Uptime_Order_By>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Query_RootValidatorArgs = {\n  distinct_on?: InputMaybe<Array<Validator_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Validator_Order_By>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Query_RootValidator_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Validator_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Validator_Order_By>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Query_RootValidator_By_PkArgs = {\n  consensus_address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootVesting_AccountArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Account_Order_By>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n\nexport type Query_RootVesting_Account_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Account_Order_By>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n\nexport type Query_RootVesting_Account_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootVesting_PeriodArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Period_Order_By>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n\nexport type Query_RootVesting_Period_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Period_Order_By>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n\nexport type Query_RootWeek_RedelegationArgs = {\n  distinct_on?: InputMaybe<Array<Week_Redelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Redelegation_Order_By>>;\n  where?: InputMaybe<Week_Redelegation_Bool_Exp>;\n};\n\n\nexport type Query_RootWeek_Redelegation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Week_Redelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Redelegation_Order_By>>;\n  where?: InputMaybe<Week_Redelegation_Bool_Exp>;\n};\n\n\nexport type Query_RootWeek_UndelegationArgs = {\n  distinct_on?: InputMaybe<Array<Week_Undelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Undelegation_Order_By>>;\n  where?: InputMaybe<Week_Undelegation_Bool_Exp>;\n};\n\n\nexport type Query_RootWeek_Undelegation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Week_Undelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Undelegation_Order_By>>;\n  where?: InputMaybe<Week_Undelegation_Bool_Exp>;\n};\n\n/** columns and relationships of \"routes\" */\nexport type Routes = {\n  /** An object relationship */\n  account: Account;\n  /** An object relationship */\n  accountBySource: Account;\n  alias: Scalars['String']['output'];\n  /** An object relationship */\n  block: Block;\n  destination: Scalars['String']['output'];\n  height: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n  source: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  transaction_hash: Scalars['String']['output'];\n  value: Array<Scalars['coin']['output']>;\n};\n\n/** aggregated selection of \"routes\" */\nexport type Routes_Aggregate = {\n  aggregate?: Maybe<Routes_Aggregate_Fields>;\n  nodes: Array<Routes>;\n};\n\nexport type Routes_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Routes_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Routes_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Routes_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Routes_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"routes\" */\nexport type Routes_Aggregate_Fields = {\n  avg?: Maybe<Routes_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Routes_Max_Fields>;\n  min?: Maybe<Routes_Min_Fields>;\n  stddev?: Maybe<Routes_Stddev_Fields>;\n  stddev_pop?: Maybe<Routes_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Routes_Stddev_Samp_Fields>;\n  sum?: Maybe<Routes_Sum_Fields>;\n  var_pop?: Maybe<Routes_Var_Pop_Fields>;\n  var_samp?: Maybe<Routes_Var_Samp_Fields>;\n  variance?: Maybe<Routes_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"routes\" */\nexport type Routes_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Routes_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"routes\" */\nexport type Routes_Aggregate_Order_By = {\n  avg?: InputMaybe<Routes_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Routes_Max_Order_By>;\n  min?: InputMaybe<Routes_Min_Order_By>;\n  stddev?: InputMaybe<Routes_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Routes_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Routes_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Routes_Sum_Order_By>;\n  var_pop?: InputMaybe<Routes_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Routes_Var_Samp_Order_By>;\n  variance?: InputMaybe<Routes_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Routes_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"routes\" */\nexport type Routes_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"routes\". All fields are combined with a logical 'AND'. */\nexport type Routes_Bool_Exp = {\n  _and?: InputMaybe<Array<Routes_Bool_Exp>>;\n  _not?: InputMaybe<Routes_Bool_Exp>;\n  _or?: InputMaybe<Array<Routes_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  accountBySource?: InputMaybe<Account_Bool_Exp>;\n  alias?: InputMaybe<String_Comparison_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  destination?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  source?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n  value?: InputMaybe<Coin_Array_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Routes_Max_Fields = {\n  alias?: Maybe<Scalars['String']['output']>;\n  destination?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  source?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  value?: Maybe<Array<Scalars['coin']['output']>>;\n};\n\n/** order by max() on columns of table \"routes\" */\nexport type Routes_Max_Order_By = {\n  alias?: InputMaybe<Order_By>;\n  destination?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  source?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Routes_Min_Fields = {\n  alias?: Maybe<Scalars['String']['output']>;\n  destination?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  source?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  value?: Maybe<Array<Scalars['coin']['output']>>;\n};\n\n/** order by min() on columns of table \"routes\" */\nexport type Routes_Min_Order_By = {\n  alias?: InputMaybe<Order_By>;\n  destination?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  source?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"routes\". */\nexport type Routes_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  accountBySource?: InputMaybe<Account_Order_By>;\n  alias?: InputMaybe<Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  destination?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  source?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"routes\" */\nexport enum Routes_Select_Column {\n  /** column name */\n  Alias = 'alias',\n  /** column name */\n  Destination = 'destination',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  Source = 'source',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TransactionHash = 'transaction_hash',\n  /** column name */\n  Value = 'value'\n}\n\n/** aggregate stddev on columns */\nexport type Routes_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"routes\" */\nexport type Routes_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Routes_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"routes\" */\nexport type Routes_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Routes_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"routes\" */\nexport type Routes_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"routes\" */\nexport type Routes_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Routes_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Routes_Stream_Cursor_Value_Input = {\n  alias?: InputMaybe<Scalars['String']['input']>;\n  destination?: InputMaybe<Scalars['String']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  id?: InputMaybe<Scalars['Int']['input']>;\n  source?: InputMaybe<Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n  value?: InputMaybe<Array<Scalars['coin']['input']>>;\n};\n\n/** aggregate sum on columns */\nexport type Routes_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** order by sum() on columns of table \"routes\" */\nexport type Routes_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Routes_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"routes\" */\nexport type Routes_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Routes_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"routes\" */\nexport type Routes_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Routes_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"routes\" */\nexport type Routes_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\nexport type Subscription_Root = {\n  /** fetch data from the table: \"_transaction\" */\n  _transaction: Array<_Transaction>;\n  /** fetch aggregated fields from the table: \"_transaction\" */\n  _transaction_aggregate: _Transaction_Aggregate;\n  /** fetch data from the table in a streaming manner: \"_transaction\" */\n  _transaction_stream: Array<_Transaction>;\n  /** fetch data from the table: \"_uptime_temp\" */\n  _uptime_temp: Array<_Uptime_Temp>;\n  /** fetch aggregated fields from the table: \"_uptime_temp\" */\n  _uptime_temp_aggregate: _Uptime_Temp_Aggregate;\n  /** fetch data from the table in a streaming manner: \"_uptime_temp\" */\n  _uptime_temp_stream: Array<_Uptime_Temp>;\n  /** fetch data from the table: \"account\" */\n  account: Array<Account>;\n  /** fetch aggregated fields from the table: \"account\" */\n  account_aggregate: Account_Aggregate;\n  /** fetch data from the table: \"account_balance\" */\n  account_balance: Array<Account_Balance>;\n  /** fetch aggregated fields from the table: \"account_balance\" */\n  account_balance_aggregate: Account_Balance_Aggregate;\n  /** fetch data from the table: \"account_balance\" using primary key columns */\n  account_balance_by_pk?: Maybe<Account_Balance>;\n  /** fetch data from the table in a streaming manner: \"account_balance\" */\n  account_balance_stream: Array<Account_Balance>;\n  /** fetch data from the table: \"account\" using primary key columns */\n  account_by_pk?: Maybe<Account>;\n  /** fetch data from the table in a streaming manner: \"account\" */\n  account_stream: Array<Account>;\n  /** fetch data from the table: \"average_block_time_from_genesis\" */\n  average_block_time_from_genesis: Array<Average_Block_Time_From_Genesis>;\n  /** fetch aggregated fields from the table: \"average_block_time_from_genesis\" */\n  average_block_time_from_genesis_aggregate: Average_Block_Time_From_Genesis_Aggregate;\n  /** fetch data from the table: \"average_block_time_from_genesis\" using primary key columns */\n  average_block_time_from_genesis_by_pk?: Maybe<Average_Block_Time_From_Genesis>;\n  /** fetch data from the table in a streaming manner: \"average_block_time_from_genesis\" */\n  average_block_time_from_genesis_stream: Array<Average_Block_Time_From_Genesis>;\n  /** fetch data from the table: \"average_block_time_per_day\" */\n  average_block_time_per_day: Array<Average_Block_Time_Per_Day>;\n  /** fetch aggregated fields from the table: \"average_block_time_per_day\" */\n  average_block_time_per_day_aggregate: Average_Block_Time_Per_Day_Aggregate;\n  /** fetch data from the table: \"average_block_time_per_day\" using primary key columns */\n  average_block_time_per_day_by_pk?: Maybe<Average_Block_Time_Per_Day>;\n  /** fetch data from the table in a streaming manner: \"average_block_time_per_day\" */\n  average_block_time_per_day_stream: Array<Average_Block_Time_Per_Day>;\n  /** fetch data from the table: \"average_block_time_per_hour\" */\n  average_block_time_per_hour: Array<Average_Block_Time_Per_Hour>;\n  /** fetch aggregated fields from the table: \"average_block_time_per_hour\" */\n  average_block_time_per_hour_aggregate: Average_Block_Time_Per_Hour_Aggregate;\n  /** fetch data from the table: \"average_block_time_per_hour\" using primary key columns */\n  average_block_time_per_hour_by_pk?: Maybe<Average_Block_Time_Per_Hour>;\n  /** fetch data from the table in a streaming manner: \"average_block_time_per_hour\" */\n  average_block_time_per_hour_stream: Array<Average_Block_Time_Per_Hour>;\n  /** fetch data from the table: \"average_block_time_per_minute\" */\n  average_block_time_per_minute: Array<Average_Block_Time_Per_Minute>;\n  /** fetch aggregated fields from the table: \"average_block_time_per_minute\" */\n  average_block_time_per_minute_aggregate: Average_Block_Time_Per_Minute_Aggregate;\n  /** fetch data from the table: \"average_block_time_per_minute\" using primary key columns */\n  average_block_time_per_minute_by_pk?: Maybe<Average_Block_Time_Per_Minute>;\n  /** fetch data from the table in a streaming manner: \"average_block_time_per_minute\" */\n  average_block_time_per_minute_stream: Array<Average_Block_Time_Per_Minute>;\n  /** fetch data from the table: \"block\" */\n  block: Array<Block>;\n  /** fetch aggregated fields from the table: \"block\" */\n  block_aggregate: Block_Aggregate;\n  /** fetch data from the table: \"block\" using primary key columns */\n  block_by_pk?: Maybe<Block>;\n  /** fetch data from the table in a streaming manner: \"block\" */\n  block_stream: Array<Block>;\n  /** fetch data from the table: \"contracts\" */\n  contracts: Array<Contracts>;\n  /** fetch aggregated fields from the table: \"contracts\" */\n  contracts_aggregate: Contracts_Aggregate;\n  /** fetch data from the table: \"contracts\" using primary key columns */\n  contracts_by_pk?: Maybe<Contracts>;\n  /** fetch data from the table in a streaming manner: \"contracts\" */\n  contracts_stream: Array<Contracts>;\n  /** fetch data from the table: \"cyb_cohort\" */\n  cyb_cohort: Array<Cyb_Cohort>;\n  /** fetch aggregated fields from the table: \"cyb_cohort\" */\n  cyb_cohort_aggregate: Cyb_Cohort_Aggregate;\n  /** fetch data from the table in a streaming manner: \"cyb_cohort\" */\n  cyb_cohort_stream: Array<Cyb_Cohort>;\n  /** fetch data from the table: \"cyber_gift_proofs\" */\n  cyber_gift_proofs: Array<Cyber_Gift_Proofs>;\n  /** fetch aggregated fields from the table: \"cyber_gift_proofs\" */\n  cyber_gift_proofs_aggregate: Cyber_Gift_Proofs_Aggregate;\n  /** fetch data from the table in a streaming manner: \"cyber_gift_proofs\" */\n  cyber_gift_proofs_stream: Array<Cyber_Gift_Proofs>;\n  /** An array relationship */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  /** fetch data from the table: \"cyberlinks\" using primary key columns */\n  cyberlinks_by_pk?: Maybe<Cyberlinks>;\n  /** fetch data from the table: \"cyberlinks_stats\" */\n  cyberlinks_stats: Array<Cyberlinks_Stats>;\n  /** fetch aggregated fields from the table: \"cyberlinks_stats\" */\n  cyberlinks_stats_aggregate: Cyberlinks_Stats_Aggregate;\n  /** fetch data from the table in a streaming manner: \"cyberlinks_stats\" */\n  cyberlinks_stats_stream: Array<Cyberlinks_Stats>;\n  /** fetch data from the table in a streaming manner: \"cyberlinks\" */\n  cyberlinks_stream: Array<Cyberlinks>;\n  /** fetch data from the table: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons: Array<Daily_Amount_Of_Active_Neurons>;\n  /** fetch aggregated fields from the table: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons_aggregate: Daily_Amount_Of_Active_Neurons_Aggregate;\n  /** fetch data from the table in a streaming manner: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons_stream: Array<Daily_Amount_Of_Active_Neurons>;\n  /** fetch data from the table: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas: Array<Daily_Amount_Of_Used_Gas>;\n  /** fetch aggregated fields from the table: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas_aggregate: Daily_Amount_Of_Used_Gas_Aggregate;\n  /** fetch data from the table in a streaming manner: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas_stream: Array<Daily_Amount_Of_Used_Gas>;\n  /** fetch data from the table: \"daily_number_of_transactions\" */\n  daily_number_of_transactions: Array<Daily_Number_Of_Transactions>;\n  /** fetch aggregated fields from the table: \"daily_number_of_transactions\" */\n  daily_number_of_transactions_aggregate: Daily_Number_Of_Transactions_Aggregate;\n  /** fetch data from the table in a streaming manner: \"daily_number_of_transactions\" */\n  daily_number_of_transactions_stream: Array<Daily_Number_Of_Transactions>;\n  /** fetch data from the table: \"first_10_cyberlink\" */\n  first_10_cyberlink: Array<First_10_Cyberlink>;\n  /** fetch aggregated fields from the table: \"first_10_cyberlink\" */\n  first_10_cyberlink_aggregate: First_10_Cyberlink_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_10_cyberlink\" */\n  first_10_cyberlink_stream: Array<First_10_Cyberlink>;\n  /** fetch data from the table: \"first_100_cyberlink\" */\n  first_100_cyberlink: Array<First_100_Cyberlink>;\n  /** fetch aggregated fields from the table: \"first_100_cyberlink\" */\n  first_100_cyberlink_aggregate: First_100_Cyberlink_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_100_cyberlink\" */\n  first_100_cyberlink_stream: Array<First_100_Cyberlink>;\n  /** fetch data from the table: \"first_cyberlink\" */\n  first_cyberlink: Array<First_Cyberlink>;\n  /** fetch aggregated fields from the table: \"first_cyberlink\" */\n  first_cyberlink_aggregate: First_Cyberlink_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_cyberlink\" */\n  first_cyberlink_stream: Array<First_Cyberlink>;\n  /** fetch data from the table: \"first_hero_hired\" */\n  first_hero_hired: Array<First_Hero_Hired>;\n  /** fetch aggregated fields from the table: \"first_hero_hired\" */\n  first_hero_hired_aggregate: First_Hero_Hired_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_hero_hired\" */\n  first_hero_hired_stream: Array<First_Hero_Hired>;\n  /** fetch data from the table: \"first_investmint\" */\n  first_investmint: Array<First_Investmint>;\n  /** fetch aggregated fields from the table: \"first_investmint\" */\n  first_investmint_aggregate: First_Investmint_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_investmint\" */\n  first_investmint_stream: Array<First_Investmint>;\n  /** fetch data from the table: \"first_neuron_activation\" */\n  first_neuron_activation: Array<First_Neuron_Activation>;\n  /** fetch aggregated fields from the table: \"first_neuron_activation\" */\n  first_neuron_activation_aggregate: First_Neuron_Activation_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_neuron_activation\" */\n  first_neuron_activation_stream: Array<First_Neuron_Activation>;\n  /** fetch data from the table: \"first_swap\" */\n  first_swap: Array<First_Swap>;\n  /** fetch aggregated fields from the table: \"first_swap\" */\n  first_swap_aggregate: First_Swap_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_swap\" */\n  first_swap_stream: Array<First_Swap>;\n  /** fetch data from the table: \"follow_stats\" */\n  follow_stats: Array<Follow_Stats>;\n  /** fetch aggregated fields from the table: \"follow_stats\" */\n  follow_stats_aggregate: Follow_Stats_Aggregate;\n  /** fetch data from the table in a streaming manner: \"follow_stats\" */\n  follow_stats_stream: Array<Follow_Stats>;\n  /** fetch data from the table: \"genesis\" */\n  genesis: Array<Genesis>;\n  /** fetch data from the table: \"genesis_accounts\" */\n  genesis_accounts: Array<Genesis_Accounts>;\n  /** fetch aggregated fields from the table: \"genesis_accounts\" */\n  genesis_accounts_aggregate: Genesis_Accounts_Aggregate;\n  /** fetch data from the table: \"genesis_accounts\" using primary key columns */\n  genesis_accounts_by_pk?: Maybe<Genesis_Accounts>;\n  /** fetch data from the table in a streaming manner: \"genesis_accounts\" */\n  genesis_accounts_stream: Array<Genesis_Accounts>;\n  /** fetch aggregated fields from the table: \"genesis\" */\n  genesis_aggregate: Genesis_Aggregate;\n  /** fetch data from the table: \"genesis\" using primary key columns */\n  genesis_by_pk?: Maybe<Genesis>;\n  /** fetch data from the table: \"genesis_neurons_activation\" */\n  genesis_neurons_activation: Array<Genesis_Neurons_Activation>;\n  /** fetch aggregated fields from the table: \"genesis_neurons_activation\" */\n  genesis_neurons_activation_aggregate: Genesis_Neurons_Activation_Aggregate;\n  /** fetch data from the table in a streaming manner: \"genesis_neurons_activation\" */\n  genesis_neurons_activation_stream: Array<Genesis_Neurons_Activation>;\n  /** fetch data from the table in a streaming manner: \"genesis\" */\n  genesis_stream: Array<Genesis>;\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  /** fetch data from the table: \"investmints\" using primary key columns */\n  investmints_by_pk?: Maybe<Investmints>;\n  /** fetch data from the table in a streaming manner: \"investmints\" */\n  investmints_stream: Array<Investmints>;\n  /** fetch data from the table: \"message\" */\n  message: Array<Message>;\n  /** fetch data from the table: \"message_155\" */\n  message_155: Array<Message_155>;\n  /** fetch aggregated fields from the table: \"message_155\" */\n  message_155_aggregate: Message_155_Aggregate;\n  /** fetch data from the table in a streaming manner: \"message_155\" */\n  message_155_stream: Array<Message_155>;\n  /** fetch aggregated fields from the table: \"message\" */\n  message_aggregate: Message_Aggregate;\n  /** fetch data from the table in a streaming manner: \"message\" */\n  message_stream: Array<Message>;\n  /** fetch data from the table: \"message_type\" */\n  message_type: Array<Message_Type>;\n  /** fetch aggregated fields from the table: \"message_type\" */\n  message_type_aggregate: Message_Type_Aggregate;\n  /** fetch data from the table in a streaming manner: \"message_type\" */\n  message_type_stream: Array<Message_Type>;\n  /** execute function \"messages_by_address\" which returns \"message\" */\n  messages_by_address: Array<Message>;\n  /** execute function \"messages_by_address\" and query aggregates on result of table type \"message\" */\n  messages_by_address_aggregate: Message_Aggregate;\n  /** execute function \"messages_by_type\" which returns \"message\" */\n  messages_by_type: Array<Message>;\n  /** execute function \"messages_by_type\" and query aggregates on result of table type \"message\" */\n  messages_by_type_aggregate: Message_Aggregate;\n  /** fetch data from the table: \"modules\" */\n  modules: Array<Modules>;\n  /** fetch aggregated fields from the table: \"modules\" */\n  modules_aggregate: Modules_Aggregate;\n  /** fetch data from the table: \"modules\" using primary key columns */\n  modules_by_pk?: Maybe<Modules>;\n  /** fetch data from the table in a streaming manner: \"modules\" */\n  modules_stream: Array<Modules>;\n  /** fetch data from the table: \"neuron_activation_source\" */\n  neuron_activation_source: Array<Neuron_Activation_Source>;\n  /** fetch aggregated fields from the table: \"neuron_activation_source\" */\n  neuron_activation_source_aggregate: Neuron_Activation_Source_Aggregate;\n  /** fetch data from the table in a streaming manner: \"neuron_activation_source\" */\n  neuron_activation_source_stream: Array<Neuron_Activation_Source>;\n  /** fetch data from the table: \"number_of_new_neurons\" */\n  number_of_new_neurons: Array<Number_Of_New_Neurons>;\n  /** fetch aggregated fields from the table: \"number_of_new_neurons\" */\n  number_of_new_neurons_aggregate: Number_Of_New_Neurons_Aggregate;\n  /** fetch data from the table in a streaming manner: \"number_of_new_neurons\" */\n  number_of_new_neurons_stream: Array<Number_Of_New_Neurons>;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  /** fetch data from the table: \"particles\" using primary key columns */\n  particles_by_pk?: Maybe<Particles>;\n  /** fetch data from the table in a streaming manner: \"particles\" */\n  particles_stream: Array<Particles>;\n  /** fetch data from the table: \"pools\" */\n  pools: Array<Pools>;\n  /** fetch aggregated fields from the table: \"pools\" */\n  pools_aggregate: Pools_Aggregate;\n  /** fetch data from the table: \"pools\" using primary key columns */\n  pools_by_pk?: Maybe<Pools>;\n  /** fetch data from the table: \"pools_liquidity\" */\n  pools_liquidity: Array<Pools_Liquidity>;\n  /** fetch aggregated fields from the table: \"pools_liquidity\" */\n  pools_liquidity_aggregate: Pools_Liquidity_Aggregate;\n  /** fetch data from the table in a streaming manner: \"pools_liquidity\" */\n  pools_liquidity_stream: Array<Pools_Liquidity>;\n  /** fetch data from the table: \"pools_rates\" */\n  pools_rates: Array<Pools_Rates>;\n  /** fetch aggregated fields from the table: \"pools_rates\" */\n  pools_rates_aggregate: Pools_Rates_Aggregate;\n  /** fetch data from the table in a streaming manner: \"pools_rates\" */\n  pools_rates_stream: Array<Pools_Rates>;\n  /** fetch data from the table in a streaming manner: \"pools\" */\n  pools_stream: Array<Pools>;\n  /** fetch data from the table: \"pools_volumes\" */\n  pools_volumes: Array<Pools_Volumes>;\n  /** fetch aggregated fields from the table: \"pools_volumes\" */\n  pools_volumes_aggregate: Pools_Volumes_Aggregate;\n  /** fetch data from the table in a streaming manner: \"pools_volumes\" */\n  pools_volumes_stream: Array<Pools_Volumes>;\n  /** fetch data from the table: \"pre_commit\" */\n  pre_commit: Array<Pre_Commit>;\n  /** fetch aggregated fields from the table: \"pre_commit\" */\n  pre_commit_aggregate: Pre_Commit_Aggregate;\n  /** fetch data from the table in a streaming manner: \"pre_commit\" */\n  pre_commit_stream: Array<Pre_Commit>;\n  /** fetch data from the table: \"pruning\" */\n  pruning: Array<Pruning>;\n  /** fetch aggregated fields from the table: \"pruning\" */\n  pruning_aggregate: Pruning_Aggregate;\n  /** fetch data from the table in a streaming manner: \"pruning\" */\n  pruning_stream: Array<Pruning>;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n  /** fetch data from the table: \"routes\" using primary key columns */\n  routes_by_pk?: Maybe<Routes>;\n  /** fetch data from the table in a streaming manner: \"routes\" */\n  routes_stream: Array<Routes>;\n  /** fetch data from the table: \"supply\" */\n  supply: Array<Supply>;\n  /** fetch aggregated fields from the table: \"supply\" */\n  supply_aggregate: Supply_Aggregate;\n  /** fetch data from the table: \"supply\" using primary key columns */\n  supply_by_pk?: Maybe<Supply>;\n  /** fetch data from the table in a streaming manner: \"supply\" */\n  supply_stream: Array<Supply>;\n  /** An array relationship */\n  swaps: Array<Swaps>;\n  /** An aggregate relationship */\n  swaps_aggregate: Swaps_Aggregate;\n  /** fetch data from the table: \"swaps\" using primary key columns */\n  swaps_by_pk?: Maybe<Swaps>;\n  /** fetch data from the table in a streaming manner: \"swaps\" */\n  swaps_stream: Array<Swaps>;\n  /** fetch data from the table: \"today_top_txs\" */\n  today_top_txs: Array<Today_Top_Txs>;\n  /** fetch aggregated fields from the table: \"today_top_txs\" */\n  today_top_txs_aggregate: Today_Top_Txs_Aggregate;\n  /** fetch data from the table in a streaming manner: \"today_top_txs\" */\n  today_top_txs_stream: Array<Today_Top_Txs>;\n  /** fetch data from the table: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week: Array<Top_10_Of_Active_Neurons_Week>;\n  /** fetch aggregated fields from the table: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week_aggregate: Top_10_Of_Active_Neurons_Week_Aggregate;\n  /** fetch data from the table in a streaming manner: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week_stream: Array<Top_10_Of_Active_Neurons_Week>;\n  /** fetch data from the table: \"top_first_txs\" */\n  top_first_txs: Array<Top_First_Txs>;\n  /** fetch aggregated fields from the table: \"top_first_txs\" */\n  top_first_txs_aggregate: Top_First_Txs_Aggregate;\n  /** fetch data from the table in a streaming manner: \"top_first_txs\" */\n  top_first_txs_stream: Array<Top_First_Txs>;\n  /** fetch data from the table: \"top_leaders\" */\n  top_leaders: Array<Top_Leaders>;\n  /** fetch aggregated fields from the table: \"top_leaders\" */\n  top_leaders_aggregate: Top_Leaders_Aggregate;\n  /** fetch data from the table in a streaming manner: \"top_leaders\" */\n  top_leaders_stream: Array<Top_Leaders>;\n  /** fetch data from the table: \"top_txs\" */\n  top_txs: Array<Top_Txs>;\n  /** fetch aggregated fields from the table: \"top_txs\" */\n  top_txs_aggregate: Top_Txs_Aggregate;\n  /** fetch data from the table in a streaming manner: \"top_txs\" */\n  top_txs_stream: Array<Top_Txs>;\n  /** fetch data from the table: \"transaction\" */\n  transaction: Array<Transaction>;\n  /** fetch data from the table: \"transaction_155\" */\n  transaction_155: Array<Transaction_155>;\n  /** fetch aggregated fields from the table: \"transaction_155\" */\n  transaction_155_aggregate: Transaction_155_Aggregate;\n  /** fetch data from the table in a streaming manner: \"transaction_155\" */\n  transaction_155_stream: Array<Transaction_155>;\n  /** fetch aggregated fields from the table: \"transaction\" */\n  transaction_aggregate: Transaction_Aggregate;\n  /** fetch data from the table in a streaming manner: \"transaction\" */\n  transaction_stream: Array<Transaction>;\n  /** fetch data from the table: \"tweets_stats\" */\n  tweets_stats: Array<Tweets_Stats>;\n  /** fetch aggregated fields from the table: \"tweets_stats\" */\n  tweets_stats_aggregate: Tweets_Stats_Aggregate;\n  /** fetch data from the table in a streaming manner: \"tweets_stats\" */\n  tweets_stats_stream: Array<Tweets_Stats>;\n  /** fetch data from the table: \"txs_ranked\" */\n  txs_ranked: Array<Txs_Ranked>;\n  /** fetch aggregated fields from the table: \"txs_ranked\" */\n  txs_ranked_aggregate: Txs_Ranked_Aggregate;\n  /** fetch data from the table in a streaming manner: \"txs_ranked\" */\n  txs_ranked_stream: Array<Txs_Ranked>;\n  /** fetch data from the table: \"uptime\" */\n  uptime: Array<Uptime>;\n  /** fetch aggregated fields from the table: \"uptime\" */\n  uptime_aggregate: Uptime_Aggregate;\n  /** fetch data from the table in a streaming manner: \"uptime\" */\n  uptime_stream: Array<Uptime>;\n  /** fetch data from the table: \"validator\" */\n  validator: Array<Validator>;\n  /** fetch aggregated fields from the table: \"validator\" */\n  validator_aggregate: Validator_Aggregate;\n  /** fetch data from the table: \"validator\" using primary key columns */\n  validator_by_pk?: Maybe<Validator>;\n  /** fetch data from the table in a streaming manner: \"validator\" */\n  validator_stream: Array<Validator>;\n  /** fetch data from the table: \"vesting_account\" */\n  vesting_account: Array<Vesting_Account>;\n  /** fetch aggregated fields from the table: \"vesting_account\" */\n  vesting_account_aggregate: Vesting_Account_Aggregate;\n  /** fetch data from the table: \"vesting_account\" using primary key columns */\n  vesting_account_by_pk?: Maybe<Vesting_Account>;\n  /** fetch data from the table in a streaming manner: \"vesting_account\" */\n  vesting_account_stream: Array<Vesting_Account>;\n  /** fetch data from the table: \"vesting_period\" */\n  vesting_period: Array<Vesting_Period>;\n  /** fetch aggregated fields from the table: \"vesting_period\" */\n  vesting_period_aggregate: Vesting_Period_Aggregate;\n  /** fetch data from the table in a streaming manner: \"vesting_period\" */\n  vesting_period_stream: Array<Vesting_Period>;\n  /** fetch data from the table: \"week_redelegation\" */\n  week_redelegation: Array<Week_Redelegation>;\n  /** fetch aggregated fields from the table: \"week_redelegation\" */\n  week_redelegation_aggregate: Week_Redelegation_Aggregate;\n  /** fetch data from the table in a streaming manner: \"week_redelegation\" */\n  week_redelegation_stream: Array<Week_Redelegation>;\n  /** fetch data from the table: \"week_undelegation\" */\n  week_undelegation: Array<Week_Undelegation>;\n  /** fetch aggregated fields from the table: \"week_undelegation\" */\n  week_undelegation_aggregate: Week_Undelegation_Aggregate;\n  /** fetch data from the table in a streaming manner: \"week_undelegation\" */\n  week_undelegation_stream: Array<Week_Undelegation>;\n};\n\n\nexport type Subscription_Root_TransactionArgs = {\n  distinct_on?: InputMaybe<Array<_Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Transaction_Order_By>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Transaction_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<_Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Transaction_Order_By>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Transaction_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<_Transaction_Stream_Cursor_Input>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Uptime_TempArgs = {\n  distinct_on?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Uptime_Temp_Order_By>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Uptime_Temp_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Uptime_Temp_Order_By>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Uptime_Temp_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<_Uptime_Temp_Stream_Cursor_Input>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccountArgs = {\n  distinct_on?: InputMaybe<Array<Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Order_By>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Order_By>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_BalanceArgs = {\n  distinct_on?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Balance_Order_By>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_Balance_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Balance_Order_By>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_Balance_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootAccount_Balance_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Account_Balance_Stream_Cursor_Input>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootAccount_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Account_Stream_Cursor_Input>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_From_GenesisArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_From_Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_From_Genesis_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_From_Genesis_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_From_Genesis_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_From_Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_From_Genesis_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_From_Genesis_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_From_Genesis_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootAverage_Block_Time_From_Genesis_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Average_Block_Time_From_Genesis_Stream_Cursor_Input>>;\n  where?: InputMaybe<Average_Block_Time_From_Genesis_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_DayArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Day_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Day_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Day_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Day_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Day_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Day_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Day_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Day_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Day_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Average_Block_Time_Per_Day_Stream_Cursor_Input>>;\n  where?: InputMaybe<Average_Block_Time_Per_Day_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_HourArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Hour_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Hour_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Hour_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Hour_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Hour_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Hour_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Hour_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Hour_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Hour_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Average_Block_Time_Per_Hour_Stream_Cursor_Input>>;\n  where?: InputMaybe<Average_Block_Time_Per_Hour_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_MinuteArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Minute_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Minute_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Minute_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Minute_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Minute_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Minute_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Minute_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Minute_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Minute_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Average_Block_Time_Per_Minute_Stream_Cursor_Input>>;\n  where?: InputMaybe<Average_Block_Time_Per_Minute_Bool_Exp>;\n};\n\n\nexport type Subscription_RootBlockArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Subscription_RootBlock_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Subscription_RootBlock_By_PkArgs = {\n  height: Scalars['bigint']['input'];\n};\n\n\nexport type Subscription_RootBlock_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Block_Stream_Cursor_Input>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Subscription_RootContractsArgs = {\n  distinct_on?: InputMaybe<Array<Contracts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Contracts_Order_By>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootContracts_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Contracts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Contracts_Order_By>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootContracts_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootContracts_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Contracts_Stream_Cursor_Input>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyb_CohortArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyb_Cohort_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyb_Cohort_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Cyb_Cohort_Stream_Cursor_Input>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyber_Gift_ProofsArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyber_Gift_Proofs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyber_Gift_Proofs_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Cyber_Gift_Proofs_Stream_Cursor_Input>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootCyberlinks_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Stats_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Stats_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_Stats_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Cyberlinks_Stats_Stream_Cursor_Input>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Cyberlinks_Stream_Cursor_Input>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Active_NeuronsArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Active_Neurons_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Active_Neurons_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Daily_Amount_Of_Active_Neurons_Stream_Cursor_Input>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Used_GasArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Used_Gas_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Used_Gas_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Daily_Amount_Of_Used_Gas_Stream_Cursor_Input>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Number_Of_TransactionsArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Number_Of_Transactions_Order_By>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Number_Of_Transactions_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Number_Of_Transactions_Order_By>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Number_Of_Transactions_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Daily_Number_Of_Transactions_Stream_Cursor_Input>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_10_CyberlinkArgs = {\n  distinct_on?: InputMaybe<Array<First_10_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_10_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_10_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_10_Cyberlink_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_10_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_10_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_10_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_10_Cyberlink_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_10_Cyberlink_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_10_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_100_CyberlinkArgs = {\n  distinct_on?: InputMaybe<Array<First_100_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_100_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_100_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_100_Cyberlink_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_100_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_100_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_100_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_100_Cyberlink_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_100_Cyberlink_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_100_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_CyberlinkArgs = {\n  distinct_on?: InputMaybe<Array<First_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Cyberlink_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Cyberlink_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_Cyberlink_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Hero_HiredArgs = {\n  distinct_on?: InputMaybe<Array<First_Hero_Hired_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Hero_Hired_Order_By>>;\n  where?: InputMaybe<First_Hero_Hired_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Hero_Hired_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Hero_Hired_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Hero_Hired_Order_By>>;\n  where?: InputMaybe<First_Hero_Hired_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Hero_Hired_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_Hero_Hired_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_Hero_Hired_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_InvestmintArgs = {\n  distinct_on?: InputMaybe<Array<First_Investmint_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Investmint_Order_By>>;\n  where?: InputMaybe<First_Investmint_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Investmint_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Investmint_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Investmint_Order_By>>;\n  where?: InputMaybe<First_Investmint_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Investmint_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_Investmint_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_Investmint_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Neuron_ActivationArgs = {\n  distinct_on?: InputMaybe<Array<First_Neuron_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Neuron_Activation_Order_By>>;\n  where?: InputMaybe<First_Neuron_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Neuron_Activation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Neuron_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Neuron_Activation_Order_By>>;\n  where?: InputMaybe<First_Neuron_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Neuron_Activation_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_Neuron_Activation_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_Neuron_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_SwapArgs = {\n  distinct_on?: InputMaybe<Array<First_Swap_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Swap_Order_By>>;\n  where?: InputMaybe<First_Swap_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Swap_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Swap_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Swap_Order_By>>;\n  where?: InputMaybe<First_Swap_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Swap_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_Swap_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_Swap_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFollow_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Follow_Stats_Order_By>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFollow_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Follow_Stats_Order_By>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFollow_Stats_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Follow_Stats_Stream_Cursor_Input>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesisArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Order_By>>;\n  where?: InputMaybe<Genesis_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_AccountsArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Accounts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Accounts_Order_By>>;\n  where?: InputMaybe<Genesis_Accounts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_Accounts_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Accounts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Accounts_Order_By>>;\n  where?: InputMaybe<Genesis_Accounts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_Accounts_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootGenesis_Accounts_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Genesis_Accounts_Stream_Cursor_Input>>;\n  where?: InputMaybe<Genesis_Accounts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Order_By>>;\n  where?: InputMaybe<Genesis_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootGenesis_Neurons_ActivationArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Neurons_Activation_Order_By>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_Neurons_Activation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Neurons_Activation_Order_By>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_Neurons_Activation_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Genesis_Neurons_Activation_Stream_Cursor_Input>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Genesis_Stream_Cursor_Input>>;\n  where?: InputMaybe<Genesis_Bool_Exp>;\n};\n\n\nexport type Subscription_RootInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Subscription_RootInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Subscription_RootInvestmints_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootInvestmints_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Investmints_Stream_Cursor_Input>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessageArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_155Args = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_155_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_155_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Message_155_Stream_Cursor_Input>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Message_Stream_Cursor_Input>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_TypeArgs = {\n  distinct_on?: InputMaybe<Array<Message_Type_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Type_Order_By>>;\n  where?: InputMaybe<Message_Type_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_Type_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Type_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Type_Order_By>>;\n  where?: InputMaybe<Message_Type_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_Type_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Message_Type_Stream_Cursor_Input>>;\n  where?: InputMaybe<Message_Type_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessages_By_AddressArgs = {\n  args: Messages_By_Address_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessages_By_Address_AggregateArgs = {\n  args: Messages_By_Address_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessages_By_TypeArgs = {\n  args: Messages_By_Type_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessages_By_Type_AggregateArgs = {\n  args: Messages_By_Type_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootModulesArgs = {\n  distinct_on?: InputMaybe<Array<Modules_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Modules_Order_By>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Subscription_RootModules_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Modules_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Modules_Order_By>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Subscription_RootModules_By_PkArgs = {\n  module_name: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootModules_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Modules_Stream_Cursor_Input>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNeuron_Activation_SourceArgs = {\n  distinct_on?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Neuron_Activation_Source_Order_By>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNeuron_Activation_Source_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Neuron_Activation_Source_Order_By>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNeuron_Activation_Source_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Neuron_Activation_Source_Stream_Cursor_Input>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNumber_Of_New_NeuronsArgs = {\n  distinct_on?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Number_Of_New_Neurons_Order_By>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNumber_Of_New_Neurons_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Number_Of_New_Neurons_Order_By>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNumber_Of_New_Neurons_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Number_Of_New_Neurons_Stream_Cursor_Input>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Subscription_RootParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Subscription_RootParticles_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootParticles_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Particles_Stream_Cursor_Input>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPoolsArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Order_By>>;\n  where?: InputMaybe<Pools_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Order_By>>;\n  where?: InputMaybe<Pools_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_By_PkArgs = {\n  pool_id: Scalars['bigint']['input'];\n};\n\n\nexport type Subscription_RootPools_LiquidityArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Liquidity_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Liquidity_Order_By>>;\n  where?: InputMaybe<Pools_Liquidity_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_Liquidity_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Liquidity_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Liquidity_Order_By>>;\n  where?: InputMaybe<Pools_Liquidity_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_Liquidity_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Pools_Liquidity_Stream_Cursor_Input>>;\n  where?: InputMaybe<Pools_Liquidity_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_RatesArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Rates_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Rates_Order_By>>;\n  where?: InputMaybe<Pools_Rates_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_Rates_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Rates_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Rates_Order_By>>;\n  where?: InputMaybe<Pools_Rates_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_Rates_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Pools_Rates_Stream_Cursor_Input>>;\n  where?: InputMaybe<Pools_Rates_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Pools_Stream_Cursor_Input>>;\n  where?: InputMaybe<Pools_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_VolumesArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Volumes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Volumes_Order_By>>;\n  where?: InputMaybe<Pools_Volumes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_Volumes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Volumes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Volumes_Order_By>>;\n  where?: InputMaybe<Pools_Volumes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_Volumes_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Pools_Volumes_Stream_Cursor_Input>>;\n  where?: InputMaybe<Pools_Volumes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_CommitArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_Commit_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_Commit_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Pre_Commit_Stream_Cursor_Input>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPruningArgs = {\n  distinct_on?: InputMaybe<Array<Pruning_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pruning_Order_By>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPruning_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pruning_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pruning_Order_By>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPruning_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Pruning_Stream_Cursor_Input>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Subscription_RootRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootRoutes_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootRoutes_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Routes_Stream_Cursor_Input>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSupplyArgs = {\n  distinct_on?: InputMaybe<Array<Supply_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Supply_Order_By>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSupply_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Supply_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Supply_Order_By>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSupply_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootSupply_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Supply_Stream_Cursor_Input>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSwapsArgs = {\n  distinct_on?: InputMaybe<Array<Swaps_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Swaps_Order_By>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSwaps_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Swaps_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Swaps_Order_By>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSwaps_By_PkArgs = {\n  pool_id: Scalars['bigint']['input'];\n};\n\n\nexport type Subscription_RootSwaps_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Swaps_Stream_Cursor_Input>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\nexport type Subscription_RootToday_Top_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Today_Top_Txs_Order_By>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootToday_Top_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Today_Top_Txs_Order_By>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootToday_Top_Txs_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Today_Top_Txs_Stream_Cursor_Input>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_10_Of_Active_Neurons_WeekArgs = {\n  distinct_on?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Order_By>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_10_Of_Active_Neurons_Week_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Order_By>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_10_Of_Active_Neurons_Week_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Top_10_Of_Active_Neurons_Week_Stream_Cursor_Input>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_First_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_First_Txs_Order_By>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_First_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_First_Txs_Order_By>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_First_Txs_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Top_First_Txs_Stream_Cursor_Input>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_LeadersArgs = {\n  distinct_on?: InputMaybe<Array<Top_Leaders_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Leaders_Order_By>>;\n  where?: InputMaybe<Top_Leaders_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_Leaders_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_Leaders_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Leaders_Order_By>>;\n  where?: InputMaybe<Top_Leaders_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_Leaders_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Top_Leaders_Stream_Cursor_Input>>;\n  where?: InputMaybe<Top_Leaders_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Txs_Order_By>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Txs_Order_By>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_Txs_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Top_Txs_Stream_Cursor_Input>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransactionArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransaction_155Args = {\n  distinct_on?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_155_Order_By>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransaction_155_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_155_Order_By>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransaction_155_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Transaction_155_Stream_Cursor_Input>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransaction_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransaction_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Transaction_Stream_Cursor_Input>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTweets_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Tweets_Stats_Order_By>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTweets_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Tweets_Stats_Order_By>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTweets_Stats_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Tweets_Stats_Stream_Cursor_Input>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTxs_RankedArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Ranked_Order_By>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTxs_Ranked_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Ranked_Order_By>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTxs_Ranked_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Txs_Ranked_Stream_Cursor_Input>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Subscription_RootUptimeArgs = {\n  distinct_on?: InputMaybe<Array<Uptime_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Uptime_Order_By>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Subscription_RootUptime_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Uptime_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Uptime_Order_By>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Subscription_RootUptime_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Uptime_Stream_Cursor_Input>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Subscription_RootValidatorArgs = {\n  distinct_on?: InputMaybe<Array<Validator_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Validator_Order_By>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Subscription_RootValidator_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Validator_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Validator_Order_By>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Subscription_RootValidator_By_PkArgs = {\n  consensus_address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootValidator_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Validator_Stream_Cursor_Input>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVesting_AccountArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Account_Order_By>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVesting_Account_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Account_Order_By>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVesting_Account_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootVesting_Account_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Vesting_Account_Stream_Cursor_Input>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVesting_PeriodArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Period_Order_By>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVesting_Period_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Period_Order_By>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVesting_Period_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Vesting_Period_Stream_Cursor_Input>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n\nexport type Subscription_RootWeek_RedelegationArgs = {\n  distinct_on?: InputMaybe<Array<Week_Redelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Redelegation_Order_By>>;\n  where?: InputMaybe<Week_Redelegation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootWeek_Redelegation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Week_Redelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Redelegation_Order_By>>;\n  where?: InputMaybe<Week_Redelegation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootWeek_Redelegation_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Week_Redelegation_Stream_Cursor_Input>>;\n  where?: InputMaybe<Week_Redelegation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootWeek_UndelegationArgs = {\n  distinct_on?: InputMaybe<Array<Week_Undelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Undelegation_Order_By>>;\n  where?: InputMaybe<Week_Undelegation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootWeek_Undelegation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Week_Undelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Undelegation_Order_By>>;\n  where?: InputMaybe<Week_Undelegation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootWeek_Undelegation_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Week_Undelegation_Stream_Cursor_Input>>;\n  where?: InputMaybe<Week_Undelegation_Bool_Exp>;\n};\n\n/** columns and relationships of \"supply\" */\nexport type Supply = {\n  coins: Array<Scalars['coin']['output']>;\n  height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n};\n\n/** aggregated selection of \"supply\" */\nexport type Supply_Aggregate = {\n  aggregate?: Maybe<Supply_Aggregate_Fields>;\n  nodes: Array<Supply>;\n};\n\n/** aggregate fields of \"supply\" */\nexport type Supply_Aggregate_Fields = {\n  avg?: Maybe<Supply_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Supply_Max_Fields>;\n  min?: Maybe<Supply_Min_Fields>;\n  stddev?: Maybe<Supply_Stddev_Fields>;\n  stddev_pop?: Maybe<Supply_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Supply_Stddev_Samp_Fields>;\n  sum?: Maybe<Supply_Sum_Fields>;\n  var_pop?: Maybe<Supply_Var_Pop_Fields>;\n  var_samp?: Maybe<Supply_Var_Samp_Fields>;\n  variance?: Maybe<Supply_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"supply\" */\nexport type Supply_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Supply_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Supply_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"supply\". All fields are combined with a logical 'AND'. */\nexport type Supply_Bool_Exp = {\n  _and?: InputMaybe<Array<Supply_Bool_Exp>>;\n  _not?: InputMaybe<Supply_Bool_Exp>;\n  _or?: InputMaybe<Array<Supply_Bool_Exp>>;\n  coins?: InputMaybe<Coin_Array_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Supply_Max_Fields = {\n  coins?: Maybe<Array<Scalars['coin']['output']>>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Supply_Min_Fields = {\n  coins?: Maybe<Array<Scalars['coin']['output']>>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"supply\". */\nexport type Supply_Order_By = {\n  coins?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"supply\" */\nexport enum Supply_Select_Column {\n  /** column name */\n  Coins = 'coins',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  OneRowId = 'one_row_id'\n}\n\n/** aggregate stddev on columns */\nexport type Supply_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Supply_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Supply_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"supply\" */\nexport type Supply_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Supply_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Supply_Stream_Cursor_Value_Input = {\n  coins?: InputMaybe<Array<Scalars['coin']['input']>>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  one_row_id?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Supply_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Supply_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Supply_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Supply_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"swaps\" */\nexport type Swaps = {\n  address: Scalars['String']['output'];\n  /** An object relationship */\n  block: Block;\n  exchanged_demand_coin: Scalars['coin_scalar']['output'];\n  exchanged_demand_coin_fee: Scalars['coin_scalar']['output'];\n  exchanged_offer_coin: Scalars['coin_scalar']['output'];\n  exchanged_offer_coin_fee: Scalars['coin_scalar']['output'];\n  height: Scalars['bigint']['output'];\n  pool_id: Scalars['bigint']['output'];\n  swap_price: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"swaps\" */\nexport type Swaps_Aggregate = {\n  aggregate?: Maybe<Swaps_Aggregate_Fields>;\n  nodes: Array<Swaps>;\n};\n\nexport type Swaps_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Swaps_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Swaps_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Swaps_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Swaps_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"swaps\" */\nexport type Swaps_Aggregate_Fields = {\n  avg?: Maybe<Swaps_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Swaps_Max_Fields>;\n  min?: Maybe<Swaps_Min_Fields>;\n  stddev?: Maybe<Swaps_Stddev_Fields>;\n  stddev_pop?: Maybe<Swaps_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Swaps_Stddev_Samp_Fields>;\n  sum?: Maybe<Swaps_Sum_Fields>;\n  var_pop?: Maybe<Swaps_Var_Pop_Fields>;\n  var_samp?: Maybe<Swaps_Var_Samp_Fields>;\n  variance?: Maybe<Swaps_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"swaps\" */\nexport type Swaps_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Swaps_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"swaps\" */\nexport type Swaps_Aggregate_Order_By = {\n  avg?: InputMaybe<Swaps_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Swaps_Max_Order_By>;\n  min?: InputMaybe<Swaps_Min_Order_By>;\n  stddev?: InputMaybe<Swaps_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Swaps_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Swaps_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Swaps_Sum_Order_By>;\n  var_pop?: InputMaybe<Swaps_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Swaps_Var_Samp_Order_By>;\n  variance?: InputMaybe<Swaps_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Swaps_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"swaps\" */\nexport type Swaps_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"swaps\". All fields are combined with a logical 'AND'. */\nexport type Swaps_Bool_Exp = {\n  _and?: InputMaybe<Array<Swaps_Bool_Exp>>;\n  _not?: InputMaybe<Swaps_Bool_Exp>;\n  _or?: InputMaybe<Array<Swaps_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  exchanged_demand_coin?: InputMaybe<Coin_Scalar_Comparison_Exp>;\n  exchanged_demand_coin_fee?: InputMaybe<Coin_Scalar_Comparison_Exp>;\n  exchanged_offer_coin?: InputMaybe<Coin_Scalar_Comparison_Exp>;\n  exchanged_offer_coin_fee?: InputMaybe<Coin_Scalar_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  pool_id?: InputMaybe<Bigint_Comparison_Exp>;\n  swap_price?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Swaps_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  exchanged_demand_coin?: Maybe<Scalars['coin_scalar']['output']>;\n  exchanged_demand_coin_fee?: Maybe<Scalars['coin_scalar']['output']>;\n  exchanged_offer_coin?: Maybe<Scalars['coin_scalar']['output']>;\n  exchanged_offer_coin_fee?: Maybe<Scalars['coin_scalar']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  swap_price?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"swaps\" */\nexport type Swaps_Max_Order_By = {\n  address?: InputMaybe<Order_By>;\n  exchanged_demand_coin?: InputMaybe<Order_By>;\n  exchanged_demand_coin_fee?: InputMaybe<Order_By>;\n  exchanged_offer_coin?: InputMaybe<Order_By>;\n  exchanged_offer_coin_fee?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n  swap_price?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Swaps_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  exchanged_demand_coin?: Maybe<Scalars['coin_scalar']['output']>;\n  exchanged_demand_coin_fee?: Maybe<Scalars['coin_scalar']['output']>;\n  exchanged_offer_coin?: Maybe<Scalars['coin_scalar']['output']>;\n  exchanged_offer_coin_fee?: Maybe<Scalars['coin_scalar']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  swap_price?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"swaps\" */\nexport type Swaps_Min_Order_By = {\n  address?: InputMaybe<Order_By>;\n  exchanged_demand_coin?: InputMaybe<Order_By>;\n  exchanged_demand_coin_fee?: InputMaybe<Order_By>;\n  exchanged_offer_coin?: InputMaybe<Order_By>;\n  exchanged_offer_coin_fee?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n  swap_price?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"swaps\". */\nexport type Swaps_Order_By = {\n  address?: InputMaybe<Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  exchanged_demand_coin?: InputMaybe<Order_By>;\n  exchanged_demand_coin_fee?: InputMaybe<Order_By>;\n  exchanged_offer_coin?: InputMaybe<Order_By>;\n  exchanged_offer_coin_fee?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n  swap_price?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"swaps\" */\nexport enum Swaps_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  ExchangedDemandCoin = 'exchanged_demand_coin',\n  /** column name */\n  ExchangedDemandCoinFee = 'exchanged_demand_coin_fee',\n  /** column name */\n  ExchangedOfferCoin = 'exchanged_offer_coin',\n  /** column name */\n  ExchangedOfferCoinFee = 'exchanged_offer_coin_fee',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  PoolId = 'pool_id',\n  /** column name */\n  SwapPrice = 'swap_price'\n}\n\n/** aggregate stddev on columns */\nexport type Swaps_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"swaps\" */\nexport type Swaps_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Swaps_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"swaps\" */\nexport type Swaps_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Swaps_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"swaps\" */\nexport type Swaps_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"swaps\" */\nexport type Swaps_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Swaps_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Swaps_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n  exchanged_demand_coin?: InputMaybe<Scalars['coin_scalar']['input']>;\n  exchanged_demand_coin_fee?: InputMaybe<Scalars['coin_scalar']['input']>;\n  exchanged_offer_coin?: InputMaybe<Scalars['coin_scalar']['input']>;\n  exchanged_offer_coin_fee?: InputMaybe<Scalars['coin_scalar']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  pool_id?: InputMaybe<Scalars['bigint']['input']>;\n  swap_price?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Swaps_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"swaps\" */\nexport type Swaps_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Swaps_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"swaps\" */\nexport type Swaps_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Swaps_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"swaps\" */\nexport type Swaps_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Swaps_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"swaps\" */\nexport type Swaps_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to compare columns of type \"timestamp\". All fields are combined with logical 'AND'. */\nexport type Timestamp_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['timestamp']['input']>;\n  _gt?: InputMaybe<Scalars['timestamp']['input']>;\n  _gte?: InputMaybe<Scalars['timestamp']['input']>;\n  _in?: InputMaybe<Array<Scalars['timestamp']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['timestamp']['input']>;\n  _lte?: InputMaybe<Scalars['timestamp']['input']>;\n  _neq?: InputMaybe<Scalars['timestamp']['input']>;\n  _nin?: InputMaybe<Array<Scalars['timestamp']['input']>>;\n};\n\n/** columns and relationships of \"today_top_txs\" */\nexport type Today_Top_Txs = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"today_top_txs\" */\nexport type Today_Top_Txs_Aggregate = {\n  aggregate?: Maybe<Today_Top_Txs_Aggregate_Fields>;\n  nodes: Array<Today_Top_Txs>;\n};\n\n/** aggregate fields of \"today_top_txs\" */\nexport type Today_Top_Txs_Aggregate_Fields = {\n  avg?: Maybe<Today_Top_Txs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Today_Top_Txs_Max_Fields>;\n  min?: Maybe<Today_Top_Txs_Min_Fields>;\n  stddev?: Maybe<Today_Top_Txs_Stddev_Fields>;\n  stddev_pop?: Maybe<Today_Top_Txs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Today_Top_Txs_Stddev_Samp_Fields>;\n  sum?: Maybe<Today_Top_Txs_Sum_Fields>;\n  var_pop?: Maybe<Today_Top_Txs_Var_Pop_Fields>;\n  var_samp?: Maybe<Today_Top_Txs_Var_Samp_Fields>;\n  variance?: Maybe<Today_Top_Txs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"today_top_txs\" */\nexport type Today_Top_Txs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Today_Top_Txs_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"today_top_txs\". All fields are combined with a logical 'AND'. */\nexport type Today_Top_Txs_Bool_Exp = {\n  _and?: InputMaybe<Array<Today_Top_Txs_Bool_Exp>>;\n  _not?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n  _or?: InputMaybe<Array<Today_Top_Txs_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Today_Top_Txs_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Today_Top_Txs_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"today_top_txs\". */\nexport type Today_Top_Txs_Order_By = {\n  count?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"today_top_txs\" */\nexport enum Today_Top_Txs_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Today_Top_Txs_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Today_Top_Txs_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Today_Top_Txs_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"today_top_txs\" */\nexport type Today_Top_Txs_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Today_Top_Txs_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Today_Top_Txs_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['bigint']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Today_Top_Txs_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Today_Top_Txs_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Today_Top_Txs_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Today_Top_Txs_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week_Aggregate = {\n  aggregate?: Maybe<Top_10_Of_Active_Neurons_Week_Aggregate_Fields>;\n  nodes: Array<Top_10_Of_Active_Neurons_Week>;\n};\n\n/** aggregate fields of \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week_Aggregate_Fields = {\n  avg?: Maybe<Top_10_Of_Active_Neurons_Week_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Top_10_Of_Active_Neurons_Week_Max_Fields>;\n  min?: Maybe<Top_10_Of_Active_Neurons_Week_Min_Fields>;\n  stddev?: Maybe<Top_10_Of_Active_Neurons_Week_Stddev_Fields>;\n  stddev_pop?: Maybe<Top_10_Of_Active_Neurons_Week_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Top_10_Of_Active_Neurons_Week_Stddev_Samp_Fields>;\n  sum?: Maybe<Top_10_Of_Active_Neurons_Week_Sum_Fields>;\n  var_pop?: Maybe<Top_10_Of_Active_Neurons_Week_Var_Pop_Fields>;\n  var_samp?: Maybe<Top_10_Of_Active_Neurons_Week_Var_Samp_Fields>;\n  variance?: Maybe<Top_10_Of_Active_Neurons_Week_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Top_10_Of_Active_Neurons_Week_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"top_10_of_active_neurons_week\". All fields are combined with a logical 'AND'. */\nexport type Top_10_Of_Active_Neurons_Week_Bool_Exp = {\n  _and?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Bool_Exp>>;\n  _not?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n  _or?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  pubkey?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Top_10_Of_Active_Neurons_Week_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Top_10_Of_Active_Neurons_Week_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"top_10_of_active_neurons_week\". */\nexport type Top_10_Of_Active_Neurons_Week_Order_By = {\n  count?: InputMaybe<Order_By>;\n  pubkey?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"top_10_of_active_neurons_week\" */\nexport enum Top_10_Of_Active_Neurons_Week_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Pubkey = 'pubkey'\n}\n\n/** aggregate stddev on columns */\nexport type Top_10_Of_Active_Neurons_Week_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Top_10_Of_Active_Neurons_Week_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Top_10_Of_Active_Neurons_Week_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Top_10_Of_Active_Neurons_Week_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Top_10_Of_Active_Neurons_Week_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['bigint']['input']>;\n  pubkey?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Top_10_Of_Active_Neurons_Week_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Top_10_Of_Active_Neurons_Week_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Top_10_Of_Active_Neurons_Week_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Top_10_Of_Active_Neurons_Week_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"top_first_txs\" */\nexport type Top_First_Txs = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"top_first_txs\" */\nexport type Top_First_Txs_Aggregate = {\n  aggregate?: Maybe<Top_First_Txs_Aggregate_Fields>;\n  nodes: Array<Top_First_Txs>;\n};\n\n/** aggregate fields of \"top_first_txs\" */\nexport type Top_First_Txs_Aggregate_Fields = {\n  avg?: Maybe<Top_First_Txs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Top_First_Txs_Max_Fields>;\n  min?: Maybe<Top_First_Txs_Min_Fields>;\n  stddev?: Maybe<Top_First_Txs_Stddev_Fields>;\n  stddev_pop?: Maybe<Top_First_Txs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Top_First_Txs_Stddev_Samp_Fields>;\n  sum?: Maybe<Top_First_Txs_Sum_Fields>;\n  var_pop?: Maybe<Top_First_Txs_Var_Pop_Fields>;\n  var_samp?: Maybe<Top_First_Txs_Var_Samp_Fields>;\n  variance?: Maybe<Top_First_Txs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"top_first_txs\" */\nexport type Top_First_Txs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Top_First_Txs_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"top_first_txs\". All fields are combined with a logical 'AND'. */\nexport type Top_First_Txs_Bool_Exp = {\n  _and?: InputMaybe<Array<Top_First_Txs_Bool_Exp>>;\n  _not?: InputMaybe<Top_First_Txs_Bool_Exp>;\n  _or?: InputMaybe<Array<Top_First_Txs_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Top_First_Txs_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Top_First_Txs_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"top_first_txs\". */\nexport type Top_First_Txs_Order_By = {\n  count?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"top_first_txs\" */\nexport enum Top_First_Txs_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Top_First_Txs_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Top_First_Txs_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Top_First_Txs_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"top_first_txs\" */\nexport type Top_First_Txs_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Top_First_Txs_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Top_First_Txs_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['bigint']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Top_First_Txs_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Top_First_Txs_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Top_First_Txs_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Top_First_Txs_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"top_leaders\" */\nexport type Top_Leaders = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"top_leaders\" */\nexport type Top_Leaders_Aggregate = {\n  aggregate?: Maybe<Top_Leaders_Aggregate_Fields>;\n  nodes: Array<Top_Leaders>;\n};\n\n/** aggregate fields of \"top_leaders\" */\nexport type Top_Leaders_Aggregate_Fields = {\n  avg?: Maybe<Top_Leaders_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Top_Leaders_Max_Fields>;\n  min?: Maybe<Top_Leaders_Min_Fields>;\n  stddev?: Maybe<Top_Leaders_Stddev_Fields>;\n  stddev_pop?: Maybe<Top_Leaders_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Top_Leaders_Stddev_Samp_Fields>;\n  sum?: Maybe<Top_Leaders_Sum_Fields>;\n  var_pop?: Maybe<Top_Leaders_Var_Pop_Fields>;\n  var_samp?: Maybe<Top_Leaders_Var_Samp_Fields>;\n  variance?: Maybe<Top_Leaders_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"top_leaders\" */\nexport type Top_Leaders_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Top_Leaders_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Top_Leaders_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"top_leaders\". All fields are combined with a logical 'AND'. */\nexport type Top_Leaders_Bool_Exp = {\n  _and?: InputMaybe<Array<Top_Leaders_Bool_Exp>>;\n  _not?: InputMaybe<Top_Leaders_Bool_Exp>;\n  _or?: InputMaybe<Array<Top_Leaders_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Top_Leaders_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Top_Leaders_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"top_leaders\". */\nexport type Top_Leaders_Order_By = {\n  count?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"top_leaders\" */\nexport enum Top_Leaders_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Neuron = 'neuron'\n}\n\n/** aggregate stddev on columns */\nexport type Top_Leaders_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Top_Leaders_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Top_Leaders_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"top_leaders\" */\nexport type Top_Leaders_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Top_Leaders_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Top_Leaders_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['bigint']['input']>;\n  neuron?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Top_Leaders_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Top_Leaders_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Top_Leaders_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Top_Leaders_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"top_txs\" */\nexport type Top_Txs = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"top_txs\" */\nexport type Top_Txs_Aggregate = {\n  aggregate?: Maybe<Top_Txs_Aggregate_Fields>;\n  nodes: Array<Top_Txs>;\n};\n\n/** aggregate fields of \"top_txs\" */\nexport type Top_Txs_Aggregate_Fields = {\n  avg?: Maybe<Top_Txs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Top_Txs_Max_Fields>;\n  min?: Maybe<Top_Txs_Min_Fields>;\n  stddev?: Maybe<Top_Txs_Stddev_Fields>;\n  stddev_pop?: Maybe<Top_Txs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Top_Txs_Stddev_Samp_Fields>;\n  sum?: Maybe<Top_Txs_Sum_Fields>;\n  var_pop?: Maybe<Top_Txs_Var_Pop_Fields>;\n  var_samp?: Maybe<Top_Txs_Var_Samp_Fields>;\n  variance?: Maybe<Top_Txs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"top_txs\" */\nexport type Top_Txs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Top_Txs_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"top_txs\". All fields are combined with a logical 'AND'. */\nexport type Top_Txs_Bool_Exp = {\n  _and?: InputMaybe<Array<Top_Txs_Bool_Exp>>;\n  _not?: InputMaybe<Top_Txs_Bool_Exp>;\n  _or?: InputMaybe<Array<Top_Txs_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Top_Txs_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Top_Txs_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"top_txs\". */\nexport type Top_Txs_Order_By = {\n  count?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"top_txs\" */\nexport enum Top_Txs_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Top_Txs_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Top_Txs_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Top_Txs_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"top_txs\" */\nexport type Top_Txs_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Top_Txs_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Top_Txs_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['bigint']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Top_Txs_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Top_Txs_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Top_Txs_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Top_Txs_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"transaction\" */\nexport type Transaction = {\n  /** An object relationship */\n  block: Block;\n  fee: Scalars['jsonb']['output'];\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash: Scalars['String']['output'];\n  height: Scalars['bigint']['output'];\n  logs?: Maybe<Scalars['jsonb']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  /** An array relationship */\n  message_155s: Array<Message_155>;\n  /** An aggregate relationship */\n  message_155s_aggregate: Message_155_Aggregate;\n  messages: Scalars['json']['output'];\n  /** An array relationship */\n  messagesByPartitionIdTransactionHash: Array<Message>;\n  /** An aggregate relationship */\n  messagesByPartitionIdTransactionHash_aggregate: Message_Aggregate;\n  partition_id: Scalars['bigint']['output'];\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures: Array<Scalars['String']['output']>;\n  signer_infos: Scalars['jsonb']['output'];\n  success: Scalars['Boolean']['output'];\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionFeeArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionLogsArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessage_155sArgs = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessage_155s_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessagesArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessagesByPartitionIdTransactionHashArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessagesByPartitionIdTransactionHash_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionSigner_InfosArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155 = {\n  /** An object relationship */\n  block: Block;\n  fee: Scalars['jsonb']['output'];\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash: Scalars['String']['output'];\n  height: Scalars['bigint']['output'];\n  logs?: Maybe<Scalars['jsonb']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  messages: Scalars['json']['output'];\n  /** An array relationship */\n  messagesByTransactionHashPartitionId: Array<Message>;\n  /** An aggregate relationship */\n  messagesByTransactionHashPartitionId_aggregate: Message_Aggregate;\n  partition_id: Scalars['bigint']['output'];\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures: Array<Scalars['String']['output']>;\n  signer_infos: Scalars['jsonb']['output'];\n  success: Scalars['Boolean']['output'];\n};\n\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155FeeArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155LogsArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155MessagesArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155MessagesByTransactionHashPartitionIdArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155MessagesByTransactionHashPartitionId_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155Signer_InfosArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregated selection of \"transaction_155\" */\nexport type Transaction_155_Aggregate = {\n  aggregate?: Maybe<Transaction_155_Aggregate_Fields>;\n  nodes: Array<Transaction_155>;\n};\n\nexport type Transaction_155_Aggregate_Bool_Exp = {\n  bool_and?: InputMaybe<Transaction_155_Aggregate_Bool_Exp_Bool_And>;\n  bool_or?: InputMaybe<Transaction_155_Aggregate_Bool_Exp_Bool_Or>;\n  count?: InputMaybe<Transaction_155_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Transaction_155_Aggregate_Bool_Exp_Bool_And = {\n  arguments: Transaction_155_Select_Column_Transaction_155_Aggregate_Bool_Exp_Bool_And_Arguments_Columns;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Transaction_155_Bool_Exp>;\n  predicate: Boolean_Comparison_Exp;\n};\n\nexport type Transaction_155_Aggregate_Bool_Exp_Bool_Or = {\n  arguments: Transaction_155_Select_Column_Transaction_155_Aggregate_Bool_Exp_Bool_Or_Arguments_Columns;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Transaction_155_Bool_Exp>;\n  predicate: Boolean_Comparison_Exp;\n};\n\nexport type Transaction_155_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Transaction_155_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"transaction_155\" */\nexport type Transaction_155_Aggregate_Fields = {\n  avg?: Maybe<Transaction_155_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Transaction_155_Max_Fields>;\n  min?: Maybe<Transaction_155_Min_Fields>;\n  stddev?: Maybe<Transaction_155_Stddev_Fields>;\n  stddev_pop?: Maybe<Transaction_155_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Transaction_155_Stddev_Samp_Fields>;\n  sum?: Maybe<Transaction_155_Sum_Fields>;\n  var_pop?: Maybe<Transaction_155_Var_Pop_Fields>;\n  var_samp?: Maybe<Transaction_155_Var_Samp_Fields>;\n  variance?: Maybe<Transaction_155_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"transaction_155\" */\nexport type Transaction_155_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"transaction_155\" */\nexport type Transaction_155_Aggregate_Order_By = {\n  avg?: InputMaybe<Transaction_155_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Transaction_155_Max_Order_By>;\n  min?: InputMaybe<Transaction_155_Min_Order_By>;\n  stddev?: InputMaybe<Transaction_155_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Transaction_155_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Transaction_155_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Transaction_155_Sum_Order_By>;\n  var_pop?: InputMaybe<Transaction_155_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Transaction_155_Var_Samp_Order_By>;\n  variance?: InputMaybe<Transaction_155_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Transaction_155_Avg_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"transaction_155\" */\nexport type Transaction_155_Avg_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"transaction_155\". All fields are combined with a logical 'AND'. */\nexport type Transaction_155_Bool_Exp = {\n  _and?: InputMaybe<Array<Transaction_155_Bool_Exp>>;\n  _not?: InputMaybe<Transaction_155_Bool_Exp>;\n  _or?: InputMaybe<Array<Transaction_155_Bool_Exp>>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  fee?: InputMaybe<Jsonb_Comparison_Exp>;\n  gas_used?: InputMaybe<Bigint_Comparison_Exp>;\n  gas_wanted?: InputMaybe<Bigint_Comparison_Exp>;\n  hash?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  logs?: InputMaybe<Jsonb_Comparison_Exp>;\n  memo?: InputMaybe<String_Comparison_Exp>;\n  messages?: InputMaybe<Json_Comparison_Exp>;\n  messagesByTransactionHashPartitionId?: InputMaybe<Message_Bool_Exp>;\n  messagesByTransactionHashPartitionId_aggregate?: InputMaybe<Message_Aggregate_Bool_Exp>;\n  partition_id?: InputMaybe<Bigint_Comparison_Exp>;\n  raw_log?: InputMaybe<String_Comparison_Exp>;\n  signatures?: InputMaybe<String_Array_Comparison_Exp>;\n  signer_infos?: InputMaybe<Jsonb_Comparison_Exp>;\n  success?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Transaction_155_Max_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n};\n\n/** order by max() on columns of table \"transaction_155\" */\nexport type Transaction_155_Max_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Transaction_155_Min_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n};\n\n/** order by min() on columns of table \"transaction_155\" */\nexport type Transaction_155_Min_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"transaction_155\". */\nexport type Transaction_155_Order_By = {\n  block?: InputMaybe<Block_Order_By>;\n  fee?: InputMaybe<Order_By>;\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  logs?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  messages?: InputMaybe<Order_By>;\n  messagesByTransactionHashPartitionId_aggregate?: InputMaybe<Message_Aggregate_Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n  signer_infos?: InputMaybe<Order_By>;\n  success?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"transaction_155\" */\nexport enum Transaction_155_Select_Column {\n  /** column name */\n  Fee = 'fee',\n  /** column name */\n  GasUsed = 'gas_used',\n  /** column name */\n  GasWanted = 'gas_wanted',\n  /** column name */\n  Hash = 'hash',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Logs = 'logs',\n  /** column name */\n  Memo = 'memo',\n  /** column name */\n  Messages = 'messages',\n  /** column name */\n  PartitionId = 'partition_id',\n  /** column name */\n  RawLog = 'raw_log',\n  /** column name */\n  Signatures = 'signatures',\n  /** column name */\n  SignerInfos = 'signer_infos',\n  /** column name */\n  Success = 'success'\n}\n\n/** select \"transaction_155_aggregate_bool_exp_bool_and_arguments_columns\" columns of table \"transaction_155\" */\nexport enum Transaction_155_Select_Column_Transaction_155_Aggregate_Bool_Exp_Bool_And_Arguments_Columns {\n  /** column name */\n  Success = 'success'\n}\n\n/** select \"transaction_155_aggregate_bool_exp_bool_or_arguments_columns\" columns of table \"transaction_155\" */\nexport enum Transaction_155_Select_Column_Transaction_155_Aggregate_Bool_Exp_Bool_Or_Arguments_Columns {\n  /** column name */\n  Success = 'success'\n}\n\n/** aggregate stddev on columns */\nexport type Transaction_155_Stddev_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"transaction_155\" */\nexport type Transaction_155_Stddev_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Transaction_155_Stddev_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"transaction_155\" */\nexport type Transaction_155_Stddev_Pop_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Transaction_155_Stddev_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"transaction_155\" */\nexport type Transaction_155_Stddev_Samp_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"transaction_155\" */\nexport type Transaction_155_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Transaction_155_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Transaction_155_Stream_Cursor_Value_Input = {\n  fee?: InputMaybe<Scalars['jsonb']['input']>;\n  gas_used?: InputMaybe<Scalars['bigint']['input']>;\n  gas_wanted?: InputMaybe<Scalars['bigint']['input']>;\n  hash?: InputMaybe<Scalars['String']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  logs?: InputMaybe<Scalars['jsonb']['input']>;\n  memo?: InputMaybe<Scalars['String']['input']>;\n  messages?: InputMaybe<Scalars['json']['input']>;\n  partition_id?: InputMaybe<Scalars['bigint']['input']>;\n  raw_log?: InputMaybe<Scalars['String']['input']>;\n  signatures?: InputMaybe<Array<Scalars['String']['input']>>;\n  signer_infos?: InputMaybe<Scalars['jsonb']['input']>;\n  success?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Transaction_155_Sum_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"transaction_155\" */\nexport type Transaction_155_Sum_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Transaction_155_Var_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"transaction_155\" */\nexport type Transaction_155_Var_Pop_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Transaction_155_Var_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"transaction_155\" */\nexport type Transaction_155_Var_Samp_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Transaction_155_Variance_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"transaction_155\" */\nexport type Transaction_155_Variance_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregated selection of \"transaction\" */\nexport type Transaction_Aggregate = {\n  aggregate?: Maybe<Transaction_Aggregate_Fields>;\n  nodes: Array<Transaction>;\n};\n\nexport type Transaction_Aggregate_Bool_Exp = {\n  bool_and?: InputMaybe<Transaction_Aggregate_Bool_Exp_Bool_And>;\n  bool_or?: InputMaybe<Transaction_Aggregate_Bool_Exp_Bool_Or>;\n  count?: InputMaybe<Transaction_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Transaction_Aggregate_Bool_Exp_Bool_And = {\n  arguments: Transaction_Select_Column_Transaction_Aggregate_Bool_Exp_Bool_And_Arguments_Columns;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Transaction_Bool_Exp>;\n  predicate: Boolean_Comparison_Exp;\n};\n\nexport type Transaction_Aggregate_Bool_Exp_Bool_Or = {\n  arguments: Transaction_Select_Column_Transaction_Aggregate_Bool_Exp_Bool_Or_Arguments_Columns;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Transaction_Bool_Exp>;\n  predicate: Boolean_Comparison_Exp;\n};\n\nexport type Transaction_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Transaction_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Transaction_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"transaction\" */\nexport type Transaction_Aggregate_Fields = {\n  avg?: Maybe<Transaction_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Transaction_Max_Fields>;\n  min?: Maybe<Transaction_Min_Fields>;\n  stddev?: Maybe<Transaction_Stddev_Fields>;\n  stddev_pop?: Maybe<Transaction_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Transaction_Stddev_Samp_Fields>;\n  sum?: Maybe<Transaction_Sum_Fields>;\n  var_pop?: Maybe<Transaction_Var_Pop_Fields>;\n  var_samp?: Maybe<Transaction_Var_Samp_Fields>;\n  variance?: Maybe<Transaction_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"transaction\" */\nexport type Transaction_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Transaction_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"transaction\" */\nexport type Transaction_Aggregate_Order_By = {\n  avg?: InputMaybe<Transaction_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Transaction_Max_Order_By>;\n  min?: InputMaybe<Transaction_Min_Order_By>;\n  stddev?: InputMaybe<Transaction_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Transaction_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Transaction_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Transaction_Sum_Order_By>;\n  var_pop?: InputMaybe<Transaction_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Transaction_Var_Samp_Order_By>;\n  variance?: InputMaybe<Transaction_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Transaction_Avg_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"transaction\" */\nexport type Transaction_Avg_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"transaction\". All fields are combined with a logical 'AND'. */\nexport type Transaction_Bool_Exp = {\n  _and?: InputMaybe<Array<Transaction_Bool_Exp>>;\n  _not?: InputMaybe<Transaction_Bool_Exp>;\n  _or?: InputMaybe<Array<Transaction_Bool_Exp>>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  fee?: InputMaybe<Jsonb_Comparison_Exp>;\n  gas_used?: InputMaybe<Bigint_Comparison_Exp>;\n  gas_wanted?: InputMaybe<Bigint_Comparison_Exp>;\n  hash?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  logs?: InputMaybe<Jsonb_Comparison_Exp>;\n  memo?: InputMaybe<String_Comparison_Exp>;\n  message_155s?: InputMaybe<Message_155_Bool_Exp>;\n  message_155s_aggregate?: InputMaybe<Message_155_Aggregate_Bool_Exp>;\n  messages?: InputMaybe<Json_Comparison_Exp>;\n  messagesByPartitionIdTransactionHash?: InputMaybe<Message_Bool_Exp>;\n  messagesByPartitionIdTransactionHash_aggregate?: InputMaybe<Message_Aggregate_Bool_Exp>;\n  partition_id?: InputMaybe<Bigint_Comparison_Exp>;\n  raw_log?: InputMaybe<String_Comparison_Exp>;\n  signatures?: InputMaybe<String_Array_Comparison_Exp>;\n  signer_infos?: InputMaybe<Jsonb_Comparison_Exp>;\n  success?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Transaction_Max_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n};\n\n/** order by max() on columns of table \"transaction\" */\nexport type Transaction_Max_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Transaction_Min_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n};\n\n/** order by min() on columns of table \"transaction\" */\nexport type Transaction_Min_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"transaction\". */\nexport type Transaction_Order_By = {\n  block?: InputMaybe<Block_Order_By>;\n  fee?: InputMaybe<Order_By>;\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  logs?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  message_155s_aggregate?: InputMaybe<Message_155_Aggregate_Order_By>;\n  messages?: InputMaybe<Order_By>;\n  messagesByPartitionIdTransactionHash_aggregate?: InputMaybe<Message_Aggregate_Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n  signer_infos?: InputMaybe<Order_By>;\n  success?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"transaction\" */\nexport enum Transaction_Select_Column {\n  /** column name */\n  Fee = 'fee',\n  /** column name */\n  GasUsed = 'gas_used',\n  /** column name */\n  GasWanted = 'gas_wanted',\n  /** column name */\n  Hash = 'hash',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Logs = 'logs',\n  /** column name */\n  Memo = 'memo',\n  /** column name */\n  Messages = 'messages',\n  /** column name */\n  PartitionId = 'partition_id',\n  /** column name */\n  RawLog = 'raw_log',\n  /** column name */\n  Signatures = 'signatures',\n  /** column name */\n  SignerInfos = 'signer_infos',\n  /** column name */\n  Success = 'success'\n}\n\n/** select \"transaction_aggregate_bool_exp_bool_and_arguments_columns\" columns of table \"transaction\" */\nexport enum Transaction_Select_Column_Transaction_Aggregate_Bool_Exp_Bool_And_Arguments_Columns {\n  /** column name */\n  Success = 'success'\n}\n\n/** select \"transaction_aggregate_bool_exp_bool_or_arguments_columns\" columns of table \"transaction\" */\nexport enum Transaction_Select_Column_Transaction_Aggregate_Bool_Exp_Bool_Or_Arguments_Columns {\n  /** column name */\n  Success = 'success'\n}\n\n/** aggregate stddev on columns */\nexport type Transaction_Stddev_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"transaction\" */\nexport type Transaction_Stddev_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Transaction_Stddev_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"transaction\" */\nexport type Transaction_Stddev_Pop_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Transaction_Stddev_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"transaction\" */\nexport type Transaction_Stddev_Samp_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"transaction\" */\nexport type Transaction_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Transaction_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Transaction_Stream_Cursor_Value_Input = {\n  fee?: InputMaybe<Scalars['jsonb']['input']>;\n  gas_used?: InputMaybe<Scalars['bigint']['input']>;\n  gas_wanted?: InputMaybe<Scalars['bigint']['input']>;\n  hash?: InputMaybe<Scalars['String']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  logs?: InputMaybe<Scalars['jsonb']['input']>;\n  memo?: InputMaybe<Scalars['String']['input']>;\n  messages?: InputMaybe<Scalars['json']['input']>;\n  partition_id?: InputMaybe<Scalars['bigint']['input']>;\n  raw_log?: InputMaybe<Scalars['String']['input']>;\n  signatures?: InputMaybe<Array<Scalars['String']['input']>>;\n  signer_infos?: InputMaybe<Scalars['jsonb']['input']>;\n  success?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Transaction_Sum_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"transaction\" */\nexport type Transaction_Sum_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Transaction_Var_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"transaction\" */\nexport type Transaction_Var_Pop_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Transaction_Var_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"transaction\" */\nexport type Transaction_Var_Samp_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Transaction_Variance_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"transaction\" */\nexport type Transaction_Variance_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"tweets_stats\" */\nexport type Tweets_Stats = {\n  date?: Maybe<Scalars['date']['output']>;\n  tweets?: Maybe<Scalars['numeric']['output']>;\n  tweets_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregated selection of \"tweets_stats\" */\nexport type Tweets_Stats_Aggregate = {\n  aggregate?: Maybe<Tweets_Stats_Aggregate_Fields>;\n  nodes: Array<Tweets_Stats>;\n};\n\n/** aggregate fields of \"tweets_stats\" */\nexport type Tweets_Stats_Aggregate_Fields = {\n  avg?: Maybe<Tweets_Stats_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Tweets_Stats_Max_Fields>;\n  min?: Maybe<Tweets_Stats_Min_Fields>;\n  stddev?: Maybe<Tweets_Stats_Stddev_Fields>;\n  stddev_pop?: Maybe<Tweets_Stats_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Tweets_Stats_Stddev_Samp_Fields>;\n  sum?: Maybe<Tweets_Stats_Sum_Fields>;\n  var_pop?: Maybe<Tweets_Stats_Var_Pop_Fields>;\n  var_samp?: Maybe<Tweets_Stats_Var_Samp_Fields>;\n  variance?: Maybe<Tweets_Stats_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"tweets_stats\" */\nexport type Tweets_Stats_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Tweets_Stats_Avg_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"tweets_stats\". All fields are combined with a logical 'AND'. */\nexport type Tweets_Stats_Bool_Exp = {\n  _and?: InputMaybe<Array<Tweets_Stats_Bool_Exp>>;\n  _not?: InputMaybe<Tweets_Stats_Bool_Exp>;\n  _or?: InputMaybe<Array<Tweets_Stats_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  tweets?: InputMaybe<Numeric_Comparison_Exp>;\n  tweets_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Tweets_Stats_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  tweets?: Maybe<Scalars['numeric']['output']>;\n  tweets_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Tweets_Stats_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  tweets?: Maybe<Scalars['numeric']['output']>;\n  tweets_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"tweets_stats\". */\nexport type Tweets_Stats_Order_By = {\n  date?: InputMaybe<Order_By>;\n  tweets?: InputMaybe<Order_By>;\n  tweets_per_day?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"tweets_stats\" */\nexport enum Tweets_Stats_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  Tweets = 'tweets',\n  /** column name */\n  TweetsPerDay = 'tweets_per_day'\n}\n\n/** aggregate stddev on columns */\nexport type Tweets_Stats_Stddev_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Tweets_Stats_Stddev_Pop_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Tweets_Stats_Stddev_Samp_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"tweets_stats\" */\nexport type Tweets_Stats_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Tweets_Stats_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Tweets_Stats_Stream_Cursor_Value_Input = {\n  date?: InputMaybe<Scalars['date']['input']>;\n  tweets?: InputMaybe<Scalars['numeric']['input']>;\n  tweets_per_day?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Tweets_Stats_Sum_Fields = {\n  tweets?: Maybe<Scalars['numeric']['output']>;\n  tweets_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Tweets_Stats_Var_Pop_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Tweets_Stats_Var_Samp_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Tweets_Stats_Variance_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"txs_ranked\" */\nexport type Txs_Ranked = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"txs_ranked\" */\nexport type Txs_Ranked_Aggregate = {\n  aggregate?: Maybe<Txs_Ranked_Aggregate_Fields>;\n  nodes: Array<Txs_Ranked>;\n};\n\n/** aggregate fields of \"txs_ranked\" */\nexport type Txs_Ranked_Aggregate_Fields = {\n  avg?: Maybe<Txs_Ranked_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Txs_Ranked_Max_Fields>;\n  min?: Maybe<Txs_Ranked_Min_Fields>;\n  stddev?: Maybe<Txs_Ranked_Stddev_Fields>;\n  stddev_pop?: Maybe<Txs_Ranked_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Txs_Ranked_Stddev_Samp_Fields>;\n  sum?: Maybe<Txs_Ranked_Sum_Fields>;\n  var_pop?: Maybe<Txs_Ranked_Var_Pop_Fields>;\n  var_samp?: Maybe<Txs_Ranked_Var_Samp_Fields>;\n  variance?: Maybe<Txs_Ranked_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"txs_ranked\" */\nexport type Txs_Ranked_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Txs_Ranked_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"txs_ranked\". All fields are combined with a logical 'AND'. */\nexport type Txs_Ranked_Bool_Exp = {\n  _and?: InputMaybe<Array<Txs_Ranked_Bool_Exp>>;\n  _not?: InputMaybe<Txs_Ranked_Bool_Exp>;\n  _or?: InputMaybe<Array<Txs_Ranked_Bool_Exp>>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n  rank?: InputMaybe<Bigint_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Txs_Ranked_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Txs_Ranked_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"txs_ranked\". */\nexport type Txs_Ranked_Order_By = {\n  height?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  rank?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"txs_ranked\" */\nexport enum Txs_Ranked_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Neuron = 'neuron',\n  /** column name */\n  Rank = 'rank',\n  /** column name */\n  Type = 'type',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Txs_Ranked_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Txs_Ranked_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Txs_Ranked_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"txs_ranked\" */\nexport type Txs_Ranked_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Txs_Ranked_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Txs_Ranked_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  neuron?: InputMaybe<Scalars['String']['input']>;\n  rank?: InputMaybe<Scalars['bigint']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Txs_Ranked_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Txs_Ranked_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Txs_Ranked_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Txs_Ranked_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"uptime\" */\nexport type Uptime = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  uptime?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"uptime\" */\nexport type Uptime_Aggregate = {\n  aggregate?: Maybe<Uptime_Aggregate_Fields>;\n  nodes: Array<Uptime>;\n};\n\n/** aggregate fields of \"uptime\" */\nexport type Uptime_Aggregate_Fields = {\n  avg?: Maybe<Uptime_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Uptime_Max_Fields>;\n  min?: Maybe<Uptime_Min_Fields>;\n  stddev?: Maybe<Uptime_Stddev_Fields>;\n  stddev_pop?: Maybe<Uptime_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Uptime_Stddev_Samp_Fields>;\n  sum?: Maybe<Uptime_Sum_Fields>;\n  var_pop?: Maybe<Uptime_Var_Pop_Fields>;\n  var_samp?: Maybe<Uptime_Var_Samp_Fields>;\n  variance?: Maybe<Uptime_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"uptime\" */\nexport type Uptime_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Uptime_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Uptime_Avg_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"uptime\". All fields are combined with a logical 'AND'. */\nexport type Uptime_Bool_Exp = {\n  _and?: InputMaybe<Array<Uptime_Bool_Exp>>;\n  _not?: InputMaybe<Uptime_Bool_Exp>;\n  _or?: InputMaybe<Array<Uptime_Bool_Exp>>;\n  consensus_address?: InputMaybe<String_Comparison_Exp>;\n  consensus_pubkey?: InputMaybe<String_Comparison_Exp>;\n  pre_commits?: InputMaybe<Bigint_Comparison_Exp>;\n  uptime?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Uptime_Max_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  uptime?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Uptime_Min_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  uptime?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"uptime\". */\nexport type Uptime_Order_By = {\n  consensus_address?: InputMaybe<Order_By>;\n  consensus_pubkey?: InputMaybe<Order_By>;\n  pre_commits?: InputMaybe<Order_By>;\n  uptime?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"uptime\" */\nexport enum Uptime_Select_Column {\n  /** column name */\n  ConsensusAddress = 'consensus_address',\n  /** column name */\n  ConsensusPubkey = 'consensus_pubkey',\n  /** column name */\n  PreCommits = 'pre_commits',\n  /** column name */\n  Uptime = 'uptime'\n}\n\n/** aggregate stddev on columns */\nexport type Uptime_Stddev_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Uptime_Stddev_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Uptime_Stddev_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"uptime\" */\nexport type Uptime_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Uptime_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Uptime_Stream_Cursor_Value_Input = {\n  consensus_address?: InputMaybe<Scalars['String']['input']>;\n  consensus_pubkey?: InputMaybe<Scalars['String']['input']>;\n  pre_commits?: InputMaybe<Scalars['bigint']['input']>;\n  uptime?: InputMaybe<Scalars['numeric']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Uptime_Sum_Fields = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  uptime?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Uptime_Var_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Uptime_Var_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Uptime_Variance_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"validator\" */\nexport type Validator = {\n  /** An array relationship */\n  blocks: Array<Block>;\n  /** An aggregate relationship */\n  blocks_aggregate: Block_Aggregate;\n  consensus_address: Scalars['String']['output'];\n  consensus_pubkey: Scalars['String']['output'];\n  /** An array relationship */\n  pre_commits: Array<Pre_Commit>;\n  /** An aggregate relationship */\n  pre_commits_aggregate: Pre_Commit_Aggregate;\n};\n\n\n/** columns and relationships of \"validator\" */\nexport type ValidatorBlocksArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"validator\" */\nexport type ValidatorBlocks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"validator\" */\nexport type ValidatorPre_CommitsArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"validator\" */\nexport type ValidatorPre_Commits_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n/** aggregated selection of \"validator\" */\nexport type Validator_Aggregate = {\n  aggregate?: Maybe<Validator_Aggregate_Fields>;\n  nodes: Array<Validator>;\n};\n\n/** aggregate fields of \"validator\" */\nexport type Validator_Aggregate_Fields = {\n  count: Scalars['Int']['output'];\n  max?: Maybe<Validator_Max_Fields>;\n  min?: Maybe<Validator_Min_Fields>;\n};\n\n\n/** aggregate fields of \"validator\" */\nexport type Validator_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Validator_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** Boolean expression to filter rows from the table \"validator\". All fields are combined with a logical 'AND'. */\nexport type Validator_Bool_Exp = {\n  _and?: InputMaybe<Array<Validator_Bool_Exp>>;\n  _not?: InputMaybe<Validator_Bool_Exp>;\n  _or?: InputMaybe<Array<Validator_Bool_Exp>>;\n  blocks?: InputMaybe<Block_Bool_Exp>;\n  blocks_aggregate?: InputMaybe<Block_Aggregate_Bool_Exp>;\n  consensus_address?: InputMaybe<String_Comparison_Exp>;\n  consensus_pubkey?: InputMaybe<String_Comparison_Exp>;\n  pre_commits?: InputMaybe<Pre_Commit_Bool_Exp>;\n  pre_commits_aggregate?: InputMaybe<Pre_Commit_Aggregate_Bool_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Validator_Max_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Validator_Min_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"validator\". */\nexport type Validator_Order_By = {\n  blocks_aggregate?: InputMaybe<Block_Aggregate_Order_By>;\n  consensus_address?: InputMaybe<Order_By>;\n  consensus_pubkey?: InputMaybe<Order_By>;\n  pre_commits_aggregate?: InputMaybe<Pre_Commit_Aggregate_Order_By>;\n};\n\n/** select columns of table \"validator\" */\nexport enum Validator_Select_Column {\n  /** column name */\n  ConsensusAddress = 'consensus_address',\n  /** column name */\n  ConsensusPubkey = 'consensus_pubkey'\n}\n\n/** Streaming cursor of the table \"validator\" */\nexport type Validator_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Validator_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Validator_Stream_Cursor_Value_Input = {\n  consensus_address?: InputMaybe<Scalars['String']['input']>;\n  consensus_pubkey?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** columns and relationships of \"vesting_account\" */\nexport type Vesting_Account = {\n  /** An object relationship */\n  account: Account;\n  address: Scalars['String']['output'];\n  end_time: Scalars['timestamp']['output'];\n  id: Scalars['Int']['output'];\n  original_vesting: Array<Scalars['coin']['output']>;\n  start_time?: Maybe<Scalars['timestamp']['output']>;\n  type: Scalars['String']['output'];\n  /** An array relationship */\n  vesting_periods: Array<Vesting_Period>;\n  /** An aggregate relationship */\n  vesting_periods_aggregate: Vesting_Period_Aggregate;\n};\n\n\n/** columns and relationships of \"vesting_account\" */\nexport type Vesting_AccountVesting_PeriodsArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Period_Order_By>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"vesting_account\" */\nexport type Vesting_AccountVesting_Periods_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Period_Order_By>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n/** aggregated selection of \"vesting_account\" */\nexport type Vesting_Account_Aggregate = {\n  aggregate?: Maybe<Vesting_Account_Aggregate_Fields>;\n  nodes: Array<Vesting_Account>;\n};\n\nexport type Vesting_Account_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Vesting_Account_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Vesting_Account_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Vesting_Account_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"vesting_account\" */\nexport type Vesting_Account_Aggregate_Fields = {\n  avg?: Maybe<Vesting_Account_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Vesting_Account_Max_Fields>;\n  min?: Maybe<Vesting_Account_Min_Fields>;\n  stddev?: Maybe<Vesting_Account_Stddev_Fields>;\n  stddev_pop?: Maybe<Vesting_Account_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Vesting_Account_Stddev_Samp_Fields>;\n  sum?: Maybe<Vesting_Account_Sum_Fields>;\n  var_pop?: Maybe<Vesting_Account_Var_Pop_Fields>;\n  var_samp?: Maybe<Vesting_Account_Var_Samp_Fields>;\n  variance?: Maybe<Vesting_Account_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"vesting_account\" */\nexport type Vesting_Account_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"vesting_account\" */\nexport type Vesting_Account_Aggregate_Order_By = {\n  avg?: InputMaybe<Vesting_Account_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Vesting_Account_Max_Order_By>;\n  min?: InputMaybe<Vesting_Account_Min_Order_By>;\n  stddev?: InputMaybe<Vesting_Account_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Vesting_Account_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Vesting_Account_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Vesting_Account_Sum_Order_By>;\n  var_pop?: InputMaybe<Vesting_Account_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Vesting_Account_Var_Samp_Order_By>;\n  variance?: InputMaybe<Vesting_Account_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Vesting_Account_Avg_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Avg_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"vesting_account\". All fields are combined with a logical 'AND'. */\nexport type Vesting_Account_Bool_Exp = {\n  _and?: InputMaybe<Array<Vesting_Account_Bool_Exp>>;\n  _not?: InputMaybe<Vesting_Account_Bool_Exp>;\n  _or?: InputMaybe<Array<Vesting_Account_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  end_time?: InputMaybe<Timestamp_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  original_vesting?: InputMaybe<Coin_Array_Comparison_Exp>;\n  start_time?: InputMaybe<Timestamp_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  vesting_periods?: InputMaybe<Vesting_Period_Bool_Exp>;\n  vesting_periods_aggregate?: InputMaybe<Vesting_Period_Aggregate_Bool_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Vesting_Account_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  end_time?: Maybe<Scalars['timestamp']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  original_vesting?: Maybe<Array<Scalars['coin']['output']>>;\n  start_time?: Maybe<Scalars['timestamp']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Max_Order_By = {\n  address?: InputMaybe<Order_By>;\n  end_time?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  original_vesting?: InputMaybe<Order_By>;\n  start_time?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Vesting_Account_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  end_time?: Maybe<Scalars['timestamp']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  original_vesting?: Maybe<Array<Scalars['coin']['output']>>;\n  start_time?: Maybe<Scalars['timestamp']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Min_Order_By = {\n  address?: InputMaybe<Order_By>;\n  end_time?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  original_vesting?: InputMaybe<Order_By>;\n  start_time?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"vesting_account\". */\nexport type Vesting_Account_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  address?: InputMaybe<Order_By>;\n  end_time?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  original_vesting?: InputMaybe<Order_By>;\n  start_time?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  vesting_periods_aggregate?: InputMaybe<Vesting_Period_Aggregate_Order_By>;\n};\n\n/** select columns of table \"vesting_account\" */\nexport enum Vesting_Account_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  EndTime = 'end_time',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  OriginalVesting = 'original_vesting',\n  /** column name */\n  StartTime = 'start_time',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Vesting_Account_Stddev_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Stddev_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Vesting_Account_Stddev_Pop_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Stddev_Pop_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Vesting_Account_Stddev_Samp_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Stddev_Samp_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"vesting_account\" */\nexport type Vesting_Account_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Vesting_Account_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Vesting_Account_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n  end_time?: InputMaybe<Scalars['timestamp']['input']>;\n  id?: InputMaybe<Scalars['Int']['input']>;\n  original_vesting?: InputMaybe<Array<Scalars['coin']['input']>>;\n  start_time?: InputMaybe<Scalars['timestamp']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Vesting_Account_Sum_Fields = {\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** order by sum() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Sum_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Vesting_Account_Var_Pop_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Var_Pop_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Vesting_Account_Var_Samp_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Var_Samp_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Vesting_Account_Variance_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Variance_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"vesting_period\" */\nexport type Vesting_Period = {\n  amount: Array<Scalars['coin']['output']>;\n  length: Scalars['bigint']['output'];\n  period_order: Scalars['bigint']['output'];\n  /** An object relationship */\n  vesting_account: Vesting_Account;\n  vesting_account_id: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"vesting_period\" */\nexport type Vesting_Period_Aggregate = {\n  aggregate?: Maybe<Vesting_Period_Aggregate_Fields>;\n  nodes: Array<Vesting_Period>;\n};\n\nexport type Vesting_Period_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Vesting_Period_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Vesting_Period_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Vesting_Period_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"vesting_period\" */\nexport type Vesting_Period_Aggregate_Fields = {\n  avg?: Maybe<Vesting_Period_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Vesting_Period_Max_Fields>;\n  min?: Maybe<Vesting_Period_Min_Fields>;\n  stddev?: Maybe<Vesting_Period_Stddev_Fields>;\n  stddev_pop?: Maybe<Vesting_Period_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Vesting_Period_Stddev_Samp_Fields>;\n  sum?: Maybe<Vesting_Period_Sum_Fields>;\n  var_pop?: Maybe<Vesting_Period_Var_Pop_Fields>;\n  var_samp?: Maybe<Vesting_Period_Var_Samp_Fields>;\n  variance?: Maybe<Vesting_Period_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"vesting_period\" */\nexport type Vesting_Period_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"vesting_period\" */\nexport type Vesting_Period_Aggregate_Order_By = {\n  avg?: InputMaybe<Vesting_Period_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Vesting_Period_Max_Order_By>;\n  min?: InputMaybe<Vesting_Period_Min_Order_By>;\n  stddev?: InputMaybe<Vesting_Period_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Vesting_Period_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Vesting_Period_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Vesting_Period_Sum_Order_By>;\n  var_pop?: InputMaybe<Vesting_Period_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Vesting_Period_Var_Samp_Order_By>;\n  variance?: InputMaybe<Vesting_Period_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Vesting_Period_Avg_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Avg_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"vesting_period\". All fields are combined with a logical 'AND'. */\nexport type Vesting_Period_Bool_Exp = {\n  _and?: InputMaybe<Array<Vesting_Period_Bool_Exp>>;\n  _not?: InputMaybe<Vesting_Period_Bool_Exp>;\n  _or?: InputMaybe<Array<Vesting_Period_Bool_Exp>>;\n  amount?: InputMaybe<Coin_Array_Comparison_Exp>;\n  length?: InputMaybe<Bigint_Comparison_Exp>;\n  period_order?: InputMaybe<Bigint_Comparison_Exp>;\n  vesting_account?: InputMaybe<Vesting_Account_Bool_Exp>;\n  vesting_account_id?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Vesting_Period_Max_Fields = {\n  amount?: Maybe<Array<Scalars['coin']['output']>>;\n  length?: Maybe<Scalars['bigint']['output']>;\n  period_order?: Maybe<Scalars['bigint']['output']>;\n  vesting_account_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by max() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Max_Order_By = {\n  amount?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Vesting_Period_Min_Fields = {\n  amount?: Maybe<Array<Scalars['coin']['output']>>;\n  length?: Maybe<Scalars['bigint']['output']>;\n  period_order?: Maybe<Scalars['bigint']['output']>;\n  vesting_account_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by min() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Min_Order_By = {\n  amount?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"vesting_period\". */\nexport type Vesting_Period_Order_By = {\n  amount?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account?: InputMaybe<Vesting_Account_Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"vesting_period\" */\nexport enum Vesting_Period_Select_Column {\n  /** column name */\n  Amount = 'amount',\n  /** column name */\n  Length = 'length',\n  /** column name */\n  PeriodOrder = 'period_order',\n  /** column name */\n  VestingAccountId = 'vesting_account_id'\n}\n\n/** aggregate stddev on columns */\nexport type Vesting_Period_Stddev_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Stddev_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Vesting_Period_Stddev_Pop_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Stddev_Pop_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Vesting_Period_Stddev_Samp_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Stddev_Samp_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"vesting_period\" */\nexport type Vesting_Period_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Vesting_Period_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Vesting_Period_Stream_Cursor_Value_Input = {\n  amount?: InputMaybe<Array<Scalars['coin']['input']>>;\n  length?: InputMaybe<Scalars['bigint']['input']>;\n  period_order?: InputMaybe<Scalars['bigint']['input']>;\n  vesting_account_id?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Vesting_Period_Sum_Fields = {\n  length?: Maybe<Scalars['bigint']['output']>;\n  period_order?: Maybe<Scalars['bigint']['output']>;\n  vesting_account_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Sum_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Vesting_Period_Var_Pop_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Var_Pop_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Vesting_Period_Var_Samp_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Var_Samp_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Vesting_Period_Variance_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Variance_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"week_redelegation\" */\nexport type Week_Redelegation = {\n  redelegation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"week_redelegation\" */\nexport type Week_Redelegation_Aggregate = {\n  aggregate?: Maybe<Week_Redelegation_Aggregate_Fields>;\n  nodes: Array<Week_Redelegation>;\n};\n\n/** aggregate fields of \"week_redelegation\" */\nexport type Week_Redelegation_Aggregate_Fields = {\n  avg?: Maybe<Week_Redelegation_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Week_Redelegation_Max_Fields>;\n  min?: Maybe<Week_Redelegation_Min_Fields>;\n  stddev?: Maybe<Week_Redelegation_Stddev_Fields>;\n  stddev_pop?: Maybe<Week_Redelegation_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Week_Redelegation_Stddev_Samp_Fields>;\n  sum?: Maybe<Week_Redelegation_Sum_Fields>;\n  var_pop?: Maybe<Week_Redelegation_Var_Pop_Fields>;\n  var_samp?: Maybe<Week_Redelegation_Var_Samp_Fields>;\n  variance?: Maybe<Week_Redelegation_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"week_redelegation\" */\nexport type Week_Redelegation_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Week_Redelegation_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Week_Redelegation_Avg_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"week_redelegation\". All fields are combined with a logical 'AND'. */\nexport type Week_Redelegation_Bool_Exp = {\n  _and?: InputMaybe<Array<Week_Redelegation_Bool_Exp>>;\n  _not?: InputMaybe<Week_Redelegation_Bool_Exp>;\n  _or?: InputMaybe<Array<Week_Redelegation_Bool_Exp>>;\n  redelegation?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Week_Redelegation_Max_Fields = {\n  redelegation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Week_Redelegation_Min_Fields = {\n  redelegation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"week_redelegation\". */\nexport type Week_Redelegation_Order_By = {\n  redelegation?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"week_redelegation\" */\nexport enum Week_Redelegation_Select_Column {\n  /** column name */\n  Redelegation = 'redelegation',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Week_Redelegation_Stddev_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Week_Redelegation_Stddev_Pop_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Week_Redelegation_Stddev_Samp_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"week_redelegation\" */\nexport type Week_Redelegation_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Week_Redelegation_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Week_Redelegation_Stream_Cursor_Value_Input = {\n  redelegation?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Week_Redelegation_Sum_Fields = {\n  redelegation?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Week_Redelegation_Var_Pop_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Week_Redelegation_Var_Samp_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Week_Redelegation_Variance_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"week_undelegation\" */\nexport type Week_Undelegation = {\n  undelegation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"week_undelegation\" */\nexport type Week_Undelegation_Aggregate = {\n  aggregate?: Maybe<Week_Undelegation_Aggregate_Fields>;\n  nodes: Array<Week_Undelegation>;\n};\n\n/** aggregate fields of \"week_undelegation\" */\nexport type Week_Undelegation_Aggregate_Fields = {\n  avg?: Maybe<Week_Undelegation_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Week_Undelegation_Max_Fields>;\n  min?: Maybe<Week_Undelegation_Min_Fields>;\n  stddev?: Maybe<Week_Undelegation_Stddev_Fields>;\n  stddev_pop?: Maybe<Week_Undelegation_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Week_Undelegation_Stddev_Samp_Fields>;\n  sum?: Maybe<Week_Undelegation_Sum_Fields>;\n  var_pop?: Maybe<Week_Undelegation_Var_Pop_Fields>;\n  var_samp?: Maybe<Week_Undelegation_Var_Samp_Fields>;\n  variance?: Maybe<Week_Undelegation_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"week_undelegation\" */\nexport type Week_Undelegation_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Week_Undelegation_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Week_Undelegation_Avg_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"week_undelegation\". All fields are combined with a logical 'AND'. */\nexport type Week_Undelegation_Bool_Exp = {\n  _and?: InputMaybe<Array<Week_Undelegation_Bool_Exp>>;\n  _not?: InputMaybe<Week_Undelegation_Bool_Exp>;\n  _or?: InputMaybe<Array<Week_Undelegation_Bool_Exp>>;\n  undelegation?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Week_Undelegation_Max_Fields = {\n  undelegation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Week_Undelegation_Min_Fields = {\n  undelegation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"week_undelegation\". */\nexport type Week_Undelegation_Order_By = {\n  undelegation?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"week_undelegation\" */\nexport enum Week_Undelegation_Select_Column {\n  /** column name */\n  Undelegation = 'undelegation',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Week_Undelegation_Stddev_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Week_Undelegation_Stddev_Pop_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Week_Undelegation_Stddev_Samp_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"week_undelegation\" */\nexport type Week_Undelegation_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Week_Undelegation_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Week_Undelegation_Stream_Cursor_Value_Input = {\n  undelegation?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Week_Undelegation_Sum_Fields = {\n  undelegation?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Week_Undelegation_Var_Pop_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Week_Undelegation_Var_Samp_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Week_Undelegation_Variance_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\nexport type TransactionsSubscriptionVariables = Exact<{ [key: string]: never; }>;\n\n\nexport type TransactionsSubscription = { transaction: Array<{ success: boolean, messages: any, height: any, hash: string }> };\n\nexport type AccountCountQueryVariables = Exact<{ [key: string]: never; }>;\n\n\nexport type AccountCountQuery = { account_aggregate: { aggregate?: { count: number } | null } };\n\nexport type BlockByHeightQueryVariables = Exact<{\n  blockId?: InputMaybe<Scalars['bigint']['input']>;\n}>;\n\n\nexport type BlockByHeightQuery = { block: Array<{ hash: string, height: any, proposer_address?: string | null, timestamp: any, transactions: Array<{ messages: any, hash: string, height: any, success: boolean }> }> };\n\nexport type BlocksQueryVariables = Exact<{\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  where?: InputMaybe<Block_Bool_Exp>;\n}>;\n\n\nexport type BlocksQuery = { block: Array<{ hash: string, height: any, proposer_address?: string | null, timestamp: any, transactions_aggregate: { aggregate?: { count: number } | null } }> };\n\nexport type ContractsCountQueryVariables = Exact<{ [key: string]: never; }>;\n\n\nexport type ContractsCountQuery = { contracts_aggregate: { aggregate?: { count: number } | null } };\n\nexport type CyberlinksByParticleQueryVariables = Exact<{\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  orderBy?: InputMaybe<Array<Cyberlinks_Order_By> | Cyberlinks_Order_By>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n}>;\n\n\nexport type CyberlinksByParticleQuery = { cyberlinks: Array<{ timestamp: any, neuron: string, transaction_hash: string, from: string, to: string }> };\n\nexport type CyberlinksCountByNeuronQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['String']['input']>;\n  particles_from?: InputMaybe<Array<Scalars['String']['input']> | Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n}>;\n\n\nexport type CyberlinksCountByNeuronQuery = { cyberlinks_aggregate: { aggregate?: { count: number } | null } };\n\nexport type CyberlinksCountByParticleQueryVariables = Exact<{\n  cid?: InputMaybe<Scalars['String']['input']>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n}>;\n\n\nexport type CyberlinksCountByParticleQuery = { cyberlinks_aggregate: { aggregate?: { count: number } | null } };\n\nexport type MessagesByAddressCountQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['_text']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n}>;\n\n\nexport type MessagesByAddressCountQuery = { messages_by_address_aggregate: { aggregate?: { count: number } | null } };\n\nexport type MessagesByAddressSenseQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['_text']['input']>;\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  timestamp_from?: InputMaybe<Scalars['timestamp']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n  order_direction?: InputMaybe<Order_By>;\n}>;\n\n\nexport type MessagesByAddressSenseQuery = { messages_by_address: Array<{ transaction_hash: string, index: any, value: any, type: string, transaction?: { success: boolean, memo?: string | null, block: { timestamp: any, height: any } } | null }> };\n\nexport type MessagesByAddressSenseWsSubscriptionVariables = Exact<{\n  address?: InputMaybe<Scalars['_text']['input']>;\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  timestamp_from?: InputMaybe<Scalars['timestamp']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n  order_direction?: InputMaybe<Order_By>;\n}>;\n\n\nexport type MessagesByAddressSenseWsSubscription = { messages_by_address: Array<{ transaction_hash: string, index: any, value: any, type: string, transaction?: { success: boolean, memo?: string | null, block: { timestamp: any, height: any } } | null }> };\n\nexport type TransactionCountQueryVariables = Exact<{ [key: string]: never; }>;\n\n\nexport type TransactionCountQuery = { transaction_aggregate: { aggregate?: { count: number } | null } };\n\nexport type UptimeByAddressQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['String']['input']>;\n}>;\n\n\nexport type UptimeByAddressQuery = { uptime: Array<{ uptime?: any | null }> };\n\nexport type WasmDashboardPageQueryVariables = Exact<{\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n}>;\n\n\nexport type WasmDashboardPageQuery = { contracts: Array<{ address: string, admin: string, code_id: any, creator: string, fees: any, gas: any, label: string, tx: any }>, contracts_aggregate: { aggregate?: { count: number, sum?: { gas?: any | null, fees?: any | null, tx?: any | null } | null } | null } };\n\nexport type MessagesByAddressQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['_text']['input']>;\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n}>;\n\n\nexport type MessagesByAddressQuery = { messages_by_address: Array<{ transaction_hash: string, value: any, type: string, transaction?: { success: boolean, height: any, logs?: any | null, memo?: string | null, block: { timestamp: any } } | null }> };\n\n\nexport const TransactionsDocument = gql`\n    subscription Transactions {\n  transaction(offset: 0, limit: 200, order_by: {height: desc}) {\n    success\n    messages\n    height\n    hash\n  }\n}\n    `;\n\n/**\n * __useTransactionsSubscription__\n *\n * To run a query within a React component, call `useTransactionsSubscription` and pass it any options that fit your needs.\n * When your component renders, `useTransactionsSubscription` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the subscription, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useTransactionsSubscription({\n *   variables: {\n *   },\n * });\n */\nexport function useTransactionsSubscription(baseOptions?: Apollo.SubscriptionHookOptions<TransactionsSubscription, TransactionsSubscriptionVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useSubscription<TransactionsSubscription, TransactionsSubscriptionVariables>(TransactionsDocument, options);\n      }\nexport type TransactionsSubscriptionHookResult = ReturnType<typeof useTransactionsSubscription>;\nexport type TransactionsSubscriptionResult = Apollo.SubscriptionResult<TransactionsSubscription>;\nexport const AccountCountDocument = gql`\n    query accountCount {\n  account_aggregate {\n    aggregate {\n      count(columns: address)\n    }\n  }\n}\n    `;\n\n/**\n * __useAccountCountQuery__\n *\n * To run a query within a React component, call `useAccountCountQuery` and pass it any options that fit your needs.\n * When your component renders, `useAccountCountQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useAccountCountQuery({\n *   variables: {\n *   },\n * });\n */\nexport function useAccountCountQuery(baseOptions?: Apollo.QueryHookOptions<AccountCountQuery, AccountCountQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<AccountCountQuery, AccountCountQueryVariables>(AccountCountDocument, options);\n      }\nexport function useAccountCountLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<AccountCountQuery, AccountCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<AccountCountQuery, AccountCountQueryVariables>(AccountCountDocument, options);\n        }\nexport function useAccountCountSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<AccountCountQuery, AccountCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<AccountCountQuery, AccountCountQueryVariables>(AccountCountDocument, options);\n        }\nexport type AccountCountQueryHookResult = ReturnType<typeof useAccountCountQuery>;\nexport type AccountCountLazyQueryHookResult = ReturnType<typeof useAccountCountLazyQuery>;\nexport type AccountCountSuspenseQueryHookResult = ReturnType<typeof useAccountCountSuspenseQuery>;\nexport type AccountCountQueryResult = Apollo.QueryResult<AccountCountQuery, AccountCountQueryVariables>;\nexport const BlockByHeightDocument = gql`\n    query blockByHeight($blockId: bigint) {\n  block(where: {height: {_eq: $blockId}}) {\n    hash\n    height\n    proposer_address\n    timestamp\n    transactions {\n      messages\n      hash\n      height\n      success\n    }\n  }\n}\n    `;\n\n/**\n * __useBlockByHeightQuery__\n *\n * To run a query within a React component, call `useBlockByHeightQuery` and pass it any options that fit your needs.\n * When your component renders, `useBlockByHeightQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useBlockByHeightQuery({\n *   variables: {\n *      blockId: // value for 'blockId'\n *   },\n * });\n */\nexport function useBlockByHeightQuery(baseOptions?: Apollo.QueryHookOptions<BlockByHeightQuery, BlockByHeightQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<BlockByHeightQuery, BlockByHeightQueryVariables>(BlockByHeightDocument, options);\n      }\nexport function useBlockByHeightLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<BlockByHeightQuery, BlockByHeightQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<BlockByHeightQuery, BlockByHeightQueryVariables>(BlockByHeightDocument, options);\n        }\nexport function useBlockByHeightSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<BlockByHeightQuery, BlockByHeightQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<BlockByHeightQuery, BlockByHeightQueryVariables>(BlockByHeightDocument, options);\n        }\nexport type BlockByHeightQueryHookResult = ReturnType<typeof useBlockByHeightQuery>;\nexport type BlockByHeightLazyQueryHookResult = ReturnType<typeof useBlockByHeightLazyQuery>;\nexport type BlockByHeightSuspenseQueryHookResult = ReturnType<typeof useBlockByHeightSuspenseQuery>;\nexport type BlockByHeightQueryResult = Apollo.QueryResult<BlockByHeightQuery, BlockByHeightQueryVariables>;\nexport const BlocksDocument = gql`\n    query blocks($limit: Int, $offset: Int, $where: block_bool_exp) {\n  block(where: $where, limit: $limit, offset: $offset, order_by: {height: desc}) {\n    hash\n    height\n    proposer_address\n    transactions_aggregate {\n      aggregate {\n        count\n      }\n    }\n    timestamp\n  }\n}\n    `;\n\n/**\n * __useBlocksQuery__\n *\n * To run a query within a React component, call `useBlocksQuery` and pass it any options that fit your needs.\n * When your component renders, `useBlocksQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useBlocksQuery({\n *   variables: {\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      where: // value for 'where'\n *   },\n * });\n */\nexport function useBlocksQuery(baseOptions?: Apollo.QueryHookOptions<BlocksQuery, BlocksQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<BlocksQuery, BlocksQueryVariables>(BlocksDocument, options);\n      }\nexport function useBlocksLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<BlocksQuery, BlocksQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<BlocksQuery, BlocksQueryVariables>(BlocksDocument, options);\n        }\nexport function useBlocksSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<BlocksQuery, BlocksQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<BlocksQuery, BlocksQueryVariables>(BlocksDocument, options);\n        }\nexport type BlocksQueryHookResult = ReturnType<typeof useBlocksQuery>;\nexport type BlocksLazyQueryHookResult = ReturnType<typeof useBlocksLazyQuery>;\nexport type BlocksSuspenseQueryHookResult = ReturnType<typeof useBlocksSuspenseQuery>;\nexport type BlocksQueryResult = Apollo.QueryResult<BlocksQuery, BlocksQueryVariables>;\nexport const ContractsCountDocument = gql`\n    query contractsCount {\n  contracts_aggregate {\n    aggregate {\n      count\n    }\n  }\n}\n    `;\n\n/**\n * __useContractsCountQuery__\n *\n * To run a query within a React component, call `useContractsCountQuery` and pass it any options that fit your needs.\n * When your component renders, `useContractsCountQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useContractsCountQuery({\n *   variables: {\n *   },\n * });\n */\nexport function useContractsCountQuery(baseOptions?: Apollo.QueryHookOptions<ContractsCountQuery, ContractsCountQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<ContractsCountQuery, ContractsCountQueryVariables>(ContractsCountDocument, options);\n      }\nexport function useContractsCountLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<ContractsCountQuery, ContractsCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<ContractsCountQuery, ContractsCountQueryVariables>(ContractsCountDocument, options);\n        }\nexport function useContractsCountSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<ContractsCountQuery, ContractsCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<ContractsCountQuery, ContractsCountQueryVariables>(ContractsCountDocument, options);\n        }\nexport type ContractsCountQueryHookResult = ReturnType<typeof useContractsCountQuery>;\nexport type ContractsCountLazyQueryHookResult = ReturnType<typeof useContractsCountLazyQuery>;\nexport type ContractsCountSuspenseQueryHookResult = ReturnType<typeof useContractsCountSuspenseQuery>;\nexport type ContractsCountQueryResult = Apollo.QueryResult<ContractsCountQuery, ContractsCountQueryVariables>;\nexport const CyberlinksByParticleDocument = gql`\n    query CyberlinksByParticle($limit: Int, $offset: Int, $orderBy: [cyberlinks_order_by!], $where: cyberlinks_bool_exp) {\n  cyberlinks(limit: $limit, offset: $offset, order_by: $orderBy, where: $where) {\n    from: particle_from\n    to: particle_to\n    timestamp\n    neuron\n    transaction_hash\n  }\n}\n    `;\n\n/**\n * __useCyberlinksByParticleQuery__\n *\n * To run a query within a React component, call `useCyberlinksByParticleQuery` and pass it any options that fit your needs.\n * When your component renders, `useCyberlinksByParticleQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useCyberlinksByParticleQuery({\n *   variables: {\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      orderBy: // value for 'orderBy'\n *      where: // value for 'where'\n *   },\n * });\n */\nexport function useCyberlinksByParticleQuery(baseOptions?: Apollo.QueryHookOptions<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>(CyberlinksByParticleDocument, options);\n      }\nexport function useCyberlinksByParticleLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>(CyberlinksByParticleDocument, options);\n        }\nexport function useCyberlinksByParticleSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>(CyberlinksByParticleDocument, options);\n        }\nexport type CyberlinksByParticleQueryHookResult = ReturnType<typeof useCyberlinksByParticleQuery>;\nexport type CyberlinksByParticleLazyQueryHookResult = ReturnType<typeof useCyberlinksByParticleLazyQuery>;\nexport type CyberlinksByParticleSuspenseQueryHookResult = ReturnType<typeof useCyberlinksByParticleSuspenseQuery>;\nexport type CyberlinksByParticleQueryResult = Apollo.QueryResult<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>;\nexport const CyberlinksCountByNeuronDocument = gql`\n    query CyberlinksCountByNeuron($address: String, $particles_from: [String!], $timestamp: timestamp) {\n  cyberlinks_aggregate(\n    where: {_and: [{neuron: {_eq: $address}}, {particle_from: {_in: $particles_from}}, {timestamp: {_gt: $timestamp}}]}\n  ) {\n    aggregate {\n      count\n    }\n  }\n}\n    `;\n\n/**\n * __useCyberlinksCountByNeuronQuery__\n *\n * To run a query within a React component, call `useCyberlinksCountByNeuronQuery` and pass it any options that fit your needs.\n * When your component renders, `useCyberlinksCountByNeuronQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useCyberlinksCountByNeuronQuery({\n *   variables: {\n *      address: // value for 'address'\n *      particles_from: // value for 'particles_from'\n *      timestamp: // value for 'timestamp'\n *   },\n * });\n */\nexport function useCyberlinksCountByNeuronQuery(baseOptions?: Apollo.QueryHookOptions<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>(CyberlinksCountByNeuronDocument, options);\n      }\nexport function useCyberlinksCountByNeuronLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>(CyberlinksCountByNeuronDocument, options);\n        }\nexport function useCyberlinksCountByNeuronSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>(CyberlinksCountByNeuronDocument, options);\n        }\nexport type CyberlinksCountByNeuronQueryHookResult = ReturnType<typeof useCyberlinksCountByNeuronQuery>;\nexport type CyberlinksCountByNeuronLazyQueryHookResult = ReturnType<typeof useCyberlinksCountByNeuronLazyQuery>;\nexport type CyberlinksCountByNeuronSuspenseQueryHookResult = ReturnType<typeof useCyberlinksCountByNeuronSuspenseQuery>;\nexport type CyberlinksCountByNeuronQueryResult = Apollo.QueryResult<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>;\nexport const CyberlinksCountByParticleDocument = gql`\n    query cyberlinksCountByParticle($cid: String, $where: cyberlinks_bool_exp) {\n  cyberlinks_aggregate(where: $where) {\n    aggregate {\n      count\n    }\n  }\n}\n    `;\n\n/**\n * __useCyberlinksCountByParticleQuery__\n *\n * To run a query within a React component, call `useCyberlinksCountByParticleQuery` and pass it any options that fit your needs.\n * When your component renders, `useCyberlinksCountByParticleQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useCyberlinksCountByParticleQuery({\n *   variables: {\n *      cid: // value for 'cid'\n *      where: // value for 'where'\n *   },\n * });\n */\nexport function useCyberlinksCountByParticleQuery(baseOptions?: Apollo.QueryHookOptions<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>(CyberlinksCountByParticleDocument, options);\n      }\nexport function useCyberlinksCountByParticleLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>(CyberlinksCountByParticleDocument, options);\n        }\nexport function useCyberlinksCountByParticleSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>(CyberlinksCountByParticleDocument, options);\n        }\nexport type CyberlinksCountByParticleQueryHookResult = ReturnType<typeof useCyberlinksCountByParticleQuery>;\nexport type CyberlinksCountByParticleLazyQueryHookResult = ReturnType<typeof useCyberlinksCountByParticleLazyQuery>;\nexport type CyberlinksCountByParticleSuspenseQueryHookResult = ReturnType<typeof useCyberlinksCountByParticleSuspenseQuery>;\nexport type CyberlinksCountByParticleQueryResult = Apollo.QueryResult<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>;\nexport const MessagesByAddressCountDocument = gql`\n    query MessagesByAddressCount($address: _text, $timestamp: timestamp) {\n  messages_by_address_aggregate(\n    args: {addresses: $address, limit: \"100000000\", offset: \"0\", types: \"{}\"}\n    where: {transaction: {block: {timestamp: {_gt: $timestamp}}}}\n  ) {\n    aggregate {\n      count\n    }\n  }\n}\n    `;\n\n/**\n * __useMessagesByAddressCountQuery__\n *\n * To run a query within a React component, call `useMessagesByAddressCountQuery` and pass it any options that fit your needs.\n * When your component renders, `useMessagesByAddressCountQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useMessagesByAddressCountQuery({\n *   variables: {\n *      address: // value for 'address'\n *      timestamp: // value for 'timestamp'\n *   },\n * });\n */\nexport function useMessagesByAddressCountQuery(baseOptions?: Apollo.QueryHookOptions<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>(MessagesByAddressCountDocument, options);\n      }\nexport function useMessagesByAddressCountLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>(MessagesByAddressCountDocument, options);\n        }\nexport function useMessagesByAddressCountSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>(MessagesByAddressCountDocument, options);\n        }\nexport type MessagesByAddressCountQueryHookResult = ReturnType<typeof useMessagesByAddressCountQuery>;\nexport type MessagesByAddressCountLazyQueryHookResult = ReturnType<typeof useMessagesByAddressCountLazyQuery>;\nexport type MessagesByAddressCountSuspenseQueryHookResult = ReturnType<typeof useMessagesByAddressCountSuspenseQuery>;\nexport type MessagesByAddressCountQueryResult = Apollo.QueryResult<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>;\nexport const MessagesByAddressSenseDocument = gql`\n    query MessagesByAddressSense($address: _text, $limit: bigint, $offset: bigint, $timestamp_from: timestamp, $types: _text, $order_direction: order_by) {\n  messages_by_address(\n    args: {addresses: $address, limit: $limit, offset: $offset, types: $types}\n    order_by: {transaction: {block: {timestamp: $order_direction}}}\n    where: {transaction: {block: {timestamp: {_gt: $timestamp_from}}}}\n  ) {\n    transaction_hash\n    index\n    value\n    transaction {\n      success\n      block {\n        timestamp\n        height\n      }\n      memo\n    }\n    type\n  }\n}\n    `;\n\n/**\n * __useMessagesByAddressSenseQuery__\n *\n * To run a query within a React component, call `useMessagesByAddressSenseQuery` and pass it any options that fit your needs.\n * When your component renders, `useMessagesByAddressSenseQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useMessagesByAddressSenseQuery({\n *   variables: {\n *      address: // value for 'address'\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      timestamp_from: // value for 'timestamp_from'\n *      types: // value for 'types'\n *      order_direction: // value for 'order_direction'\n *   },\n * });\n */\nexport function useMessagesByAddressSenseQuery(baseOptions?: Apollo.QueryHookOptions<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>(MessagesByAddressSenseDocument, options);\n      }\nexport function useMessagesByAddressSenseLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>(MessagesByAddressSenseDocument, options);\n        }\nexport function useMessagesByAddressSenseSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>(MessagesByAddressSenseDocument, options);\n        }\nexport type MessagesByAddressSenseQueryHookResult = ReturnType<typeof useMessagesByAddressSenseQuery>;\nexport type MessagesByAddressSenseLazyQueryHookResult = ReturnType<typeof useMessagesByAddressSenseLazyQuery>;\nexport type MessagesByAddressSenseSuspenseQueryHookResult = ReturnType<typeof useMessagesByAddressSenseSuspenseQuery>;\nexport type MessagesByAddressSenseQueryResult = Apollo.QueryResult<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>;\nexport const MessagesByAddressSenseWsDocument = gql`\n    subscription MessagesByAddressSenseWs($address: _text, $limit: bigint, $offset: bigint, $timestamp_from: timestamp, $types: _text, $order_direction: order_by) {\n  messages_by_address(\n    args: {addresses: $address, limit: $limit, offset: $offset, types: $types}\n    order_by: {transaction: {block: {timestamp: $order_direction}}}\n    where: {transaction: {block: {timestamp: {_gt: $timestamp_from}}}}\n  ) {\n    transaction_hash\n    index\n    value\n    transaction {\n      success\n      block {\n        timestamp\n        height\n      }\n      memo\n    }\n    type\n  }\n}\n    `;\n\n/**\n * __useMessagesByAddressSenseWsSubscription__\n *\n * To run a query within a React component, call `useMessagesByAddressSenseWsSubscription` and pass it any options that fit your needs.\n * When your component renders, `useMessagesByAddressSenseWsSubscription` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the subscription, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useMessagesByAddressSenseWsSubscription({\n *   variables: {\n *      address: // value for 'address'\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      timestamp_from: // value for 'timestamp_from'\n *      types: // value for 'types'\n *      order_direction: // value for 'order_direction'\n *   },\n * });\n */\nexport function useMessagesByAddressSenseWsSubscription(baseOptions?: Apollo.SubscriptionHookOptions<MessagesByAddressSenseWsSubscription, MessagesByAddressSenseWsSubscriptionVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useSubscription<MessagesByAddressSenseWsSubscription, MessagesByAddressSenseWsSubscriptionVariables>(MessagesByAddressSenseWsDocument, options);\n      }\nexport type MessagesByAddressSenseWsSubscriptionHookResult = ReturnType<typeof useMessagesByAddressSenseWsSubscription>;\nexport type MessagesByAddressSenseWsSubscriptionResult = Apollo.SubscriptionResult<MessagesByAddressSenseWsSubscription>;\nexport const TransactionCountDocument = gql`\n    query transactionCount {\n  transaction_aggregate {\n    aggregate {\n      count(columns: hash)\n    }\n  }\n}\n    `;\n\n/**\n * __useTransactionCountQuery__\n *\n * To run a query within a React component, call `useTransactionCountQuery` and pass it any options that fit your needs.\n * When your component renders, `useTransactionCountQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useTransactionCountQuery({\n *   variables: {\n *   },\n * });\n */\nexport function useTransactionCountQuery(baseOptions?: Apollo.QueryHookOptions<TransactionCountQuery, TransactionCountQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<TransactionCountQuery, TransactionCountQueryVariables>(TransactionCountDocument, options);\n      }\nexport function useTransactionCountLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<TransactionCountQuery, TransactionCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<TransactionCountQuery, TransactionCountQueryVariables>(TransactionCountDocument, options);\n        }\nexport function useTransactionCountSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<TransactionCountQuery, TransactionCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<TransactionCountQuery, TransactionCountQueryVariables>(TransactionCountDocument, options);\n        }\nexport type TransactionCountQueryHookResult = ReturnType<typeof useTransactionCountQuery>;\nexport type TransactionCountLazyQueryHookResult = ReturnType<typeof useTransactionCountLazyQuery>;\nexport type TransactionCountSuspenseQueryHookResult = ReturnType<typeof useTransactionCountSuspenseQuery>;\nexport type TransactionCountQueryResult = Apollo.QueryResult<TransactionCountQuery, TransactionCountQueryVariables>;\nexport const UptimeByAddressDocument = gql`\n    query uptimeByAddress($address: String) {\n  uptime(where: {consensus_address: {_eq: $address}}) {\n    uptime\n  }\n}\n    `;\n\n/**\n * __useUptimeByAddressQuery__\n *\n * To run a query within a React component, call `useUptimeByAddressQuery` and pass it any options that fit your needs.\n * When your component renders, `useUptimeByAddressQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useUptimeByAddressQuery({\n *   variables: {\n *      address: // value for 'address'\n *   },\n * });\n */\nexport function useUptimeByAddressQuery(baseOptions?: Apollo.QueryHookOptions<UptimeByAddressQuery, UptimeByAddressQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<UptimeByAddressQuery, UptimeByAddressQueryVariables>(UptimeByAddressDocument, options);\n      }\nexport function useUptimeByAddressLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<UptimeByAddressQuery, UptimeByAddressQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<UptimeByAddressQuery, UptimeByAddressQueryVariables>(UptimeByAddressDocument, options);\n        }\nexport function useUptimeByAddressSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<UptimeByAddressQuery, UptimeByAddressQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<UptimeByAddressQuery, UptimeByAddressQueryVariables>(UptimeByAddressDocument, options);\n        }\nexport type UptimeByAddressQueryHookResult = ReturnType<typeof useUptimeByAddressQuery>;\nexport type UptimeByAddressLazyQueryHookResult = ReturnType<typeof useUptimeByAddressLazyQuery>;\nexport type UptimeByAddressSuspenseQueryHookResult = ReturnType<typeof useUptimeByAddressSuspenseQuery>;\nexport type UptimeByAddressQueryResult = Apollo.QueryResult<UptimeByAddressQuery, UptimeByAddressQueryVariables>;\nexport const WasmDashboardPageDocument = gql`\n    query wasmDashboardPage($offset: Int, $limit: Int) {\n  contracts(limit: $limit, offset: $offset, order_by: {tx: desc}) {\n    address\n    admin\n    code_id\n    creator\n    fees\n    gas\n    label\n    tx\n  }\n  contracts_aggregate {\n    aggregate {\n      sum {\n        gas\n        fees\n        tx\n      }\n      count(columns: address)\n    }\n  }\n}\n    `;\n\n/**\n * __useWasmDashboardPageQuery__\n *\n * To run a query within a React component, call `useWasmDashboardPageQuery` and pass it any options that fit your needs.\n * When your component renders, `useWasmDashboardPageQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useWasmDashboardPageQuery({\n *   variables: {\n *      offset: // value for 'offset'\n *      limit: // value for 'limit'\n *   },\n * });\n */\nexport function useWasmDashboardPageQuery(baseOptions?: Apollo.QueryHookOptions<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>(WasmDashboardPageDocument, options);\n      }\nexport function useWasmDashboardPageLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>(WasmDashboardPageDocument, options);\n        }\nexport function useWasmDashboardPageSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>(WasmDashboardPageDocument, options);\n        }\nexport type WasmDashboardPageQueryHookResult = ReturnType<typeof useWasmDashboardPageQuery>;\nexport type WasmDashboardPageLazyQueryHookResult = ReturnType<typeof useWasmDashboardPageLazyQuery>;\nexport type WasmDashboardPageSuspenseQueryHookResult = ReturnType<typeof useWasmDashboardPageSuspenseQuery>;\nexport type WasmDashboardPageQueryResult = Apollo.QueryResult<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>;\nexport const MessagesByAddressDocument = gql`\n    query MessagesByAddress($address: _text, $limit: bigint, $offset: bigint, $types: _text) {\n  messages_by_address(\n    args: {addresses: $address, limit: $limit, offset: $offset, types: $types}\n    order_by: {transaction: {block: {height: desc}}}\n  ) {\n    transaction_hash\n    value\n    transaction {\n      success\n      height\n      logs\n      memo\n      block {\n        timestamp\n      }\n    }\n    type\n  }\n}\n    `;\n\n/**\n * __useMessagesByAddressQuery__\n *\n * To run a query within a React component, call `useMessagesByAddressQuery` and pass it any options that fit your needs.\n * When your component renders, `useMessagesByAddressQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useMessagesByAddressQuery({\n *   variables: {\n *      address: // value for 'address'\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      types: // value for 'types'\n *   },\n * });\n */\nexport function useMessagesByAddressQuery(baseOptions?: Apollo.QueryHookOptions<MessagesByAddressQuery, MessagesByAddressQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<MessagesByAddressQuery, MessagesByAddressQueryVariables>(MessagesByAddressDocument, options);\n      }\nexport function useMessagesByAddressLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<MessagesByAddressQuery, MessagesByAddressQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<MessagesByAddressQuery, MessagesByAddressQueryVariables>(MessagesByAddressDocument, options);\n        }\nexport function useMessagesByAddressSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<MessagesByAddressQuery, MessagesByAddressQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<MessagesByAddressQuery, MessagesByAddressQueryVariables>(MessagesByAddressDocument, options);\n        }\nexport type MessagesByAddressQueryHookResult = ReturnType<typeof useMessagesByAddressQuery>;\nexport type MessagesByAddressLazyQueryHookResult = ReturnType<typeof useMessagesByAddressLazyQuery>;\nexport type MessagesByAddressSuspenseQueryHookResult = ReturnType<typeof useMessagesByAddressSuspenseQuery>;\nexport type MessagesByAddressQueryResult = Apollo.QueryResult<MessagesByAddressQuery, MessagesByAddressQueryVariables>;","import { Coin } from 'cosmjs-types/cosmos/base/v1beta1/coin';\nimport { CyberLinkSimple, NeuronAddress } from 'src/types/base';\n\ninterface GenericIndexerTransaction<T> {\n  value: T;\n  type: string;\n  transaction_hash: string;\n  index: number;\n  transaction: {\n    memo?: string;\n    success: boolean;\n    block: {\n      timestamp: string;\n    };\n  };\n}\nexport const MSG_SEND_TRANSACTION_TYPE = 'cosmos.bank.v1beta1.MsgSend';\n\nexport const MSG_MULTI_SEND_TRANSACTION_TYPE =\n  'cosmos.bank.v1beta1.MsgMultiSend';\n\nexport const CYBER_LINK_TRANSACTION_TYPE = 'cyber.graph.v1beta1.MsgCyberlink';\n\ninterface Input {\n  address: NeuronAddress;\n  coins: Coin[];\n}\n\ninterface Output {\n  address: NeuronAddress;\n  coins: Coin[];\n}\n\nexport interface MsgMultiSendValue {\n  inputs: Input[];\n  outputs: Output[];\n}\n\nexport interface MsgSendValue {\n  amount: Coin[];\n  from_address: NeuronAddress;\n  to_address: NeuronAddress;\n}\n\ninterface MsgDelegateValue {\n  amount: Coin;\n  delegator_address: NeuronAddress;\n  validator_address: NeuronAddress;\n}\n\nexport interface CyberLinkValue {\n  neuron: NeuronAddress;\n  links: CyberLinkSimple[];\n}\n\nexport interface CyberLinkTransaction\n  extends GenericIndexerTransaction<CyberLinkValue> {\n  type: typeof CYBER_LINK_TRANSACTION_TYPE;\n}\n\nexport interface MsgMultiSendTransaction\n  extends GenericIndexerTransaction<MsgMultiSendValue> {\n  type: typeof MSG_MULTI_SEND_TRANSACTION_TYPE;\n}\n\nexport interface MsgSendTransaction\n  extends GenericIndexerTransaction<MsgSendValue> {\n  type: typeof MSG_SEND_TRANSACTION_TYPE;\n}\n\nexport type Transaction =\n  // | DelegateTransaction\n  CyberLinkTransaction | MsgMultiSendTransaction | MsgSendTransaction;\n","import { Tx } from 'cosmjs-types/cosmos/tx/v1beta1/tx';\nimport { MsgSend, MsgMultiSend } from 'cosmjs-types/cosmos/bank/v1beta1/tx';\n\nimport { fromBase64 } from '@cosmjs/encoding';\nimport {\n  MSG_MULTI_SEND_TRANSACTION_TYPE,\n  MSG_SEND_TRANSACTION_TYPE,\n} from 'src/services/backend/services/indexer/types';\nimport { NeuronAddress } from 'src/types/base';\nimport { TransactionDto } from 'src/services/CozoDb/types/dto';\nimport { getNowUtcNumber } from 'src/utils/date';\n\n// eslint-disable-next-line import/no-unused-modules\nexport const extractTxData = (data: string) => {\n  const result = Tx.decode(fromBase64(data));\n  const memo = result.body?.memo;\n  const messages = result.body?.messages\n    .map((message) => {\n      const msgType = message.typeUrl.slice(1);\n      if (msgType === MSG_SEND_TRANSACTION_TYPE) {\n        return MsgSend.decode(message.value);\n      }\n\n      if (msgType === MSG_MULTI_SEND_TRANSACTION_TYPE) {\n        return MsgMultiSend.decode(message.value);\n      }\n      return undefined;\n    })\n    .filter((message) => message !== undefined);\n\n  return { memo, messages };\n};\n\n// eslint-disable-next-line import/no-unused-modules\nexport const mapWebsocketTxToTransactions = (\n  neuron: NeuronAddress,\n  result: any\n) => {\n  const { data, events } = result;\n\n  const hash = events['tx.hash'][0];\n  const transactionType = events['message.action'][0].slice(1);\n  const timestamp = getNowUtcNumber();\n  const blockHeight = events['tx.height'][0];\n\n  const { memo = '', messages } = extractTxData(data.value.TxResult.tx);\n\n  const transactions: TransactionDto[] = [];\n  messages!.forEach((message, index) => {\n    transactions.push({\n      hash,\n      index,\n      type: transactionType,\n      timestamp,\n      success: true,\n      value: message!,\n      memo,\n      neuron,\n      blockHeight,\n    });\n  });\n\n  return transactions;\n};\n","import { ApolloClient, DocumentNode, InMemoryCache } from '@apollo/client';\n\nimport { GraphQLWsLink } from '@apollo/client/link/subscriptions';\nimport { GraphQLClient } from 'graphql-request';\nimport { createClient } from 'graphql-ws';\nimport { Observable } from 'rxjs';\nimport { INDEX_WEBSOCKET, INDEX_HTTPS } from 'src/constants/config';\n\nconst cyberGraphQLWsLink = new GraphQLWsLink(\n  createClient({\n    url: INDEX_WEBSOCKET,\n    shouldRetry: (errOrCloseEvent: unknown) => true,\n    retryAttempts: 10,\n    retryWait: async (retries: number): Promise<void> => {\n      setTimeout(() => Promise.resolve(), Math.min(1000 * 2 ** retries, 10000));\n    },\n    // on: {\n    //   error: (err) => {\n    //     console.log('---ws errr', err);\n    //   },\n    //   message: (msg) => {\n    //     console.log('---ws message', msg);\n    //   },\n    //   // Handle connection opened event\n    //   opened: () => {\n    //     console.log('---ws opened');\n    //   },\n    //   // Handle connection closed event\n    //   closed: () => {\n    //     console.log('---ws closed');\n    //   },\n    // },\n  })\n);\n\nexport const createIndexerClient = (abortSignal: AbortSignal) =>\n  new GraphQLClient(INDEX_HTTPS, {\n    signal: abortSignal,\n  });\n\n// eslint-disable-next-line import/no-unused-modules\nexport function createIndexerWebsocket<T>(\n  query: DocumentNode,\n  variables: object\n): Observable<T> {\n  const client = new ApolloClient({\n    link: cyberGraphQLWsLink,\n    cache: new InMemoryCache(),\n  });\n\n  const apolloObservable = client.subscribe({ query, variables });\n  return new Observable((subscriber) => {\n    const subscription = apolloObservable.subscribe({\n      next(result) {\n        subscriber.next(result.data as T);\n      },\n      error(err) {\n        subscriber.error(err);\n      },\n      complete() {\n        subscriber.complete();\n      },\n    });\n\n    // Cleanup subscription on unsubscribe\n    return () => subscription.unsubscribe();\n  });\n}\n","/* eslint-disable import/no-unused-modules */\n\nimport { ParticleCid, NeuronAddress } from 'src/types/base';\nimport { numberToUtcDate } from 'src/utils/date';\n\nimport { CYBERLINKS_BATCH_LIMIT } from './consts';\nimport { createIndexerClient } from './utils/graphqlClient';\nimport { fetchIterableByOffset } from 'src/utils/async/iterable';\nimport {\n  CyberlinksByParticleDocument,\n  CyberlinksByParticleQuery,\n  CyberlinksByParticleQueryVariables,\n  CyberlinksCountByNeuronDocument,\n  CyberlinksCountByNeuronQuery,\n  CyberlinksCountByNeuronQueryVariables,\n  Order_By,\n} from 'src/generated/graphql';\n\nconst fetchCyberlinks = async ({\n  particleCid,\n  timestampFrom,\n  offset = 0,\n  abortSignal,\n}: {\n  particleCid: ParticleCid;\n  timestampFrom: number;\n  offset?: number;\n  abortSignal: AbortSignal;\n}) => {\n  const res = await createIndexerClient(abortSignal).request<\n    CyberlinksByParticleQuery,\n    CyberlinksByParticleQueryVariables\n  >(CyberlinksByParticleDocument, {\n    limit: CYBERLINKS_BATCH_LIMIT,\n    offset,\n    orderBy: [{ timestamp: Order_By.Asc }],\n    where: {\n      _or: [\n        { particle_to: { _eq: particleCid } },\n        { particle_from: { _eq: particleCid } },\n      ],\n      timestamp: { _gt: numberToUtcDate(timestampFrom) },\n    },\n  });\n\n  return res.cyberlinks;\n};\n\nconst fetchCyberlinksCount = async (\n  address: NeuronAddress,\n  particlesFrom: ParticleCid[],\n  timestampFrom: number,\n  abortSignal: AbortSignal\n) => {\n  const res = await createIndexerClient(abortSignal).request<\n    CyberlinksCountByNeuronQuery,\n    CyberlinksCountByNeuronQueryVariables\n  >(CyberlinksCountByNeuronDocument, {\n    address,\n    particles_from: particlesFrom,\n    timestamp: numberToUtcDate(timestampFrom),\n  });\n\n  return res.cyberlinks_aggregate.aggregate?.count;\n};\n\nconst fetchCyberlinksByNeroun = async ({\n  neuron,\n  particlesFrom,\n  timestampFrom,\n  batchSize,\n  offset = 0,\n  abortSignal,\n}: {\n  neuron: NeuronAddress;\n  particlesFrom: ParticleCid[];\n  timestampFrom: number;\n  batchSize: number;\n  offset: number;\n  abortSignal: AbortSignal;\n}) => {\n  const where = {\n    _and: [\n      {\n        timestamp: {\n          _gt: numberToUtcDate(timestampFrom),\n        },\n      },\n      {\n        neuron: {\n          _eq: neuron,\n        },\n      },\n      { particle_from: { _in: particlesFrom } },\n    ],\n  };\n\n  const res = await createIndexerClient(abortSignal).request<\n    CyberlinksByParticleQuery,\n    CyberlinksByParticleQueryVariables\n  >(CyberlinksByParticleDocument, {\n    limit: batchSize,\n    offset,\n    orderBy: [\n      {\n        timestamp: Order_By.Asc,\n      },\n    ],\n    where,\n  });\n\n  return res.cyberlinks;\n};\n\nexport const fetchCyberlinksByNerounIterable = async (\n  neuron: NeuronAddress,\n  particlesFrom: ParticleCid[],\n  timestampFrom: number,\n  batchSize: number,\n  abortSignal: AbortSignal\n) =>\n  fetchIterableByOffset(fetchCyberlinksByNeroun, {\n    neuron,\n    particlesFrom,\n    timestampFrom,\n    batchSize,\n    abortSignal,\n  });\n\nconst fetchCyberlinksIterable = (\n  particleCid: ParticleCid,\n  timestampFrom: number,\n  abortSignal: AbortSignal\n) =>\n  fetchIterableByOffset(fetchCyberlinks, {\n    particleCid,\n    timestampFrom,\n    abortSignal,\n  });\n\nexport { fetchCyberlinksIterable, fetchCyberlinksCount };\n","const TRANSACTIONS_BATCH_LIMIT = 500;\nconst CYBERLINKS_BATCH_LIMIT = 200;\n\nexport { TRANSACTIONS_BATCH_LIMIT, CYBERLINKS_BATCH_LIMIT };\n","import { CyberLinkSimple, CyberlinkTxHash, ParticleCid } from 'src/types/base';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { asyncIterableBatchProcessor } from 'src/utils/async/iterable';\nimport { CID_TWEET } from 'src/constants/app';\nimport { LinkDto, TransactionDto } from 'src/services/CozoDb/types/dto';\n\nimport { fetchCyberlinksIterable } from '../../../indexer/cyberlinks';\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { MAX_LINKS_RESOLVE_BATCH } from '../consts';\nimport {\n  CYBER_LINK_TRANSACTION_TYPE,\n  CyberLinkValue,\n} from '../../../indexer/types';\nimport { SyncQueueJobType } from 'src/services/CozoDb/types/entities';\n\nconst getUniqueParticlesFromLinks = (links: CyberLinkSimple[]) =>\n  [\n    ...new Set([\n      ...links.map((link) => link.to),\n      ...links.map((link) => link.from),\n    ]),\n  ] as ParticleCid[];\n\n// eslint-disable-next-line import/no-unused-modules\nexport const fetchCyberlinksAndResolveParticles = async (\n  cid: ParticleCid,\n  timestampUpdate: number,\n  particlesResolver: ParticlesResolverQueue,\n  queuePriority: QueuePriority,\n  abortSignal: AbortSignal\n) => {\n  const cyberlinksIterable = fetchCyberlinksIterable(\n    cid,\n    timestampUpdate,\n    abortSignal\n  );\n  const links = [];\n  // eslint-disable-next-line no-restricted-syntax\n  for await (const batch of cyberlinksIterable) {\n    links.push(...batch);\n    const particles = getUniqueParticlesFromLinks(batch);\n    if (particles.length > 0) {\n      await asyncIterableBatchProcessor(\n        particles,\n        (cids: ParticleCid[]) =>\n          particlesResolver!.enqueueBatch(\n            cids,\n            SyncQueueJobType.particle,\n            queuePriority\n          ),\n        MAX_LINKS_RESOLVE_BATCH\n      );\n    }\n  }\n\n  return links;\n};\n\nexport function extractCybelinksFromTransaction(batch: TransactionDto[]) {\n  const cyberlinks = batch.filter(\n    (l) => l.type === CYBER_LINK_TRANSACTION_TYPE\n  );\n  const particlesFound = new Set<string>();\n  const links: LinkDto[] = [];\n  // Get links: only from TWEETS\n  const tweets: Record<ParticleCid, LinkDto> = cyberlinks.reduce<\n    Record<ParticleCid, LinkDto>\n  >((acc, { value, hash, timestamp }: TransactionDto) => {\n    (value as CyberLinkValue).links.forEach((link) => {\n      particlesFound.add(link.to);\n      particlesFound.add(link.from);\n      const txLink = {\n        ...link,\n        timestamp,\n        neuron: (value as CyberLinkValue).neuron,\n        transactionHash: hash,\n      };\n      links.push(txLink);\n\n      if (link.from === CID_TWEET) {\n        acc[txLink.to] = txLink;\n      }\n    });\n    return acc;\n  }, {});\n\n  return {\n    tweets,\n    particlesFound: [...particlesFound],\n    links,\n  };\n}\n","import { NeuronAddress } from 'src/types/base';\nimport { numberToUtcDate } from 'src/utils/date';\nimport { fetchIterableByOffset } from 'src/utils/async/iterable';\nimport {\n  MessagesByAddressCountDocument,\n  MessagesByAddressCountQuery,\n  MessagesByAddressCountQueryVariables,\n  MessagesByAddressSenseDocument,\n  MessagesByAddressSenseQuery,\n  MessagesByAddressSenseQueryVariables,\n} from 'src/generated/graphql';\n\nimport { createIndexerClient } from './utils/graphqlClient';\nimport { Transaction } from './types';\n\ntype OrderDirection = 'desc' | 'asc';\ntype Abortable = { abortSignal: AbortSignal };\n\nexport type MessagesByAddressVariables = {\n  neuron: NeuronAddress;\n  timestampFrom: number;\n  offset?: number;\n  types: Transaction['type'][];\n  orderDirection: OrderDirection;\n  limit: number;\n} & Abortable;\n\nexport const mapMessagesByAddressVariables = ({\n  neuron,\n  timestampFrom,\n  offset = 0,\n  types = [],\n  orderDirection = 'desc',\n  limit,\n}: MessagesByAddressVariables) => ({\n  address: `{${neuron}}`,\n  limit,\n  timestamp_from: numberToUtcDate(timestampFrom),\n  offset,\n  types: `{${types.map((t) => `\"${t}\"`).join(' ,')}}`,\n  order_direction: orderDirection,\n});\n\nconst fetchTransactions = async ({\n  neuron,\n  timestampFrom,\n  offset = 0,\n  types = [],\n  orderDirection = 'desc',\n  limit,\n  abortSignal,\n}: MessagesByAddressVariables) => {\n  const res = await createIndexerClient(abortSignal).request<\n    MessagesByAddressSenseQuery,\n    MessagesByAddressSenseQueryVariables\n  >(\n    MessagesByAddressSenseDocument,\n    mapMessagesByAddressVariables({\n      neuron,\n      timestampFrom,\n      offset,\n      types,\n      orderDirection,\n      limit,\n      abortSignal,\n    }) as MessagesByAddressSenseQueryVariables\n  );\n\n  return res?.messages_by_address as Transaction[];\n};\n\nexport const fetchTransactionMessagesCount = async (\n  address: NeuronAddress,\n  timestampFrom: number,\n  abortSignal: AbortSignal\n) => {\n  const res = await createIndexerClient(abortSignal).request<\n    MessagesByAddressCountQuery,\n    MessagesByAddressCountQueryVariables\n  >(MessagesByAddressCountDocument, {\n    address: `{${address}}`,\n    timestamp: numberToUtcDate(timestampFrom),\n  });\n\n  return res?.messages_by_address_aggregate.aggregate?.count;\n};\n\nexport const fetchTransactionsIterable = ({\n  neuron,\n  timestampFrom,\n  types,\n  orderDirection,\n  limit,\n  abortSignal,\n}: MessagesByAddressVariables) =>\n  fetchIterableByOffset(fetchTransactions, {\n    neuron,\n    timestampFrom,\n    types,\n    orderDirection,\n    limit,\n    abortSignal,\n  });\n","import { TransactionDto } from 'src/services/CozoDb/types/dto';\nimport { SenseChat } from 'src/services/backend/types/sense';\nimport { NeuronAddress } from 'src/types/base';\nimport { Coin } from 'cosmjs-types/cosmos/base/v1beta1/coin';\n\nimport {\n  MSG_SEND_TRANSACTION_TYPE,\n  MSG_MULTI_SEND_TRANSACTION_TYPE,\n  MsgSendTransaction,\n} from '../../../indexer/types';\n\nexport const extractSenseChats = (\n  myAddress: NeuronAddress,\n  transactions: TransactionDto[]\n) => {\n  const sendTransactions =\n    transactions!.filter(\n      (t) =>\n        t.type === MSG_SEND_TRANSACTION_TYPE ||\n        t.type === MSG_MULTI_SEND_TRANSACTION_TYPE\n    ) || [];\n\n  if (sendTransactions.length === 0) {\n    return [];\n  }\n  const chats = new Map<NeuronAddress, SenseChat>();\n  transactions.forEach((t) => {\n    let userAddress = '';\n    if (t.type === MSG_MULTI_SEND_TRANSACTION_TYPE) {\n      const { inputs, outputs } = t.value;\n      const isSender = inputs.find((i) => i.address === myAddress);\n      const userMessages = isSender ? outputs : inputs;\n      userMessages.forEach((msg) =>\n        updateSenseChat(chats, msg.address, t, msg.coins, isSender)\n      );\n    } else if (t.type === MSG_SEND_TRANSACTION_TYPE) {\n      const { fromAddress, toAddress, amount } =\n        t.value as MsgSendTransaction['value'];\n      const isSender = fromAddress === myAddress;\n      userAddress = isSender ? toAddress : fromAddress;\n      updateSenseChat(chats, userAddress, t, amount, isSender);\n    }\n  });\n\n  return chats;\n};\n\nconst updateSenseChat = (\n  chats: Map<NeuronAddress, SenseChat>,\n  addr: string,\n  t: TransactionDto,\n  amount: Coin[],\n  isSender: boolean\n): Map<string, SenseChat> => {\n  const chat = chats.get(addr);\n  const transactions = chat?.transactions || [];\n\n  transactions.push(t);\n  chats.set(addr, {\n    userAddress: addr,\n    lastSendTimestamp: isSender ? t.timestamp : chat?.lastSendTimestamp || 0,\n    last: { amount, memo: t.memo, direction: isSender ? 'to' : 'from' },\n    transactions,\n  });\n  return chats;\n};\n","import { EntryType } from 'src/services/CozoDb/types/entities';\nimport DbApiWrapper from 'src/services/backend/services/DbApi/DbApi';\nimport { NeuronAddress } from 'src/types/base';\nimport {\n  SenseListItem,\n  SenseTransactionMeta,\n} from 'src/services/backend/types/sense';\nimport { throwIfAborted } from 'src/utils/async/promise';\nimport { extractSenseChats } from '../../utils/sense';\n\n// eslint-disable-next-line import/prefer-default-export\nexport const syncMyChats = async (\n  db: DbApiWrapper,\n  myAddress: NeuronAddress,\n  timestampFrom: number,\n  signal: AbortSignal,\n  shouldUpdateTimestamp = true\n) => {\n  const syncItems = await db.findSyncStatus({\n    ownerId: myAddress,\n    entryType: EntryType.chat,\n  });\n\n  const syncItemsMap = new Map(syncItems?.map((i) => [i.id, i]));\n\n  const myTransactions = await db.getTransactions(myAddress, {\n    order: 'asc',\n    timestampFrom,\n  });\n\n  const myChats = extractSenseChats(myAddress, myTransactions!);\n\n  const results: SenseListItem[] = [];\n\n  // eslint-disable-next-line no-restricted-syntax\n  for (const chat of myChats.values()) {\n    const syncItem = syncItemsMap.get(chat.userAddress);\n    const lastTransaction = chat.transactions.at(-1)!;\n\n    const { timestamp: transactionTimestamp, hash, index } = lastTransaction;\n    const syncItemHeader = {\n      entryType: EntryType.chat,\n      ownerId: myAddress,\n      meta: {\n        transactionHash: hash,\n        index,\n      } as SenseTransactionMeta,\n    };\n\n    // if no sync item(first message/initial)\n    if (!syncItem) {\n      const unreadCount = chat.transactions.filter(\n        (t) => t.timestamp > chat.lastSendTimestamp\n      ).length; // uread count on top of my last send message\n\n      const newItem = {\n        ...syncItemHeader,\n        id: chat.userAddress,\n        unreadCount,\n        // if 'fast' then no shift update poiter till 'slow' reupdate\n        timestampUpdate: shouldUpdateTimestamp ? transactionTimestamp : 0,\n        timestampRead: chat.lastSendTimestamp,\n        disabled: false,\n      };\n\n      // eslint-disable-next-line no-await-in-loop\n      await throwIfAborted(db.putSyncStatus.bind(db), signal)(newItem);\n\n      results.push({ ...newItem, meta: lastTransaction });\n    } else {\n      const {\n        id,\n        timestampRead,\n        timestampUpdate,\n        meta,\n        unreadCount: prevUnreadCount,\n      } = syncItem;\n\n      const lastTimestampRead = Math.max(\n        timestampRead!,\n        chat.lastSendTimestamp\n      );\n      const { timestampUpdateContent = 0, timestampUpdateChat = 0 } = meta;\n      const timestampUnreadFrom = Math.max(\n        chat.lastSendTimestamp,\n        timestampUpdateChat\n      );\n      const unreadCount =\n        prevUnreadCount +\n        chat.transactions.filter((t) => t.timestamp > timestampUnreadFrom) // + new messages count\n          .length;\n\n      if (timestampUpdate < transactionTimestamp) {\n        // if message source is 'fast' then no update till 'slow' reupdate\n        const newTimestampUpdateChat = shouldUpdateTimestamp\n          ? transactionTimestamp\n          : timestampUpdateChat;\n\n        const syncStatusChanges = {\n          ...syncItemHeader,\n          id: id!,\n          unreadCount,\n          timestampRead: lastTimestampRead,\n          // show max timestamp to use in sorting, in sense list\n          // real timestamp shold be resynced with 'slow' data source by timestampUpdateChat\n          timestampUpdate: Math.max(\n            transactionTimestamp,\n            timestampUpdateContent,\n            newTimestampUpdateChat\n          ),\n\n          meta: {\n            ...syncItemHeader.meta,\n            timestampUpdateChat: newTimestampUpdateChat,\n            timestampUpdateContent,\n          },\n        };\n\n        // eslint-disable-next-line no-await-in-loop\n        await throwIfAborted(\n          db.updateSyncStatus.bind(db),\n          signal\n        )(syncStatusChanges);\n\n        results.push({\n          ...syncItem,\n          ...syncStatusChanges,\n          meta: lastTransaction,\n        } as SenseListItem);\n      }\n    }\n  }\n  return results;\n};\n","import { ProgressTracking } from 'src/services/backend/types/services';\n\nconst ROLLING_WINDOW = 10;\n\ntype onProgressUpdateFunc = (progress: ProgressTracking) => void;\n\ntype RequestRecord = {\n  timestamp: number;\n  itemCount: number;\n};\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport class ProgressTracker {\n  private requestRecords: RequestRecord[] = [];\n\n  private totalRequests = 0;\n\n  private completedRequests = 0;\n\n  private estimatedTime = -1;\n\n  private batchSize = 1;\n\n  private onProgressUpdate?: onProgressUpdateFunc;\n\n  public get progress(): ProgressTracking {\n    return {\n      totalCount: this.totalRequests,\n      completeCount: this.completedRequests,\n      estimatedTime: this.estimatedTime,\n    };\n  }\n\n  constructor(onProgressUpdate?: onProgressUpdateFunc) {\n    this.onProgressUpdate = onProgressUpdate;\n  }\n\n  public start(totalRequests: number, batchSize = 1) {\n    this.totalRequests = totalRequests;\n    this.requestRecords = [];\n    this.completedRequests = 0;\n    this.estimatedTime = -1;\n    this.batchSize = batchSize;\n\n    return this.progress;\n  }\n\n  public add(extraRequests: number) {\n    this.totalRequests += extraRequests;\n\n    return this.progress;\n  }\n\n  public trackProgress(processedCount: number) {\n    this.addRequestRecord(processedCount);\n\n    if (this.requestRecords.length > ROLLING_WINDOW) {\n      this.requestRecords.shift();\n    }\n\n    if (this.requestRecords.length > 1) {\n      const averageTimePerItem = this.calculateAverageTimePerItem();\n      const remainingRequests = this.totalRequests - this.completedRequests;\n      const estimatedRemainingItems = remainingRequests * processedCount; // Assuming remaining requests will process the same number of items\n      const estimatedRemainingTime =\n        averageTimePerItem * estimatedRemainingItems;\n\n      this.completedRequests += processedCount;\n      this.estimatedTime = Math.round(estimatedRemainingTime); // Convert to seconds;\n      this.onProgressUpdate && this.onProgressUpdate(this.progress);\n    }\n\n    return this.progress;\n  }\n\n  private addRequestRecord(itemCount: number) {\n    this.requestRecords.push({ timestamp: Date.now(), itemCount });\n  }\n\n  private calculateAverageTimePerItem(): number {\n    let totalDiff = 0;\n    let totalItems = 0;\n\n    for (let i = 1; i < this.requestRecords.length; i++) {\n      const timeDiff =\n        this.requestRecords[i].timestamp - this.requestRecords[i - 1].timestamp;\n      const { itemCount } = this.requestRecords[i];\n\n      totalDiff += timeDiff * itemCount;\n      totalItems += itemCount;\n    }\n\n    return totalItems === 0 ? 0 : totalDiff / totalItems;\n  }\n}\n","import {\n  Observable,\n  filter,\n  distinctUntilChanged,\n  map,\n  switchMap,\n  take,\n  tap,\n} from 'rxjs';\n\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport { broadcastStatus } from 'src/services/backend/channels/broadcastStatus';\nimport { SyncEntryName } from 'src/services/backend/types/services';\nimport { CyblogChannel, createCyblogChannel } from 'src/utils/logging/cyblog';\n\nimport DbApiWrapper from '../../../DbApi/DbApi';\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { ProgressTracker } from '../ProgressTracker/ProgressTracker';\nimport { ServiceDeps } from '../types';\nimport { SyncServiceParams } from '../../types';\n\nabstract class BaseSync {\n  protected name: string;\n\n  protected abortController: AbortController;\n\n  protected db: DbApiWrapper | undefined;\n\n  protected progressTracker = new ProgressTracker();\n\n  protected channelApi = new BroadcastChannelSender();\n\n  protected particlesResolver: ParticlesResolverQueue | undefined;\n\n  protected statusApi: ReturnType<typeof broadcastStatus>;\n\n  protected params: SyncServiceParams = {\n    myAddress: null,\n  };\n\n  protected readonly isInitialized$: Observable<boolean>;\n\n  protected cyblogCh: CyblogChannel;\n\n  constructor(\n    name: SyncEntryName,\n    deps: ServiceDeps,\n    particlesResolver: ParticlesResolverQueue\n  ) {\n    this.name = name;\n\n    this.abortController = new AbortController();\n\n    this.statusApi = broadcastStatus(name, this.channelApi);\n    this.particlesResolver = particlesResolver;\n    this.cyblogCh = createCyblogChannel({ thread: 'bckd', module: name });\n    if (!deps.params$) {\n      throw new Error('params$ is not defined');\n    }\n\n    deps.dbInstance$.subscribe((db) => {\n      this.db = db;\n    });\n\n    this.particlesResolver = particlesResolver;\n\n    this.isInitialized$ = this.createIsInitializedObserver(deps);\n\n    this.isInitialized$.subscribe((isInitialized) => {\n      this.cyblogCh.info(\n        `>>> ${this.name} - ${isInitialized ? 'initialized' : 'inactive'}`\n      );\n      this.statusApi.sendStatus(isInitialized ? 'initialized' : 'inactive');\n    });\n\n    this.isInitialized$\n      .pipe(switchMap(() => deps.params$!))\n      .subscribe((params) => {\n        this.params = params;\n        this.cyblogCh.info(`>>> ${this.name} - params updated`, {\n          data: params,\n        });\n      });\n\n    // Restart observer\n    this.isInitialized$\n      .pipe(\n        filter((isInitialized) => !!isInitialized),\n        switchMap(() => this.createRestartObserver(deps.params$!))\n      )\n      .subscribe(() => {\n        this.restart();\n      });\n  }\n\n  protected initAbortController() {\n    this.abortController = new AbortController();\n  }\n\n  protected abstract createIsInitializedObserver(\n    deps: ServiceDeps\n  ): Observable<boolean>;\n\n  // eslint-disable-next-line class-methods-use-this\n  protected createRestartObserver(params$: Observable<SyncServiceParams>) {\n    return params$.pipe(\n      map((params) => params.myAddress),\n      distinctUntilChanged((addrBefore, addrAfter) => addrBefore === addrAfter),\n      map((v) => !!v),\n      filter((v) => !!v)\n    );\n  }\n\n  public abstract restart(): void;\n\n  public abstract start(): void;\n}\n\nexport default BaseSync;\n","/* eslint-disable import/prefer-default-export */\nimport {\n  distinctUntilChanged,\n  filter,\n  Observable,\n  share,\n  switchMap,\n  tap,\n} from 'rxjs';\n\nexport const switchWhenInitialized = (\n  isInitialized$: Observable<boolean>,\n  actionObservable$: Observable<any>,\n  onChange?: (isInitialized: boolean) => void\n) =>\n  isInitialized$.pipe(\n    distinctUntilChanged(),\n    tap((isInitialized) => onChange?.(isInitialized)),\n    filter((initialized) => initialized),\n    switchMap(() => actionObservable$),\n    share()\n  );\n","import { Observable, Subject, from, startWith, switchMap, tap } from 'rxjs';\n\nimport { SyncEntryName } from 'src/services/backend/types/services';\n\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { ServiceDeps } from '../types';\nimport BaseSync from './BaseSync';\nimport { switchWhenInitialized } from '../utils/rxjs/withInitializer';\nimport { SyncServiceParams } from '../../types';\n\nabstract class BaseSyncClient extends BaseSync {\n  protected readonly source$: Observable<any>;\n\n  protected readonly reloadTrigger$ = new Subject<void>();\n\n  constructor(\n    name: SyncEntryName,\n    deps: ServiceDeps,\n    particlesResolver: ParticlesResolverQueue\n  ) {\n    super(name, deps, particlesResolver);\n\n    const source$ = switchWhenInitialized(\n      this.isInitialized$!,\n      this.reloadTrigger$.pipe(\n        startWith(null),\n        tap(() => {\n          // initialize abort conteoller for restart strategy\n          this.initAbortController();\n        }),\n        switchMap(() =>\n          this.createInitObservable().pipe(\n            switchMap((timestampFrom: number) =>\n              this.createClientObservable(timestampFrom).pipe(\n                tap(() => this.statusApi.sendStatus('listen')),\n                switchMap((data) => from(this.onUpdate(data, this.params)))\n              )\n            )\n          )\n        )\n      ),\n      (isInitialized) => {\n        console.log(`>>> ${name} isInitialized`, isInitialized);\n        this.statusApi.sendStatus(isInitialized ? 'initialized' : 'inactive');\n      }\n    );\n\n    source$.subscribe({\n      next: () => {\n        this.statusApi.sendStatus('listen');\n      },\n      error: (err) => {\n        this.statusApi.sendStatus('error', err);\n      },\n    });\n    this.source$ = source$;\n  }\n\n  protected abstract createClientObservable(\n    timestampFrom: number\n  ): Observable<any>;\n\n  protected abstract createInitObservable(): Observable<number>;\n\n  public restart() {\n    this.abortController?.abort();\n    this.reloadTrigger$.next();\n    console.log(`>>> ${this.name} client restart`);\n  }\n\n  protected abstract onUpdate(\n    data: any,\n    params: SyncServiceParams\n  ): Promise<void>;\n\n  public start() {\n    this.source$.subscribe(() => {\n      // dummy subscriber to keep pipeline running - don't remove\n    });\n    return this;\n  }\n}\n\nexport default BaseSyncClient;\n","/* eslint-disable camelcase */\nimport {\n  map,\n  combineLatest,\n  Observable,\n  from,\n  defer,\n  distinctUntilChanged,\n  merge,\n  filter,\n} from 'rxjs';\nimport { isEmpty } from 'lodash';\n\nimport {\n  EntryType,\n  SyncQueueJobType,\n} from 'src/services/CozoDb/types/entities';\nimport { mapIndexerTransactionToEntity } from 'src/services/CozoDb/mapping';\nimport { numberToUtcDate } from 'src/utils/date';\nimport { NeuronAddress } from 'src/types/base';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { SyncStatusDto, TransactionDto } from 'src/services/CozoDb/types/dto';\nimport { asyncIterableBatchProcessor } from 'src/utils/async/iterable';\nimport { throwIfAborted } from 'src/utils/async/promise';\nimport {\n  createNodeWebsocketObservable,\n  getIncomingTransfersQuery,\n} from 'src/services/lcd/websocket';\nimport {\n  MessagesByAddressSenseQueryVariables,\n  MessagesByAddressSenseWsDocument,\n  MessagesByAddressSenseWsSubscription,\n} from 'src/generated/graphql';\n\nimport { mapWebsocketTxToTransactions } from 'src/services/lcd/utils/mapping';\n\nimport { ServiceDeps } from '../types';\nimport { extractCybelinksFromTransaction } from '../utils/links';\n\nimport {\n  fetchTransactionsIterable,\n  mapMessagesByAddressVariables,\n  fetchTransactionMessagesCount,\n} from '../../../indexer/transactions';\nimport { syncMyChats } from './services/chat';\nimport { TRANSACTIONS_BATCH_LIMIT } from '../../../indexer/consts';\nimport BaseSyncClient from '../BaseSyncLoop/BaseSyncClient';\nimport { createIndexerWebsocket } from '../../../indexer/utils/graphqlClient';\nimport { SyncServiceParams } from '../../types';\nimport { MAX_DATABASE_PUT_SIZE } from '../consts';\n\ntype DataStreamResult = {\n  source: 'indexer' | 'node';\n  transactions: TransactionDto[];\n};\n\nclass SyncTransactionsLoop extends BaseSyncClient {\n  protected createIsInitializedObserver(deps: ServiceDeps) {\n    const isInitialized$ = combineLatest([\n      deps.dbInstance$,\n      deps.params$!.pipe(\n        map((params) => params.myAddress),\n        distinctUntilChanged()\n      ),\n      this.particlesResolver!.isInitialized$,\n    ]).pipe(\n      map(\n        ([dbInstance, myAddress, syncQueueInitialized]) =>\n          !!dbInstance && !!syncQueueInitialized && !!myAddress\n      )\n    );\n\n    return isInitialized$;\n  }\n\n  // eslint-disable-next-line class-methods-use-this\n  protected createClientObservable(\n    timestampFrom: number\n  ): Observable<DataStreamResult> {\n    const { myAddress } = this.params;\n    this.cyblogCh.info(\n      `>>> ${this.name} subscribe ${myAddress} from ${numberToUtcDate(\n        timestampFrom\n      )}`\n    );\n\n    const variables = mapMessagesByAddressVariables({\n      neuron: myAddress!,\n      timestampFrom,\n      types: [],\n      orderDirection: 'desc',\n      limit: 100,\n    }) as MessagesByAddressSenseQueryVariables;\n\n    const indexerObservable$ =\n      createIndexerWebsocket<MessagesByAddressSenseWsSubscription>(\n        MessagesByAddressSenseWsDocument,\n        variables\n      ).pipe(\n        map((response: MessagesByAddressSenseWsSubscription) => {\n          return {\n            source: 'indexer',\n            transactions: response.messages_by_address.map((i) =>\n              mapIndexerTransactionToEntity(myAddress!, i)\n            ),\n          };\n        })\n      );\n\n    const nodeObservample$ = createNodeWebsocketObservable(\n      myAddress!,\n      getIncomingTransfersQuery(myAddress!),\n      (message, ctx) => this.cyblogCh.info(message, { unit: 'node-ws', ...ctx })\n    ).pipe(\n      filter((data) => !isEmpty(data)),\n      map((data) => {\n        return {\n          source: 'node',\n          transactions: mapWebsocketTxToTransactions(myAddress!, data),\n        };\n      })\n    );\n\n    return merge(\n      indexerObservable$,\n      nodeObservample$\n    ) as Observable<DataStreamResult>;\n  }\n\n  protected createInitObservable() {\n    return defer(() => from(this.initSync()));\n    // return from(this.initSync());\n  }\n\n  public async initSync() {\n    const { myAddress } = this.params;\n    const { signal } = this.abortController;\n    const syncItem = await this.db!.getSyncStatus(myAddress!, myAddress!);\n\n    const lastTransactionTimestamp = await this.syncTransactions(\n      myAddress!,\n      myAddress!,\n      syncItem\n    );\n\n    this.statusApi.sendStatus('in-progress', `sync my chats`);\n    const syncStatusItems = await syncMyChats(\n      this.db!,\n      myAddress!,\n      syncItem.timestampUpdate,\n      signal\n    );\n\n    this.channelApi.postSenseUpdate(syncStatusItems);\n    this.statusApi.sendStatus('active');\n\n    return lastTransactionTimestamp;\n  }\n\n  protected async onUpdate(\n    { source, transactions }: DataStreamResult,\n    params: SyncServiceParams\n  ) {\n    const { myAddress } = params;\n    const { signal } = this.abortController;\n    if (transactions.length === 0) {\n      this.cyblogCh.info(`>>> ${this.name} ${myAddress} recived 0 updates `);\n      return;\n    }\n    const syncItem = await this.db!.getSyncStatus(myAddress!, myAddress!);\n\n    await this.processBatchTransactions(\n      myAddress!,\n      myAddress!,\n      transactions,\n      syncItem,\n      source\n    );\n\n    this.statusApi.sendStatus('in-progress', `sync my chats`);\n    const syncStatusItems = await syncMyChats(\n      this.db!,\n      myAddress!,\n      syncItem.timestampUpdate,\n      signal,\n      source !== 'node'\n    );\n\n    this.channelApi.postSenseUpdate(syncStatusItems);\n    this.statusApi.sendStatus('listen');\n  }\n\n  public async processBatchTransactions(\n    myAddress: NeuronAddress,\n    address: NeuronAddress,\n    transactions: TransactionDto[],\n    { timestampRead, unreadCount, timestampUpdate }: SyncStatusDto,\n    source: DataStreamResult['source']\n  ) {\n    const { signal } = this.abortController;\n\n    // node transaction is limited by incoming messages,\n    // to prevent missing of other msg types let's avoid to change ts\n    const shouldUpdateTimestamp = source !== 'node';\n\n    this.cyblogCh.info(\n      `   syncTransactions - process ${address}[${source}],  count: ${\n        transactions.length\n      }, from: ${transactions.at(0)?.timestamp}, to: ${\n        transactions.at(-1)?.timestamp\n      }`\n    );\n\n    // save transaction\n    await throwIfAborted(this.db!.putTransactions, signal)(transactions);\n\n    // save links\n    this.syncLinks(transactions, signal);\n\n    const {\n      hash,\n      index,\n\n      timestamp,\n    } = transactions.at(-1)!;\n\n    const lastTimestampFrom = timestamp;\n\n    // Update transaction sync items\n    const newSyncItem = {\n      ownerId: myAddress,\n      entryType: EntryType.transactions,\n      id: address,\n      timestampUpdate: shouldUpdateTimestamp\n        ? lastTimestampFrom\n        : timestampUpdate!,\n      unreadCount: unreadCount! + transactions.length,\n      timestampRead: timestampRead || 0,\n      disabled: false,\n      meta: {\n        transactionHash: hash,\n        index,\n      },\n    };\n\n    await throwIfAborted(this.db!.putSyncStatus, signal)(newSyncItem);\n\n    return lastTimestampFrom;\n  }\n\n  public async syncTransactions(\n    myAddress: NeuronAddress,\n    address: NeuronAddress,\n    syncItem: SyncStatusDto\n  ) {\n    const { unreadCount, timestampUpdate } = syncItem;\n    const timestampFrom = timestampUpdate + 1; // ofsset + 1 to fix milliseconds precision bug\n\n    this.statusApi.sendStatus('estimating');\n\n    const totalMessageCount = await fetchTransactionMessagesCount(\n      address,\n      timestampFrom,\n      this.abortController!.signal\n    );\n\n    this.cyblogCh.info(\n      `>>> syncTransactions - start ${address},  count: ${totalMessageCount}, from: ${timestampFrom}`\n    );\n\n    if (totalMessageCount === 0) {\n      return timestampFrom;\n    }\n\n    this.statusApi.sendStatus(\n      'in-progress',\n      `sync ${address}...`,\n      this.progressTracker.start(\n        Math.ceil(totalMessageCount / TRANSACTIONS_BATCH_LIMIT)\n      )\n    );\n\n    const transactionsAsyncIterable = fetchTransactionsIterable({\n      neuron: address,\n      timestampFrom,\n      types: [], // SENSE_TRANSACTIONS,\n      orderDirection: 'asc',\n      limit: TRANSACTIONS_BATCH_LIMIT,\n      abortSignal: this.abortController?.signal,\n    });\n\n    let transactionCount = 0;\n    let lastTimestampFrom = timestampFrom;\n\n    // eslint-disable-next-line no-restricted-syntax\n    for await (const batch of transactionsAsyncIterable) {\n      this.statusApi.sendStatus(\n        'in-progress',\n        `sync ${address}...`,\n        this.progressTracker.trackProgress(1)\n      );\n\n      transactionCount += batch.length;\n\n      const transactions = batch.map((i) =>\n        mapIndexerTransactionToEntity(address, i)\n      );\n\n      lastTimestampFrom = await this.processBatchTransactions(\n        myAddress,\n        address,\n        transactions,\n        {\n          ...syncItem,\n          unreadCount: unreadCount + transactionCount,\n        },\n        'indexer'\n      );\n    }\n\n    return lastTimestampFrom;\n  }\n\n  private async syncLinks(batch: TransactionDto[], signal: AbortSignal) {\n    const { tweets, particlesFound, links } =\n      extractCybelinksFromTransaction(batch);\n    if (links.length > 0) {\n      await asyncIterableBatchProcessor(\n        links,\n        (links) => throwIfAborted(this.db!.putCyberlinks, signal)(links),\n        MAX_DATABASE_PUT_SIZE\n      );\n    }\n\n    const tweetParticles = Object.keys(tweets);\n\n    const nonTweetParticles = particlesFound.filter(\n      (cid) => !tweetParticles.includes(cid)\n    );\n\n    // pre-resolve 'tweets' particles\n    await this.particlesResolver!.enqueueBatch(\n      tweetParticles,\n      SyncQueueJobType.particle,\n      QueuePriority.HIGH\n    );\n\n    // pre-resolve all the rest particles\n    if (nonTweetParticles.length > 0) {\n      await this.particlesResolver!.enqueueBatch(\n        nonTweetParticles,\n        SyncQueueJobType.particle,\n        QueuePriority.LOW\n      );\n    }\n  }\n}\n\nexport default SyncTransactionsLoop;\n","import { Observable } from 'rxjs';\nimport { WEBSOCKET_URL } from 'src/constants/config';\nimport { NeuronAddress } from 'src/types/base';\nimport { LogFunc } from 'src/utils/logging/cyblog';\n\nexport const getIncomingTransfersQuery = (address: NeuronAddress) =>\n  `tm.event='Tx' AND transfer.recipient='${address}'`;\n\n// eslint-disable-next-line import/no-unused-modules\nexport function createNodeWebsocketObservable(\n  address: NeuronAddress,\n  query: string,\n  log: LogFunc\n) {\n  return new Observable((subscriber) => {\n    const ws = new WebSocket(WEBSOCKET_URL);\n\n    ws.onopen = () => {\n      log(`node ws connected to ${WEBSOCKET_URL} with ${query}`);\n      ws.send(\n        JSON.stringify({\n          jsonrpc: '2.0',\n          method: 'subscribe',\n          id: '0',\n          params: { query },\n        })\n      );\n    };\n\n    ws.onmessage = (event) => {\n      const message = JSON.parse(event.data);\n      log(`node ws ${address} onmessage`, message);\n      subscriber.next(message.result);\n    };\n\n    ws.onerror = (event) => {\n      log(`node ws ${address} error`, { error: event });\n      subscriber.error(event);\n    };\n\n    ws.onclose = () => {\n      log(`node ws ${address} closed`);\n      subscriber.complete();\n    };\n\n    return () => {\n      ws.close();\n    };\n  });\n}\n","import { EntityToDto, DtoToEntity } from 'src/types/dto';\nimport { deserializeString } from './string';\n\nexport const snakeToCamel = (str: string) =>\n  str.replace(/([-_][a-z])/g, (group) =>\n    group.toUpperCase().replace('-', '').replace('_', '')\n  );\n\nexport const camelToSnake = (str: string) =>\n  str.replace(/[A-Z]/g, (letter) => `_${letter.toLowerCase()}`);\n// Function to transform a DB entity to a DTO\n\n// eslint-disable-next-line import/no-unused-modules\nexport function entityToDto<T extends Record<string, any>>(\n  dbEntity: T\n): EntityToDto<T> {\n  if (!dbEntity || typeof dbEntity !== 'object') {\n    return dbEntity;\n  }\n  const dto: Record<string, any> = {}; // Specify the type for dto\n  Object.keys(dbEntity).forEach((key) => {\n    if (Object.prototype.hasOwnProperty.call(dbEntity, key)) {\n      const camelCaseKey = snakeToCamel(key);\n      let value = dbEntity[key];\n      if (Array.isArray(dbEntity[key])) {\n        value = dbEntity[key].map((item) => entityToDto(item));\n      } else if (typeof dbEntity[key] === 'object') {\n        value = entityToDto(dbEntity[key]);\n      } else if (typeof dbEntity[key] === 'string') {\n        value = deserializeString(value);\n      }\n      dto[camelCaseKey] = value;\n    }\n  });\n  return dto as EntityToDto<T>;\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport function dtoToEntity<T extends Record<string, any>>(\n  dto: T\n): DtoToEntity<T> {\n  // in case of recursive calls\n  if (!dto || typeof dto !== 'object') {\n    return dto;\n  }\n  const dbEntity: any = {};\n\n  Object.keys(dto).forEach((key) => {\n    if (Object.prototype.hasOwnProperty.call(dto, key)) {\n      const snakeCaseKey = camelToSnake(key);\n      let value = dto[key];\n      if (Array.isArray(value)) {\n        value = value.map((item) => dtoToEntity(item));\n      } else if (typeof value === 'object') {\n        value = dtoToEntity(value);\n      }\n      //  else if (typeof value === 'string') {\n      //   value = replaceQuotes(value);\n      // }\n      dbEntity[snakeCaseKey] = value;\n    }\n  });\n  return dbEntity as DtoToEntity<T>; // Replace T with the appropriate DB Entity type if known\n}\n\nexport function dtoListToEntity<T extends Record<string, any>>(\n  array: T[]\n): DtoToEntity<T>[] {\n  return array.map((dto) => dtoToEntity(dto));\n}\n\nexport function entityListToDto<T extends Record<string, any>>(\n  array: T[]\n): EntityToDto<T>[] {\n  return array.map((dto) => entityToDto(dto));\n}\n\nexport function removeUndefinedFields(entity: Record<string, any>) {\n  Object.keys(entity).forEach((key) => {\n    if (entity[key] === undefined) {\n      delete entity[key];\n    }\n  });\n  return entity;\n}\n","import { NeuronAddress } from 'src/types/base';\nimport { LinkDto, SyncStatusDto } from 'src/services/CozoDb/types/dto';\nimport { EntryType } from 'src/services/CozoDb/types/entities';\n\nimport { findLastIndex } from 'lodash';\nimport { entityToDto } from 'src/utils/dto';\n\nimport { SenseItemLinkMeta } from '../../types/sense';\nimport { SyncEntryName } from '../../types/services';\n\nexport function getLastReadInfo(\n  links: LinkDto[],\n  ownerId: NeuronAddress,\n  prevTimestampRead = 0,\n  prevUnreadCount = 0\n) {\n  const lastUnreadLinks = links.filter(\n    (link) => link.timestamp > prevTimestampRead\n  );\n  const lastMyLinkIndex = findLastIndex(\n    lastUnreadLinks,\n    (link) => link.neuron === ownerId\n  );\n\n  const unreadCount =\n    lastMyLinkIndex < 0\n      ? prevUnreadCount + lastUnreadLinks.length\n      : lastUnreadLinks.length - lastMyLinkIndex - 1;\n\n  const timestampRead =\n    lastMyLinkIndex < 0 ? prevTimestampRead : links[lastMyLinkIndex].timestamp;\n\n  return {\n    timestampRead,\n    unreadCount,\n  };\n}\n\nexport function changeParticleSyncStatus(\n  syncStatus: Partial<SyncStatusDto>,\n  links: LinkDto[],\n  ownerId: NeuronAddress,\n  shouldUpdateTimestamp = true\n) {\n  const { timestampRead, unreadCount } = getLastReadInfo(\n    links,\n    ownerId,\n    syncStatus.timestampRead,\n    syncStatus.unreadCount\n  );\n\n  const lastLink = entityToDto(links[links.length - 1]);\n  const timestampUpdate = lastLink.timestamp;\n  return {\n    ...syncStatus,\n    ownerId,\n    entryType: EntryType.particle,\n    disabled: false,\n    unreadCount,\n    meta: {\n      ...lastLink,\n      timestamp: timestampUpdate,\n    } as SenseItemLinkMeta,\n    timestampRead,\n    timestampUpdate: shouldUpdateTimestamp\n      ? timestampUpdate\n      : syncStatus.timestampUpdate,\n  } as SyncStatusDto;\n}\n\nconst mapSyncEntryReadable: Record<SyncEntryName, string> = {\n  'my-friends': \"friend's logs\",\n  particles: 'log cyberlinks',\n  resolver: 'particles',\n  transactions: 'transactions',\n  pin: 'ipfs pins',\n};\n\nexport const syncEntryNameToReadable = (name: SyncEntryName) =>\n  mapSyncEntryReadable[name] || name;\n","export const isAbortException = (e: Error) =>\n  e instanceof DOMException && e.name === 'AbortError';\n","import { Observable, defer, filter, from, tap } from 'rxjs';\n\nimport { SyncEntryName } from 'src/services/backend/types/services';\nimport { isAbortException } from 'src/utils/exceptions/helpers';\nimport { clone } from 'ramda';\n\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { ServiceDeps } from '../types';\nimport { createLoopObservable } from '../utils/rxjs/loop';\nimport BaseSync from './BaseSync';\nimport { SyncServiceParams } from '../../types';\n\nabstract class BaseSyncLoop extends BaseSync {\n  private restartLoop: (() => void) | undefined;\n\n  public readonly loop$: Observable<boolean>;\n\n  constructor(\n    name: SyncEntryName,\n    intervalMs: number,\n    deps: ServiceDeps,\n    particlesResolver: ParticlesResolverQueue,\n    {\n      warmupMs,\n    }: {\n      warmupMs: number;\n    } = { warmupMs: 0 }\n  ) {\n    super(name, deps, particlesResolver);\n\n    const { loop$, restartLoop } = createLoopObservable(\n      this.isInitialized$,\n      // defer(() => from(this.sync())),\n      defer(() => from(this.doSync())),\n      {\n        intervalMs,\n        warmupMs,\n        // onStartInterval: () => this.initAbortController(),\n        onError: (error) => {\n          this.cyblogCh.info(`>>> ${name} error`, error.toString());\n          this.statusApi.sendStatus('error', error.toString());\n        },\n        onChange: (isInitialized) => {\n          this.cyblogCh.info(`>>> ${name} initialized: ${isInitialized}`);\n          this.statusApi.sendStatus(isInitialized ? 'initialized' : 'inactive');\n        },\n      }\n    );\n\n    this.loop$ = loop$;\n    this.restartLoop = restartLoop;\n  }\n\n  public restart() {\n    this.abortController?.abort();\n    this.restartLoop?.();\n    this.cyblogCh.info(`>>> ${this.name} loop restart`);\n  }\n\n  public start() {\n    this.loop$.subscribe(() => this.statusApi.sendStatus('active'));\n    return this;\n  }\n\n  private async doSync() {\n    const params = clone(this.params);\n    this.initAbortController();\n    try {\n      await this.sync(params);\n    } catch (e) {\n      const isAborted = isAbortException(e);\n      this.cyblogCh.info(\n        `>>> ${this.name} ${params.myAddress} sync error [abrt:${isAborted}]:`,\n        {\n          error: e,\n        }\n      );\n\n      if (!isAborted) {\n        throw e;\n      }\n    }\n  }\n\n  protected abstract sync(params: SyncServiceParams): Promise<void>;\n}\n\nexport default BaseSyncLoop;\n","/* eslint-disable import/prefer-default-export */\nimport {\n  Observable,\n  switchMap,\n  interval,\n  startWith,\n  tap,\n  retry,\n  delay,\n  exhaustMap,\n  Subject,\n} from 'rxjs';\nimport { switchWhenInitialized } from './withInitializer';\n\ntype LoopObservableOptions = {\n  warmupMs?: number;\n  retryDelayMs?: number;\n  onStartInterval?: () => void;\n  onError?: (error: any) => void;\n  onChange?: (isInitialized: boolean) => void;\n  intervalMs?: number;\n};\n\nexport const createLoopObservable = (\n  isInitialized$: Observable<boolean>,\n  actionObservable$: Observable<any>,\n  options: LoopObservableOptions = {}\n) => {\n  const {\n    intervalMs,\n    warmupMs = 0,\n    onStartInterval,\n    onError,\n    retryDelayMs = 0,\n    onChange,\n  } = options;\n\n  const restartTrigger$ = new Subject<void>();\n\n  const intervalOrRestart$ = restartTrigger$.pipe(\n    startWith(null),\n    switchMap(() => interval(intervalMs).pipe(startWith(0), delay(warmupMs)))\n  );\n\n  const source$ = switchWhenInitialized(\n    isInitialized$,\n    intervalOrRestart$.pipe(\n      tap(() => onStartInterval && onStartInterval()),\n      exhaustMap(() =>\n        actionObservable$.pipe(\n          retry({\n            delay: (error) => {\n              console.log('retry', error);\n              onError && onError(error);\n              return interval(retryDelayMs);\n            },\n          })\n        )\n      )\n    ),\n    (isInitialized) => onChange?.(isInitialized)\n  );\n\n  return {\n    loop$: source$,\n    restartLoop: () => {\n      // console.log('>>> createLoopObservable restart');\n      // Trigger a restart by emitting a new value\n      restartTrigger$.next();\n    },\n  };\n};\n","import { map, combineLatest, distinctUntilChanged } from 'rxjs';\nimport { EntryType } from 'src/services/CozoDb/types/entities';\nimport { SyncStatusDto } from 'src/services/CozoDb/types/dto';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { NeuronAddress } from 'src/types/base';\n\nimport { mapLinkFromIndexerToDto } from 'src/services/CozoDb/mapping';\nimport { CID_TWEET } from 'src/constants/app';\nimport { dateToUtcNumber } from 'src/utils/date';\nimport { SenseListItem } from 'src/services/backend/types/sense';\nimport { asyncIterableBatchProcessor } from 'src/utils/async/iterable';\nimport { throwIfAborted } from 'src/utils/async/promise';\nimport { entityToDto } from 'src/utils/dto';\n\nimport { ServiceDeps } from '../types';\nimport { fetchCyberlinksAndResolveParticles } from '../utils/links';\n\nimport { changeParticleSyncStatus } from '../../utils';\nimport {\n  fetchCyberlinksByNerounIterable,\n  fetchCyberlinksCount,\n} from '../../../indexer/cyberlinks';\nimport { CYBERLINKS_BATCH_LIMIT } from '../../../indexer/consts';\nimport BaseSyncLoop from '../BaseSyncLoop/BaseSyncLoop';\nimport { MAX_DATABASE_PUT_SIZE } from '../consts';\nimport { SyncServiceParams } from '../../types';\n\nclass SyncParticlesLoop extends BaseSyncLoop {\n  protected createIsInitializedObserver(deps: ServiceDeps) {\n    const isInitialized$ = combineLatest([\n      deps.dbInstance$,\n      deps.ipfsInstance$,\n      deps.params$!.pipe(\n        map((params) => params.myAddress),\n        distinctUntilChanged()\n      ),\n      this.particlesResolver!.isInitialized$,\n    ]).pipe(\n      map(\n        ([dbInstance, ipfsInstance, myAddress, particleResolverInitialized]) =>\n          !!ipfsInstance &&\n          !!dbInstance &&\n          !!particleResolverInitialized &&\n          !!myAddress\n      )\n    );\n\n    return isInitialized$;\n  }\n\n  protected async sync(params: SyncServiceParams): Promise<void> {\n    const { myAddress } = params;\n    const { signal } = this.abortController;\n    this.statusApi.sendStatus('estimating');\n\n    const syncItemParticles = await this.db!.findSyncStatus({\n      ownerId: myAddress!,\n      entryType: EntryType.particle,\n    });\n\n    const timestampUpdate = syncItemParticles.at(0)?.timestampUpdate || 0;\n\n    // Get count of new links after last update\n    const newLinkCount = await fetchCyberlinksCount(\n      myAddress!,\n      [CID_TWEET],\n      timestampUpdate,\n      signal\n    );\n\n    this.cyblogCh.info(\n      `>>> syncMyParticles ${myAddress} count ${newLinkCount}`\n    );\n    this.progressTracker.start(newLinkCount + syncItemParticles.length);\n    this.statusApi.sendStatus(\n      'in-progress',\n      'preparing...',\n      this.progressTracker.progress\n    );\n\n    if (newLinkCount > 0) {\n      // fetch and save new particles\n      const newSyncItemParticles = await this.fetchNewTweets(\n        myAddress!,\n        timestampUpdate,\n        signal\n      );\n\n      // add to fetch-sync linked particles\n      syncItemParticles.push(...newSyncItemParticles);\n    }\n    await this.syncParticles(myAddress!, syncItemParticles, signal);\n  }\n\n  private async fetchNewTweets(\n    myAddress: NeuronAddress,\n    timestampUpdate: number,\n    signal: AbortSignal\n  ) {\n    const tweetsAsyncIterable = await fetchCyberlinksByNerounIterable(\n      myAddress,\n      [CID_TWEET],\n      timestampUpdate,\n      CYBERLINKS_BATCH_LIMIT,\n      this.abortController?.signal\n    );\n\n    const newTweets: SyncStatusDto[] = [];\n    const existingParticles = await this.db!.findSyncStatus({\n      ownerId: myAddress,\n      entryType: EntryType.particle,\n    });\n    const existingParticlesMap = new Map(\n      existingParticles.map((i) => [i.id, i])\n    );\n    // eslint-disable-next-line no-await-in-loop, no-restricted-syntax\n    for await (const tweetsBatch of tweetsAsyncIterable) {\n      this.statusApi.sendStatus(\n        'in-progress',\n        `fetching new tweets...`,\n        this.progressTracker.trackProgress(1)\n      );\n      const syncStatusEntities = tweetsBatch.map(entityToDto).map((item) => {\n        const { timestamp, to } = item;\n        const timestampUpdate = dateToUtcNumber(timestamp);\n\n        // In case my tweet already linked from other neuron, resync from beginning\n        const timestampSyncFrom = existingParticlesMap.get(to)\n          ? dateToUtcNumber(timestamp)\n          : 0;\n\n        // Initial state\n        return {\n          ownerId: myAddress,\n          id: to,\n          entryType: EntryType.particle,\n          timestampUpdate: timestampSyncFrom,\n          timestampRead: timestampUpdate,\n          unreadCount: 0,\n          disabled: false,\n          meta: { ...item, timestamp: timestampUpdate },\n        } as SyncStatusDto;\n      });\n\n      if (syncStatusEntities.length > 0) {\n        await throwIfAborted(\n          this.db!.putSyncStatus,\n          signal\n        )(syncStatusEntities);\n        newTweets.push(...syncStatusEntities);\n      }\n    }\n\n    return newTweets;\n  }\n\n  private async syncParticles(\n    myAddress: NeuronAddress,\n    syncItems: SyncStatusDto[],\n    signal: AbortSignal\n  ) {\n    const updatedSyncItems: SyncStatusDto[] = [];\n\n    // eslint-disable-next-line no-restricted-syntax\n    for (const syncItem of syncItems) {\n      const { id, timestampUpdate } = syncItem;\n\n      this.statusApi.sendStatus(\n        'in-progress',\n        `fetching tweet updates...`,\n        this.progressTracker.trackProgress(1)\n      );\n      // eslint-disable-next-line no-await-in-loop\n      const linksIndexer = await fetchCyberlinksAndResolveParticles(\n        id,\n        timestampUpdate,\n        this.particlesResolver!,\n        QueuePriority.MEDIUM,\n        this.abortController?.signal\n      );\n\n      if (linksIndexer.length > 0) {\n        const links = linksIndexer.map(mapLinkFromIndexerToDto);\n\n        // save links\n        // eslint-disable-next-line no-await-in-loop\n        await asyncIterableBatchProcessor(\n          links,\n          (links) => throwIfAborted(this.db!.putCyberlinks, signal)(links),\n          MAX_DATABASE_PUT_SIZE\n        );\n\n        const newItem = changeParticleSyncStatus(syncItem, links, myAddress);\n\n        updatedSyncItems.push(newItem);\n      }\n    }\n\n    if (updatedSyncItems.length > 0) {\n      await throwIfAborted(this.db!.putSyncStatus, signal)(updatedSyncItems);\n    }\n    this.channelApi.postSenseUpdate(updatedSyncItems as SenseListItem[]);\n  }\n}\n\nexport default SyncParticlesLoop;\n","/* eslint-disable camelcase */\nimport {\n  map,\n  combineLatest,\n  distinctUntilChanged,\n  BehaviorSubject,\n} from 'rxjs';\n\nimport {\n  EntryType,\n  SyncQueueJobType,\n} from 'src/services/CozoDb/types/entities';\n\nimport { NeuronAddress } from 'src/types/base';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { isAbortException } from 'src/utils/exceptions/helpers';\n\nimport { mapLinkFromIndexerToDto } from 'src/services/CozoDb/mapping';\nimport { throwIfAborted } from 'src/utils/async/promise';\n\nimport { SyncEntryName } from 'src/services/backend/types/services';\nimport { SenseItemLinkMeta } from 'src/services/backend/types/sense';\nimport { entityToDto } from 'src/utils/dto';\nimport { ServiceDeps } from '../types';\n\nimport { fetchCyberlinksByNerounIterable } from '../../../indexer/cyberlinks';\nimport { CYBERLINKS_BATCH_LIMIT } from '../../../indexer/consts';\nimport BaseSyncLoop from '../BaseSyncLoop/BaseSyncLoop';\nimport { SyncServiceParams } from '../../types';\nimport { getLastReadInfo } from '../../utils';\n\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { SENSE_FRIEND_PARTICLES } from '../consts';\n\nclass SyncMyFriendsLoop extends BaseSyncLoop {\n  protected followings: NeuronAddress[] = [];\n\n  constructor(\n    name: SyncEntryName,\n    intervalMs: number,\n    deps: ServiceDeps,\n    particlesResolver: ParticlesResolverQueue,\n    { warmupMs }: { warmupMs: number } = { warmupMs: 0 }\n  ) {\n    if (!deps.followings$) {\n      throw new Error('followings$ is required');\n    }\n\n    super(name, intervalMs, deps, particlesResolver, {\n      warmupMs,\n    });\n  }\n\n  protected createIsInitializedObserver(deps: ServiceDeps) {\n    const followingsInitialized$ = new BehaviorSubject<boolean>(false);\n    deps.params$\n      ?.pipe(\n        map((params) => params.myAddress),\n        distinctUntilChanged()\n      )\n      .subscribe(() => {\n        followingsInitialized$.next(false);\n      });\n\n    deps.followings$!.subscribe((followings) => {\n      this.followings = followings;\n      followingsInitialized$.next(true);\n\n      this.restart();\n    });\n\n    const isInitialized$ = combineLatest([\n      deps.dbInstance$,\n      deps.params$!,\n      this.particlesResolver!.isInitialized$,\n      followingsInitialized$!,\n    ]).pipe(\n      map(\n        ([dbInstance, params, syncQueueInitialized, followingsInitialized]) =>\n          !!dbInstance &&\n          !!params.myAddress &&\n          !!syncQueueInitialized &&\n          followingsInitialized\n      )\n    );\n\n    return isInitialized$;\n  }\n\n  protected async sync(params: SyncServiceParams) {\n    const { signal } = this.abortController;\n\n    this.statusApi.sendStatus('in-progress', 'preparing...');\n    const { myAddress } = params;\n\n    const { followings } = this;\n\n    this.statusApi.sendStatus('estimating');\n\n    this.cyblogCh.info(\n      `>>> syncMyFriends ${myAddress} count ${followings.length}`,\n      {\n        unit: 'friends-sync',\n        data: followings,\n      }\n    );\n\n    this.progressTracker.start(followings.length);\n    this.statusApi.sendStatus(\n      'in-progress',\n      `sync...`,\n      this.progressTracker.progress\n    );\n\n    // eslint-disable-next-line no-restricted-syntax\n    for (const addr of followings) {\n      // eslint-disable-next-line no-await-in-loop\n      await this.syncLinks(myAddress!, addr, signal);\n    }\n  }\n\n  public async syncLinks(\n    myAddress: NeuronAddress,\n    address: NeuronAddress,\n    signal: AbortSignal\n  ) {\n    let syncUpdates = [];\n    try {\n      this.statusApi.sendStatus(\n        'in-progress',\n        `starting sync ${address}...`,\n        this.progressTracker.progress\n      );\n      const { timestampRead, unreadCount, meta } = await this.db!.getSyncStatus(\n        myAddress,\n        address\n      );\n\n      const { timestampUpdateChat = 0, timestampUpdateContent = 0 } =\n        meta || {};\n\n      const timestampFrom = timestampUpdateContent + 1; // ofsset + 1 to fix milliseconds precision bug\n\n      const linksAsyncIterable = await fetchCyberlinksByNerounIterable(\n        address,\n        SENSE_FRIEND_PARTICLES,\n        timestampFrom,\n        CYBERLINKS_BATCH_LIMIT,\n        signal\n      );\n\n      // eslint-disable-next-line no-restricted-syntax\n      for await (const linksBatch of linksAsyncIterable) {\n        this.statusApi.sendStatus(\n          'in-progress',\n          `sync ${address}...`,\n          this.progressTracker.trackProgress(1)\n        );\n\n        const links = linksBatch.map(mapLinkFromIndexerToDto);\n\n        const { timestampRead: newTimestampRead, unreadCount: newUnreadCount } =\n          getLastReadInfo(links, myAddress, timestampRead, unreadCount);\n\n        // const unreadItemsCount = unreadCount + links.length;\n\n        if (links.length > 0) {\n          const lastLink = entityToDto(links.at(-1)!);\n          const newTimestampUpdateContent = lastLink!.timestamp;\n\n          await throwIfAborted(this.db!.putCyberlinks, signal)(links);\n\n          const particles = links.map((t) => t.to);\n          await this.particlesResolver!.enqueueBatch(\n            particles,\n            SyncQueueJobType.particle,\n            QueuePriority.HIGH\n          );\n\n          const newSyncItem = {\n            ownerId: myAddress,\n            entryType: EntryType.chat,\n            id: address,\n            timestampUpdate: Math.max(\n              newTimestampUpdateContent,\n              timestampUpdateChat\n            ),\n            unreadCount: newUnreadCount,\n            timestampRead: newTimestampRead,\n            disabled: false,\n            meta: {\n              ...lastLink!,\n              timestampUpdateContent: newTimestampUpdateContent,\n              timestampUpdateChat,\n            } as SenseItemLinkMeta,\n          };\n          // Update transaction\n          await throwIfAborted(this.db!.putSyncStatus, signal)(newSyncItem);\n\n          syncUpdates.push(newSyncItem);\n        }\n      }\n    } catch (err) {\n      this.cyblogCh.error(`>>> SyncMyFriends ${address} error`, {\n        error: err,\n      });\n      if (!isAbortException(err)) {\n        this.statusApi.sendStatus('error', err.toString());\n      } else {\n        syncUpdates = [];\n        throw err;\n      }\n    } finally {\n      // console.log('-----syncUpdates with redux', syncUpdates);\n      this.channelApi.postSenseUpdate(syncUpdates);\n    }\n  }\n\n  // eslint-disable-next-line class-methods-use-this\n  // protected createRestartObserver(\n  //   params$: Observable<SyncServiceParams>\n  // ): Observable<boolean> {\n  //   return super\n  //     .createRestartObserver(params$)\n  //     .pipe(switchMap((addressChanged) => this.isInitialized$));\n  // }\n}\n\nexport default SyncMyFriendsLoop;\n","import { NeuronAddress, ParticleCid } from 'src/types/base';\nimport { getIpfsHash } from 'src/utils/ipfs/helpers';\nimport { PATTERN_CYBER } from 'src/constants/patterns';\nimport { Subject, Observable } from 'rxjs';\n\nimport DbApiWrapper from '../backend/services/DbApi/DbApi';\nimport { getFollowsAsCid, getFollowers } from './lcd';\nimport { FetchParticleAsync, QueuePriority } from '../QueueManager/types';\nimport { CommunityDto } from '../CozoDb/types/dto';\nimport { FetchIpfsFunc } from '../backend/services/sync/types';\nimport { createCyblogChannel } from 'src/utils/logging/cyblog';\n\nexport type SyncCommunityResult = {\n  action: 'reset' | 'add' | 'complete';\n  items: CommunityDto[];\n};\n\nconst cyblogCh = createCyblogChannel({\n  thread: 'bckd',\n  unit: 'fetchStoredSyncCommunity',\n});\n\n// eslint-disable-next-line import/prefer-default-export, import/no-unused-modules\nexport const fetchStoredSyncCommunity$ = (\n  dbApi: DbApiWrapper,\n  address: NeuronAddress,\n  fetchParticleAsync?: FetchIpfsFunc,\n  signal?: AbortSignal\n): Observable<SyncCommunityResult> => {\n  return new Observable<SyncCommunityResult>((subscriber) => {\n    subscriber.next({ action: 'reset', items: [] });\n\n    (async () => {\n      const storedCommunity = await dbApi.getCommunity(address);\n\n      subscriber.next({ action: 'add', items: storedCommunity });\n\n      const communityUpdatesMap = new Map<ParticleCid, CommunityDto>(\n        storedCommunity.map((c) => [c.particle, c])\n      );\n\n      const getExistingOrDefault = (cid: ParticleCid): Partial<CommunityDto> =>\n        communityUpdatesMap.get(cid) || {\n          ownerId: address,\n          name: '',\n          following: false,\n          follower: false,\n        };\n\n      const followsCids = await getFollowsAsCid(address, signal);\n      const followers = await getFollowers(address, signal);\n\n      const newFollowerCids = followsCids.filter(\n        (cid) => !storedCommunity.some((i) => i.particle === cid && i.following)\n      );\n\n      const newFollowingNeurons = followers.filter(\n        (addr) => !storedCommunity.some((i) => i.neuron === addr && i.follower)\n      );\n\n      cyblogCh.info(\n        `>>>$ sync community ${address} processing, stored ${storedCommunity.length} new followers: ${newFollowerCids.length} new following: ${newFollowingNeurons.length}`\n      );\n\n      const followersCommunity = await Promise.all(\n        newFollowingNeurons.map(async (neuron) => {\n          const cid = await getIpfsHash(neuron);\n\n          const communityItem = {\n            ...getExistingOrDefault(cid),\n            particle: cid,\n            neuron,\n            follower: true,\n          } as CommunityDto;\n\n          await dbApi.putCommunity(communityItem);\n          communityUpdatesMap.set(cid, communityItem);\n          return communityItem;\n        })\n      );\n\n      subscriber.next({ action: 'add', items: followersCommunity });\n\n      await Promise.all(\n        newFollowerCids.map(async (cid: ParticleCid) => {\n          const neuron = (await fetchParticleAsync!(cid, QueuePriority.URGENT))\n            ?.result?.textPreview;\n          if (neuron && neuron.match(PATTERN_CYBER)) {\n            const communityItem = {\n              ...getExistingOrDefault(cid),\n              neuron,\n              particle: cid,\n              following: true,\n            } as CommunityDto;\n\n            await dbApi.putCommunity(communityItem);\n            communityUpdatesMap.set(cid, communityItem);\n            subscriber.next({ action: 'add', items: [communityItem] });\n          }\n        })\n      );\n\n      cyblogCh.info(`>>>$ sync community ${address}, done`);\n      // const communityUpdates = [...communityUpdatesMap.values()];\n\n      // if (communityUpdates.length > 0) {\n      //   subscriber.next(communityUpdates);\n      // }\n      subscriber.next({ action: 'complete', items: [] });\n\n      subscriber.complete();\n    })().catch((err) => {\n      cyblogCh.error(`>>>$ sync community ${address}, error`, { error: err });\n      subscriber.error(err);\n    });\n  });\n};\n\n// eslint-disable-next-line import/no-unused-modules\nexport const fetchCommunity = async (\n  address: NeuronAddress,\n  fetchParticleAsync?: FetchParticleAsync,\n  onResolve?: (community: CommunityDto[]) => void,\n  signal?: AbortSignal\n) => {\n  const communityUpdatesMap = new Map<ParticleCid, CommunityDto>();\n\n  const getExistingOrDefault = (cid: ParticleCid): Partial<CommunityDto> =>\n    communityUpdatesMap.get(cid) || {\n      ownerId: address,\n      name: '',\n      following: false,\n      follower: false,\n    };\n\n  const followsCids = await getFollowsAsCid(address, signal);\n  const followers = await getFollowers(address, signal);\n\n  console.log(`>>> sync community ${address} processing without store`);\n\n  const followsPromise = Promise.all(\n    followsCids.map(async (cid) => {\n      const neuron = (await fetchParticleAsync!(cid))?.result?.textPreview;\n      if (neuron && neuron.match(PATTERN_CYBER)) {\n        const communityItem = {\n          ...getExistingOrDefault(cid),\n          neuron,\n          particle: cid,\n          following: true,\n        } as CommunityDto;\n        communityUpdatesMap.set(cid, communityItem);\n        onResolve && !signal?.aborted && onResolve([communityItem]);\n      }\n    })\n  );\n\n  const followersPromise = Promise.all(\n    followers.map(async (neuron) => {\n      const cid = await getIpfsHash(neuron);\n\n      const communityItem = {\n        ...getExistingOrDefault(cid),\n        particle: cid,\n        neuron,\n        follower: true,\n      } as CommunityDto;\n\n      communityUpdatesMap.set(cid, communityItem);\n      onResolve && !signal?.aborted && onResolve([communityItem]);\n    })\n  );\n\n  await Promise.all([followersPromise, followsPromise]);\n};\n","import { NeuronAddress, ParticleCid } from 'src/types/base';\nimport { CID_FOLLOW } from 'src/constants/app';\nimport { getIpfsHash } from 'src/utils/ipfs/helpers';\nimport { getTransactions } from 'src/services/transactions/lcd';\n\nexport const getFollowsAsCid = async (\n  address: NeuronAddress,\n  signal?: AbortSignal\n): Promise<ParticleCid[]> => {\n  const response = await getTransactions({\n    events: [\n      {\n        key: 'cyberlink.neuron',\n        value: address,\n      },\n      {\n        key: 'cyberlink.particleFrom',\n        value: CID_FOLLOW,\n      },\n    ],\n    pagination: {\n      limit: 1000000000,\n    },\n    config: {\n      signal,\n    },\n  });\n\n  if (!response.txResponses.length) {\n    return [];\n  }\n\n  return response.txResponses.map(\n    (item) => item?.tx?.body.messages[0].links[0].to\n  );\n};\n\nexport const getFollowers = async (\n  address: NeuronAddress,\n  signal?: AbortSignal\n): Promise<NeuronAddress[]> => {\n  const addressHash = await getIpfsHash(address);\n\n  const response = await getTransactions({\n    events: [\n      {\n        key: 'cyberlink.particleFrom',\n        value: CID_FOLLOW,\n      },\n      {\n        key: 'cyberlink.particleTo',\n        value: addressHash,\n      },\n    ],\n    pagination: {\n      limit: 1000000000,\n    },\n    config: {\n      signal,\n    },\n  });\n\n  if (!response.txResponses.length) {\n    return [];\n  }\n\n  return response.txResponses.map((item) => item?.tx?.body.messages[0].neuron);\n};\n","import { BehaviorSubject, Observable, first } from 'rxjs';\nimport { LinkDto } from 'src/services/CozoDb/types/dto';\nimport { IPFSContent } from 'src/services/ipfs/types';\nimport { mapParticleToEntity } from 'src/services/CozoDb/mapping';\nimport { QueueChannelMessage } from './types';\nimport { CYB_QUEUE_CHANNEL } from '../consts';\n\nimport { enqueueParticleEmbeddingMaybe } from './backendQueueSenders';\nimport ParticlesResolverQueue from '../../services/sync/services/ParticlesResolverQueue/ParticlesResolverQueue';\nimport DbApi from '../../services/DbApi/DbApi';\n\nimport { SyncQueueItem } from '../../services/sync/services/ParticlesResolverQueue/types';\nimport { Option } from 'src/types';\n\nclass BackendQueueChannelListener {\n  private channel = new BroadcastChannel(CYB_QUEUE_CHANNEL);\n\n  private particlesResolver: ParticlesResolverQueue;\n\n  private dbInstance$: BehaviorSubject<Option<DbApi>>;\n\n  constructor(\n    particlesResolver: ParticlesResolverQueue,\n    dbInstance$: Observable<DbApi | undefined>\n  ) {\n    this.particlesResolver = particlesResolver;\n    dbInstance$.subscribe((v) => {\n      this.dbInstance$.next(v);\n    });\n    this.dbInstance$ = new BehaviorSubject<Option<DbApi>>(undefined);\n\n    this.channel.onmessage = (event) => this.onMessage(event);\n\n    this.channel.onmessageerror = (event) =>\n      console.error(`${CYB_QUEUE_CHANNEL} error`, event);\n  }\n\n  private async getDeffredDbApi(): Promise<DbApi> {\n    return new Promise((resolve) => {\n      const dbApi = this.dbInstance$.getValue();\n      if (dbApi) {\n        resolve(dbApi);\n      }\n\n      this.dbInstance$\n        .pipe(\n          first((value) => value !== undefined) // Automatically unsubscribes after the first valid value\n        )\n        .subscribe((value) => {\n          resolve(value as DbApi);\n        });\n    });\n  }\n\n  private async saveLinks(links: LinkDto[]) {\n    const dbApi = await this.getDeffredDbApi();\n    const res = await dbApi.putCyberlinks(links);\n    // console.log('---saveLinks done', links, res);\n  }\n\n  private async saveParticles(content: IPFSContent) {\n    try {\n      const dbApi = await this.getDeffredDbApi();\n      const entity = mapParticleToEntity(content);\n      const result = await dbApi.putParticles(entity);\n      if (result.ok) {\n        await enqueueParticleEmbeddingMaybe(content);\n      }\n    } catch (e) {\n      console.log(\n        '---saveParticle e',\n        content,\n        content.textPreview,\n        e.toString()\n      );\n      throw e;\n    }\n  }\n\n  private async enquueSync(data: SyncQueueItem | SyncQueueItem[]) {\n    // TODO: TMP ASYNC WAIT TO INIT DB\n    await this.getDeffredDbApi();\n\n    this.particlesResolver.enqueue(Array.isArray(data) ? data : [data]);\n  }\n\n  private onMessage(msg: MessageEvent<QueueChannelMessage>) {\n    const { type, data } = msg.data;\n    if (type === 'link') {\n      this.saveLinks(data);\n    } else if (type === 'particle') {\n      this.saveParticles(data);\n    } else if (type === 'sync') {\n      this.enquueSync(data);\n    }\n  }\n}\n\nexport default BackendQueueChannelListener;\n","/* eslint-disable no-restricted-syntax */\nimport { Observable, combineLatest } from 'rxjs';\nimport { map } from 'rxjs/operators';\n\nimport BroadcastChannelSender from '../../channels/BroadcastChannelSender';\n\nimport ParticlesResolverQueue from './services/ParticlesResolverQueue/ParticlesResolverQueue';\n\n// import SyncIpfsLoop from './services/SyncIpfsLoop/SyncIpfsLoop';\nimport SyncTransactionsLoop from './services/SyncTransactionsLoop/SyncTransactionsLoop';\nimport SyncParticlesLoop from './services/SyncParticlesLoop/SyncParticlesLoop';\n\nimport { ServiceDeps } from './services/types';\nimport {\n  MY_FRIENDS_SYNC_INTERVAL,\n  MY_PARTICLES_SYNC_INTERVAL,\n} from './services/consts';\nimport SyncMyFriendsLoop from './services/SyncMyFriendsLoop/SyncMyFriendsLoop';\nimport { SyncEntryName } from '../../types/services';\nimport BaseSyncLoop from './services/BaseSyncLoop/BaseSyncLoop';\nimport createCommunitySync$ from './services/CommunitySync/CommunitySync';\nimport { createCyblogChannel } from 'src/utils/logging/cyblog';\nimport BackendQueueChannelListener from '../../channels/BackendQueueChannel/BackendQueueChannel';\n\nconst cyblogCh = createCyblogChannel({ thread: 'bckd' });\n\n// eslint-disable-next-line import/prefer-default-export\nexport class SyncService {\n  private isInitialized$: Observable<boolean>;\n\n  private channelApi = new BroadcastChannelSender();\n\n  private loops: Partial<Record<SyncEntryName, BaseSyncLoop>> = {};\n\n  constructor(deps: ServiceDeps) {\n    const { dbInstance$, ipfsInstance$ } = deps;\n\n    const particlesResolver = new ParticlesResolverQueue(deps).start();\n\n    const queueListener = new BackendQueueChannelListener(\n      particlesResolver,\n      dbInstance$\n    );\n\n    this.isInitialized$ = combineLatest([dbInstance$, ipfsInstance$]).pipe(\n      map(([dbInstance, ipfsInstance]) => !!dbInstance && !!ipfsInstance)\n    );\n    // subscribe when started\n    this.isInitialized$.subscribe({\n      next: (result) => {\n        return result && this.channelApi.postServiceStatus('sync', 'started');\n      },\n      error: (err) => this.channelApi.postServiceStatus('sync', 'error', err),\n    });\n\n    const communitySync$ = createCommunitySync$(deps);\n    communitySync$.subscribe((community) => {\n      cyblogCh.info('--> community fetched', {\n        unit: 'community',\n        data: community,\n      });\n    });\n\n    const followings$ = communitySync$.pipe(\n      map((c) => c.filter((i) => i.following)),\n      map((c) => c.map((i) => i.neuron))\n    );\n\n    // new SyncIpfsLoop(deps, particlesResolver).start();\n\n    new SyncTransactionsLoop('transactions', deps, particlesResolver).start();\n\n    new SyncParticlesLoop(\n      'particles',\n      MY_PARTICLES_SYNC_INTERVAL,\n      deps,\n      particlesResolver\n    ).start();\n\n    new SyncMyFriendsLoop(\n      'my-friends',\n      MY_FRIENDS_SYNC_INTERVAL,\n      { ...deps, followings$ },\n      particlesResolver\n      // { warmupMs: 1000 }\n    ).start();\n  }\n\n  public restart(name: SyncEntryName) {\n    this.loops[name]?.restart();\n  }\n}\n","import {\n  Observable,\n  combineLatest,\n  defer,\n  distinctUntilChanged,\n  filter,\n  map,\n  switchMap,\n} from 'rxjs';\n\nimport {\n  SyncCommunityResult,\n  fetchStoredSyncCommunity$,\n} from 'src/services/community/community';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport { CommunityDto } from 'src/services/CozoDb/types/dto';\nimport { ServiceDeps } from '../types';\n\n// eslint-disable-next-line import/no-unused-modules\nexport default function createCommunitySync$(\n  deps: ServiceDeps\n): Observable<CommunityDto[]> {\n  const { dbInstance$, ipfsInstance$, params$ } = deps;\n  const channel = new BroadcastChannelSender();\n\n  return combineLatest([\n    dbInstance$,\n    params$!.pipe(\n      map((params) => params.myAddress),\n      distinctUntilChanged()\n    ),\n    ipfsInstance$,\n  ]).pipe(\n    filter(\n      ([dbInstance, myAddress, ipfsInstance]) =>\n        !!dbInstance && !!ipfsInstance && !!myAddress\n    ),\n    switchMap(([dbApi, myAddress, ipfsInstance]) => {\n      const { waitForParticleResolve } = deps;\n      let community: CommunityDto[] = []; // Fix: Add type declaration for community array\n      return new Observable<CommunityDto[]>((observer) => {\n        observer.next([]);\n\n        fetchStoredSyncCommunity$(\n          dbApi!,\n          myAddress!,\n          waitForParticleResolve!\n        ).subscribe(({ action, items }: SyncCommunityResult) => {\n          channel.post({ type: 'load_community', value: { action, items } });\n\n          if (action === 'reset') {\n            community = [];\n          } else if (['add', 'complete'].some((s) => s === action)) {\n            community.push(...items);\n          }\n\n          if (action === 'complete') {\n            observer.next(community);\n            observer.complete();\n          }\n        });\n      });\n    })\n  );\n}\n","/* eslint-disable valid-jsdoc */\n/* eslint-disable import/no-unused-modules */\nimport { fileTypeFromBuffer } from 'file-type';\nimport { concat as uint8ArrayConcat } from 'uint8arrays/concat';\nimport { Uint8ArrayLike } from '../types';\n\ntype ResultWithMime = {\n  result: Uint8ArrayLike;\n  mime: string | undefined;\n  firstChunk: Uint8Array | undefined;\n};\n\ntype StreamDoneCallback = (\n  chunks: Array<Uint8Array>,\n  mime: string | undefined\n) => Promise<void> | void;\n\n// interface AsyncIterableWithReturn<T> extends AsyncIterable<T> {\n//   return?: (value?: unknown) => Promise<IteratorResult<T>>;\n// }\n\nexport const getMimeFromUint8Array = async (\n  raw: Uint8Array | undefined\n): Promise<string | undefined> => {\n  if (!raw) {\n    return 'unknown';\n  }\n  // TODO: try to pass only first N-bytes\n  const fileType = await fileTypeFromBuffer(raw);\n\n  return fileType?.mime || 'text/plain';\n};\n\nexport async function toAsyncIterableWithMime(\n  stream: ReadableStream<Uint8Array>,\n  flush?: StreamDoneCallback\n): Promise<ResultWithMime> {\n  const [firstChunkStream, fullStream] = stream.tee();\n  const chunks: Array<Uint8Array> = []; // accumulate all the data to pim/save\n\n  // Read the first chunk from the stream\n  const firstReader = firstChunkStream.getReader();\n  const { value } = await firstReader.read();\n  const mime = value ? await getMimeFromUint8Array(value) : undefined;\n\n  const restReader = fullStream.getReader();\n\n  const asyncIterable: AsyncIterable<Uint8Array> = {\n    async *[Symbol.asyncIterator]() {\n      while (true) {\n        const { done, value } = await restReader.read();\n        if (done) {\n          flush && flush(chunks, mime);\n          return; // Exit the loop when done\n        }\n        flush && chunks.push(value);\n        yield value; // Yield the value to the consumer\n      }\n    },\n  };\n\n  return { mime, result: asyncIterable, firstChunk: value };\n}\n\nexport async function toReadableStreamWithMime(\n  stream: ReadableStream<Uint8Array>,\n  flush?: StreamDoneCallback\n): Promise<ResultWithMime> {\n  const [firstChunkStream, fullStream] = stream.tee();\n  const chunks: Array<Uint8Array> = []; // accumulate all the data to pim/save\n\n  // Read the first chunk from the stream\n  const firstReader = firstChunkStream.getReader();\n  const { value } = await firstReader.read();\n  const mime = value ? await getMimeFromUint8Array(value) : undefined;\n\n  const modifiedStream = new ReadableStream<Uint8Array>({\n    async pull(controller) {\n      const restReader = fullStream.getReader();\n      const { done, value } = await restReader.read();\n      if (done) {\n        controller.close();\n        flush && flush(chunks, mime);\n      } else {\n        controller.enqueue(value);\n        flush && chunks.push(value);\n      }\n      restReader.releaseLock();\n    },\n    cancel() {\n      firstChunkStream.cancel();\n      fullStream.cancel();\n    },\n  });\n\n  return { mime, result: modifiedStream, firstChunk: value };\n}\n\nexport type onProgressCallback = (progress: number) => void;\n\nexport const getResponseResult = async (\n  response: Uint8ArrayLike,\n  onProgress?: onProgressCallback\n) => {\n  let bytesDownloaded = 0;\n  try {\n    if (response instanceof Uint8Array) {\n      onProgress && onProgress(response.byteLength);\n      return response;\n    }\n    const chunks: Array<Uint8Array> = [];\n\n    if (response instanceof ReadableStream) {\n      const reader = response.getReader();\n\n      const readStream = async ({\n        done,\n        value,\n      }: ReadableStreamReadResult<Uint8Array>): Promise<Uint8Array> => {\n        if (done) {\n          return uint8ArrayConcat(chunks);\n        }\n\n        chunks.push(value!);\n        bytesDownloaded += value!.byteLength;\n        onProgress && onProgress(bytesDownloaded);\n        return reader.read().then(readStream);\n      };\n\n      const readArray: Uint8Array = await reader.read().then(readStream);\n\n      return readArray;\n    }\n\n    if (Symbol.asyncIterator in response) {\n      const reader = response[Symbol.asyncIterator]();\n\n      // if (cid === 'QmRqms6Utkk6L4mtyLQXY2spcQ8Pk7fBBTNjvxa9jTNrXp') {\n      //   debugger;\n      // }\n      // eslint-disable-next-line no-restricted-syntax\n      for await (const chunk of reader) {\n        if (chunk instanceof Uint8Array) {\n          chunks.push(chunk);\n          bytesDownloaded += chunk.byteLength;\n          onProgress && onProgress(bytesDownloaded);\n        }\n      }\n      const result = uint8ArrayConcat(chunks);\n      return result;\n    }\n    return undefined;\n  } catch (error) {\n    console.error(\n      `Error reading stream/iterable.\\r\\n Probably Hot reload error!`,\n      error\n    );\n\n    // throw error;\n\n    return undefined;\n  }\n};\n","import Dexie from 'dexie';\n\nconst db = new Dexie('cyber-page-cash');\ndb.version(3).stores({\n  cid: 'cid',\n  following: 'cid',\n});\n\nexport default db;\n","import db from 'src/db';\n\nconst ipfsCacheDb = () => {\n  const add = async (cid: string, raw: Uint8Array): Promise<void> => {\n    const dbValue = await db.table('cid').get({ cid });\n\n    if (!dbValue) {\n      const ipfsContentAddtToInddexdDB = {\n        cid,\n        data: raw,\n      };\n      db.table('cid').add(ipfsContentAddtToInddexdDB);\n    }\n  };\n\n  const get = async (cid: string): Promise<Uint8Array | undefined> => {\n    // TODO: use cursor\n    const dbValue = await db.table('cid').get({ cid });\n\n    // backward compatibility\n    return dbValue?.data || dbValue?.content;\n  };\n\n  return { add, get };\n};\n\nexport default ipfsCacheDb();\n","import { IPFSNodes, IpfsOptsType } from './types';\n\nexport const CYBER_NODE_SWARM_PEER_ID =\n  'QmUgmRxoLtGERot7Y6G7UyF6fwvnusQZfGR15PuE6pY3aB';\n\nexport const CYBERNODE_SWARM_ADDR_WSS = `/dns4/swarm.io.cybernode.ai/tcp/443/wss/p2p/${CYBER_NODE_SWARM_PEER_ID}`;\nexport const CYBERNODE_SWARM_ADDR_TCP = `/ip4/88.99.105.146/tcp/4001/p2p/${CYBER_NODE_SWARM_PEER_ID}`;\n\nexport const IPFS_CLUSTER_URL = 'https://io.cybernode.ai';\n\nexport const CYBER_GATEWAY_URL = 'https://gateway.ipfs.cybernode.ai';\n\nexport const FILE_SIZE_DOWNLOAD = 20 * 10 ** 6;\n\nexport const getIpfsOpts = () => {\n  let ipfsOpts: IpfsOptsType = {\n    ipfsNodeType: IPFSNodes.HELIA,\n    urlOpts: '/ip4/127.0.0.1/tcp/5001', // default url\n    userGateway: 'http://127.0.0.1:8080',\n  };\n\n  // get type ipfs\n  const lsTypeIpfs = localStorage.getItem('ipfsState');\n  if (lsTypeIpfs !== null) {\n    const lsTypeIpfsData = JSON.parse(lsTypeIpfs);\n    ipfsOpts = { ...ipfsOpts, ...lsTypeIpfsData };\n  }\n\n  localStorage.setItem('ipfsState', JSON.stringify(ipfsOpts));\n\n  return ipfsOpts as IpfsOptsType;\n};\n","import {\n  AddResponse,\n  PinResponse,\n} from '@nftstorage/ipfs-cluster/dist/src/interface';\n\nimport { Cluster } from '@nftstorage/ipfs-cluster';\nimport { IPFS_CLUSTER_URL } from '../config';\n\nconst cyberCluster = () => {\n  const cluster = new Cluster(IPFS_CLUSTER_URL);\n\n  const add = async (\n    file: File | string\n  ): Promise<AddResponse | PinResponse | undefined> => {\n    const dataFile =\n      typeof file === 'string' ? new File([file], 'file.txt') : file;\n    return cluster.add(dataFile, { cidVersion: 0, rawLeaves: false });\n  };\n\n  const status = async (cid: string) => cluster.status(cid);\n  return { add, status };\n};\n\nexport default cyberCluster();\n","import { toString as uint8ArrayToAsciiString } from 'uint8arrays/to-string';\nimport isSvg from 'is-svg';\nimport { PATTERN_HTTP, PATTERN_IPFS_HASH } from 'src/constants/patterns';\nimport { Option } from 'src/types';\n\nimport { shortenString } from 'src/utils/string';\nimport {\n  IPFSContentDetails,\n  IPFSContent,\n  IpfsContentType,\n  IpfsGatewayContentType,\n} from '../types';\nimport { getResponseResult, onProgressCallback } from './stream';\n\nfunction createObjectURL(rawData: Uint8Array, type: string) {\n  const blob = new Blob([rawData], { type });\n  return URL.createObjectURL(blob);\n}\n\nfunction createImgData(rawData: Uint8Array, type: string) {\n  const imgBase64 = uint8ArrayToAsciiString(rawData, 'base64');\n  const file = `data:${type};base64,${imgBase64}`;\n  return file;\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport const detectGatewayContentType = (\n  mime: string | undefined\n): Option<IpfsGatewayContentType> => {\n  if (mime) {\n    if (mime.includes('video')) {\n      return 'video';\n    }\n\n    if (mime.includes('audio')) {\n      return 'audio';\n    }\n\n    if (mime.includes('epub')) {\n      return 'epub';\n    }\n  }\n  return undefined;\n};\n\nconst basic = /\\s?<!doctype html>|(<html\\b[^>]*>|<body\\b[^>]*>|<x-[^>]+>)+/i;\n\nfunction isHtml(string: string) {\n  const newString = string.trim().slice(0, 1000);\n  return basic.test(newString);\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport const chunksToBlob = (\n  chunks: Array<Uint8Array>,\n  mime: string | undefined\n) => new Blob(chunks, mime ? { type: mime } : {});\n\n// eslint-disable-next-line import/no-unused-modules\nexport const mimeToBaseContentType = (\n  mime: string | undefined\n): IpfsContentType => {\n  if (!mime) {\n    return 'other';\n  }\n\n  const initialType = detectGatewayContentType(mime);\n  if (initialType) {\n    return initialType;\n  }\n\n  if (\n    mime.indexOf('text/plain') !== -1 ||\n    mime.indexOf('application/xml') !== -1\n  ) {\n    return 'text';\n  }\n  if (mime.indexOf('image') !== -1) {\n    return 'image';\n  }\n  if (mime.indexOf('application/pdf') !== -1) {\n    return 'pdf';\n  }\n  return 'other';\n};\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport const parseArrayLikeToDetails = async (\n  content: Option<IPFSContent>,\n  cid: string,\n  onProgress?: onProgressCallback\n): Promise<IPFSContentDetails> => {\n  // try {\n  if (!content || !content?.result) {\n    return {\n      gateway: true,\n      text: cid.toString(),\n      cid,\n    };\n  }\n\n  const { result, meta } = content;\n\n  const { mime, contentType } = meta;\n\n  if (!mime) {\n    return {\n      cid,\n      gateway: true,\n      text: `Can't detect MIME for ${cid.toString()}`,\n    };\n  }\n  const contentCid = content.cid;\n\n  const response: IPFSContentDetails = {\n    link: `/ipfs/${cid}`,\n    gateway: false,\n    cid: contentCid,\n    type: contentType,\n  };\n\n  if (detectGatewayContentType(mime)) {\n    return { ...response, gateway: true };\n  }\n\n  const rawData =\n    typeof result !== 'string'\n      ? await getResponseResult(result, onProgress)\n      : result;\n\n  const isStringData = typeof rawData === 'string';\n\n  // console.log(rawData);\n  if (!rawData) {\n    return {\n      ...response,\n      gateway: true,\n      text: `Can't parse content for ${cid.toString()}`,\n    };\n  }\n\n  // clarify text-content subtypes\n  if (response.type === 'text') {\n    // render svg as image\n    if (!isStringData && isSvg(Buffer.from(rawData))) {\n      return {\n        ...response,\n        type: 'image',\n        content: createImgData(rawData, 'image/svg+xml'),\n      };\n    }\n\n    const str = isStringData ? rawData : uint8ArrayToAsciiString(rawData);\n\n    if (str.match(PATTERN_IPFS_HASH)) {\n      return {\n        ...response,\n        type: 'cid',\n        content: str,\n      };\n    }\n    if (str.match(PATTERN_HTTP)) {\n      return {\n        ...response,\n        type: 'link',\n        content: str,\n      };\n    }\n    if (isHtml(str)) {\n      return {\n        ...response,\n        type: 'html',\n        gateway: true,\n        content: cid.toString(),\n      };\n    }\n\n    // TODO: search can bel longer for 42???!\n    // also cover ipns links\n    return {\n      ...response,\n      link: str.length > 42 ? `/ipfs/${cid}` : `/search/${str}`,\n      type: 'text',\n      text: shortenString(str),\n      content: str,\n    };\n  }\n\n  if (!isStringData) {\n    if (response.type === 'image') {\n      return { ...response, content: createImgData(rawData, mime) }; // file\n    }\n    if (response.type === 'pdf') {\n      return {\n        ...response,\n        content: createObjectURL(rawData, mime),\n        gateway: true,\n      }; // file\n    }\n  }\n\n  return response;\n  // } catch (e) {\n  //   console.log('----parseRawIpfsData', e, cid);\n  //   return undefined;\n  // }\n};\n\nexport const contentToUint8Array = async (\n  content: File | string\n): Promise<Uint8Array> => {\n  return new Uint8Array(\n    typeof content === 'string'\n      ? Buffer.from(content)\n      : await content.arrayBuffer()\n  );\n};\n\nexport const createTextPreview = (\n  array: Uint8Array | undefined | string,\n  contentType: IpfsContentType,\n  previewLength = 150\n) => {\n  if (!array) {\n    return undefined;\n  }\n  if (typeof array === 'string') {\n    return array.slice(0, previewLength);\n  }\n  return contentType && contentType === 'text'\n    ? uint8ArrayToAsciiString(array).slice(0, previewLength)\n    : undefined;\n};\n","/* eslint-disable import/no-unused-modules */\nimport { concat as uint8ArrayConcat } from 'uint8arrays/concat';\n\nimport { Option } from 'src/types';\nimport {\n  // getIpfsUserGatewanAndNodeType,\n  IPFSContentMeta,\n  CallBackFuncStatus,\n  IpfsContentSource,\n  IpfsNode,\n  IpfsFileStats,\n  IPFSContent,\n} from '../types';\n\nimport { getMimeFromUint8Array, toAsyncIterableWithMime } from './stream';\n\nimport ipfsCacheDb from './ipfsCacheDb';\nimport cyberCluster from './cluster';\n\nimport {\n  contentToUint8Array,\n  createTextPreview,\n  mimeToBaseContentType,\n} from './content';\n\nimport { CYBER_GATEWAY_URL, FILE_SIZE_DOWNLOAD } from '../config';\n\n// Get data by CID from local storage\nconst loadIPFSContentFromDb = async (\n  cid: string\n): Promise<Option<IPFSContent>> => {\n  // TODO: enable, disabled for tests\n\n  // TODO: use cursor\n  const data = await ipfsCacheDb.get(cid);\n  if (data && data.length) {\n    // TODO: use cursor\n    const mime = await getMimeFromUint8Array(data);\n    const contentType = mimeToBaseContentType(mime);\n\n    const textPreview = createTextPreview(data, contentType);\n\n    const meta: IPFSContentMeta = {\n      type: 'file', // `TODO: ipfs refactor dir support ?\n      size: data.length,\n      sizeLocal: data.length,\n      mime,\n      contentType,\n    };\n    return { result: data, cid, meta, source: 'db', textPreview };\n  }\n\n  return undefined;\n};\n\nconst emptyStats: IpfsFileStats = {\n  type: 'file',\n  size: undefined,\n  sizeLocal: undefined,\n  blocks: undefined,\n};\n\nconst fetchIPFSContentStat = async (\n  cid: string,\n  node?: IpfsNode,\n  signal?: AbortSignal\n): Promise<IpfsFileStats> => {\n  if (node) {\n    const stats = await node.stat(cid, { signal });\n    return stats;\n  }\n  return emptyStats;\n};\n\nconst fetchIPFSContentFromNode = async (\n  cid: string,\n  node?: IpfsNode,\n  controller?: AbortController\n): Promise<Option<IPFSContent>> => {\n  const controllerLegacy = controller || new AbortController();\n  const { signal } = controllerLegacy;\n  let timer: NodeJS.Timeout | undefined;\n\n  if (!node) {\n    console.log('--------fetchIPFSContentFromNode NO NODE INTIALIZED--------');\n    return undefined;\n  }\n\n  if (!controller) {\n    timer = setTimeout(() => {\n      controllerLegacy.abort();\n    }, 1000 * 60 * 1);\n  } // 1 min\n\n  // TODO: cover ipns case\n  try {\n    // const stat = await node.files.stat(path, { signal });\n    const startTime = Date.now();\n    const stats = await fetchIPFSContentStat(cid, node, signal);\n    const meta = stats as IPFSContentMeta;\n    const statsDoneTime = Date.now();\n    meta.statsTime = statsDoneTime - startTime;\n    const allowedSize = stats.size ? stats.size < FILE_SIZE_DOWNLOAD : false;\n    timer && clearTimeout(timer);\n\n    switch (stats.type) {\n      case 'directory': {\n        // TODO: return directory structure\n        return { cid, availableDownload: true, source: 'node', meta: stats };\n      }\n      default: {\n        // Get sample of content\n        const { value: firstChunk } = await node\n          .cat(cid, { signal, length: 2048, offset: 0 })\n          [Symbol.asyncIterator]()\n          .next();\n\n        meta.mime = await getMimeFromUint8Array(firstChunk);\n        meta.contentType = mimeToBaseContentType(meta.mime);\n        const fullyDownloaded =\n          stats.size && stats.size > -1 && firstChunk.length >= stats.size;\n\n        const textPreview = createTextPreview(firstChunk, meta.contentType);\n\n        if (fullyDownloaded) {\n          await ipfsCacheDb.add(cid, uint8ArrayConcat([firstChunk]));\n        }\n\n        // If all content fits in first chunk return byte-array instead iterable\n        const stream = fullyDownloaded\n          ? firstChunk\n          : allowedSize\n          ? node.cat(cid, { signal })\n          : undefined;\n\n        meta.catTime = Date.now() - statsDoneTime;\n\n        // TODO: add to db flag that content is pinned TO local node\n        // if already pinned skip pin\n        if (!meta.local && allowedSize) {\n          node.pin(cid);\n\n          meta.pinTime = Date.now() - meta.catTime;\n        } else {\n          meta.pinTime = -1;\n        }\n\n        return {\n          result: stream,\n          textPreview,\n          cid,\n          meta,\n          source: 'node',\n        };\n        // }\n      }\n    }\n  } catch (error) {\n    console.debug('error fetchIPFSContentFromNode', error);\n    return {\n      cid,\n      availableDownload: true,\n      source: 'node',\n      meta: { ...emptyStats } as IPFSContentMeta,\n    };\n  }\n};\n\nconst fetchIPFSContentFromGateway = async (\n  cid: string,\n  node?: IpfsNode,\n  controller?: AbortController,\n  headers?: Record<string, string>\n): Promise<Option<IPFSContent>> => {\n  // fetch META only from external node(toooo slow), TODO: fetch meta from cybernode\n  const isExternalNode = node?.nodeType === 'external';\n\n  const stats = isExternalNode\n    ? await fetchIPFSContentStat(cid, node, controller?.signal)\n    : emptyStats;\n\n  const contentUrl = `${CYBER_GATEWAY_URL}/ipfs/${cid}`;\n  const response = await fetch(contentUrl, {\n    method: 'GET',\n    signal: controller?.signal,\n    headers,\n  });\n  if (response && response.body) {\n    // fetch doesn't provide any headers in our case :(\n\n    // const contentLength = parseInt(\n    //   response.headers['content-length'] || '-1',\n    //   10\n    // );\n    // const contentType = response.headers['content-type'];\n\n    // Extract meta if ipfs prob/node not started yet\n    // if (!meta.mime) {\n    //   meta = { ...meta, mime: contentType };\n    // }\n\n    // TODO: fix\n    const flushResults = (chunks: Uint8Array[]) =>\n      !isExternalNode\n        ? ipfsCacheDb.add(cid, uint8ArrayConcat(chunks))\n        : Promise.resolve();\n\n    const { mime, result, firstChunk } = await toAsyncIterableWithMime(\n      response.body,\n      flushResults\n    );\n\n    const contentType = mimeToBaseContentType(mime);\n\n    const textPreview = createTextPreview(firstChunk, contentType);\n    return {\n      cid,\n      textPreview,\n      meta: { ...stats, mime, contentType },\n      result,\n      source: 'gateway',\n      contentUrl,\n    };\n  }\n\n  return undefined;\n};\n\ntype fetchContentOptions = {\n  controller?: AbortController;\n  node?: IpfsNode;\n  headers?: Record<string, string>;\n};\n\nasync function fetchIpfsContent(\n  cid: string,\n  source: IpfsContentSource,\n  options: fetchContentOptions\n): Promise<Option<IPFSContent>> {\n  const { node, controller, headers } = options;\n\n  try {\n    switch (source) {\n      case 'db':\n        return loadIPFSContentFromDb(cid);\n      case 'node':\n        return fetchIPFSContentFromNode(cid, node, controller);\n      case 'gateway':\n        return fetchIPFSContentFromGateway(cid, node, controller, headers);\n      default:\n        return undefined;\n    }\n  } catch (e) {\n    console.log('----fetchIpfsContent error', e);\n    return undefined;\n  }\n}\n\nconst getIPFSContent = async (\n  cid: string,\n  node?: IpfsNode,\n  controller?: AbortController,\n  callBackFuncStatus?: CallBackFuncStatus\n): Promise<Option<IPFSContent>> => {\n  const dataRsponseDb = await loadIPFSContentFromDb(cid);\n  if (dataRsponseDb !== undefined) {\n    return dataRsponseDb;\n  }\n\n  if (node) {\n    callBackFuncStatus && callBackFuncStatus('trying to get with a node');\n    // console.log('----Fetch from node', cid);\n    const ipfsContent = await fetchIPFSContentFromNode(cid, node, controller);\n\n    return ipfsContent;\n  }\n\n  callBackFuncStatus && callBackFuncStatus('trying to get with a gatway');\n  // console.log('----Fetch from gateway', cid);\n  const respnseGateway = await fetchIPFSContentFromGateway(\n    cid,\n    node,\n    controller\n  );\n\n  return respnseGateway;\n};\n\nconst catIPFSContentFromNode = (\n  cid: string,\n  node?: IpfsNode,\n  offset?: number,\n  controller?: AbortController\n): AsyncIterable<Uint8Array> | undefined => {\n  if (!node) {\n    console.log(\n      '--------fetchIPFSContentFromNode NO NODE INTIALIZED TODO: cover case--------'\n    );\n    return undefined;\n  }\n\n  // TODO: cover ipns case\n\n  return node.cat(cid, { offset, signal: controller?.signal });\n};\n\n// const nodeContentFindProvs = async (\n//   node: AppIPFS,\n//   cid: string,\n//   offset: number,\n//   controller?: AbortController\n// ): AsyncIterable<number> | undefined => {\n//   if (!node) {\n//     console.log(\n//       '--------fetchIPFSContentFromNode NO NODE INTIALIZED TODO: cover case--------'\n//     );\n//     return undefined;\n//   }\n\n//   // TODO: cover ipns case\n//   const path = `/ipfs/${cid}`;\n\n//   const providers = node.dht.findProvs(path, {\n//     signal: controller?.signal,\n//   });\n\n//   let count = 0;\n//   for await (const provider of providers) {\n//     //  console.log(provider.id.toString())\n//     //  id: PeerId\n//     // multiaddrs: Multiaddr[]\n//     // protocols: string[]\n//     count++;\n//   }\n\n//   return count;\n// };\n\nconst addContenToIpfs = async (\n  node: IpfsNode,\n  content: File | string\n): Promise<Option<string>> => {\n  let cid;\n  if (node) {\n    cid = await node.add(content);\n  }\n  // TODO: WARN - TMP solution make cluster call non-awaitable\n  cyberCluster.add(content);\n  // Save to local cache\n  cid && (await ipfsCacheDb.add(cid, await contentToUint8Array(content)));\n  return cid;\n};\n\nexport {\n  getIPFSContent,\n  catIPFSContentFromNode,\n  fetchIpfsContent,\n  addContenToIpfs,\n};\n","import { IQueueStrategy, QueueSettings, QueueSource } from './types';\n\nexport class QueueStrategy implements IQueueStrategy {\n  settings: QueueSettings;\n\n  order: QueueSource[];\n\n  constructor(settings: QueueSettings, order: QueueSource[]) {\n    this.settings = settings;\n    this.order = order;\n  }\n\n  getNextSource(source: QueueSource): QueueSource | undefined {\n    const index = this.order.indexOf(source);\n    return index < this.order.length ? this.order[index + 1] : undefined;\n  }\n}\n","export class QueueItemTimeoutError extends Error {\n  constructor(timeoutMs: number) {\n    super(`Timeout after ${timeoutMs}`);\n    Object.setPrototypeOf(this, QueueItemTimeoutError.prototype);\n  }\n}\n","/* eslint-disable import/prefer-default-export */\nexport const CustomHeaders = {\n  XCybSource: 'X-Cyb-Source',\n};\n\nexport enum XCybSourceValues {\n  sharedWorker = 'shared-worker',\n}\n","import {\n  BehaviorSubject,\n  EMPTY,\n  Observable,\n  catchError,\n  combineLatest,\n  debounceTime,\n  distinctUntilChanged,\n  filter,\n  interval,\n  map,\n  merge,\n  mergeMap,\n  of,\n  share,\n  tap,\n  throwError,\n  timeout,\n  withLatestFrom,\n} from 'rxjs';\n\nimport * as R from 'ramda';\n\nimport { CybIpfsNode, IpfsContentSource } from 'src/services/ipfs/types';\nimport { fetchIpfsContent } from 'src/services/ipfs/utils/utils-ipfs';\nimport { ParticleCid } from 'src/types/base';\n\nimport { promiseToObservable } from '../../utils/rxjs/helpers';\n\nimport type {\n  QueueItem,\n  QueueItemAsyncResult,\n  QueueItemCallback,\n  QueueItemOptions,\n  QueueItemResult,\n  QueueSource,\n  QueueStats,\n} from './types';\n\nimport { QueueStrategy } from './QueueStrategy';\n\nimport { enqueueParticleSave } from '../backend/channels/BackendQueueChannel/backendQueueSenders';\nimport BroadcastChannelSender from '../backend/channels/BroadcastChannelSender';\nimport { RuneEngine } from '../scripting/engine';\n\nimport { QueueItemTimeoutError } from './QueueItemTimeoutError';\nimport { CustomHeaders, XCybSourceValues } from './constants';\n\nconst QUEUE_DEBOUNCE_MS = 33;\nconst CONNECTION_KEEPER_RETRY_MS = 15000;\n\nfunction getQueueItemTotalPriority(item: QueueItem): number {\n  return (item.priority || 0) + (item.viewPortPriority || 0);\n}\n\nconst debugCid = (cid: ParticleCid, prefix: string, ...args) => {\n  console.log(`>>> ${prefix}: ${cid}`, ...args);\n};\n\nconst strategies = {\n  external: new QueueStrategy(\n    {\n      db: { timeout: 5000, maxConcurrentExecutions: 999 },\n      node: { timeout: 60 * 1000, maxConcurrentExecutions: 30 },\n      gateway: { timeout: 10000, maxConcurrentExecutions: 11 },\n    },\n    ['db', 'node', 'gateway']\n  ),\n  embedded: new QueueStrategy(\n    {\n      db: { timeout: 5000, maxConcurrentExecutions: 999 },\n      node: { timeout: 60 * 1000, maxConcurrentExecutions: 30 },\n      gateway: { timeout: 21000, maxConcurrentExecutions: 11 },\n    },\n    ['db', 'gateway', 'node']\n  ),\n  helia: new QueueStrategy(\n    {\n      db: { timeout: 5000, maxConcurrentExecutions: 999 },\n      node: { timeout: 60 * 1000, maxConcurrentExecutions: 50 },\n      gateway: { timeout: 10000, maxConcurrentExecutions: 11 },\n    },\n    ['db', 'node', 'gateway']\n  ),\n};\n\ntype QueueMap = Map<ParticleCid, QueueItem>;\n\nclass QueueManager {\n  private queue$ = new BehaviorSubject<QueueMap>(new Map());\n\n  private node: CybIpfsNode | undefined = undefined;\n\n  private strategy: QueueStrategy;\n\n  private queueDebounceMs: number;\n\n  private lastNodeCallTime: number = Date.now();\n\n  private channel = new BroadcastChannelSender();\n\n  private executing: Record<QueueSource, Set<ParticleCid>> = {\n    db: new Set(),\n    node: new Set(),\n    gateway: new Set(),\n  };\n\n  private switchStrategy(strategy: QueueStrategy): void {\n    this.strategy = strategy;\n  }\n\n  public async setNode(node: CybIpfsNode, customStrategy?: QueueStrategy) {\n    console.log(\n      `* switch node from ${this.node?.nodeType || '<none>'} to ${\n        node.nodeType\n      }`\n    );\n    this.node = node;\n    this.switchStrategy(customStrategy || strategies[node.nodeType]);\n  }\n\n  private getItemBySourceAndPriority(queue: QueueMap) {\n    const pendingItems = [...queue.values()].filter(\n      (i) => i.status === 'pending'\n    );\n\n    const pendingBySource = R.groupBy((i) => i.source, pendingItems);\n\n    const itemsToExecute: QueueItem[] = [];\n    // eslint-disable-next-line no-loop-func, no-restricted-syntax\n    for (const [queueSource, items] of Object.entries(pendingBySource)) {\n      const settings = this.strategy.settings[queueSource as IpfsContentSource];\n\n      const executeCount =\n        settings.maxConcurrentExecutions -\n        this.executing[queueSource as IpfsContentSource].size;\n      const itemsByPriority = items\n        .sort(\n          (a, b) => getQueueItemTotalPriority(b) - getQueueItemTotalPriority(a)\n        )\n        .slice(0, executeCount);\n\n      itemsToExecute.push(...itemsByPriority);\n    }\n\n    return itemsToExecute;\n  }\n\n  private postSummary() {\n    const summary = `(total: ${this.queue$.value.size} |  db - ${this.executing.db.size} node - ${this.executing.node.size} gateway - ${this.executing.gateway.size})`;\n    this.channel.postServiceStatus('ipfs', 'started', summary);\n  }\n\n  private fetchData$(item: QueueItem) {\n    const { cid, source, callbacks, controller } = item;\n    // const abortController = controller || new AbortController();\n    const settings = this.strategy.settings[source];\n    this.executing[source].add(cid);\n    this.postSummary();\n    const queueItem = this.queue$.value.get(cid);\n    // Mutate item without next\n    this.queue$.value.set(cid, {\n      ...queueItem,\n      status: 'executing',\n      executionTime: Date.now(),\n      controller: new AbortController(),\n    } as QueueItem);\n    // debugCid(cid, 'fetchData', cid, source);\n    callbacks.map((callback) => callback(cid, 'executing', source));\n\n    return promiseToObservable(async () => {\n      return fetchIpfsContent(cid, source, {\n        controller,\n        node: this.node,\n        headers: {\n          [CustomHeaders.XCybSource]: XCybSourceValues.sharedWorker,\n        },\n      }).then((content) => {\n        // put saveto db msg into bus\n        if (content && source !== 'db') {\n          enqueueParticleSave(content);\n        }\n\n        return content;\n      });\n    }).pipe(\n      timeout({\n        each: settings.timeout,\n        with: () =>\n          throwError(() => {\n            controller?.abort('timeout');\n\n            return new QueueItemTimeoutError(settings.timeout);\n          }),\n      }),\n      map((result): QueueItemResult => {\n        return {\n          item,\n          status: result ? 'completed' : 'error',\n          source,\n          result,\n        };\n      }),\n      catchError((error): Observable<QueueItemResult> => {\n        // debugCid(cid, 'fetchData - fetchIpfsContent catchErr', error);\n        if (error instanceof QueueItemTimeoutError) {\n          return of({\n            item,\n            status: 'timeout',\n            source,\n          });\n        }\n\n        if (error?.name === 'AbortError') {\n          return of({ item, status: 'cancelled', source });\n        }\n        return of({ item, status: 'error', source });\n      })\n    );\n  }\n\n  /**\n   * Mutate queue item, and return new queue\n   * @param cid\n   * @param changes\n   * @returns\n   */\n  private mutateQueueItem(cid: string, changes: Partial<QueueItem>) {\n    const queue = this.queue$.value;\n    const item = queue.get(cid);\n    if (item) {\n      queue.set(cid, { ...item, ...changes });\n    }\n\n    return this.queue$.next(queue);\n  }\n\n  private removeAndNext(cid: string): void {\n    const queue = this.queue$.value;\n    queue.delete(cid);\n    this.queue$.next(queue);\n  }\n\n  // reset status and switch to next source\n  private switchSourceAndNext(item: QueueItem, nextSource: QueueSource): void {\n    item.callbacks.map((callback) => callback(item.cid, 'pending', nextSource));\n\n    this.mutateQueueItem(item.cid, { status: 'pending', source: nextSource });\n  }\n\n  private cancelDeprioritizedItems(queue: QueueMap): QueueMap {\n    (['node', 'gateway'] as IpfsContentSource[]).forEach((source) => {\n      Array.from(this.executing[source]).forEach((cid) => {\n        const item = queue.get(cid);\n        if (item && getQueueItemTotalPriority(item) < 0 && item.controller) {\n          // abort request and move to pending\n          item.controller.abort('cancelled');\n          item.callbacks.map((callback) =>\n            callback(item.cid, 'pending', item.source)\n          );\n\n          queue.set(cid, { ...item, status: 'pending' });\n          // console.log('-----cancel item', item, queue);\n\n          this.executing[source].delete(cid);\n        }\n      });\n    });\n\n    return queue;\n  }\n\n  private releaseExecution(cid: string) {\n    // eslint-disable-next-line no-restricted-syntax\n    Object.keys(this.executing).forEach((key) =>\n      this.executing[key as IpfsContentSource].delete(cid)\n    );\n  }\n\n  constructor(\n    ipfsInstance$: Observable<CybIpfsNode | undefined>,\n    {\n      strategy,\n      queueDebounceMs,\n    }: {\n      strategy?: QueueStrategy;\n      queueDebounceMs?: number;\n    }\n  ) {\n    ipfsInstance$.subscribe((node) => {\n      if (node) {\n        this.setNode(node);\n      }\n    });\n\n    this.strategy = strategy || strategies.embedded;\n    this.queueDebounceMs = queueDebounceMs || QUEUE_DEBOUNCE_MS;\n\n    // Little hack to handle keep-alive connection to swarm cyber node\n    // Fix some lag with node peers(when it shown swarm node in peers but not  connected anymore)\n    interval(CONNECTION_KEEPER_RETRY_MS)\n      .pipe(\n        filter(\n          () =>\n            !!this.node &&\n            !![...this.queue$.value.values()].find((i) => i.source === 'node')\n        )\n      )\n      .subscribe(() => {\n        // console.log(\n        //   '-----reconnect cnt',\n        //   this.queue$.value.size,\n        //   this.queue$.value\n        // );\n        this.node!.reconnectToSwarm(true);\n      });\n\n    const isInitialized$ = combineLatest([ipfsInstance$]).pipe(\n      map(([ipfsInstance]) => !!ipfsInstance && ipfsInstance?.isStarted),\n      distinctUntilChanged(),\n      share()\n    );\n\n    isInitialized$.subscribe((isInitialized) => {\n      isInitialized && console.log(' ipfs queue initialized');\n      this.node?.reconnectToSwarm(true);\n    });\n\n    this.queue$\n      .pipe(\n        withLatestFrom(isInitialized$),\n        filter(([, isInitialized]) => isInitialized),\n        debounceTime(this.queueDebounceMs),\n        map(([queue]) => this.cancelDeprioritizedItems(queue)),\n        mergeMap((queue) => {\n          const workItems = this.getItemBySourceAndPriority(queue);\n          // console.log('---workItems', workItems);\n          if (workItems.length > 0) {\n            // wake up connnection to swarm cyber node\n            this.node?.reconnectToSwarm(false);\n\n            return merge(...workItems.map((item) => this.fetchData$(item)));\n          }\n          return EMPTY;\n        })\n      )\n      .subscribe(({ item, status, source, result }) => {\n        const { cid } = item;\n        const callbacks = this.queue$.value.get(cid)?.callbacks || [];\n        // fix to process dublicated items\n        // debugCid(cid, 'subscribe', cid, source, status, result, callbacks);\n\n        callbacks.map((callback) => callback(cid, status, source, result));\n\n        // HACK to use with GracePeriod for reconnection\n        if (source === 'node') {\n          this.lastNodeCallTime = Date.now();\n        }\n\n        this.executing[source].delete(cid);\n\n        // success execution -> next\n        if (status === 'completed' || status === 'cancelled') {\n          // debugCid(cid, '------done', item, status, source, result);\n          this.removeAndNext(cid);\n        } else {\n          // debugCid(cid, '------error', item, status, source, result);\n\n          // Retry -> (next sources) or -> next\n          const nextSource = this.strategy.getNextSource(source);\n\n          if (nextSource) {\n            this.switchSourceAndNext(item, nextSource);\n          } else {\n            this.removeAndNext(cid);\n            // notify thatn nothing found from all sources\n            callbacks.map((callback) =>\n              callback(cid, 'not_found', source, result)\n            );\n          }\n        }\n\n        this.postSummary();\n      });\n  }\n\n  public enqueue(\n    cid: string,\n    callback: QueueItemCallback,\n    options: QueueItemOptions = {}\n  ): void {\n    const queue = this.queue$.value;\n    const existingItem = queue.get(cid);\n    // debugCid(cid, '----/--enqueue ', cid, existingItem);\n\n    // In case if item already in queue,\n    // just attach one more callback to quieued item\n    if (existingItem) {\n      this.mutateQueueItem(cid, {\n        callbacks: [...existingItem.callbacks, callback],\n      });\n    } else {\n      const source = options.initialSource || this.strategy.order[0];\n      const item: QueueItem = {\n        cid,\n        callbacks: [callback],\n        source, // initial method to fetch\n        status: 'pending',\n        postProcessing: true, // by default rune-post-processing enabled\n        ...options,\n      };\n\n      callback(cid, 'pending', source);\n\n      queue.set(cid, item);\n      this.queue$.next(queue);\n    }\n  }\n\n  public enqueueAndWait(\n    cid: string,\n    options: QueueItemOptions = {}\n  ): Promise<QueueItemAsyncResult> {\n    return new Promise((resolve) => {\n      const callback = ((cid, status, source, result) => {\n        if (status === 'completed' || status === 'not_found') {\n          resolve({ status, source, result });\n        }\n      }) as QueueItemCallback;\n\n      this.enqueue(cid, callback, options);\n    });\n  }\n\n  public updateViewPortPriority(cid: string, viewPortPriority: number) {\n    this.mutateQueueItem(cid, { viewPortPriority });\n  }\n\n  public cancel(cid: string): void {\n    const queue = this.queue$.value;\n    const item = queue.get(cid);\n    // console.log('-----cancel item', item, item?.controller);\n    if (item) {\n      // If item has no abortController we can just remove it,\n      // otherwise abort&keep-to-finalize\n      if (!item.controller) {\n        this.removeAndNext(cid);\n      } else {\n        item.controller.abort('cancelled');\n      }\n    }\n  }\n\n  public cancelByParent(parent: string): void {\n    const queue = this.queue$.value;\n\n    queue.forEach((item, cid) => {\n      if (item.parent === parent) {\n        this.releaseExecution(cid);\n        item.controller?.abort('cancelled');\n        queue.delete(cid);\n      }\n    });\n\n    this.queue$.next(queue);\n  }\n\n  public clear(): void {\n    const queue = this.queue$.value;\n\n    queue.forEach((item, cid) => {\n      this.releaseExecution(cid);\n      item.controller?.abort('cancelled');\n      queue.delete(cid);\n    });\n\n    this.queue$.next(new Map());\n  }\n\n  public getQueueMap(): QueueMap {\n    return this.queue$.value;\n  }\n\n  public getQueueList(): QueueItem[] {\n    return Array.from(this.queue$.value.values());\n  }\n\n  public getStats(): QueueStats[] {\n    const fn = R.pipe(\n      R.countBy<QueueItem>(R.prop('status')),\n      R.toPairs,\n      R.map(R.zipObj(['status', 'count']))\n    );\n\n    return fn(this.getQueueList()) as QueueStats[];\n  }\n}\n\nexport default QueueManager;\n","import { Observable } from 'rxjs';\n\n/**\n * Convert promise to observable\n * @param promiseFactory\n * @returns\n */\nexport function promiseToObservable<T>(promiseFactory: () => Promise<T>) {\n  return new Observable<T>((observer) => {\n    promiseFactory()\n      .then((response) => {\n        observer.next(response);\n        observer.complete();\n      })\n      .catch((error) => {\n        console.debug('----promiseToObservable error', error); //, error\n        observer.error(error);\n      });\n  });\n}\n","import { CID } from 'multiformats/cid';\n\nexport const stringToCid = (s: string) => CID.parse(s);\nexport const stringToIpfsPath = (s: string) => `/ipfs/${s}`;\n","import { IPFSHTTPClient, create as createKuboClient } from 'kubo-rpc-client';\nimport { multiaddr } from '@multiformats/multiaddr';\n\nimport { stringToCid, stringToIpfsPath } from '../../utils/cid';\nimport {\n  AbortOptions,\n  CatOptions,\n  IpfsNodeType,\n  InitOptions,\n  IpfsFileStats,\n  IpfsNode,\n  IpfsNodePrperties,\n} from '../../types';\nimport { CYBER_GATEWAY_URL } from '../../config';\n\nclass KuboNode implements IpfsNode {\n  readonly nodeType: IpfsNodeType = 'external';\n\n  private node?: IPFSHTTPClient;\n\n  private _config: IpfsNodePrperties = {};\n\n  get config() {\n    return this._config;\n  }\n\n  private _isStarted: boolean = false;\n\n  get isStarted() {\n    return this._isStarted;\n  }\n\n  private async initConfig() {\n    const response = await this.node!.config.get('Addresses.Gateway');\n    if (!response) {\n      return { gatewayUrl: CYBER_GATEWAY_URL };\n    }\n    const address = multiaddr(response as string).nodeAddress();\n\n    return { gatewayUrl: `http://${address.address}:${address.port}` };\n  }\n\n  async init(options?: InitOptions) {\n    this.node = createKuboClient(options);\n    this._config = await this.initConfig();\n\n    if (typeof window !== 'undefined') {\n      window.node = this.node;\n      window.toCid = stringToCid;\n    }\n    console.log(\n      'IPFS - Kubo addrs',\n      (await this.node.swarm.localAddrs()).map((a) => a.toString())\n    );\n    this._isStarted = true;\n  }\n\n  async stat(cid: string, options: AbortOptions = {}): Promise<IpfsFileStats> {\n    return this.node!.files.stat(stringToIpfsPath(cid), {\n      ...options,\n      withLocal: true,\n      size: true,\n    }).then((result) => {\n      const { type, size, sizeLocal, local, blocks } = result;\n      return {\n        type,\n        size: size || -1,\n        sizeLocal: sizeLocal || -1,\n        blocks,\n      };\n    });\n  }\n\n  cat(cid: string, options: CatOptions = {}) {\n    return this.node!.cat(stringToCid(cid), options);\n  }\n\n  async add(content: File | string, options: AbortOptions = {}) {\n    return (await this.node!.add(content, options)).cid.toString();\n  }\n\n  async pin(cid: string, options: AbortOptions = {}) {\n    return (await this.node!.pin.add(stringToCid(cid), options)).toString();\n  }\n\n  async getPeers() {\n    return (await this.node!.swarm.peers()).map((c) => c.peer.toString());\n  }\n\n  async stop() {}\n  async start() {}\n\n  async connectPeer(address: string) {\n    const addr = multiaddr(address);\n    await this.node!.bootstrap.add(addr);\n\n    await this.node!.swarm.connect(addr);\n    return true;\n  }\n\n  ls() {\n    return this.node!.pin.ls();\n  }\n\n  async info() {\n    const { repoSize } = await this.node!.stats.repo();\n\n    const responseId = await this.node!.id();\n    const { agentVersion, id } = responseId;\n    return { id: id.toString(), agentVersion, repoSize };\n  }\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport default KuboNode;\n","import { Helia, Pin, createHelia } from 'helia';\nimport { IDBBlockstore } from 'blockstore-idb';\nimport { IDBDatastore } from 'datastore-idb';\nimport { Libp2p, createLibp2p } from 'libp2p';\nimport { noise } from '@chainsafe/libp2p-noise';\nimport { yamux } from '@chainsafe/libp2p-yamux';\n// import { mplex } from '@libp2p/mplex';\n\nimport { circuitRelayTransport } from 'libp2p/circuit-relay';\nimport { UnixFS, unixfs, AddOptions } from '@helia/unixfs';\nimport { bootstrap } from '@libp2p/bootstrap';\nimport { webRTC, webRTCDirect } from '@libp2p/webrtc';\nimport { webSockets } from '@libp2p/websockets';\nimport { webTransport } from '@libp2p/webtransport';\nimport { identifyService } from 'libp2p/identify';\nimport { multiaddr, protocols } from '@multiformats/multiaddr';\nimport { LsResult } from 'ipfs-core-types/src/pin';\n\nimport {\n  AbortOptions,\n  CatOptions,\n  IpfsNodeType,\n  IpfsFileStats,\n  IpfsNode,\n} from '../../types';\n// import { all } from '@libp2p/websockets/filters';\nimport { stringToCid } from '../../utils/cid';\nimport { CYBER_GATEWAY_URL } from '../../config';\n\nasync function* mapToLsResult(\n  iterable: AsyncIterable<Pin>\n): AsyncIterable<LsResult> {\n  // eslint-disable-next-line no-restricted-syntax\n  for await (const item of iterable) {\n    const { cid, metadata } = item;\n    yield { cid: cid.toV0(), metadata, type: 'recursive' };\n  }\n}\n\nconst libp2pFactory = async (\n  datastore: IDBDatastore,\n  bootstrapList: string[] = []\n) => {\n  const libp2p = await createLibp2p({\n    datastore,\n    // addresses: {\n    //   listen: [\n    //     '/ip4/127.0.0.1/tcp/0',\n    //     '/dns4/swarm.io.cybernode.ai/tcp/443/wss/p2p/QmUgmRxoLtGERot7Y6G7UyF6fwvnusQZfGR15PuE6pY3aB',\n    //   ],\n    // },\n    transports: [\n      webSockets(),\n      webTransport(),\n      webRTC({\n        rtcConfiguration: {\n          iceServers: [\n            {\n              urls: [\n                'stun:stun.l.google.com:19302',\n                'stun:global.stun.twilio.com:3478',\n                'STUN:freestun.net:3479',\n                'STUN:stun.bernardoprovenzano.net:3478',\n                'STUN:stun.aa.net.uk:3478',\n              ],\n            },\n            {\n              credential: 'free',\n              username: 'free',\n              urls: ['TURN:freestun.net:3479', 'TURNS:freestun.net:5350'],\n            },\n          ],\n        },\n      }),\n      webRTCDirect(),\n      circuitRelayTransport({\n        discoverRelays: 1,\n      }),\n    ],\n    connectionEncryption: [noise()],\n    streamMuxers: [yamux()],\n    connectionGater: {\n      denyDialMultiaddr: () => {\n        return false;\n        // by default we refuse to dial local addresses from the browser since they\n        // are usually sent by remote peers broadcasting undialable multiaddrs but\n        // here we are explicitly connecting to a local node so do not deny dialing\n        // any discovered address\n      },\n    },\n    peerDiscovery: [\n      bootstrap({\n        list: bootstrapList,\n      }),\n    ],\n    services: {\n      identify: identifyService(),\n    },\n  });\n  return libp2p;\n};\n\nconst addOptionsV0: Partial<AddOptions> = {\n  cidVersion: 0,\n  rawLeaves: false,\n};\n\nclass HeliaNode implements IpfsNode {\n  readonly nodeType: IpfsNodeType = 'helia';\n\n  get config() {\n    return { gatewayUrl: CYBER_GATEWAY_URL };\n  }\n\n  private _isStarted = false;\n\n  get isStarted() {\n    return this._isStarted;\n  }\n\n  private node?: Helia;\n\n  private fs?: UnixFS;\n\n  async init() {\n    const blockstore = new IDBBlockstore('helia-bs');\n    await blockstore.open();\n\n    const datastore = new IDBDatastore('helia-ds');\n    await datastore.open();\n\n    const bootstrapList = [\n      '/dnsaddr/bootstrap.libp2p.io/p2p/QmNnooDu7bfjPFoTZYxMNLWUQJyrVwtbZg5gBMjTezGAJN',\n      '/dnsaddr/bootstrap.libp2p.io/p2p/QmQCU2EcMqAqQPR2i9bChDtGNJchTbq5TbXJJ16u19uLTa',\n      '/dnsaddr/bootstrap.libp2p.io/p2p/QmbLHAnMoJPWSCR5Zhtx6BHJX9KiKNN6tpvbUcqanj75Nb',\n      '/dnsaddr/bootstrap.libp2p.io/p2p/QmcZf59bWwK5XFi76CZX8cbJ4BhTzzA3gU1ZjYZcYW3dwt',\n      '/dns4/swarm.io.cybernode.ai/tcp/443/wss/p2p/QmUgmRxoLtGERot7Y6G7UyF6fwvnusQZfGR15PuE6pY3aB',\n    ];\n    const libp2p = await libp2pFactory(datastore, bootstrapList);\n\n    this.node = await createHelia({ blockstore, datastore, libp2p });\n\n    this.fs = unixfs(this.node);\n\n    if (typeof window !== 'undefined') {\n      window.libp2p = libp2p;\n      window.node = this.node;\n      window.fs = this.fs;\n      window.toCid = stringToCid;\n    }\n\n    // DEBUG\n    libp2p.addEventListener('peer:connect', (evt) => {\n      const peerId = evt.detail.toString();\n      const conn = libp2p.getConnections(peerId) || [];\n      const transportsByAddr = Object.fromEntries(\n        conn.map((c) => [\n          c.remoteAddr.toString(),\n          c.remoteAddr.protoCodes().map((v) => protocols(v)?.name),\n        ])\n      );\n      console.debug(`Connected to ${peerId}`, transportsByAddr);\n\n      // console.log(\n      //   '---------ppppp',\n      //   peerId,\n      //   conn,\n      //   conn?.remoteAddr.protoCodes().map((v) => protocols(v)?.name)\n      // ); //.includes(WEBRTC_CODE)\n      // if (conn && conn.stat) {\n      //   const transport = conn.stat.transport; // This might vary based on libp2p version\n      //   console.log(`Connected to ${peerId} using transport ${transport}`);\n      // } else {\n      //   console.log(`Connected to ${peerId}`);\n      // }\n    });\n    libp2p.addEventListener('peer:disconnect', (evt) => {\n      console.debug(`Disconnected from ${evt.detail.toString()}`);\n    });\n    console.log(\n      'IPFS - Helia addrs',\n      libp2p.getMultiaddrs().map((a) => a.toString())\n    );\n    // const webrtcConn = await libp2p.dial(\n    //   multiaddr(\n    //     '/ip4/127.0.0.1/udp/4001/quic-v1/webtransport/certhash/uEiDHumbyZRFV1Av7qH9-2l5HGgU2a2UqM6eloqO0vYz5pQ/certhash/uEiDD_TuVgih5_ua31Z4MVbNq7WSw095UAQmZqdUFMDTVRA/p2p/12D3KooWEYGfgK4dEY3spfuDKVq6Jpiyj4KxP1r6HS5RFp5WHebz'\n    //   )\n    // );\n    // console.log('----webrtcConn', webrtcConn);\n\n    this._isStarted = true;\n  }\n\n  async stat(cid: string, options: AbortOptions = {}): Promise<IpfsFileStats> {\n    return this.fs!.stat(stringToCid(cid), options).then((result) => {\n      const { type, fileSize, localFileSize, blocks, dagSize, mtime } = result;\n      return {\n        type,\n        size: fileSize || -1,\n        sizeLocal: localFileSize || -1,\n        blocks,\n      };\n    });\n  }\n\n  cat(cid: string, options: CatOptions = {}) {\n    return this.fs!.cat(stringToCid(cid), options);\n  }\n\n  async add(content: File | string, options: AbortOptions = {}) {\n    // Options to keep CID in V0 format 'Qm....';\n    const optionsV0 = {\n      ...options,\n      ...addOptionsV0,\n    } as Partial<AddOptions>;\n\n    let cid;\n\n    if (content instanceof File) {\n      const fileName = content.name;\n      const arrayBuffer = await content.arrayBuffer();\n      const data = new Uint8Array(arrayBuffer);\n      cid = await this.fs!.addFile(\n        { path: fileName, content: data },\n        optionsV0\n      );\n    } else {\n      const data = new TextEncoder().encode(content);\n      cid = await this.fs!.addBytes(data, optionsV0);\n    }\n    // console.log('----added to helia', cid.toString());\n    this.pin(cid.toString(), options);\n    return cid.toString();\n  }\n\n  async pin(cid: string, options: AbortOptions = {}) {\n    const cid_ = stringToCid(cid);\n    const isPinned = await this.node?.pins.isPinned(cid_, options);\n    if (!isPinned) {\n      const pinResult = (\n        await this.node?.pins.add(cid_, options)\n      )?.cid.toString();\n      // console.log('------pin', pinResult);\n    }\n    // console.log('------pinned', cid, isPinned);\n    return undefined;\n  }\n\n  async getPeers() {\n    return this.node!.libp2p!.getConnections().map((c) =>\n      c.remotePeer.toString()\n    );\n  }\n\n  async stop() {\n    await this.node?.stop();\n  }\n\n  async start() {\n    await this.node?.start();\n  }\n\n  async connectPeer(address: string) {\n    const conn = await this.node!.libp2p!.dial(multiaddr(address));\n    return true;\n  }\n\n  ls() {\n    const result = mapToLsResult(this.node!.pins.ls());\n    return result;\n  }\n\n  async info() {\n    const id = this.node!.libp2p.peerId.toString();\n    const agentVersion = this.node!.libp2p!.services!.identify!.host!\n      .agentVersion as string;\n    return { id, agentVersion, repoSize: -1 };\n  }\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport default HeliaNode;\n","// eslint-disable-next-line import/no-unresolved\nimport { webSockets } from '@libp2p/websockets';\nimport * as filters from '@libp2p/websockets/filters';\nimport { Options } from 'ipfs-core/dist/src/types';\n\nconst configIpfs = (): Options => ({\n  start: true,\n  repo: 'ipfs-repo-cyber-v2',\n  relay: {\n    enabled: false,\n    hop: {\n      enabled: false,\n    },\n  },\n  preload: {\n    enabled: false,\n  },\n  config: {\n    API: {\n      HTTPHeaders: {\n        'Access-Control-Allow-Methods': ['PUT', 'POST'],\n        'Access-Control-Allow-Origin': [\n          'http://localhost:3000',\n          'http://127.0.0.1:5001',\n          'http://127.0.0.1:8888',\n          'http://localhost:8888',\n        ],\n      },\n    },\n    Addresses: {\n      Gateway: '/ip4/127.0.0.1/tcp/8080',\n      Swarm: [\n        // '/dns4/ws-star.discovery.cybernode.ai/tcp/443/wss/p2p-webrtc-star',\n        // '/dns4/wrtc-star1.par.dwebops.pub/tcp/443/wss/p2p-webrtc-star',\n        // '/dns4/wrtc-star2.sjc.dwebops.pub/tcp/443/wss/p2p-webrtc-star',\n      ],\n      Delegates: [\n        // '/dns4/node0.delegate.ipfs.io/tcp/443/https',\n        // '/dns4/node1.delegate.ipfs.io/tcp/443/https',\n        // '/dns4/node2.delegate.ipfs.io/tcp/443/https',\n      ],\n    },\n    Discovery: {\n      MDNS: {\n        Enabled: true,\n        Interval: 10,\n      },\n      webRTCStar: {\n        Enabled: false,\n      },\n    },\n    Bootstrap: [\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmNnooDu7bfjPFoTZYxMNLWUQJyrVwtbZg5gBMjTezGAJN',\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmbLHAnMoJPWSCR5Zhtx6BHJX9KiKNN6tpvbUcqanj75Nb',\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmZa1sAxajnQjVM8WjWXoMbmPd7NsWhfKsPkErzpm9wGkp',\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmQCU2EcMqAqQPR2i9bChDtGNJchTbq5TbXJJ16u19uLTa',\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmcZf59bWwK5XFi76CZX8cbJ4BhTzzA3gU1ZjYZcYW3dwt',\n      // '/dns4/ws-star.discovery.cybernode.ai/tcp/4430/wss/p2p/QmUgmRxoLtGERot7Y6G7UyF6fwvnusQZfGR15PuE6pY3aB',\n    ],\n    Pubsub: {\n      Enabled: false,\n    },\n    Swarm: {\n      ConnMgr: {\n        HighWater: 300,\n        LowWater: 50,\n      },\n      DisableNatPortMap: false,\n    },\n    Routing: {\n      Type: 'dhtclient',\n    },\n  },\n  libp2p: {\n    transports: [\n      // This is added for local demo!\n      // In a production environment the default filter should be used\n      // where only DNS + WSS addresses will be dialed by websockets in the browser.\n      webSockets({\n        filter: filters.dnsWss,\n      }),\n    ],\n    nat: {\n      enabled: false,\n    },\n  },\n  EXPERIMENTAL: {\n    ipnsPubsub: false,\n  },\n});\n\nexport default configIpfs;\n","import {\n  AbortOptions,\n  CatOptions,\n  IpfsNodeType,\n  IpfsFileStats,\n  IpfsNode,\n  IpfsNodePrperties,\n} from '../../types';\nimport { create as createJsIpfsClient, IPFS } from 'ipfs-core';\nimport { stringToCid, stringToIpfsPath } from '../../utils/cid';\nimport { multiaddr } from '@multiformats/multiaddr';\n\nimport configIpfs from './configs/jsIpfsConfig';\nimport { CYBER_GATEWAY_URL } from '../../config';\n\nclass JsIpfsNode implements IpfsNode {\n  readonly nodeType: IpfsNodeType = 'embedded';\n\n  get config() {\n    return { gatewayUrl: CYBER_GATEWAY_URL };\n  }\n\n  private _isStarted = false;\n\n  get isStarted() {\n    return this._isStarted;\n  }\n\n  private node?: IPFS;\n\n  async init() {\n    this.node = await createJsIpfsClient(configIpfs());\n    if (typeof window !== 'undefined') {\n      window.node = this.node;\n      window.toCid = stringToCid;\n    }\n\n    this._isStarted = true;\n  }\n\n  async stat(cid: string, options: AbortOptions = {}): Promise<IpfsFileStats> {\n    return this.node!.files.stat(stringToIpfsPath(cid), {\n      ...options,\n      withLocal: true,\n      size: true,\n    }).then((result) => {\n      const { type, size, sizeLocal, local, blocks } = result;\n      return {\n        type,\n        size: size || -1,\n        sizeLocal: sizeLocal || -1,\n        blocks,\n      };\n    });\n  }\n\n  cat(cid: string, options: CatOptions = {}) {\n    return this.node!.cat(stringToCid(cid), options);\n  }\n\n  async add(content: File | string, options: AbortOptions = {}) {\n    return (await this.node!.add(content, options)).cid.toString();\n  }\n\n  async pin(cid: string, options: AbortOptions = {}) {\n    return (await this.node!.pin.add(stringToCid(cid), options)).toString();\n  }\n\n  async getPeers() {\n    return (await this.node!.swarm.peers()).map((c) => c.peer.toString());\n  }\n\n  async stop() {}\n  async start() {}\n\n  async connectPeer(address: string) {\n    const addr = multiaddr(address);\n    await this.node!.bootstrap.add(addr);\n\n    await this.node!.swarm.connect(addr);\n    return true;\n  }\n\n  ls() {\n    return this.node!.pin.ls();\n  }\n\n  async info() {\n    const response = await this.node!.stats.repo();\n    const repoSize = Number(response.repoSize);\n\n    const responseId = await this.node!.id();\n    const { agentVersion, id } = responseId;\n    return { id: id.toString(), agentVersion, repoSize };\n  }\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport default JsIpfsNode;\n","// import { getNodeAutoDialInterval } from './utils-ipfs';\nimport { IpfsNodeType, IpfsNode, CybIpfsNode, IpfsOptsType } from '../types';\nimport KuboNode from './impl/kubo';\nimport HeliaNode from './impl/helia';\nimport JsIpfsNode from './impl/js-ipfs';\n// import EnhancedIpfsNode from './node/enhancedNode';\nimport {\n  CYBERNODE_SWARM_ADDR_TCP,\n  CYBERNODE_SWARM_ADDR_WSS,\n  CYBER_NODE_SWARM_PEER_ID,\n} from '../config';\nimport { withCybFeatures } from './mixins/withCybFeatures';\n\nconst nodeClassMap: Record<IpfsNodeType, new () => IpfsNode> = {\n  helia: HeliaNode,\n  embedded: JsIpfsNode,\n  external: KuboNode,\n};\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport async function initIpfsNode(\n  options: IpfsOptsType\n): Promise<CybIpfsNode> {\n  const { ipfsNodeType, ...restOptions } = options;\n\n  const swarmPeerId = CYBER_NODE_SWARM_PEER_ID;\n\n  const swarmPeerAddress =\n    ipfsNodeType === 'external'\n      ? CYBERNODE_SWARM_ADDR_TCP\n      : CYBERNODE_SWARM_ADDR_WSS;\n\n  const EnhancedClass = withCybFeatures(nodeClassMap[ipfsNodeType], {\n    swarmPeerId,\n    swarmPeerAddress,\n  });\n\n  const instance = new EnhancedClass();\n\n  await instance.init({ url: restOptions.urlOpts });\n  // TODO: REFACT\n  //   instance.connMgrGracePeriod = await getNodeAutoDialInterval(instance);\n  // window.ipfs = instance;\n\n  await instance.reconnectToSwarm();\n  return instance;\n}\n","import { IpfsNode, CybIpfsNode, IpfsContentType } from '../../types';\nimport { parseArrayLikeToDetails } from '../../utils/content';\nimport { addContenToIpfs, getIPFSContent } from '../../utils/utils-ipfs';\n\ntype WithCybFeaturesOptions = {\n  swarmPeerId: string;\n  swarmPeerAddress: string;\n};\n\nfunction withCybFeatures<TBase extends new (...args: any[]) => IpfsNode>(\n  Base: TBase,\n  options: WithCybFeaturesOptions\n) {\n  return class CybIpfsNodeMixin extends Base implements CybIpfsNode {\n    async fetchWithDetails(\n      cid: string,\n      parseAs?: IpfsContentType,\n      abortController?: AbortController\n    ) {\n      const content = await getIPFSContent(cid, this, abortController);\n\n      const details = await parseArrayLikeToDetails(content, cid);\n      return !parseAs\n        ? details\n        : details?.type === parseAs\n        ? details\n        : undefined;\n    }\n\n    async addContent(content: File | string) {\n      return addContenToIpfs(this, content);\n    }\n\n    async isConnectedToSwarm() {\n      const peers = await super.getPeers();\n      return !!peers.find((peerId) => peerId === options.swarmPeerId);\n    }\n\n    async reconnectToSwarm(forced = false) {\n      const isConnectedToSwarm = await this.isConnectedToSwarm();\n      if (!isConnectedToSwarm || forced) {\n        // TODO: refactor using timeout for node config\n\n        //   const needToReconnect =\n        //     Date.now() - lastConnectedTimestamp <\n        //     DEFAULT_CONNECTION_LIFETIME_SECONDS;\n        super\n          .connectPeer(options.swarmPeerAddress)\n          .then(() => {\n            console.log(` connected to swarm - ${options.swarmPeerAddress}`);\n            return true;\n          })\n          .catch((err) => {\n            console.log(\n              `Can't connect to swarm ${options.swarmPeerAddress}: ${err.message}`\n            );\n            return false;\n          });\n      }\n    }\n  };\n}\n\nexport { withCybFeatures };\n","import {\n  PipelineType,\n  pipeline,\n  env,\n  FeatureExtractionPipeline,\n} from '@xenova/transformers';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport DbApiWrapper from 'src/services/backend/services/DbApi/DbApi';\nimport {\n  Subject,\n  combineLatest,\n  Observable,\n  shareReplay,\n  ReplaySubject,\n  filter,\n} from 'rxjs';\nimport { proxy } from 'comlink';\n\nenv.allowLocalModels = false;\n\ntype MlModelParams = {\n  name: PipelineType;\n  model: string;\n};\nconst mlModelMap: Record<string, MlModelParams> = {\n  featureExtractor: {\n    name: 'feature-extraction',\n    model: 'Xenova/all-MiniLM-L6-v2',\n  },\n  // summarization: {\n  //   name: 'summarization',\n  //   model: 'ahmedaeb/distilbart-cnn-6-6-optimised',\n  // },\n  // qa: {\n  //   name: 'question-answering',\n  //   model: 'Xenova/distilbert-base-uncased-distilled-squad',\n  // },\n};\n\nconst loadPipeline = (\n  name: PipelineType,\n  model: string,\n  onProgress: (data: any) => void\n) => {\n  return pipeline(name, model, {\n    progress_callback: (progressData: any) => {\n      try {\n        const {\n          status,\n          progress,\n          // name: modelName,\n          loaded,\n          total,\n        } = progressData;\n\n        const message = loaded ? `${model} - ${loaded}/${total} bytes` : model;\n\n        const done = ['ready', 'error'].some((s) => s === status);\n\n        const progrssStateItem = {\n          status,\n          message,\n          done,\n          progress: progress ? Math.round(progress) : 0,\n        };\n        // console.log('progress_callback', name, progressData);\n\n        onProgress(progrssStateItem);\n      } catch (e) {\n        console.log('-------progresss error', model, e.toString());\n      }\n    },\n  });\n};\n\nexport type EmbeddingApi = {\n  createEmbedding: (text: string) => Promise<number[]>;\n  searchByEmbedding: (\n    text: string,\n    count?: number\n  ) => ReturnType<DbApiWrapper['searchByEmbedding']>;\n};\n\nconst createEmbeddingApi$ = (\n  dbInstance$: Subject<DbApiWrapper>,\n  featureExtractor$: Subject<FeatureExtractionPipeline>\n) => {\n  const replaySubject = new ReplaySubject(1);\n\n  combineLatest([dbInstance$, featureExtractor$]).subscribe(\n    ([dbInstance, featureExtractor]) => {\n      if (dbInstance && featureExtractor) {\n        const createEmbedding = async (text: string) => {\n          const output = await featureExtractor(text, {\n            pooling: 'mean',\n            normalize: true,\n          });\n\n          return output.data as number[];\n        };\n\n        const searchByEmbedding = async (text: string, count?: number) => {\n          const vec = await createEmbedding(text);\n          // console.log('----searchByEmbedding', vec);\n\n          const rows = await dbInstance.searchByEmbedding(vec, count);\n          //   console.log('----searcByEmbedding rows', rows);\n\n          return rows;\n        };\n\n        const api = {\n          createEmbedding,\n          searchByEmbedding,\n        };\n        replaySubject.next(proxy(api));\n      }\n    }\n  );\n  // .pipe(filter((v) => !!v))\n  return replaySubject as Observable<EmbeddingApi>;\n};\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport const createMlApi = (\n  dbInstance$: Subject<DbApiWrapper>,\n  broadcastApi: BroadcastChannelSender\n) => {\n  const featureExtractor$ = new Subject<FeatureExtractionPipeline>();\n  const embeddingApi$ = createEmbeddingApi$(dbInstance$, featureExtractor$);\n\n  const initPipelineInstance = async (alias: keyof typeof mlModelMap) => {\n    const { name, model } = mlModelMap[alias];\n\n    const pipeline = await loadPipeline(name, model, (data) =>\n      broadcastApi.postMlSyncEntryProgress(alias, data)\n    );\n    if (name === 'feature-extraction') {\n      featureExtractor$.next(pipeline as FeatureExtractionPipeline);\n    }\n    console.log(`${alias} - loaded`);\n  };\n\n  const init = async () => {\n    broadcastApi.postServiceStatus('ml', 'starting');\n    console.time(' ml initialized');\n\n    return Promise.all([\n      initPipelineInstance('featureExtractor'),\n      // initMlInstance('summarization'),\n      // initMlInstance('qa'),\n    ])\n      .then((result) => {\n        setTimeout(() => broadcastApi.postServiceStatus('ml', 'started'), 0);\n        console.timeEnd(' ml initialized');\n\n        return result;\n      })\n      .catch((e) =>\n        broadcastApi.postServiceStatus('ml', 'error', e.toString())\n      );\n  };\n\n  init();\n\n  return { embeddingApi$, init };\n};\n","import initAsync, { compile } from 'cyb-rune-wasm';\n\nimport { v4 as uuidv4 } from 'uuid';\n\nimport { TabularKeyValues } from 'src/types/data';\nimport { keyValuesToObject } from 'src/utils/localStorage';\nimport { entityToDto } from 'src/utils/dto';\n\nimport { mapObjIndexed } from 'ramda';\nimport { removeBrokenUnicode } from 'src/utils/string';\n\nimport {\n  BehaviorSubject,\n  ReplaySubject,\n  combineLatest,\n  distinctUntilChanged,\n  map,\n} from 'rxjs';\n\nimport defaultParticleScript from 'src/services/scripting/rune/default/particle.rn';\nimport runtimeScript from 'src/services/scripting/rune/runtime.rn';\n\nimport {\n  ScriptCallback,\n  ScriptParticleParams,\n  ScriptContext,\n  ScriptParticleResult,\n  // ScriptMyParticleParams,\n  ScriptEntrypoints,\n  EntrypointParams,\n  EngineContext,\n  ScriptMyCampanion,\n} from './types';\n\nimport { extractRuneScript } from './helpers';\n\ntype RuneEntrypoint = {\n  readOnly: boolean;\n  execute: boolean;\n  funcName: string;\n  funcParams: any[];\n  params: Object; // context data\n  input: string; // main code\n  script: string; // runtime code\n};\n\ntype RuneRunResult = {\n  output: string;\n  diagnosticsOutput: string;\n  diagnostics: any[];\n  error: string;\n  result: any;\n};\n\nconst compileConfig = {\n  budget: 1_000_000,\n  experimental: false,\n  instructions: false,\n  options: [],\n};\n\nconst defaultRuneEntrypoint: RuneEntrypoint = {\n  readOnly: false,\n  execute: true,\n  funcName: 'main',\n  funcParams: {},\n  params: {},\n  input: defaultParticleScript,\n  script: runtimeScript,\n};\n\nconst toRecord = (item: TabularKeyValues) =>\n  keyValuesToObject(Object.values(item));\n\nexport type LoadParams = {\n  entrypoints: ScriptEntrypoints;\n  secrets: TabularKeyValues;\n};\n\n// eslint-disable-next-line import/prefer-default-export\nfunction enigine() {\n  let entrypoints: Partial<ScriptEntrypoints> = {};\n  let context: EngineContext = { params: {}, user: {}, secrets: {} };\n  const isInitialized$ = new BehaviorSubject<boolean>(false);\n  const entrypoints$ = new BehaviorSubject<Partial<ScriptEntrypoints>>({});\n\n  const scriptCallbacks = new Map<string, ScriptCallback>();\n\n  const isSoulInitialized$ = new ReplaySubject(1);\n  combineLatest([isInitialized$, entrypoints$])\n    .pipe(\n      map(\n        ([isInitialized, entrypoints]) =>\n          !!(isInitialized && entrypoints.particle)\n      ),\n      distinctUntilChanged()\n    )\n    .subscribe((v) => {\n      isSoulInitialized$.next(v);\n    });\n\n  entrypoints$.subscribe((v) => {\n    entrypoints = v;\n  });\n\n  const init = async () => {\n    console.time(' rune initialized');\n    await initAsync();\n    // window.rune = rune; // debug\n    console.timeEnd(' rune initialized');\n    isInitialized$.next(true);\n  };\n\n  const pushContext = <K extends keyof ScriptContext>(\n    name: K,\n    value: ScriptContext[K] //| TabularKeyValues\n  ) => {\n    // context[name] =  toRecord(value as TabularKeyValues);\n    context[name] = value;\n  };\n\n  const popContext = (names: (keyof ScriptContext)[]) => {\n    const newContext = context;\n    names.forEach((name) => {\n      newContext[name] = {};\n    });\n    context = newContext;\n  };\n\n  const setEntrypoints = (scriptEntrypoints: ScriptEntrypoints) => {\n    entrypoints = mapObjIndexed(\n      (v) => ({ ...v, script: extractRuneScript(v.script) }),\n      scriptEntrypoints\n    );\n    entrypoints$.next(entrypoints);\n  };\n\n  const run = async (\n    script: string,\n    compileParams: Partial<RuneEntrypoint>,\n    callback?: ScriptCallback,\n    scripts: Record<string, string> = {}\n  ) => {\n    const refId = uuidv4().toString();\n\n    callback && scriptCallbacks.set(refId, callback);\n    const scriptParams = {\n      app: context,\n      refId,\n    };\n    const compilerParams = {\n      ...defaultRuneEntrypoint,\n      ...compileParams,\n      input: script,\n      scripts: { ...scripts, runtime: runtimeScript },\n      params: scriptParams,\n    };\n    // console.log('-----run', scriptParams);\n    const outputData = await compile(compilerParams, compileConfig);\n\n    // Parse the JSON string\n    const { result, error } = outputData;\n\n    try {\n      scriptCallbacks.delete(refId);\n\n      return {\n        ...entityToDto(outputData),\n        error,\n        result: result\n          ? result === '()'\n            ? ''\n            : JSON.parse(removeBrokenUnicode(result))\n          : { action: 'error', message: 'No result' },\n      } as RuneRunResult;\n    } catch (e) {\n      scriptCallbacks.delete(refId);\n\n      console.log(\n        `engine.run err ${compilerParams.funcName}`,\n        e,\n        outputData,\n        compilerParams\n      );\n      return {\n        diagnosticsOutput: `scripting engine error ${e}`,\n        ...outputData,\n        result: { action: 'error', message: e?.toString() || 'Unknown error' },\n      } as RuneRunResult;\n    }\n  };\n\n  const getParticleScriptOrAction = ():\n    | ['error' | 'pass' | 'script', string] => {\n    if (!entrypoints.particle) {\n      return ['error', ''];\n    }\n\n    const { script, enabled } = entrypoints.particle;\n\n    if (!enabled) {\n      return ['pass', ''];\n    }\n\n    return ['script', script];\n  };\n\n  const personalProcessor = async (\n    params: ScriptParticleParams\n  ): Promise<ScriptParticleResult> => {\n    const [resultType, script] = getParticleScriptOrAction();\n\n    if (resultType === 'error') {\n      return { action: 'error', message: 'No particle entrypoint' };\n    }\n\n    if (resultType !== 'script') {\n      return { action: 'pass' };\n    }\n\n    const { cid, contentType, content } = params;\n    const output = await run(script, {\n      funcName: 'personal_processor',\n      funcParams: [cid, contentType, content], //params as EntrypointParams,\n    });\n    const { action, content: outputContent } = output.result;\n\n    if (action === 'error') {\n      console.error(\n        `RUNE: personalProcessor error: ${params.cid}`,\n        params,\n        output\n      );\n    }\n\n    if (outputContent) {\n      return { ...output.result, content: outputContent };\n    }\n\n    return output.result;\n  };\n\n  const executeFunction = async (\n    script: string,\n    funcName: string,\n    funcParams: EntrypointParams\n  ): Promise<ScriptParticleResult> => {\n    console.log('-----executeFunction rune', funcName, funcParams);\n    const output = await run(script, {\n      funcName,\n      funcParams,\n      readOnly: true, // block to sign tx and add to ipfs\n    });\n\n    return output.result;\n  };\n\n  // const particleInference = async (\n  //   userScript: string,\n  //   funcParams: EntrypointParams\n  // ): Promise<ScriptMyParticleResult> => {\n  //   const output = await run(userScript, {\n  //     funcName: 'particle_inference',\n  //     funcParams,\n  //   });\n\n  //   return output.result;\n  // };\n\n  const askCompanion = async (\n    cid: string,\n    contentType: string,\n    content: string,\n    callback?: ScriptCallback\n  ): Promise<ScriptMyCampanion> => {\n    const [resultType, script] = getParticleScriptOrAction();\n    if (resultType === 'error') {\n      return {\n        action: 'error',\n        metaItems: [[{ type: 'text', text: 'No particle entrypoint' }]],\n      };\n    }\n\n    if (resultType === 'pass') {\n      return { action: 'pass', metaItems: [] };\n    }\n\n    const output = await run(\n      script,\n      {\n        funcName: 'ask_companion',\n        funcParams: [cid, contentType, content],\n      },\n      callback\n    );\n\n    if (output.result.action === 'error') {\n      console.error('---askCompanion error', output);\n      return {\n        action: 'error',\n        metaItems: [[{ type: 'text', text: output.error }]],\n      };\n    }\n\n    return { action: 'answer', metaItems: output.result.content };\n  };\n\n  const executeCallback = async (refId: string, data: any) => {\n    const callback = scriptCallbacks.get(refId);\n\n    if (callback) {\n      await callback(data);\n    }\n  };\n\n  return {\n    init,\n    isSoulInitialized$,\n    run,\n    // particleInference,\n    askCompanion,\n    personalProcessor,\n    setEntrypoints,\n    pushContext,\n    popContext,\n    executeFunction,\n    executeCallback,\n    getDebug: () => ({\n      context,\n      entrypoints,\n    }),\n  };\n}\n\nconst scriptEngine = enigine();\n\nexport type RuneEngine = typeof scriptEngine;\n\nexport default scriptEngine;\n","import { Observable } from 'rxjs';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport rune from 'src/services/scripting/engine';\nimport runeDeps, { RuneInnerDeps } from 'src/services/scripting/runeDeps';\nimport DbApiWrapper from 'src/services/backend/services/DbApi/DbApi';\nimport { EmbeddingApi } from './mlApi';\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport const createRuneApi = (\n  embeddingApi$: Observable<EmbeddingApi>,\n  dbInstance$: Observable<DbApiWrapper>,\n  broadcastApi: BroadcastChannelSender\n) => {\n  const setInnerDeps = (deps: Partial<RuneInnerDeps>) =>\n    runeDeps.setInnerDeps(deps);\n\n  embeddingApi$.subscribe((embeddingApi) => {\n    setInnerDeps({ embeddingApi });\n  });\n\n  dbInstance$.subscribe((dbApi) => {\n    setInnerDeps({ dbApi });\n  });\n\n  rune.isSoulInitialized$.subscribe((value) => {\n    value\n      ? setTimeout(() => broadcastApi.postServiceStatus('rune', 'started'), 0)\n      : broadcastApi.postServiceStatus('rune', 'inactive');\n  });\n\n  const init = async () => {\n    broadcastApi.postServiceStatus('rune', 'starting');\n\n    await rune.init();\n    setInnerDeps({ rune });\n  };\n\n  init();\n\n  return { rune, setInnerDeps, abort: runeDeps.abort };\n};\n","import { proxy } from 'comlink';\n\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { ParticleCid } from 'src/types/base';\nimport { BehaviorSubject, Subject } from 'rxjs';\nimport { RuneInnerDeps } from 'src/services/scripting/runeDeps';\n\nimport { exposeWorkerApi } from '../factoryMethods';\n\nimport { SyncService } from '../../services/sync/sync';\nimport { SyncServiceParams } from '../../services/sync/types';\n\nimport DbApi from '../../services/DbApi/DbApi';\n\nimport BroadcastChannelSender from '../../channels/BroadcastChannelSender';\nimport { createIpfsApi } from './api/ipfsApi';\nimport { createMlApi } from './api/mlApi';\nimport { createRuneApi } from './api/runeApi';\n\n// import { initRuneDeps } from 'src/services/scripting/wasmBindings';\n\nconst createBackgroundWorkerApi = () => {\n  const broadcastApi = new BroadcastChannelSender();\n\n  const dbInstance$ = new Subject<DbApi>();\n\n  const injectDb = (db: DbApi) => dbInstance$.next(db);\n\n  const params$ = new BehaviorSubject<SyncServiceParams>({\n    myAddress: null,\n  });\n\n  const { embeddingApi$ } = createMlApi(dbInstance$, broadcastApi);\n\n  const { setInnerDeps, rune } = createRuneApi(\n    embeddingApi$,\n    dbInstance$,\n    broadcastApi\n  );\n\n  const {\n    ipfsQueue,\n    ipfsInstance$,\n    api: ipfsApi,\n  } = createIpfsApi(rune, broadcastApi);\n\n  const waitForParticleResolve = (\n    cid: ParticleCid,\n    priority: QueuePriority = QueuePriority.MEDIUM\n  ) => ipfsQueue.enqueueAndWait(cid, { postProcessing: false, priority });\n\n  const serviceDeps = {\n    waitForParticleResolve,\n    dbInstance$,\n    ipfsInstance$,\n    embeddingApi$,\n    params$,\n  };\n\n  // service to sync updates about cyberlinks, transactions, swarm etc.\n  const syncService = new SyncService(serviceDeps);\n\n  // INITIALIZATION\n  setInnerDeps({ ipfsApi });\n\n  return {\n    injectDb,\n    isIpfsInitialized: () => !!ipfsInstance$.getValue(),\n    // syncDrive,\n    ipfsApi: proxy(ipfsApi),\n    rune: proxy(rune),\n    embeddingApi$,\n    // ipfsInstance$,\n    ipfsQueue: proxy(ipfsQueue),\n    setRuneDeps: (\n      deps: Partial<Omit<RuneInnerDeps, 'embeddingApi' | 'dbApi'>>\n    ) => setInnerDeps(deps),\n    // restartSync: (name: SyncEntryName) => syncService.restart(name),\n    setParams: (params: Partial<SyncServiceParams>) =>\n      params$.next({ ...params$.value, ...params }),\n  };\n};\n\nconst backgroundWorker = createBackgroundWorkerApi();\n\nexport type BackgroundWorker = typeof backgroundWorker;\n\n// Expose the API to the main thread as shared/regular worker\nexposeWorkerApi(self, backgroundWorker);\n","import { proxy } from 'comlink';\nimport { BehaviorSubject, Subject } from 'rxjs';\nimport QueueManager from 'src/services/QueueManager/QueueManager';\nimport {\n  QueueItemCallback,\n  QueueItemOptions,\n} from 'src/services/QueueManager/types';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport { initIpfsNode } from 'src/services/ipfs/node/factory';\n\nimport {\n  CybIpfsNode,\n  IpfsContentType,\n  IpfsOptsType,\n} from 'src/services/ipfs/types';\nimport { RuneEngine } from 'src/services/scripting/engine';\n\n// eslint-disable-next-line import/prefer-default-export\nexport const createIpfsApi = (\n  rune: RuneEngine,\n  broadcastApi: BroadcastChannelSender\n) => {\n  const ipfsInstance$ = new BehaviorSubject<CybIpfsNode | undefined>(undefined);\n  const ipfsQueue = new QueueManager(ipfsInstance$, {\n    rune,\n  });\n  const stopIpfs = async () => {\n    const ipfsNode = ipfsInstance$.getValue();\n\n    if (ipfsNode) {\n      await ipfsNode.stop();\n    }\n    ipfsInstance$.next(undefined);\n    broadcastApi.postServiceStatus('ipfs', 'inactive');\n  };\n\n  const startIpfs = async (ipfsOpts: IpfsOptsType) => {\n    try {\n      const ipfsNode = ipfsInstance$.getValue();\n      if (ipfsNode) {\n        // console.log('Ipfs node already started!');\n        setTimeout(() => broadcastApi.postServiceStatus('ipfs', 'started'), 0);\n        return Promise.resolve();\n        // await ipfsNode.stop();\n      }\n      broadcastApi.postServiceStatus('ipfs', 'starting');\n      console.time(' ipfs initialized');\n\n      const newIpfsNode = await initIpfsNode(ipfsOpts);\n      console.timeEnd(' ipfs initialized');\n\n      ipfsInstance$.next(newIpfsNode);\n      setTimeout(() => broadcastApi.postServiceStatus('ipfs', 'started'), 0);\n      return true;\n    } catch (err) {\n      console.log('----ipfs node init error ', err);\n      const msg = err instanceof Error ? err.message : (err as string);\n      broadcastApi.postServiceStatus('ipfs', 'error', msg);\n      throw Error(msg);\n    }\n  };\n\n  const api = {\n    start: startIpfs,\n    stop: stopIpfs,\n    config: async () => ipfsInstance$.getValue()?.config,\n    info: async () => ipfsInstance$.getValue()?.info(),\n    fetchWithDetails: async (\n      cid: string,\n      parseAs?: IpfsContentType,\n      controller?: AbortController\n    ) => {\n      const ipfsNode = ipfsInstance$.getValue();\n      if (!ipfsNode) {\n        throw new Error('ipfs node not initialized');\n      }\n      return ipfsNode.fetchWithDetails(cid, parseAs, controller);\n    },\n    enqueue: async (\n      cid: string,\n      callback: QueueItemCallback,\n      options: QueueItemOptions\n    ) => ipfsQueue.enqueue(cid, callback, options),\n    enqueueAndWait: async (cid: string, options?: QueueItemOptions) =>\n      ipfsQueue!.enqueueAndWait(cid, options),\n    dequeue: async (cid: string) => ipfsQueue.cancel(cid),\n    dequeueByParent: async (parent: string) => ipfsQueue.cancelByParent(parent),\n    clearQueue: async () => ipfsQueue.clear(),\n    addContent: async (content: string | File) =>\n      ipfsInstance$.getValue()?.addContent(content),\n  };\n\n  return { ipfsInstance$, ipfsQueue, api };\n};\n\nexport type IpfsApi = ReturnType<typeof createIpfsApi>['api'];\n","import { Nullable } from 'src/types';\nimport { v4 as uuidv4 } from 'uuid';\n\nexport async function getScriptFromParticle(cid?: Nullable<string>) {\n  throw new Error('Not implemented');\n  // if (!cid || !isCID(cid)) {\n  //   // throw new Error('cid is not valid');\n  //   return undefined;\n  // }\n\n  // const queueResult = await queueManager.enqueueAndWait(cid, {\n  //   postProcessing: false,\n  // });\n  // const result = queueResult?.result;\n  // if (!result?.result || result?.contentType !== 'text') {\n  //   // throw new Error('content is not valid');\n  //   return undefined;\n  // }\n\n  // return getTextFromIpfsContent(result.result);\n}\n\nexport function extractRuneContent(markdown: string) {\n  // Regular expression to match the content between ```rune``` tags\n  const runeRegex = /```rune\\s*([\\s\\S]*?)```/g;\n\n  let match;\n  let runeScript = '';\n  let modifiedMarkdown = markdown;\n  let hasRune = false;\n  // Iterate through all matches of the regular expression\n  while ((match = runeRegex.exec(markdown)) !== null) {\n    hasRune = true;\n    // Append the matched content between ```rune``` tags to runeContent variable\n    runeScript += match[1] + '\\n';\n\n    // Replace the entire matched block, including the tags, with an empty string\n    modifiedMarkdown = modifiedMarkdown.replace(match[0], '');\n  }\n\n  // Returning both the extracted content and the modified markdown without the tags\n  return {\n    script: runeScript.trim(),\n    markdown: modifiedMarkdown,\n    hasRune,\n  };\n}\n\nexport function extractRuneScript(markdown: string) {\n  const { script, markdown: md, hasRune } = extractRuneContent(markdown);\n  // if no rune tag, consider this like pure script\n  return hasRune ? script : md;\n}\n\nexport const generateRefId = () => uuidv4().toString();\n","import React, { useContext } from 'react';\nimport { CyberClient } from '@cybercongress/cyber-js';\nimport { Option } from 'src/types';\nimport { useQuery } from '@tanstack/react-query';\nimport { RPC_URL } from 'src/constants/config';\n\nconst QueryClientContext = React.createContext<Option<CyberClient>>(undefined);\n\n/**\n * @deprecated use queryCyberClient\n */\nexport function useQueryClient() {\n  return useContext(QueryClientContext);\n}\n\nfunction QueryClientProvider({ children }: { children: React.ReactNode }) {\n  const {\n    data: client,\n    error,\n    isFetching,\n  } = useQuery({\n    queryKey: ['cyberClient', 'connect'],\n    queryFn: async () => {\n      return CyberClient.connect(RPC_URL);\n    },\n  });\n\n  if (isFetching) {\n    return null;\n  }\n\n  if (error) {\n    console.error('Error queryClient connect: ', error.message);\n  }\n\n  return (\n    <QueryClientContext.Provider value={client}>\n      {children}\n    </QueryClientContext.Provider>\n  );\n}\n\nexport default QueryClientProvider;\n","import { useEffect, useState, useMemo } from 'react';\nimport axios from 'axios';\nimport { useQueryClient } from 'src/contexts/queryClient';\nimport { AccountValue } from 'src/types/defaultAccount';\nimport { useQuery } from '@tanstack/react-query';\nimport { Nullable } from 'src/types';\nimport { Citizenship } from 'src/types/citizenship';\nimport { CyberClient } from '@cybercongress/cyber-js';\nimport { getPassport } from 'src/services/passports/lcd';\n\nconst AMOUNT_ALL_STAGE = 90;\nconst NEW_RELEASE = 1000; // release 1% every 1k claims\nconst CONSTITUTION_HASH = 'QmcHB9GKHAKCLQhmSj71qNJhENJJg8Gymd1PvvsCQBhG7M';\n\n// test root\n// const CONTRACT_ADDRESS_GIFT =\n//   'bostrom1dwzfa74hzpt6393czajlldnxjup8zk3xh3skegnm67yzqx33k2cssyduk8';\n// const CONTRACT_ADDRESS_PASSPORT =\n//   'bostrom1fzm6gzyccl8jvdv3qq6hp9vs6ylaruervs4m06c7k0ntzn2f8faq7ha2z2';\n\n// prod root\nconst CONTRACT_ADDRESS_GIFT =\n  'bostrom16t6tucgcqdmegye6c9ltlkr237z8yfndmasrhvh7ucrfuqaev6xq7cpvek';\nconst CONTRACT_ADDRESS_PASSPORT =\n  'bostrom1xut80d09q0tgtch8p0z4k5f88d3uvt8cvtzm5h3tu3tsy4jk9xlsfzhxel';\n\nconst DICTIONARY = {\n  Astronauts: 'Astronaut',\n  'Average Citizens. ETH Analysis': 'Average Citizens',\n  'Cyberpunks. ERC20 and ERC721 Analysis': 'Cyberpunk',\n  'Extraordinary Hackers. Gas Analysis': 'Extraordinary Hacker',\n  'Key Opinion Leaders. ERC20 Analysis': 'Key Opinion Leader',\n  'Masters of the Great Web. Gas and ERC721 Analysis':\n    'Master of the Great Web',\n  'Passionate Investors. ERC20 Analysis': 'Passionate Investor',\n  'Heroes of the Great Web. Genesis and ETH2 Stakers':\n    'True Hero of the Great Web',\n  Leeches: 'Devil',\n};\n\nconst GIFT_ICON = '';\nconst BOOT_ICON = '';\n\nconst useGetActivePassport = (\n  addressActive: Nullable<AccountValue>,\n  updateFunc?: number\n) => {\n  const queryClient = useQueryClient();\n  const data = useQuery(\n    ['active_passport', addressActive?.bech32],\n    () => activePassport(queryClient, addressActive?.bech32),\n    {\n      enabled: Boolean(addressActive) && Boolean(queryClient),\n    }\n  );\n\n  useEffect(() => {\n    if (updateFunc) {\n      data.refetch();\n    }\n  }, [updateFunc]);\n\n  return {\n    citizenship: data.data,\n    loading: data.isFetching,\n  };\n};\n\n// TODO: replace with hook\nconst activePassport = async (\n  client: CyberClient,\n  address: string\n): Promise<Nullable<Citizenship>> => {\n  try {\n    const query = {\n      active_passport: {\n        address,\n      },\n    };\n    const response = await client.queryContractSmart(\n      CONTRACT_ADDRESS_PASSPORT,\n      query\n    );\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst parseValue = (data) => {\n  if (data.length > 0) {\n    const newData = data.replace(/'/g, '\"');\n    return JSON.parse(newData);\n  }\n  return null;\n};\n\nconst parseValueDetails = (data) => {\n  const value = parseValue(data);\n  if (value !== null) {\n    const details = {};\n    value.forEach((item) => {\n      details[DICTIONARY[item.audience]] = { gift: item.gift };\n    });\n    return details;\n  }\n  return null;\n};\n\nconst parseResponse = (obj) => {\n  return {\n    ...obj,\n    details: parseValueDetails(obj.details),\n    proof: parseValue(obj.proof),\n  };\n};\n\nconst checkGift = async (address) => {\n  try {\n    const response = await axios({\n      method: 'GET',\n      url: `https://titan.cybernode.ai/graphql/api/rest/get-cybergift/${address}`, // prod root\n      // url: `https://titan.cybernode.ai/graphql/api/rest/get-test-gift/${address}`, // test root\n    });\n\n    if (response && response.data) {\n      const { data } = response;\n      if (\n        Object.prototype.hasOwnProperty.call(data, 'cyber_gift_proofs') &&\n        Object.keys(data.cyber_gift_proofs).length > 0\n      ) {\n        const { cyber_gift_proofs: cyberGiftData } = data;\n        return parseResponse(cyberGiftData[0]);\n      }\n      if (\n        Object.prototype.hasOwnProperty.call(data, 'test_gift') &&\n        Object.keys(data.test_gift).length > 0\n      ) {\n        const { test_gift: cyberGiftData } = data;\n        return parseResponse(cyberGiftData[0]);\n      }\n    }\n    return null;\n  } catch (error) {\n    return null;\n  }\n};\n\nconst queryContractSmartPassport = async (client, query) => {\n  try {\n    const response = await getPassport(query);\n    // const response = await client.queryContractSmart(\n    //   CONTRACT_ADDRESS_PASSPORT,\n    //   query\n    // );\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst queryContractSmartGift = async (client, query) => {\n  try {\n    const response = await client.queryContractSmart(\n      CONTRACT_ADDRESS_GIFT,\n      query\n    );\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getStateGift = async (client) => {\n  try {\n    const query = {\n      state: {},\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getConfigGift = async (client) => {\n  try {\n    const query = {\n      config: {},\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getReleaseState = async (client, address) => {\n  try {\n    const query = {\n      release_state: {\n        address,\n      },\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getClaimedAmount = async (client, address) => {\n  try {\n    const query = {\n      claim: {\n        address,\n      },\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getIsClaimed = async (client, address) => {\n  try {\n    const query = {\n      is_claimed: {\n        address,\n      },\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getPassportByNickname = async (client, nickname) => {\n  try {\n    const query = {\n      passport_by_nickname: {\n        nickname,\n      },\n    };\n    const response = await queryContractSmartPassport(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getNumTokens = async (client) => {\n  try {\n    const query = {\n      num_tokens: {},\n    };\n    const response = await queryContractSmartPassport(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst tooMuthAddressError =\n  'failed to execute message; message index: 0: Address is not eligible to claim airdrop, Too many addresses: execute wasm contract failed';\n\nconst canProve8AddressNewError =\n  'You can prove only 8 addresses for one passport';\n\nconst parseRowLog = (rawlog) => {\n  if (rawlog === tooMuthAddressError) {\n    return canProve8AddressNewError;\n  }\n\n  return rawlog;\n};\n\nexport {\n  activePassport,\n  CONTRACT_ADDRESS_PASSPORT,\n  useGetActivePassport,\n  CONSTITUTION_HASH,\n  CONTRACT_ADDRESS_GIFT,\n  GIFT_ICON,\n  BOOT_ICON,\n  AMOUNT_ALL_STAGE,\n  NEW_RELEASE,\n  checkGift,\n  getConfigGift,\n  getStateGift,\n  getReleaseState,\n  getClaimedAmount,\n  getIsClaimed,\n  getPassportByNickname,\n  parseRowLog,\n  getNumTokens,\n};\n","import { CONTRACT_ADDRESS_PASSPORT } from 'src/containers/portal/utils';\nimport { toAscii, toBase64 } from '@cosmjs/encoding';\nimport { PassportContractQuery } from 'src/services/soft.js/api/passport';\n\nimport axios from 'axios';\nimport defaultNetworks from 'src/constants/defaultNetworks';\n\n// need this request to query passports with any queryClient chain\n// eslint-disable-next-line import/prefer-default-export\nexport async function getPassport(query: PassportContractQuery) {\n  const response = await axios.get(\n    `${\n      defaultNetworks.bostrom.LCD_URL\n    }/cosmwasm/wasm/v1/contract/${CONTRACT_ADDRESS_PASSPORT}/smart/${toBase64(\n      toAscii(JSON.stringify(query))\n    )}`\n  );\n  return response.data.data;\n}\n","import { DeliverTxResponse } from '@cosmjs/stargate';\n\nexport class SigningCyberClientError extends Error {\n  public code: number;\n\n  constructor(response: string[] | DeliverTxResponse) {\n    let message = '';\n    let code = -1;\n    if (response instanceof Array) {\n      message = response.join('\\r\\n');\n    } else if (response.rawLog) {\n      message = response.rawLog.toString();\n      code = response.code;\n    } else {\n      message = message?.error;\n    }\n\n    super(message);\n    cyblog.error(message, { error: response });\n\n    this.code = code;\n  }\n}\n\nexport const throwErrorOrResponse = (\n  response: string[] | DeliverTxResponse\n) => {\n  const isResponseError = response instanceof Array || response.code !== 0;\n  if (isResponseError) {\n    throw new SigningCyberClientError(response);\n  }\n  return response as DeliverTxResponse;\n};\n","/* eslint-disable import/no-unused-modules */\nimport { Coin, OfflineSigner, StdFee } from '@cosmjs/launchpad';\nimport { SigningCyberClient } from '@cybercongress/cyber-js';\nimport { SenseApi } from 'src/contexts/backend/services/senseApi';\nimport { CyberLinkSimple, NeuronAddress, ParticleCid } from 'src/types/base';\nimport { getNowUtcNumber } from 'src/utils/date';\n\nimport { DEFAULT_GAS_LIMITS } from 'src/constants/config';\nimport { CONTRACT_ADDRESS_PASSPORT } from 'src/containers/portal/utils';\nimport BigNumber from 'bignumber.js';\nimport { asyncForEach } from 'src/utils/utils';\nimport { LinkDto } from '../CozoDb/types/dto';\nimport { throwErrorOrResponse } from './errors';\n\nimport Soft3MessageFactory from '../soft.js/api/msgs';\n\nconst defaultFee = {\n  amount: [],\n  gas: DEFAULT_GAS_LIMITS.toString(),\n} as StdFee;\n\nexport const sendCyberlink = async (\n  neuron: NeuronAddress,\n  from: ParticleCid,\n  to: ParticleCid,\n  {\n    senseApi,\n    signingClient,\n  }: {\n    senseApi: SenseApi;\n    signingClient: SigningCyberClient;\n  },\n  fee: StdFee = defaultFee\n) => {\n  const response = await signingClient!.cyberlink(neuron, from, to, fee);\n  const result = throwErrorOrResponse(response);\n\n  const { transactionHash } = result;\n  const link = {\n    from,\n    to,\n    transactionHash,\n    timestamp: getNowUtcNumber(),\n    neuron,\n  } as LinkDto;\n\n  // TODO: add from/toparticle to DB ??\n  await senseApi?.putCyberlink(link);\n  await senseApi?.addCyberlinkLocal(link);\n\n  return transactionHash;\n};\n\nexport const sendCyberlinkArray = async (\n  neuron: NeuronAddress,\n  arrLinks: CyberLinkSimple[],\n  {\n    signingClient,\n    senseApi,\n  }: {\n    senseApi: SenseApi;\n    signingClient: SigningCyberClient;\n  }\n) => {\n  const multiplier = new BigNumber(2).multipliedBy(arrLinks.length);\n\n  const cyberlinkMsg = {\n    typeUrl: '/cyber.graph.v1beta1.MsgCyberlink',\n    value: {\n      neuron,\n      links: arrLinks,\n    },\n  };\n\n  const response = await signingClient.signAndBroadcast(\n    neuron,\n    [cyberlinkMsg],\n    Soft3MessageFactory.fee(multiplier.toNumber())\n  );\n\n  const result = throwErrorOrResponse(response);\n\n  const { transactionHash } = result;\n\n  const links = arrLinks.map((item) => {\n    return {\n      from: item.from,\n      to: item.to,\n      transactionHash,\n      timestamp: getNowUtcNumber(),\n      neuron,\n    } as LinkDto;\n  });\n\n  await senseApi?.putCyberlink(links);\n\n  await asyncForEach(links, async (item: LinkDto) => {\n    await senseApi?.addCyberlinkLocal(item);\n  });\n\n  return transactionHash;\n};\n\nexport const sendTokensWithMessage = async (\n  address: NeuronAddress,\n  recipient: string,\n  offerCoin: Coin[],\n  memo: string | ParticleCid,\n  {\n    senseApi,\n    signingClient,\n  }: { signingClient: SigningCyberClient; senseApi: SenseApi }\n) => {\n  const response = await signingClient.sendTokens(\n    address,\n    recipient,\n    offerCoin,\n    'auto',\n    memo\n  );\n  const result = throwErrorOrResponse(response);\n  const { transactionHash } = result;\n\n  await senseApi?.addMsgSendAsLocal({\n    transactionHash,\n    fromAddress: address,\n    toAddress: recipient,\n    amount: offerCoin,\n    memo,\n  });\n\n  return transactionHash;\n};\n\nexport const investmint = async (\n  address: NeuronAddress,\n  amount: Coin,\n  resource: string,\n  length: number,\n  signingClient: SigningCyberClient\n) => {\n  const response = await signingClient.investmint(\n    address,\n    amount,\n    resource,\n    length,\n    'auto'\n  );\n\n  const { transactionHash } = throwErrorOrResponse(response);\n  return transactionHash;\n};\n\nexport const updatePassportParticle = async (\n  nickname: string,\n  particle: ParticleCid,\n  {\n    signer,\n    signingClient,\n  }: {\n    signer: OfflineSigner;\n    signingClient: SigningCyberClient;\n  }\n) => {\n  const [{ address }] = await signer.getAccounts();\n\n  const msgObject = {\n    update_particle: {\n      nickname,\n      particle,\n    },\n  };\n  return signingClient.execute(\n    address,\n    CONTRACT_ADDRESS_PASSPORT,\n    msgObject,\n    'auto'\n  );\n};\n","import { ProxyMarked, Remote } from 'comlink';\n\nimport { BehaviorSubject, Subject, first, tap } from 'rxjs';\nimport { CyberClient, SigningCyberClient } from '@cybercongress/cyber-js';\nimport { RPC_URL } from 'src/constants/config';\nimport { SenseApi } from 'src/contexts/backend/services/senseApi';\nimport { Option } from 'src/types';\nimport { getSearchQuery, searchByHash } from 'src/utils/search/utils';\nimport { NeuronAddress, ParticleCid } from 'src/types/base';\nimport { getPassportByNickname } from 'src/containers/portal/utils';\nimport { sendCyberlink } from '../neuron/neuronApi';\n\nimport { extractRuneScript } from './helpers';\nimport { RuneEngine } from './engine';\nimport DbApiWrapper from '../backend/services/DbApi/DbApi';\nimport { IpfsApi } from '../backend/workers/background/api/ipfsApi';\nimport { EmbeddingApi } from '../backend/workers/background/api/mlApi';\n\nexport type RuneInnerDeps = {\n  ipfsApi: Option<IpfsApi>;\n  rune: Option<RuneEngine>;\n  queryClient: Option<CyberClient>;\n  embeddingApi: Option<EmbeddingApi>;\n  dbApi: Option<DbApiWrapper>;\n  signingClient: Option<SigningCyberClient & ProxyMarked>;\n  // signer?: Option<OfflineSigner>;\n  senseApi: Option<SenseApi & ProxyMarked>;\n  address: Option<NeuronAddress>;\n};\n\ntype SubjectDeps<T> = {\n  [K in keyof T]: BehaviorSubject<T[K]> | Subject<T[K]>;\n};\n\nconst createRuneDeps = () => {\n  const subjectDeps: SubjectDeps<RuneInnerDeps> = {\n    // Initialize subjects for each dependency\n    ipfsApi: new BehaviorSubject<RuneInnerDeps['ipfsApi']>(undefined),\n    rune: new BehaviorSubject<RuneInnerDeps['rune']>(undefined),\n    queryClient: new BehaviorSubject<RuneInnerDeps['queryClient']>(undefined),\n    embeddingApi: new BehaviorSubject<Option<EmbeddingApi>>(undefined),\n    signingClient: new BehaviorSubject<RuneInnerDeps['signingClient']>(\n      undefined\n    ),\n    senseApi: new BehaviorSubject<RuneInnerDeps['senseApi']>(undefined),\n    address: new BehaviorSubject<RuneInnerDeps['address']>(undefined),\n    dbApi: new BehaviorSubject<RuneInnerDeps['dbApi']>(undefined),\n  };\n\n  let abortController: Option<AbortController>;\n\n  const defferedDependency = (\n    name: keyof RuneInnerDeps\n  ): Promise<RuneInnerDeps[typeof name]> => {\n    return new Promise((resolve) => {\n      const item$ = subjectDeps[name] as BehaviorSubject<\n        RuneInnerDeps[typeof name]\n      >;\n      if (item$.getValue()) {\n        resolve(item$.getValue());\n      }\n\n      item$\n        .pipe(\n          first((value) => !!value) // Automatically unsubscribes after the first valid value\n          // tap((v) => console.log('------defferedDependency', name, v))\n        )\n        .subscribe((value) => {\n          resolve(value);\n        });\n    });\n  };\n\n  CyberClient.connect(RPC_URL).then((client) => {\n    subjectDeps.queryClient?.next(client);\n  });\n\n  const setInnerDeps = (externalDeps: Partial<RuneInnerDeps>) => {\n    Object.keys(externalDeps)\n      .filter((name) => externalDeps[name as keyof RuneInnerDeps] !== undefined)\n      .forEach((name) => {\n        const item = externalDeps[name as keyof RuneInnerDeps];\n        subjectDeps[name as keyof RuneInnerDeps].next(item);\n      });\n  };\n\n  const graphSearch = async (query: string, page = 0) => {\n    const queryClient = (await defferedDependency(\n      'queryClient'\n    )) as CyberClient;\n\n    const keywordHash = await getSearchQuery(query);\n\n    return searchByHash(queryClient, keywordHash, page);\n  };\n\n  const getIpfsTextConent = async (cid: string) => {\n    const ipfsApi = (await defferedDependency('ipfsApi')) as IpfsApi;\n    return ipfsApi.fetchWithDetails(cid, 'text');\n  };\n\n  const evalScriptFromIpfs = async (\n    cid: ParticleCid,\n    funcName: string,\n    params = {}\n  ) => {\n    try {\n      const result = await getIpfsTextConent(cid);\n      if (result?.content === undefined) {\n        return { action: 'error', message: 'Particle not found' };\n      }\n      // in case of soul script is mixed with markdown\n      // need to extract pure script\n      const pureScript = extractRuneScript(result.content);\n\n      const rune = (await defferedDependency('rune')) as RuneEngine;\n\n      return rune.executeFunction(pureScript, funcName, params);\n    } catch (e) {\n      return { action: 'error', message: e.toString() };\n    }\n  };\n\n  const executeScriptCallback = async (refId: string, data = {}) => {\n    try {\n      const rune = (await defferedDependency('rune')) as RuneEngine;\n      return rune.executeCallback(refId, data);\n    } catch (e) {\n      return { action: 'error', message: e.toString() };\n    }\n  };\n\n  const createAbortController = () => {\n    abortController = new AbortController();\n    return abortController;\n  };\n\n  const abort = () => {\n    abortController?.abort();\n  };\n\n  const cybApi = {\n    createAbortController,\n    graphSearch,\n    cyberlink: async (from: string, to: string) => {\n      const address = subjectDeps.address.getValue();\n      if (!address) {\n        throw new Error('Connect your wallet first');\n      }\n      const senseApi = (await defferedDependency('senseApi')) as SenseApi;\n      const signingClient = (await defferedDependency(\n        'signingClient'\n      )) as SigningCyberClient;\n\n      return sendCyberlink(address, from, to, {\n        senseApi,\n        signingClient,\n      });\n    },\n    getPassportByNickname: async (nickname: string) => {\n      const queryClient = await defferedDependency('queryClient');\n      const passport = await getPassportByNickname(queryClient, nickname);\n\n      return passport;\n    },\n    searcByEmbedding: async (text: string, count = 10) => {\n      const embeddingApi = (await defferedDependency(\n        'embeddingApi'\n      )) as EmbeddingApi;\n      await defferedDependency('dbApi');\n      // console.log('----searcByEmbedding', text);\n      return embeddingApi.searchByEmbedding(text, count);\n    },\n    evalScriptFromIpfs,\n    getIpfsTextConent,\n    addContenToIpfs: async (content: string) => {\n      const ipfsApi = (await defferedDependency('ipfsApi')) as IpfsApi;\n\n      return ipfsApi.addContent(content);\n    },\n    executeScriptCallback,\n  };\n\n  return { setInnerDeps, cybApi, abort };\n};\n\nconst runeDeps = createRuneDeps();\n\nexport type RuneDeps = typeof runeDeps;\n\n// export type EngineDeps = ReturnType<typeof createRuneDeps>;\n\nexport default runeDeps;\n","/* eslint-disable import/prefer-default-export */\n/* eslint-disable import/no-unused-modules */\n\n// https://platform.openai.com/docs/models/overview\n// gpt-3.5-turbo\n\ntype OpenAiMessage = {\n  role: 'system' | 'user' | 'assistant';\n  content: string;\n};\n\ninterface OpenAIParams {\n  model: string;\n  messages: OpenAiMessage[];\n  stream?: boolean;\n  [key: string]: any;\n}\n\nconst defaultOpenAIParams: Partial<OpenAIParams> = {\n  model: 'gpt-3.5-turbo',\n};\n\nexport const openAICompletion = async (\n  messages: OpenAiMessage[],\n  apiKey: string,\n  params: Partial<OpenAIParams> = {},\n  cb: (s: string) => Promise<void>,\n  abortController?: AbortController\n): Promise<string> => {\n  const body = JSON.stringify({\n    messages,\n    ...defaultOpenAIParams,\n    ...params,\n  });\n\n  const headers = {\n    'Content-Type': 'application/json',\n    Authorization: `Bearer ${apiKey}`,\n  };\n\n  const response = await fetch('https://api.openai.com/v1/chat/completions', {\n    method: 'POST',\n    signal: abortController?.signal,\n    headers,\n    body,\n  });\n\n  if (!params.stream) {\n    // Non-streaming request\n    const data = await response.json();\n    return data.choices[0].message.content;\n  }\n  // Streaming request\n  const reader = response.body?.getReader();\n  const decoder = new TextDecoder();\n  let result = '';\n  let buffer = '';\n\n  if (reader) {\n    // eslint-disable-next-line no-constant-condition\n    while (true) {\n      // eslint-disable-next-line no-await-in-loop\n      const { done, value } = await reader.read();\n      if (done || abortController?.signal.aborted) {\n        break;\n      }\n\n      buffer += decoder.decode(value, { stream: true });\n      const lines = buffer.split('\\n');\n\n      // Keep the last partial line in the buffer\n      buffer = lines.pop() || '';\n\n      // eslint-disable-next-line no-restricted-syntax\n      for (const line of lines) {\n        const message = line.replace(/^data: /, '');\n        if (message === '[DONE]') {\n          return result;\n        }\n        try {\n          const parsed = JSON.parse(message);\n          if (parsed.choices && parsed.choices.length > 0) {\n            const { content } = parsed.choices[0].delta;\n            result += content;\n            if (content) {\n              // eslint-disable-next-line no-await-in-loop\n              await cb(content);\n            }\n          }\n        } catch (error) {\n          console.error('Error parsing stream message:', message, error);\n        }\n      }\n    }\n  }\n\n  return result;\n};\n","/* eslint-disable import/no-unused-modules */\nimport { getFromLink, getToLink } from 'src/utils/search/utils';\nimport runeDeps from './runeDeps';\nimport { openAICompletion } from './services/llmRequests/openai';\n\n// let runeDeps;\n\n// export const initRuneDeps = (deps) => {\n//   console.log('---initRuneDeps', deps);\n\n//   runeDeps = deps;\n// };\nexport async function jsCyberSearch(query) {\n  return runeDeps.cybApi.graphSearch(query);\n}\n\nexport async function jsCyberLink(fromCid, toCid) {\n  return runeDeps.cybApi.cyberlink(fromCid, toCid);\n}\n\nexport async function jsGetPassportByNickname(nickname) {\n  return runeDeps.cybApi.getPassportByNickname(nickname);\n}\n\nexport async function jsEvalScriptFromIpfs(cid, funcName, params = {}) {\n  return runeDeps.cybApi.evalScriptFromIpfs(cid, funcName, params);\n}\n\nexport async function jsGetIpfsTextContent(cid) {\n  return runeDeps.getIpfsTextConent(cid);\n}\n\nexport async function jsAddContenToIpfs(content) {\n  return runeDeps.addContenToIpfs(content);\n}\n\nexport async function jsExecuteScriptCallback(refId, data) {\n  console.log('exec deps callback', refId);\n  return runeDeps.cybApi.executeScriptCallback(refId, data);\n}\n\nexport async function jsOpenAICompletions(messages, apiKey, params, refId) {\n  const callback = async (data) => jsExecuteScriptCallback(refId, data);\n  const result = await openAICompletion(\n    messages,\n    apiKey,\n    params,\n    callback,\n    runeDeps.cybApi.createAbortController()\n  );\n  return result;\n}\n\nexport async function jsSearchByEmbedding(text, count) {\n  return runeDeps.cybApi.searcByEmbedding(text, count);\n}\n\nexport async function jsCyberLinksFrom(cid) {\n  const result = await getFromLink(cid);\n  return result;\n}\n\nexport async function jsCyberLinksTo(cid) {\n  const result = await getToLink(cid);\n  return result;\n}\n","import {\n  GetTxsEventRequest,\n  GetTxsEventResponse,\n  GetTxsEventResponseAmino,\n} from '@cybercongress/cyber-ts/cosmos/tx/v1beta1/service';\nimport axios from 'axios';\nimport { LCD_URL } from 'src/constants/config';\n\ntype PropsTx = {\n  events: ReadonlyArray<{ key: string; value: string }>;\n  pagination?: GetTxsEventRequest['pagination'];\n  orderBy?: GetTxsEventRequest['orderBy'];\n};\n\n// eslint-disable-next-line import/prefer-default-export\nexport async function getTransactions({\n  events,\n  pagination = { limit: 20, offset: 0 },\n  orderBy,\n  config,\n}: PropsTx) {\n  const { offset, limit } = pagination;\n  const response = await axios.get<GetTxsEventResponseAmino>(\n    `${LCD_URL}/cosmos/tx/v1beta1/txs`,\n    {\n      params: {\n        'pagination.offset': offset,\n        'pagination.limit': limit,\n        orderBy,\n        events: events.map((evn) => `${evn.key}='${evn.value}'`),\n      },\n      paramsSerializer: {\n        indexes: null,\n      },\n      signal: config?.signal,\n    }\n  );\n\n  const { txs } = response.data;\n\n  // bullshit formatting FIXME:\n  // const formatted = GetTxsEventResponse.fromAmino(response.data);\n  // from amino to protobuf\n  const formatted = {\n    txs,\n    pagination: response.data.pagination || {\n      total: response.data.total,\n    },\n    txResponses: response.data.tx_responses,\n  } as GetTxsEventResponse;\n\n  if (!formatted.pagination?.total) {\n    formatted.pagination.total = formatted.txResponses.length;\n  }\n\n  return formatted;\n}\n","export const enum Networks {\n  BOSTROM = 'bostrom',\n  SPACE_PUSSY = 'space-pussy',\n  ETH = 'eth',\n  OSMO = 'osmo',\n  TERRA = 'terra',\n  COSMOS = 'cosmoshub-4',\n}\n\nexport type NetworkConfig = {\n  CHAIN_ID: Networks;\n  BASE_DENOM: string;\n  DENOM_LIQUID: string;\n  RPC_URL: string;\n  LCD_URL: string;\n  WEBSOCKET_URL: string;\n  INDEX_HTTPS: string;\n  INDEX_WEBSOCKET: string;\n  BECH32_PREFIX: string;\n  MEMO_KEPLR: string;\n};\n\nexport type NetworksList = {\n  [key in Networks]: NetworkConfig;\n};\n","const LEDGER = {\n  STAGE_INIT: 0,\n  STAGE_SELECTION: 1,\n  STAGE_LEDGER_INIT: 2,\n  STAGE_READY: 3,\n  STAGE_WAIT: 4,\n  STAGE_GENERATED: 5,\n  STAGE_SUBMITTED: 6,\n  STAGE_CONFIRMING: 7,\n  STAGE_CONFIRMED: 8,\n  STAGE_ERROR: 15,\n  HDPATH: [44, 118, 0, 0, 0],\n};\n\nconst GENESIS_SUPPLY = 1000000000000000;\nconst TOTAL_GOL_GENESIS_SUPPLY = 50000000000000;\n\nconst POCKET = {\n  STAGE_TWEET_ACTION_BAR: {\n    ADD_AVATAR: 'addAvatar',\n    FOLLOW: 'follow',\n    TWEET: 'tweet',\n  },\n};\n\nexport { LEDGER, GENESIS_SUPPLY, TOTAL_GOL_GENESIS_SUPPLY, POCKET };\n","import dateFormat from 'dateformat';\n\nexport const numberToUtcDate = (timestamp: number) =>\n  dateFormat(new Date(timestamp), 'yyyy-mm-dd\"T\"HH:MM:ss.l', true);\n\nexport const dateToUtcNumber = (isoString: string) =>\n  Date.parse(isoString.endsWith('Z') ? isoString : `${isoString}Z`);\n\nexport const getNowUtcNumber = () => Date.now();\n\nfunction roundMilliseconds(dateTimeString: string) {\n  const date = new Date(dateTimeString);\n  const roundedMilliseconds = Math.round(date.getMilliseconds() / 1000) * 1000;\n  date.setMilliseconds(roundedMilliseconds);\n  return dateFormat(date, 'yyyy-mm-dd\"T\"HH:MM:ss.l');\n}\nfunction getCurrentTimezoneOffset() {\n  const now = new Date();\n  return -now.getTimezoneOffset() / 60;\n}\n\nfunction pluralizeUnit(quantity: number, unit: string): string {\n  return quantity === 1 ? unit : `${unit}s`;\n}\n\nconst minuteInMs = 60000; // 60 seconds * 1000 milliseconds\nconst hourInMs = 3600000; // 60 minutes * 60 seconds * 1000 milliseconds\nconst dayInMs = 86400000; // 24 hours * 60 minutes * 60 seconds * 1000 milliseconds\n\nfunction convertTimestampToString(timestamp: number): string {\n  if (timestamp < minuteInMs) {\n    const seconds = Math.floor(timestamp / 1000);\n    return `${seconds} ${pluralizeUnit(seconds, 'second')}`;\n  }\n  if (timestamp < hourInMs) {\n    const minutes = Math.floor(timestamp / minuteInMs);\n    return `${minutes} ${pluralizeUnit(minutes, 'minute')}`;\n  }\n  if (timestamp < dayInMs) {\n    const hours = Math.floor(timestamp / hourInMs);\n    return `${hours} ${pluralizeUnit(hours, 'hour')}`;\n  }\n\n  const days = Math.floor(timestamp / dayInMs);\n  return `${days} ${pluralizeUnit(days, 'day')}`;\n}\n\nexport { roundMilliseconds, convertTimestampToString };\n","import Unixfs from 'ipfs-unixfs';\nimport { DAGNode, util as DAGUtil } from 'ipld-dag-pb';\nimport { isString } from 'lodash';\nimport { RemoteIpfsApi } from 'src/services/backend/workers/background/worker';\nimport { ParticleCid } from 'src/types/base';\nimport { PATTERN_IPFS_HASH } from 'src/constants/patterns';\nimport { Remote } from 'comlink';\nimport { IpfsApi } from 'src/services/backend/workers/background/api/ipfsApi';\n\nexport const isCID = (cid: string): boolean => {\n  return cid.match(PATTERN_IPFS_HASH) !== null;\n};\n\n// eslint-disable-next-line import/prefer-default-export\nexport const getIpfsHash = (string: string): Promise<ParticleCid> =>\n  new Promise((resolve, reject) => {\n    const unixFsFile = new Unixfs('file', Buffer.from(string));\n\n    const buffer = unixFsFile.marshal();\n    DAGNode.create(buffer, (err, dagNode) => {\n      if (err) {\n        reject(new Error('Cannot create ipfs DAGNode'));\n      }\n\n      DAGUtil.cid(dagNode, (error, cid) => {\n        resolve(cid.toBaseEncodedString());\n      });\n    });\n  });\nexport const addIfpsMessageOrCid = async (\n  message: string | ParticleCid | File,\n  { ipfsApi }: { ipfsApi: Remote<IpfsApi> | null }\n) => {\n  if (!ipfsApi) {\n    throw Error('IpfsApi is not initialized');\n  }\n\n  return (\n    isString(message) && message.match(PATTERN_IPFS_HASH)\n      ? message\n      : ((await ipfsApi!.addContent(message)) as string)\n  ) as ParticleCid;\n};\n","export const CYBLOG_LOG_SHOW = 'cyblog_show';\n\nexport const CYBLOG_BROADCAST_CHANNEL_NAME = 'CYBLOG_BROADCST_CHANNEL';\n\nexport const CYBLOG_CONSOLE_PARAMS_DEFAULT = {\n  thread: 'all',\n  unit: 'all',\n  module: 'all',\n};\n","import _, { isEmpty } from 'lodash';\nimport { ConsoleLogParams, LogContext, LogItem, LogLevel } from './types';\nimport { CYBLOG_BROADCAST_CHANNEL_NAME } from './constants';\n\nconst logList: LogItem[] = [];\n\nfunction createCybLog<T>(defaultContext: Partial<LogContext<T>> = {}) {\n  function appendLog(logItem: LogItem, truncate = true) {\n    logList.push(logItem);\n\n    while (truncate && logList.length > 1000) {\n      logList.shift(); // Remove the first element to keep the list size <= 1000\n    }\n  }\n  let consoleLogParams = {} as ConsoleLogParams;\n\n  const channel = new BroadcastChannel(CYBLOG_BROADCAST_CHANNEL_NAME);\n\n  channel.onmessage = (event) => {\n    if (event.data.type === 'params') {\n      consoleLogParams = { ...consoleLogParams, ...event.data.value };\n    }\n  };\n\n  const getConsoleLogParams = () => consoleLogParams;\n\n  function consoleLog<T>(\n    level: LogLevel,\n    message: T,\n    context: Partial<LogContext<T>>\n  ) {\n    const ctx = _.omit(context, [\n      'formatter',\n      'thread',\n      'module',\n      'unit',\n      'data',\n    ]);\n    const { thread = '', module = '', unit = '', data = '' } = context;\n    const ctxItem = isEmpty(ctx) ? '' : ctx;\n\n    if (Array.isArray(message)) {\n      console[level](...message, ctxItem);\n      return;\n    }\n\n    if (context?.formatter) {\n      console[level](context?.formatter(message), ctxItem);\n      return;\n    }\n\n    console[level](`[${thread}:${module}:${unit}] ${message}`, data, ctxItem);\n  }\n\n  // eslint-disable-next-line import/no-unused-modules\n  function log<T>(\n    level: LogLevel,\n    message: string | T,\n    context: LogContext<any> = defaultContext\n  ) {\n    try {\n      const formattedMessage = context?.formatter\n        ? context?.formatter(message)\n        : message;\n\n      const logEntry = {\n        timestamp: new Date(),\n        level,\n        message: formattedMessage,\n        stacktrace: context?.stacktrace,\n        context: _.omit(context, ['formatter', 'stacktrace']),\n      };\n\n      appendLog(logEntry);\n      // !!localStorage.getItem(LOCAL_STORAGE_USE_CONSOLE_LOG_KEY) &&\n      const showConsoleLog = Object.keys(consoleLogParams).reduce(\n        (acc: boolean, key: string) => {\n          const params = consoleLogParams[key];\n          const contextItem = context[key];\n          if (params && contextItem) {\n            return (\n              acc ||\n              params === 'all' ||\n              params.length === 0 ||\n              params.some((p) => p === contextItem)\n            );\n          }\n          return acc;\n        },\n        false\n      );\n\n      if (showConsoleLog) {\n        consoleLog(level, message, context);\n      }\n    } catch (error) {\n      console.log('cyblog error', error);\n    }\n  }\n\n  function info<T>(message: T, context?: LogContext<string | T>) {\n    return log('info', message, context);\n  }\n\n  function error<T>(message: T, context?: LogContext<string | T>) {\n    return log('error', message, context);\n  }\n\n  function warn<T>(message: T, context?: LogContext<string | T>) {\n    return log('warn', message, context);\n  }\n\n  function trace<T>(message: T, context?: LogContext<string | T>) {\n    return log('warn', message, context);\n  }\n\n  function normalizeLog() {\n    return logList.map((logItem) => {\n      const { context, ...rest } = logItem;\n      const {\n        unit = '',\n        module = '',\n        thread = '',\n        data = '',\n        error = '',\n        stacktrace = '',\n      } = context || {};\n      return {\n        ...rest,\n        unit,\n        module,\n        thread,\n        data, //: JSON.stringify(data),\n        error,\n        stacktrace,\n      };\n    });\n  }\n\n  return {\n    log,\n    info,\n    error,\n    warn,\n    trace,\n    logList,\n    getLogs: () => normalizeLog(),\n    clear: () => logList.splice(0, logList.length),\n    getConsoleLogParams,\n  };\n}\n\nexport const createCyblogChannel = (\n  defaultContext: Partial<LogContext<T>> = {}\n) => {\n  const channel = new BroadcastChannel(CYBLOG_BROADCAST_CHANNEL_NAME);\n\n  function postLogToChannel<T>(\n    level: LogLevel,\n    message: T,\n    context?: LogContext<string | T>\n  ) {\n    const ctx = { ...defaultContext, ...context };\n    if (context?.error) {\n      ctx.error = JSON.stringify(context.error);\n    }\n    channel.postMessage({\n      type: 'log',\n      value: { level, message, context: ctx },\n    });\n  }\n\n  function info<T>(message: T, context?: LogContext<string | T>) {\n    return postLogToChannel('info', message, context);\n  }\n\n  function error<T>(message: T, context?: LogContext<string | T>) {\n    return postLogToChannel('error', message, context);\n  }\n\n  function warn<T>(message: T, context?: LogContext<string | T>) {\n    return postLogToChannel('warn', message, context);\n  }\n\n  function trace<T>(message: T, context?: LogContext<string | T>) {\n    return postLogToChannel('warn', message, context);\n  }\n\n  return { info, error, warn, trace };\n};\n\nconst cyblog = createCybLog({ thread: 'main' });\n\nexport type LogFunc = (message: T, context?: LogContext<string | T>) => void;\n\nexport type CyblogChannel = ReturnType<typeof createCyblogChannel>;\n\nexport default cyblog;\n","import axios from 'axios';\n\nimport { CyberClient } from '@cybercongress/cyber-js';\nimport { DelegationResponse } from 'cosmjs-types/cosmos/staking/v1beta1/staking';\nimport { CID_FOLLOW, CID_TWEET } from 'src/constants/app';\nimport { LCD_URL } from 'src/constants/config';\nimport { LinksType, LinksTypeFilter } from 'src/containers/Search/types';\nimport { ParticleCid } from 'src/types/base';\nimport { PATTERN_IPFS_HASH } from 'src/constants/patterns';\nimport { getTransactions } from 'src/services/transactions/lcd';\nimport { OrderBy } from 'cosmjs-types/cosmos/tx/v1beta1/service';\nimport { getIpfsHash } from '../ipfs/helpers';\nimport { encodeSlash } from '../utils';\n\nexport const formatNumber = (number, toFixed) => {\n  let formatted = +number;\n\n  if (toFixed) {\n    formatted = +formatted.toFixed(toFixed);\n  }\n\n  return formatted.toLocaleString('en').replace(/,/g, ' ');\n};\n\nexport const getRankGrade = (rank) => {\n  let from;\n  let to;\n  let value;\n\n  if (rank > 0.00000276) {\n    from = 0.00000276;\n    to = 0.01;\n    value = 1;\n  } else if (rank > 0.00000254879356777504 && rank <= 0.00000276) {\n    from = 0.00000254879356777504;\n    to = 0.00000276;\n    value = 2;\n  } else if (rank > 0.00000233758713555007 && rank <= 0.00000254879356777504) {\n    from = 0.00000233758713555007;\n    to = 0.00000254879356777504;\n    value = 3;\n  } else if (rank > 0.00000191517427110014 && rank <= 0.00000233758713555007) {\n    from = 0.00000191517427110014;\n    to = 0.00000233758713555007;\n    value = 4;\n  } else if (rank > 0.00000128155497442525 && rank <= 0.00000191517427110014) {\n    from = 0.00000128155497442525;\n    to = 0.00000191517427110014;\n    value = 5;\n  } else if (rank > 0.00000022552281330043 && rank <= 0.00000128155497442525) {\n    from = 0.00000022552281330043;\n    to = 0.00000128155497442525;\n    value = 6;\n  } else if (rank > 0 && rank <= 0.00000022552281330043) {\n    from = 0;\n    to = 0.00000022552281330043;\n    value = 7;\n  } else {\n    from = 'n/a';\n    to = 'n/a';\n    value = 'n/a';\n  }\n\n  return {\n    from,\n    to,\n    value,\n  };\n};\n\nexport const selfDelegationShares = async (\n  delegatorAddress,\n  operatorAddress\n) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/staking/delegators/${delegatorAddress}/delegations/${operatorAddress}`,\n    });\n    return response.data.result.balance.amount;\n  } catch (e) {\n    console.log(e);\n    return 0;\n  }\n};\n\nexport const stakingPool = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/staking/pool`,\n    });\n\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return 0;\n  }\n};\n\nexport const getRelevance = async (page = 0, limit = 50) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cyber/rank/v1beta1/rank/top`,\n      params: {\n        'pagination.page': page,\n        'pagination.perPage': limit,\n      },\n    });\n    return response.data;\n  } catch (error) {\n    return {};\n  }\n};\n\nexport const getTxs = async (txsHash) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cosmos/tx/v1beta1/txs/${txsHash}`,\n    });\n    debugger;\n    return response.data;\n  } catch (e) {\n    console.error(e);\n    return null;\n  }\n};\n\nexport const getValidatorsInfo = async (address) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/staking/validators/${address}`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const keybaseCheck = async (identity) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `https://keybase.io/_/api/1.0/user/lookup.json?key_suffix=${identity}&fields=basics`,\n    });\n    return response.data;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const keybaseAvatar = async (identity) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `https://keybase.io/_/api/1.0/user/lookup.json?key_suffix=${identity}&fields=pictures`,\n    });\n    return response.data;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const getDelegators = async (validatorAddr) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/staking/validators/${validatorAddr}/delegations`,\n    });\n    return response.data;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamSlashing = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/slashing/parameters`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamDistribution = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/distribution/parameters`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamBandwidth = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/bandwidth/parameters`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamGov = async () => {\n  try {\n    const responseGovDeposit = await axios({\n      method: 'get',\n      url: `${LCD_URL}/gov/parameters/deposit`,\n    });\n\n    const responseGovTallying = await axios({\n      method: 'get',\n      url: `${LCD_URL}/gov/parameters/tallying`,\n    });\n\n    const responseGovVoting = await axios({\n      method: 'get',\n      url: `${LCD_URL}/gov/parameters/voting`,\n    });\n\n    const response = {\n      deposit: responseGovDeposit.data.result,\n      voting: responseGovTallying.data.result,\n      tallying: responseGovVoting.data.result,\n    };\n\n    return response;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamRank = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/rank/parameters`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamInlfation = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/minting/parameters`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamResources = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cyber/resources/v1beta1/resources/params`,\n    });\n    return response.data.params;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamStaking = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/staking/parameters`,\n    });\n    return response.data.result;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamLiquidity = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cosmos/liquidity/v1beta1/params`,\n    });\n    return response.data.params;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamGrid = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cyber/grid/v1beta1/grid/params`,\n    });\n    return response.data.params;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getParamDmn = async () => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cyber/dmn/v1beta1/dmn/params`,\n    });\n    return response.data.params;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const getParamNetwork = async (address, node) => {\n  try {\n    let staking = null;\n    let slashing = null;\n    let distribution = null;\n    let bandwidth = null;\n    let gov = null;\n    let rank = null;\n    let mint = null;\n    let resources = null;\n    let liquidity = null;\n    let grid = null;\n    let dmn = null;\n\n    const dataStaking = await getParamStaking();\n    if (dataStaking !== null) {\n      staking = dataStaking;\n    }\n    const dataSlashing = await getParamSlashing();\n    if (dataSlashing !== null) {\n      slashing = dataSlashing;\n    }\n    const dataDistribution = await getParamDistribution();\n    if (dataDistribution !== null) {\n      distribution = dataDistribution;\n    }\n    const dataGov = await getParamGov();\n    if (dataGov !== null) {\n      gov = dataGov;\n    }\n    const dataBandwidth = await getParamBandwidth();\n    if (dataBandwidth !== null) {\n      bandwidth = dataBandwidth;\n    }\n\n    const dataRank = await getParamRank();\n    if (dataRank !== null) {\n      rank = dataRank;\n    }\n\n    const dataInlfation = await getParamInlfation();\n    if (dataInlfation !== null) {\n      mint = dataInlfation;\n    }\n\n    const dataResources = await getParamResources();\n    if (dataResources !== null) {\n      resources = dataResources;\n    }\n\n    const dataLiquidity = await getParamLiquidity();\n    if (dataLiquidity !== null) {\n      liquidity = dataLiquidity;\n    }\n\n    const dataGrid = await getParamGrid();\n    if (dataGrid !== null) {\n      grid = dataGrid;\n    }\n\n    const dataDmn = await getParamDmn();\n    if (dataDmn !== null) {\n      dmn = dataDmn;\n    }\n\n    const response = {\n      staking,\n      slashing,\n      distribution,\n      bandwidth,\n      gov,\n      rank,\n      mint,\n      resources,\n      liquidity,\n      grid,\n      dmn,\n    };\n\n    return response;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nconst getLink = async (\n  cid: string,\n  type: LinksType = LinksTypeFilter.from,\n  { offset, limit, order = OrderBy.ORDER_BY_DESC }\n) => {\n  try {\n    const response = await getTransactions({\n      events: [\n        {\n          key: `cyberlink.particle${\n            type === LinksTypeFilter.to ? 'To' : 'From'\n          }`,\n          value: cid,\n        },\n      ],\n      pagination: {\n        limit,\n        offset,\n      },\n      orderBy: order,\n    });\n\n    return response;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const getFromLink = async (cid, offset, limit) => {\n  return getLink(cid, LinksTypeFilter.from, { offset, limit });\n};\n\nexport const getToLink = async (cid, offset, limit) => {\n  return getLink(cid, LinksTypeFilter.to, { offset, limit });\n};\n\nexport const getSendBySenderRecipient = async (\n  address,\n  offset = 0,\n  limit = 5\n) => {\n  try {\n    const { recipient, sender } = address;\n\n    const response = await getTransactions({\n      events: [\n        { key: 'message.action', value: '/cosmos.bank.v1beta1.MsgSend' },\n        { key: 'transfer.sender', value: sender },\n        { key: 'transfer.recipient', value: recipient },\n      ],\n      pagination: { limit, offset },\n      orderBy: OrderBy.ORDER_BY_DESC,\n    });\n\n    return response;\n  } catch (e) {\n    console.log(e);\n    return undefined;\n  }\n};\n\nexport const getFollows = async (address) => {\n  try {\n    const response = await getTransactions({\n      events: [\n        { key: 'cyberlink.particleFrom', value: CID_FOLLOW },\n        { key: 'cyberlink.neuron', value: address },\n      ],\n      pagination: { limit: 1000000000 },\n    });\n\n    return response;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const getTweet = async (address) => {\n  try {\n    const response = await getTransactions({\n      events: [\n        { key: 'cyberlink.particleFrom', value: CID_TWEET },\n        { key: 'cyberlink.neuron', value: address },\n      ],\n      pagination: { limit: 1000000000 },\n    });\n    return response;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\nexport const chekFollow = async (address, addressFollowHash) => {\n  try {\n    const response = await getTransactions({\n      events: [\n        { key: 'cyberlink.particleFrom', value: CID_FOLLOW },\n        { key: 'cyberlink.neuron', value: address },\n        { key: 'cyberlink.particleTo', value: addressFollowHash },\n      ],\n      pagination: { limit: 1000000000 },\n    });\n    return response;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\nexport async function getCyberlinksTotal(address: string) {\n  try {\n    const response = await getTransactions({\n      events: [\n        { key: 'message.action', value: '/cyber.graph.v1beta1.MsgCyberlink' },\n        { key: 'cyberlink.neuron', value: address },\n      ],\n      pagination: { limit: 5, offset: 0 },\n    });\n\n    return response?.pagination?.total;\n  } catch (error) {\n    console.log(error);\n    return undefined;\n  }\n}\n\nexport const getFollowers = async (addressHash) => {\n  try {\n    const response = await getTransactions({\n      events: [\n        {\n          key: 'cyberlink.particleFrom',\n          value: CID_FOLLOW,\n        },\n        {\n          key: 'cyberlink.particleTo',\n          value: addressHash,\n        },\n      ],\n      pagination: {\n        limit: 1000000000,\n      },\n    });\n\n    return response;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\nexport const getCreator = async (cid) => {\n  try {\n    // TODO: refactor this\n    const response = await getTransactions({\n      events: [\n        {\n          key: 'cyberlink.particleTo',\n          value: cid,\n        },\n      ],\n      pagination: {\n        limit: 1,\n        offset: 0,\n      },\n    });\n\n    const response2 = await getTransactions({\n      events: [\n        {\n          key: 'cyberlink.particleFrom',\n          value: cid,\n        },\n      ],\n      pagination: {\n        limit: 1,\n        offset: 0,\n      },\n    });\n\n    const h1 = Number(response.txResponses?.[0]?.height || 0);\n    const h2 = Number(response2.txResponses?.[0]?.height || 0);\n\n    if (h1 === 0) {\n      return response2;\n    }\n    if (h2 === 0) {\n      return response;\n    }\n\n    return h1 < h2 ? response : response2;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\nexport const authAccounts = async (address) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cosmos/auth/v1beta1/accounts/${address}`,\n    });\n    return response.data;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\n// Access-Control-Allow-Origin\nexport const getCredit = async (address) => {\n  try {\n    const headers = {\n      'Content-Type': 'application/json',\n    };\n    const fromData = {\n      denom: 'boot',\n      address,\n    };\n    const response = await axios({\n      method: 'post',\n      // url: 'http://localhost:8000/credit',\n      url: 'https://titan.cybernode.ai/credit',\n      headers,\n      data: JSON.stringify(fromData),\n    });\n\n    return response;\n  } catch (error) {\n    return null;\n  }\n};\n\nexport const getSearchQuery = async (query: ParticleCid | string) =>\n  query.match(PATTERN_IPFS_HASH) ? query : getIpfsHash(encodeSlash(query));\n\nexport const searchByHash = async (\n  client: CyberClient,\n  hash: string,\n  page: number\n) => {\n  try {\n    const results = await client.search(hash, page);\n\n    return results;\n  } catch (error) {\n    // TODO: handle\n    console.error(error);\n    return undefined;\n  }\n};\n\nexport const getDelegatorDelegations = async (\n  client: CyberClient,\n  addressBech32: string\n): Promise<DelegationResponse[]> => {\n  let nextKey;\n  const delegationData: DelegationResponse[] = [];\n\n  let done = false;\n  while (!done) {\n    // eslint-disable-next-line no-await-in-loop\n    const responsedelegatorDelegations = await client.delegatorDelegations(\n      addressBech32,\n      nextKey\n    );\n\n    delegationData.push(...responsedelegatorDelegations.delegationResponses);\n\n    const key = responsedelegatorDelegations?.pagination?.nextKey;\n\n    if (key) {\n      nextKey = key;\n    } else {\n      done = true;\n    }\n  }\n\n  return delegationData;\n};\n","/* eslint-disable no-await-in-loop */\nimport bech32 from 'bech32';\nimport { fromBase64, fromUtf8, toBech32 } from '@cosmjs/encoding';\nimport { Sha256 } from '@cosmjs/crypto';\nimport BigNumber from 'bignumber.js';\nimport { ObjKeyValue } from 'src/types/data';\nimport { Pool } from '@cybercongress/cyber-js/build/codec/tendermint/liquidity/v1beta1/liquidity';\nimport { Option } from 'src/types';\nimport { Key } from '@keplr-wallet/types';\nimport { AccountValue } from 'src/types/defaultAccount';\nimport { BECH32_PREFIX, BECH32_PREFIX_VAL_CONS } from 'src/constants/config';\nimport { LEDGER } from './config';\n\nimport cyberSpace from '../image/large-purple-circle.png';\nimport customNetwork from '../image/large-orange-circle.png';\nimport cyberBostrom from '../image/large-green.png';\n\nconst DEFAULT_DECIMAL_DIGITS = 3;\nconst DEFAULT_CURRENCY = 'GoL';\n\nconst roundNumber = (num, scale) => {\n  if (!`${num}`.includes('e')) {\n    return +`${Math.floor(`${num}e+${scale}`)}e-${scale}`;\n  }\n  const arr = `${num}`.split('e');\n  let sig = '';\n  if (+arr[1] + scale > 0) {\n    sig = '+';\n  }\n  const i = `${+arr[0]}e${sig}${+arr[1] + scale}`;\n  const j = Math.floor(i);\n  const k = +`${j}e-${scale}`;\n  return k;\n};\n\nfunction numberWithCommas(x) {\n  const parts = x.split('.');\n  parts[0] = parts[0].replace(/\\B(?=(\\d{3})+(?!\\d))/g, ' ');\n  return parts.join('.');\n}\n\nconst formatNumber = (number: number | string, toFixed?: number): string => {\n  let formatted = number;\n\n  if (toFixed) {\n    formatted = roundNumber(formatted, toFixed);\n    formatted = formatted.toFixed(toFixed + 1);\n  }\n\n  if (typeof number === 'string') {\n    return numberWithCommas(formatted);\n  }\n\n  return formatted\n    .toLocaleString('en')\n    .replace(/(\\.\\d{0,})0+$/, '$1')\n    .replace(/,/g, ' ');\n};\n\nconst PREFIXES = [\n  {\n    prefix: 'T',\n    power: 10 ** 12,\n  },\n  {\n    prefix: 'G',\n    power: 10 ** 9,\n  },\n  {\n    prefix: 'M',\n    power: 10 ** 6,\n  },\n  {\n    prefix: 'K',\n    power: 10 ** 3,\n  },\n];\n\nexport function formatCurrency(\n  value,\n  currency = DEFAULT_CURRENCY,\n  decimalDigits = DEFAULT_DECIMAL_DIGITS,\n  prefixCustom = PREFIXES\n) {\n  const { prefix = '', power = 1 } =\n    prefixCustom.find((obj) => value >= obj.power) || {};\n\n  return `${roundNumber(\n    Number(value) / power,\n    decimalDigits\n  )} ${prefix}${currency.toLocaleUpperCase()}`;\n}\n\nconst getDecimal = (number, toFixed) => {\n  const nstring = number.toString();\n  const narray = nstring.split('.');\n  const result = narray.length > 1 ? narray[1] : '000';\n  return result;\n};\n\nconst asyncForEach = async (array, callback) => {\n  for (let index = 0; index < array.length; index++) {\n    await callback(array[index], index, array);\n  }\n};\n\nconst fromBech32 = (operatorAddr, prefix = BECH32_PREFIX) => {\n  const address = bech32.decode(operatorAddr);\n  return bech32.encode(prefix, address.words);\n};\n\nexport const consensusPubkey = (pubKey: string) => {\n  const ed25519PubkeyRaw = fromBase64(pubKey);\n  const addressData = sha256(ed25519PubkeyRaw).slice(0, 20);\n  return toBech32(BECH32_PREFIX_VAL_CONS, addressData);\n};\n\nconst trimString = (address: string, first = 3, second = 8) => {\n  if (address && address.length > 11) {\n    return `${address.substring(0, first)}...${address.substring(\n      address.length - second\n    )}`;\n  }\n  if (address && address.length < 11) {\n    return address;\n  }\n  return '';\n};\n\nconst exponentialToDecimal = (exponential) => {\n  let decimal = exponential.toString().toLowerCase();\n  if (decimal.includes('e+')) {\n    const exponentialSplitted = decimal.split('e+');\n    let postfix = '';\n    for (\n      let i = 0;\n      i <\n      +exponentialSplitted[1] -\n        (exponentialSplitted[0].includes('.')\n          ? exponentialSplitted[0].split('.')[1].length\n          : 0);\n      i++\n    ) {\n      postfix += '0';\n    }\n    decimal = exponentialSplitted[0].replace('.', '') + postfix;\n  }\n  if (decimal.toLowerCase().includes('e-')) {\n    const exponentialSplitted = decimal.split('e-');\n    let prefix = '0.';\n    for (let i = 0; i < +exponentialSplitted[1] - 1; i++) {\n      prefix += '0';\n    }\n    decimal = prefix + exponentialSplitted[0].replace('.', '');\n  }\n  return decimal;\n};\n\nfunction dhm(t) {\n  const cd = 24 * 60 * 60 * 1000;\n  const ch = 60 * 60 * 1000;\n  let d = Math.floor(t / cd);\n  let h = Math.floor((t - d * cd) / ch);\n  let m = Math.round((t - d * cd - h * ch) / 60000);\n  const pad = (n, unit) => {\n    return n < 10 ? `0${n}${unit}` : `${n}${unit}`;\n  };\n  if (m === 60) {\n    h += 1;\n    m = 0;\n  }\n  if (h === 24) {\n    d += 1;\n    h = 0;\n  }\n  return [`${d}d`, pad(h, 'h'), pad(m, 'm')].join(':');\n}\n\nconst downloadObjectAsJson = (exportObj, exportName) => {\n  const dataStr = `data:text/json;charset=utf-8,${encodeURIComponent(\n    JSON.stringify(exportObj)\n  )}`;\n  const downloadAnchorNode = document.createElement('a');\n\n  downloadAnchorNode.setAttribute('href', dataStr);\n  downloadAnchorNode.setAttribute('download', `${exportName}.json`);\n  document.body.appendChild(downloadAnchorNode);\n  downloadAnchorNode.click();\n  downloadAnchorNode.remove();\n};\n\nconst isMobileTablet = () => {\n  let hasTouchScreen = false;\n  if ('maxTouchPoints' in navigator) {\n    hasTouchScreen = navigator.maxTouchPoints > 0;\n  } else if ('msMaxTouchPoints' in navigator) {\n    hasTouchScreen = navigator.msMaxTouchPoints > 0;\n  } else {\n    const mQ = window.matchMedia && matchMedia('(pointer:coarse)');\n    if (mQ && mQ.media === '(pointer:coarse)') {\n      hasTouchScreen = !!mQ.matches;\n    } else if ('orientation' in window) {\n      hasTouchScreen = true; // deprecated, but good fallback\n    } else {\n      // Only as a last resort, fall back to user agent sniffing\n      const UA = navigator.userAgent;\n      hasTouchScreen =\n        /\\b(BlackBerry|webOS|iPhone|IEMobile)\\b/i.test(UA) ||\n        /\\b(Android|Windows Phone|iPad|iPod)\\b/i.test(UA);\n    }\n  }\n  return hasTouchScreen;\n};\n\nconst coinDecimals = (number) => {\n  return number * 10 ** -18;\n};\n\nconst convertResources = (number) => {\n  return Math.floor(number * 10 ** -3);\n};\n\nfunction timeSince(timeMS: number) {\n  const seconds = Math.floor(timeMS / 1000);\n\n  if (seconds === 0) {\n    return 'now';\n  }\n\n  let interval = Math.floor(seconds / 31536000);\n\n  if (interval > 1) {\n    return `${interval} years`;\n  }\n  interval = Math.floor(seconds / 2592000);\n  if (interval > 1) {\n    return `${interval} months`;\n  }\n  interval = Math.floor(seconds / 86400);\n  if (interval > 1) {\n    return `${interval} days`;\n  }\n  interval = Math.floor(seconds / 3600);\n  if (interval > 1) {\n    return `${interval} hours`;\n  }\n  interval = Math.floor(seconds / 60);\n  if (interval > 1) {\n    return `${interval} min`;\n  }\n  return `${Math.floor(seconds)} sec`;\n}\n\nconst reduceBalances = (data): ObjKeyValue => {\n  try {\n    let balances = {};\n    if (Object.keys(data).length > 0) {\n      balances = data.reduce(\n        (obj, item) => ({\n          ...obj,\n          [item.denom]: parseFloat(item.amount),\n        }),\n        {}\n      );\n    }\n    return balances;\n  } catch (error) {\n    console.log(`error reduceBalances`, error);\n    return {};\n  }\n};\n\n// example: oneLiner -> message.module=wasm&message.action=/cosmwasm.wasm.v1.MsgStoreCode&store_code.code_id=${codeId}\nfunction makeTags(oneLiner) {\n  return oneLiner.split('&').map((pair) => {\n    if (pair.indexOf('=') === -1) {\n      throw new Error('Parsing error: Equal sign missing');\n    }\n    const parts = pair.split('=');\n    if (parts.length > 2) {\n      throw new Error(\n        'Parsing error: Multiple equal signs found. If you need escaping support, please create a PR.'\n      );\n    }\n    const [key, value] = parts;\n    if (!key) {\n      throw new Error('Parsing error: Key must not be empty');\n    }\n    return { key, value };\n  });\n}\n\nfunction parseMsgContract(msg) {\n  const json = fromUtf8(msg);\n\n  return JSON.parse(json);\n}\nconst replaceSlash = (text) => text.replace(/\\//g, '%2F');\n\nconst encodeSlash = (text) => text.replace(/%2F/g, '/');\n\nconst groupMsg = (ArrMsg, size = 2) => {\n  const link = [];\n  for (let i = 0; i < Math.ceil(ArrMsg.length / size); i += 1) {\n    link[i] = ArrMsg.slice(i * size, i * size + size);\n  }\n  return link;\n};\n\nconst selectNetworkImg = (network) => {\n  switch (network) {\n    case 'bostrom':\n      return cyberBostrom;\n    case 'space-pussy':\n      return cyberSpace;\n\n    default:\n      return customNetwork;\n  }\n};\n\nconst sha256 = (data) => {\n  return new Uint8Array(new Sha256().update(data).digest());\n};\n\nfunction getDenomHash(path, baseDenom) {\n  const parts = path.split('/');\n  parts.push(baseDenom);\n  const newPath = parts.slice().join('/');\n  return `ibc/${Buffer.from(sha256(Buffer.from(newPath)))\n    .toString('hex')\n    .toUpperCase()}`;\n}\n\nfunction convertAmount(rawAmount, precision) {\n  return new BigNumber(rawAmount)\n    .shiftedBy(-precision)\n    .dp(precision, BigNumber.ROUND_FLOOR)\n    .toNumber();\n}\n\nfunction convertAmountReverce(rawAmount, precision) {\n  return new BigNumber(rawAmount)\n    .shiftedBy(precision)\n    .dp(precision, BigNumber.ROUND_FLOOR)\n    .toNumber();\n}\n\nfunction getDisplayAmount(\n  rawAmount: number | string,\n  precision: number\n): number {\n  return parseFloat(\n    new BigNumber(rawAmount)\n      .shiftedBy(-precision)\n      .dp(precision, BigNumber.ROUND_FLOOR)\n      .toFixed(precision > 0 ? 3 : 0, BigNumber.ROUND_FLOOR)\n  );\n}\n\nfunction getDisplayAmountReverce(rawAmount, precision) {\n  return new BigNumber(rawAmount)\n    .shiftedBy(precision)\n    .dp(precision, BigNumber.ROUND_FLOOR)\n    .toFixed(precision > 0 ? 3 : 0, BigNumber.ROUND_FLOOR);\n}\n\nfunction isNative(denom) {\n  if (denom && denom.includes('ibc')) {\n    return false;\n  }\n  return true;\n}\n\nconst findPoolDenomInArr = (\n  baseDenom: string,\n  dataPools: Pool[]\n): Option<Pool> => {\n  const findObj = dataPools.find((item) => item.poolCoinDenom === baseDenom);\n  return findObj;\n};\n\n// REFACTOR: Probably wrong timestamp\nconst getNowUtcTime = (): number => {\n  const now = new Date();\n  const utcTime = new Date(\n    now.getUTCFullYear(),\n    now.getUTCMonth(),\n    now.getUTCDate(),\n    now.getUTCHours(),\n    now.getUTCMinutes(),\n    now.getUTCSeconds()\n  );\n\n  return utcTime.getTime();\n};\n\nconst accountsKeplr = (accounts: Key): AccountValue => {\n  const { pubKey, bech32Address, name } = accounts;\n  const pk = Buffer.from(pubKey).toString('hex');\n\n  return {\n    bech32: bech32Address,\n    keys: 'keplr',\n    pk,\n    path: LEDGER.HDPATH,\n    name,\n  };\n};\n\nexport function covertUint8ArrayToString(data: Uint8Array): string {\n  return new TextDecoder().decode(data);\n}\n\nexport {\n  formatNumber,\n  asyncForEach,\n  getDecimal,\n  fromBech32,\n  trimString,\n  exponentialToDecimal,\n  dhm,\n  downloadObjectAsJson,\n  isMobileTablet,\n  coinDecimals,\n  convertResources,\n  timeSince,\n  reduceBalances,\n  makeTags,\n  parseMsgContract,\n  replaceSlash,\n  encodeSlash,\n  groupMsg,\n  selectNetworkImg,\n  getDenomHash,\n  getDisplayAmount,\n  getDisplayAmountReverce,\n  convertAmount,\n  convertAmountReverce,\n  isNative,\n  findPoolDenomInArr,\n  getNowUtcTime,\n  accountsKeplr,\n};\n","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\tid: moduleId,\n\t\tloaded: false,\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n\t// Flag the module as loaded\n\tmodule.loaded = true;\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n// expose the modules object (__webpack_modules__)\n__webpack_require__.m = __webpack_modules__;\n\n// the startup function\n__webpack_require__.x = function() {\n\t// Load entry module and return exports\n\t// This entry module depends on other loaded chunks and execution need to be delayed\n\tvar __webpack_exports__ = __webpack_require__.O(undefined, [444,1,922,964,948,112,235,187], function() { return __webpack_require__(55723); })\n\t__webpack_exports__ = __webpack_require__.O(__webpack_exports__);\n\treturn __webpack_exports__;\n};\n\n","__webpack_require__.amdO = {};","// getDefaultExport function for compatibility with non-harmony modules\n__webpack_require__.n = function(module) {\n\tvar getter = module && module.__esModule ?\n\t\tfunction() { return module['default']; } :\n\t\tfunction() { return module; };\n\t__webpack_require__.d(getter, { a: getter });\n\treturn getter;\n};","// define getter functions for harmony exports\n__webpack_require__.d = function(exports, definition) {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.f = {};\n// This file contains only the entry chunk.\n// The chunk loading function for additional chunks\n__webpack_require__.e = function(chunkId) {\n\treturn Promise.all(Object.keys(__webpack_require__.f).reduce(function(promises, key) {\n\t\t__webpack_require__.f[key](chunkId, promises);\n\t\treturn promises;\n\t}, []));\n};","// This function allow to reference async chunks and sibling chunks for the entrypoint\n__webpack_require__.u = function(chunkId) {\n\t// return url for filenames not based on template\n\tif (chunkId === 444) return \"444.861b1960.js\";\n\tif (chunkId === 1) return \"1.10cfeed4.js\";\n\tif (chunkId === 922) return \"922.980c9385.js\";\n\tif (chunkId === 964) return \"964.a9755641.js\";\n\t// return url for filenames based on template\n\treturn \"\" + chunkId + \".\" + {\"112\":\"4614d4ee\",\"187\":\"1146adee\",\"198\":\"44cc2170\",\"235\":\"c8ba16cc\",\"948\":\"cda404bf\"}[chunkId] + \".chunk.js\";\n};","// This function allow to reference async chunks and sibling chunks for the entrypoint\n__webpack_require__.miniCssF = function(chunkId) {\n\t// return url for filenames based on template\n\treturn undefined;\n};","__webpack_require__.g = (function() {\n\tif (typeof globalThis === 'object') return globalThis;\n\ttry {\n\t\treturn this || new Function('return this')();\n\t} catch (e) {\n\t\tif (typeof window === 'object') return window;\n\t}\n})();","__webpack_require__.hmd = function(module) {\n\tmodule = Object.create(module);\n\tif (!module.children) module.children = [];\n\tObject.defineProperty(module, 'exports', {\n\t\tenumerable: true,\n\t\tset: function() {\n\t\t\tthrow new Error('ES Modules may not assign module.exports or exports.*, Use ESM export syntax, instead: ' + module.id);\n\t\t}\n\t});\n\treturn module;\n};","__webpack_require__.o = function(obj, prop) { return Object.prototype.hasOwnProperty.call(obj, prop); }","// define __esModule on exports\n__webpack_require__.r = function(exports) {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","__webpack_require__.nmd = function(module) {\n\tmodule.paths = [];\n\tif (!module.children) module.children = [];\n\treturn module;\n};","__webpack_require__.p = \"/\";","__webpack_require__.b = self.location + \"\";\n\n// object to store loaded chunks\n// \"1\" means \"already loaded\"\nvar installedChunks = {\n\t621: 1\n};\n\n// importScripts chunk loading\nvar installChunk = function(data) {\n\tvar chunkIds = data[0];\n\tvar moreModules = data[1];\n\tvar runtime = data[2];\n\tfor(var moduleId in moreModules) {\n\t\tif(__webpack_require__.o(moreModules, moduleId)) {\n\t\t\t__webpack_require__.m[moduleId] = moreModules[moduleId];\n\t\t}\n\t}\n\tif(runtime) runtime(__webpack_require__);\n\twhile(chunkIds.length)\n\t\tinstalledChunks[chunkIds.pop()] = 1;\n\tparentChunkLoadingFunction(data);\n};\n__webpack_require__.f.i = function(chunkId, promises) {\n\t// \"1\" is the signal for \"already loaded\"\n\tif(!installedChunks[chunkId]) {\n\t\tif(true) { // all chunks have JS\n\t\t\timportScripts(__webpack_require__.p + __webpack_require__.u(chunkId));\n\t\t}\n\t}\n};\n\nvar chunkLoadingGlobal = self[\"webpackChunkcyb\"] = self[\"webpackChunkcyb\"] || [];\nvar parentChunkLoadingFunction = chunkLoadingGlobal.push.bind(chunkLoadingGlobal);\nchunkLoadingGlobal.push = installChunk;\n\n// no HMR\n\n// no HMR manifest","// run startup\nvar __webpack_exports__ = __webpack_require__.x();\n"],"names":["deferred","leafPrototypes","getProto","next","DEFAULT_CHAIN_ID","WorkerGlobalScope","self","localStorage","getItem","LCD_URL","RPC_URL","WEBSOCKET_URL","INDEX_HTTPS","INDEX_WEBSOCKET","BECH32_PREFIX","BECH32_PREFIX_VAL","BECH32_PREFIX_VALOPER","DEFAULT_GAS_LIMITS","BASE_DENOM","DENOM_LIQUID","MEMO_KEPLR","defaultNetworks","bostrom","CHAIN_ID","BOSTROM","SPACE_PUSSY","PATTERN_CYBER","RegExp","PATTERN_IPFS_HASH","PATTERN_COSMOS","PATTERN_HTTP","LinksTypeFilter","QueuePriority","createAsyncIterable","port","async","Symbol","asyncIterator","done","promise","Promise","resolve","onmessage","event","data","value","IPFSContentTransferHandler","canHandle","obj","result","serialize","rest","port1","port2","MessageChannel","postMessage","close","deserialize","serializedObj","SharedWorker","process","env","IS_DEV","installTransferHandlers","set","Observable","observer","remote","get","subscribe","error","complete","then","subscription","add","unsubscribe","Subscription","overrideLogging","worker","consoleLogMap","log","original","console","warn","replaceConsoleLog","method","args","apply","serializableArgs","map","arg","JSON","stringify","String","safeStringify","type","Object","keys","forEach","isParticle","Boolean","match","EntryType","SyncQueueStatus","SyncQueueJobType","initialState","list","isLoading","chats","summary","unreadCount","total","particles","neurons","formatApiData","item","entryType","chat","meta","to","particle","formatted","timestamp","Date","toISOString","transactionHash","hash","transaction_hash","memo","senseChatId","id","transactions","from","ownerId","fromAddress","inputs","address","assign","neuron","fromLog","getSenseList","senseApi","getList","getSenseChat","getLinks","filter","getFriendItems","markAsRead","newChatStructure","checkIfMessageExists","newMessage","slice","some","msg","name","reducers","updateSenseList","reducer","state","action","payload","message","concat","caseReducers","orderSenseList","prepare","addSenseItem","push","status","newList","unshift","updateSenseItem","chatId","txHash","isSuccess","find","sorted","reduce","acc","length","lastMsg","sort","a","b","parse","i","reset","extraReducers","builder","addCase","pending","fulfilled","rejected","sense","unreadCountParticle","unreadCountNeuron","values","actions","POCKET","POCKET_ACCOUNT","actionBar","tweet","STAGE_TWEET_ACTION_BAR","TWEET","isInitialized","defaultAccount","account","accounts","saveToLocalStorage","setItem","setDefaultAccount","setAccounts","setInitialized","setStageTweetActionBar","deleteAddress","accountKey","networkKey","bech32","cyber","entryCyber","entries","CYB_QUEUE_CHANNEL","constructor","this","channel","BroadcastChannel","postServiceStatus","postSyncEntryProgress","entry","postMlSyncEntryProgress","postSenseUpdate","senseList","postSetDefaultAccount","post","broadcastStatus","channelApi","sendStatus","progress","s","asyncIterableBatchProcessor","items","batchProcess","batchSize","batch","fetchIterableByOffset","fetchFunction","params","offset","busSender","enqueue","createBackendQueueSender","enqueueParticleEmbeddingMaybe","content","contentToEmbed","getTextContentIfShouldEmbed","cid","jobType","embedding","priority","MEDIUM","CID_TWEET","CID_FOLLOW","SENSE_FRIEND_PARTICLES","contentType","textPreview","getContentToEmbed","shouldEmbed","deps","statusApi","_syncQueue$","BehaviorSubject","Map","waitForParticleResolve","Error","embeddingApi$","embeddingApi","queue","size","dbInstance$","pipe","first","db","loadSyncQueue","isInitialized$","combineLatest","ipfsInstance$","dbInstance","ipfsInstance","canEmbed","getValue","loop$","_loop$","catch","text","existEmbedding","vec","createEmbedding","putEmbedding","err","toString","pendingItems","all","jobPromise","saveEmbedding","resolveIpfsParticle","removeSyncQueue","updateSyncQueue","delete","start","source$","tap","q","mergeMap","executing","jobTypeFilter","processSyncQueue","share","cids","putSyncQueue","getSyncQueue","statuses","shortenString","string","specialCharsRegexe","mapIndexerTransactionToEntity","tx","index","transaction","block","height","success","date","blockHeight","mapLinkFromIndexerToDto","throwIfAborted","func","signal","aborted","DOMException","Order_By","CyberlinksByParticleDocument","CyberlinksCountByNeuronDocument","MessagesByAddressCountDocument","MessagesByAddressSenseDocument","MessagesByAddressSenseWsDocument","MSG_SEND_TRANSACTION_TYPE","MSG_MULTI_SEND_TRANSACTION_TYPE","mapWebsocketTxToTransactions","events","transactionType","messages","Tx","decode","fromBase64","body","msgType","typeUrl","MsgSend","MsgMultiSend","extractTxData","TxResult","cyberGraphQLWsLink","url","shouldRetry","errOrCloseEvent","retryAttempts","retryWait","retries","setTimeout","Math","min","createIndexerClient","abortSignal","fetchCyberlinks","particleCid","timestampFrom","request","limit","orderBy","Asc","where","_or","particle_to","_eq","particle_from","_gt","cyberlinks","fetchCyberlinksByNeroun","particlesFrom","_and","_in","fetchCyberlinksByNerounIterable","getUniqueParticlesFromLinks","links","Set","link","fetchCyberlinksAndResolveParticles","timestampUpdate","particlesResolver","queuePriority","cyberlinksIterable","fetchCyberlinksIterable","enqueueBatch","mapMessagesByAddressVariables","types","orderDirection","timestamp_from","t","join","order_direction","fetchTransactions","res","messages_by_address","updateSenseChat","addr","amount","isSender","userAddress","lastSendTimestamp","last","direction","syncMyChats","myAddress","shouldUpdateTimestamp","syncItems","findSyncStatus","syncItemsMap","myChats","outputs","coins","toAddress","extractSenseChats","getTransactions","order","results","syncItem","lastTransaction","at","transactionTimestamp","syncItemHeader","timestampRead","prevUnreadCount","lastTimestampRead","max","timestampUpdateContent","timestampUpdateChat","timestampUnreadFrom","newTimestampUpdateChat","syncStatusChanges","updateSyncStatus","bind","newItem","disabled","putSyncStatus","ProgressTracker","onProgressUpdate","requestRecords","totalRequests","completedRequests","estimatedTime","totalCount","completeCount","extraRequests","trackProgress","processedCount","addRequestRecord","shift","estimatedRemainingTime","calculateAverageTimePerItem","round","itemCount","now","totalDiff","totalItems","timeDiff","progressTracker","abortController","AbortController","cyblogCh","thread","module","params$","createIsInitializedObserver","info","switchMap","createRestartObserver","restart","initAbortController","distinctUntilChanged","addrBefore","addrAfter","v","switchWhenInitialized","actionObservable$","onChange","initialized","super","reloadTrigger$","Subject","startWith","createInitObservable","createClientObservable","onUpdate","abort","syncQueueInitialized","variables","indexerObservable$","query","apolloObservable","ApolloClient","cache","subscriber","createIndexerWebsocket","response","source","nodeObservample$","ws","WebSocket","onopen","send","jsonrpc","onerror","onclose","createNodeWebsocketObservable","ctx","unit","isEmpty","merge","defer","initSync","getSyncStatus","lastTransactionTimestamp","syncTransactions","syncStatusItems","processBatchTransactions","putTransactions","syncLinks","lastTimestampFrom","newSyncItem","totalMessageCount","messages_by_address_aggregate","aggregate","count","fetchTransactionMessagesCount","ceil","transactionsAsyncIterable","fetchTransactionsIterable","transactionCount","tweets","particlesFound","l","txLink","extractCybelinksFromTransaction","putCyberlinks","tweetParticles","nonTweetParticles","includes","HIGH","LOW","snakeToCamel","str","replace","group","toUpperCase","entityToDto","dbEntity","dto","key","prototype","hasOwnProperty","call","camelCaseKey","Array","isArray","getLastReadInfo","prevTimestampRead","lastUnreadLinks","lastMyLinkIndex","findLastIndex","changeParticleSyncStatus","syncStatus","lastLink","isAbortException","e","intervalMs","warmupMs","restartLoop","options","onStartInterval","onError","retryDelayMs","restartTrigger$","intervalOrRestart$","interval","delay","exhaustMap","retry","createLoopObservable","doSync","sync","isAborted","particleResolverInitialized","syncItemParticles","newLinkCount","particles_from","cyberlinks_aggregate","fetchCyberlinksCount","newSyncItemParticles","fetchNewTweets","syncParticles","tweetsAsyncIterable","newTweets","existingParticles","existingParticlesMap","tweetsBatch","syncStatusEntities","timestampSyncFrom","updatedSyncItems","linksIndexer","followings$","followings","followingsInitialized$","followingsInitialized","syncUpdates","linksAsyncIterable","linksBatch","newTimestampRead","newUnreadCount","newTimestampUpdateContent","fetchStoredSyncCommunity$","dbApi","fetchParticleAsync","storedCommunity","getCommunity","communityUpdatesMap","c","getExistingOrDefault","following","follower","followsCids","pagination","config","txResponses","followers","addressHash","newFollowerCids","newFollowingNeurons","followersCommunity","communityItem","putCommunity","URGENT","onMessage","onmessageerror","getDeffredDbApi","entity","mime","blocks","sizeLocal","markdown","removeMarkdownFormatting","size_local","mapParticleToEntity","putParticles","ok","saveLinks","saveParticles","enquueSync","SyncService","loops","BackendQueueChannel","communitySync$","community","createCommunitySync$","getMimeFromUint8Array","raw","fileType","version","stores","table","ipfsContentAddtToInddexdDB","dbValue","CYBER_NODE_SWARM_PEER_ID","CYBERNODE_SWARM_ADDR_WSS","CYBERNODE_SWARM_ADDR_TCP","CYBER_GATEWAY_URL","cluster","file","dataFile","File","cidVersion","rawLeaves","createObjectURL","rawData","blob","Blob","URL","createImgData","detectGatewayContentType","basic","mimeToBaseContentType","initialType","indexOf","parseArrayLikeToDetails","onProgress","gateway","bytesDownloaded","Uint8Array","byteLength","chunks","ReadableStream","reader","getReader","readStream","read","chunk","getResponseResult","isStringData","Buffer","newString","trim","test","isHtml","createTextPreview","array","previewLength","loadIPFSContentFromDb","emptyStats","fetchIPFSContentStat","node","stat","fetchIPFSContentFromNode","controller","controllerLegacy","timer","startTime","stats","statsDoneTime","statsTime","allowedSize","clearTimeout","availableDownload","firstChunk","cat","fullyDownloaded","stream","catTime","local","pin","pinTime","debug","fetchIPFSContentFromGateway","headers","isExternalNode","nodeType","contentUrl","fetch","flushResults","flush","firstChunkStream","fullStream","tee","firstReader","restReader","asyncIterable","toAsyncIterableWithMime","getIPFSContent","callBackFuncStatus","dataRsponseDb","addContenToIpfs","arrayBuffer","contentToUint8Array","QueueStrategy","settings","getNextSource","QueueItemTimeoutError","timeoutMs","setPrototypeOf","CustomHeaders","XCybSourceValues","getQueueItemTotalPriority","viewPortPriority","strategies","external","timeout","maxConcurrentExecutions","embedded","helia","strategy","queueDebounceMs","queue$","lastNodeCallTime","setNode","reconnectToSwarm","isStarted","withLatestFrom","debounceTime","cancelDeprioritizedItems","workItems","getItemBySourceAndPriority","fetchData$","callbacks","callback","removeAndNext","nextSource","switchSourceAndNext","postSummary","switchStrategy","customStrategy","pendingBySource","itemsToExecute","queueSource","executeCount","itemsByPriority","queueItem","executionTime","promiseFactory","fetchIpfsContent","sharedWorker","enqueueParticleSave","each","with","throwError","catchError","of","mutateQueueItem","changes","releaseExecution","existingItem","initialSource","postProcessing","enqueueAndWait","updateViewPortPriority","cancel","cancelByParent","parent","clear","getQueueMap","getQueueList","getStats","fn","stringToCid","stringToIpfsPath","_config","_isStarted","gatewayUrl","nodeAddress","initConfig","window","toCid","swarm","localAddrs","files","withLocal","peers","peer","bootstrap","connect","ls","repoSize","repo","responseId","agentVersion","addOptionsV0","blockstore","open","datastore","libp2p","bootstrapList","transports","rtcConfiguration","iceServers","urls","credential","username","discoverRelays","connectionEncryption","streamMuxers","connectionGater","denyDialMultiaddr","peerDiscovery","services","identify","libp2pFactory","fs","addEventListener","evt","peerId","detail","conn","getConnections","transportsByAddr","fromEntries","remoteAddr","protoCodes","getMultiaddrs","fileSize","localFileSize","dagSize","mtime","optionsV0","fileName","addFile","path","TextEncoder","encode","addBytes","cid_","pins","isPinned","remotePeer","stop","dial","iterable","metadata","toV0","mapToLsResult","host","relay","enabled","hop","preload","API","HTTPHeaders","Addresses","Gateway","Swarm","Delegates","Discovery","MDNS","Enabled","Interval","webRTCStar","Bootstrap","Pubsub","ConnMgr","HighWater","LowWater","DisableNatPortMap","Routing","Type","filters","nat","EXPERIMENTAL","ipnsPubsub","Number","nodeClassMap","initIpfsNode","ipfsNodeType","restOptions","EnhancedClass","Base","parseAs","details","getPeers","swarmPeerId","forced","isConnectedToSwarm","connectPeer","swarmPeerAddress","withCybFeatures","instance","init","urlOpts","allowLocalModels","mlModelMap","featureExtractor","model","createMlApi","broadcastApi","featureExtractor$","replaySubject","ReplaySubject","pooling","normalize","searchByEmbedding","api","createEmbeddingApi$","initPipelineInstance","alias","pipeline","progress_callback","progressData","loaded","progrssStateItem","loadPipeline","time","timeEnd","compileConfig","budget","experimental","instructions","defaultRuneEntrypoint","readOnly","execute","funcName","funcParams","input","script","scriptEngine","entrypoints","context","user","secrets","entrypoints$","scriptCallbacks","isSoulInitialized$","run","compileParams","scripts","refId","compilerParams","runtime","app","outputData","diagnosticsOutput","getParticleScriptOrAction","askCompanion","resultType","metaItems","output","personalProcessor","outputContent","setEntrypoints","scriptEntrypoints","pushContext","popContext","names","newContext","executeFunction","executeCallback","getDebug","enigine","backgroundWorker","setInnerDeps","rune","runeDeps","createRuneApi","ipfsQueue","ipfsApi","ipfsOpts","newIpfsNode","ipfsNode","fetchWithDetails","dequeue","dequeueByParent","clearQueue","addContent","createIpfsApi","serviceDeps","injectDb","isIpfsInitialized","setRuneDeps","setParams","createBackgroundWorkerApi","onconnect","ports","extractRuneScript","md","hasRune","runeRegex","runeScript","modifiedMarkdown","exec","extractRuneContent","queryContractSmartPassport","client","toBase64","toAscii","getPassport","SigningCyberClientError","code","rawLog","cyblog","defaultFee","gas","sendCyberlink","signingClient","fee","cyberlink","putCyberlink","addCyberlinkLocal","subjectDeps","queryClient","defferedDependency","item$","CyberClient","getIpfsTextConent","cybApi","createAbortController","graphSearch","page","keywordHash","getPassportByNickname","nickname","passport","passport_by_nickname","searcByEmbedding","evalScriptFromIpfs","pureScript","executeScriptCallback","externalDeps","createRuneDeps","defaultOpenAIParams","openAICompletion","apiKey","cb","Authorization","json","choices","decoder","TextDecoder","buffer","lines","split","pop","line","parsed","delta","jsCyberSearch","jsCyberLink","fromCid","jsGetPassportByNickname","jsEvalScriptFromIpfs","jsGetIpfsTextContent","jsAddContenToIpfs","jsExecuteScriptCallback","jsOpenAICompletions","jsSearchByEmbedding","jsCyberLinksFrom","jsCyberLinksTo","evn","paramsSerializer","indexes","txs","tx_responses","Networks","ADD_AVATAR","FOLLOW","numberToUtcDate","dateToUtcNumber","isoString","endsWith","getNowUtcNumber","getIpfsHash","reject","marshal","DAGNode","dagNode","toBaseEncodedString","CYBLOG_BROADCAST_CHANNEL_NAME","logList","createCyblogChannel","defaultContext","postLogToChannel","level","trace","consoleLogParams","formattedMessage","formatter","logItem","truncate","appendLog","stacktrace","contextItem","p","ctxItem","consoleLog","getLogs","splice","getConsoleLogParams","createCybLog","getLink","OrderBy","ORDER_BY_DESC","getFromLink","getToLink","getSearchQuery","searchByHash","search","encodeSlash","__webpack_module_cache__","__webpack_require__","moduleId","cachedModule","undefined","exports","__webpack_modules__","m","x","__webpack_exports__","O","amdO","chunkIds","notFulfilled","Infinity","j","every","r","n","getter","__esModule","d","getPrototypeOf","__proto__","mode","ns","create","def","current","getOwnPropertyNames","definition","o","defineProperty","enumerable","f","chunkId","promises","u","miniCssF","g","globalThis","Function","hmd","children","prop","toStringTag","nmd","paths","location","installedChunks","importScripts","chunkLoadingGlobal","parentChunkLoadingFunction","moreModules"],"sourceRoot":""}