{"version":3,"file":"585.6a309c69.chunk.js","mappings":"gBAAIA,ECCAC,EADAC,ECAAC,E,+RCIG,SAASC,IACd,MAC+B,oBAAtBC,mBACPC,gBAAgBD,iBAEpB,CAEA,MAEME,GAFyBH,KAAcI,aAAaC,QAAQ,YAGvC,UAIdC,EACY,IAAgBH,GAAkBG,QAE9CC,EACY,IAAgBJ,GAAkBI,QAE9CC,EACkB,IAAgBL,GAAkBK,cAEpDC,EACgB,IAAgBN,GAAkBM,YAElDC,EAEX,IAAgBP,GAAkBO,gBAEvBC,EACkB,IAAgBR,GAAkBQ,cAE3DC,EAAoB,GAAGD,OAEhBE,EAAwB,GAAGD,QAe3BE,GAVe,IAAgBX,GAAkBY,WAGhC,IAAgBZ,GAAkBa,aAO9B,MAIrB,WAAEC,GAAe,IAAgBd,E,oDCpD9C,MAAMe,EAAgC,CACpCC,QAAS,CACPC,SAAU,IAASC,QACnBN,WAAY,OACZC,aAAc,WACdT,QAAS,mCACTD,QAAS,mCACTE,cAAe,2CACfC,YAAa,gDACbC,gBAAiB,8CACjBC,cAAe,UACfM,WAAY,iCAEdK,aAAc,CACZF,SAAU,IAASG,cACnBR,WAAY,OACZC,aAAc,WACdT,QAAS,wCACTD,QAAS,wCACTE,cAAe,gDACfC,YAAa,qDACbC,gBAAiB,mDACjBC,cAAe,UACfM,WAAY,iCAGd,cAAe,CACbG,SAAU,IAASI,YACnBT,WAAY,QACZC,aAAc,cACdT,QAAS,wCACTD,QAAS,uCACTE,cAAe,+CACfC,YAAa,oDACbC,gBAAiB,kDACjBC,cAAe,QACfM,WAAY,sCAIhB,K,6JC5CO,MAAMQ,EAAgB,IAAIC,OAC/B,IAAI,uBACJ,KAKWC,EAAoB,uBAYpBC,GAVyB,IAAIF,OACxC,IAAI,uBACJ,KAGmC,IAAIA,OACvC,IAAI,8BACJ,KAG4B,4BAYjBG,EAAe,yB,qECjCrB,IAAKC,EAAL,CAAKA,IACVA,EAAA,GAAK,KACLA,EAAA,KAAO,OACPA,EAAA,IAAM,MAHIA,GAAL,CAAKA,GAAA,G,oDCiCAC,EAAL,CAAKA,IACVA,EAAAA,EAAA,KAAO,GAAP,OACAA,EAAAA,EAAA,IAAM,IAAN,MACAA,EAAAA,EAAA,OAAS,IAAT,SACAA,EAAAA,EAAA,KAAO,IAAP,OACAA,EAAAA,EAAA,OAAS,GAAT,SALUA,GAAL,CAAKA,GAAA,I,sBC1BZ,SAASC,EAAoBC,GAC3B,MAAO,CACLC,OAAQC,OAAOC,iBAKb,IAAIC,GAAO,EACX,MAAQA,GAAM,CAEZ,MAAMC,EAAU,IAAIC,SAA4BC,IAE9CP,EAAKQ,UAAaC,IACG,OAAfA,EAAMC,MACRN,GAAO,EACPG,EAAQ,OAERA,EAAQE,EAAMC,KAChB,CACD,IAGGC,QAAcN,EAEN,OAAVM,UACIA,EAEV,CACF,EAEJ,CAEA,MAAMC,EAGF,CACFC,UAAYC,GACVA,GAAOA,EAAIC,QAAsD,mBAArCD,EAAIC,OAAOb,OAAOC,eAChDa,UAAUF,GACR,QAAY,IAARA,EACF,MAAO,CAAC,KAAM,IAEhB,MAAM,OAAEC,KAAWE,GAASH,GACtB,MAAEI,EAAK,MAAEC,GAAU,IAAIC,eAY7B,OAXIL,GACF,WAEE,gBAAiBJ,KAASI,EACxBG,EAAMG,YAAYV,GAEpBO,EAAMG,YAAY,MAElBH,EAAMI,OACP,EARD,GAUK,CAAC,IAAKL,EAAMjB,KAAMmB,GAAS,CAACA,GACrC,EACAI,YAAYC,GACV,IAAKA,EACH,OAEF,MAAM,KAAExB,KAASiB,GAASO,EAE1B,MAAO,IACFP,EACHF,OAAQhB,EAAoBC,GAEhC,G,gDC9DuD,oBAAjByB,cAEgBC,EAAQC,IAAIC,OAGpE,SAASC,IACP,KAAiBC,IAAI,cAAelB,GACpC,KAAiBkB,IAAI,aAAc,CACjCjB,UAAYF,GACHA,aAAiBoB,EAAA,EAE1BR,YAAcZ,GACL,IAAIoB,EAAA,GAAqBC,IAC9B,MAAMC,EAAS,KACZC,IAAI,SACJX,YAAYZ,GAEfsB,EACGE,WACC,QAAM,CACJrE,KAAOA,GAAkBkE,EAASlE,KAAKA,GACvCsE,MAAQA,GAAmBJ,EAASI,MAAMA,GAC1CC,SAAU,IAAML,EAASK,cAG5BC,MAAMC,GACLP,EAASQ,KAAI,KACXD,EAAaE,cACbR,EAAO,OAAe,KAEzB,IAGPjB,UAAYL,GACH,KAAiBuB,IAAI,SAAUlB,UAAU,CAC9CmB,UAAYH,GACVrB,EAAMwB,UAAU,CACdrE,KAAOA,GAAkBkE,EAASlE,KAAKA,GAAMwE,OAC7CF,MAAQA,GAAmBJ,EAASI,MAAMA,GAAOE,OACjDD,SAAU,IAAML,EAASK,WAAWC,aAM9C,KAAiBR,IAAI,eAAgB,CACnCjB,UAAYF,GACHA,aAAiB+B,EAAA,GAE1BnB,YAAcZ,GACL,IAAI+B,EAAA,IAAa,KACtB,MAAMT,EAAS,KACZC,IAAI,SACJX,YAAYZ,GAEfsB,EAAOQ,cAAcH,MAAK,KACxBL,EAAO,OAAe,GACtB,IAGNjB,UAAYL,GACH,KAAiBuB,IAAI,SAAUlB,UAAU,CAC9CyB,YAAa,IAAM9B,EAAM8B,iBAIjC,CAWA,SAASE,EAAgBC,GACvB,MAAMC,EAAgB,CACpBC,IAAK,CAAEC,SAAUC,EAAQF,KACzBV,MAAO,CAAEW,SAAUC,EAAQZ,OAC3Ba,KAAM,CAAEF,SAAUC,EAAQC,OAEtBC,EAAqBC,IACzB,MAAM,SAAEJ,GAAaF,EAAcM,GAEnCN,EAAcM,GAAQJ,SAAWC,EAAQG,GAEzCH,EAAQG,GAAU,IAAIC,KACpBL,EAASM,MAAML,EAASI,GACxB,MAAME,EAAmBF,EAAKG,KAAKC,GAtBzC,SAAuB1C,GACrB,IACE,OAAO2C,KAAKC,UAAU5C,EACxB,CAAE,MAAOsB,GACP,OAAOuB,OAAO7C,EAChB,CACF,CAgBiD8C,CAAcJ,KAEzDZ,EAAOvB,YAAY,CAAEwC,KAAM,UAAWV,SAAQC,KAAME,GAAmB,CACxE,EAGHQ,OAAOC,KAAKlB,GAAemB,SAASb,GAClCD,EAAkBC,IAEtB,C,gDCzFO,IAAKc,EAAL,CAAKA,IACVA,EAAAA,EAAA,aAAe,GAAf,eACAA,EAAAA,EAAA,SAAW,GAAX,WACAA,EAAAA,EAAA,KAAO,GAAP,OAHUA,GAAL,CAAKA,GAAA,IAiEAC,EAAL,CAAKA,IACVA,EAAAA,EAAA,QAAU,GAAV,UACAA,EAAAA,EAAA,UAAY,GAAZ,YACAA,EAAAA,EAAA,KAAO,GAAP,OACAA,EAAAA,EAAA,OAAS,GAAT,QAJUA,GAAL,CAAKA,GAAA,IAOAC,EAAL,CAAKA,IACVA,EAAAA,EAAA,SAAW,GAAX,WACAA,EAAAA,EAAA,UAAY,GAAZ,YAFUA,GAAL,CAAKA,GAAA,IC9FL,SAASC,EAAWzD,GAGzB,OAAO0D,QAAQ1D,EAAM2D,MAAM,wBAC7B,C,0BCoEA,MAAMC,EAA2B,CAC/BC,KAAM,CACJC,WAAW,EACX/D,KAAM,GACN0B,WAAO,GAETsC,MAAO,CAAC,EACRC,QAAS,CACPC,YAAa,CACXC,MAAO,EACPC,UAAW,EACXC,QAAS,IAGbC,IAAK,CAEHC,SAAU,UAEN,GADCxB,KAAKyB,MAAM/G,aAAaC,QAAQ,eAAiB,MAEtD+G,gBAAiB,OAIrB,SAASC,EAAcC,GACjBA,EAAKC,YAAcrB,EAAUsB,MAAQF,EAAKG,KAAKC,KACjDJ,EAAKC,UAAYrB,EAAUyB,UAG7B,MAAM,KAAEF,GAASH,EAEXM,EAAuB,CAC3BC,UAAW,IAAIC,KAAKL,EAAKI,WAAWE,cAGpCC,gBACEV,EAAKU,iBACLV,EAAKW,MACLX,EAAKG,KAAKS,kBACVZ,EAAKG,KAAKQ,MACVX,EAAKG,KAAKO,gBAEZG,KAAMb,EAAKa,MAAQV,EAAKU,KAExBC,YAAad,EAAKe,GAElBxB,YAAaS,EAAKT,aAAe,GAGnC,OAAQS,EAAKC,WACX,KAAKrB,EAAUsB,KACf,KAAKtB,EAAUoC,aAAc,CAC3B,MAAMb,EAAOH,EAAKG,MACZ,KAAE3B,GAAS2B,EAEjB,IAAIc,EAAOjB,EAAKkB,QAEhB,GAAa,gCAAT1C,EAAwC,CAE1CyC,EADcd,EAAK7E,MACN6F,WACf,MAAO,GAAa,qCAAT3C,EAA6C,CAGtDyC,EAFcd,EAAK7E,MAEN8F,OAAO,GAAGC,OACzB,CAEA5C,OAAO6C,OAAOhB,EAAW,CACvB9B,OACAyC,OACAd,KAAMH,EAAKG,KAAK7E,QAGlB,KACF,CAEA,KAAKsD,EAAUyB,SAAU,CACvB,MAAMF,EAAOH,EAAKG,KAElB1B,OAAO6C,OAAOhB,EAAW,CACvB9B,KAAM,mCACNyC,KAAMd,EAAKoB,OACXpB,KAAAA,EACAqB,SAAS,IAGX,KACF,CAEA,QAGE,MAAO,CAAC,EAGZ,OAAOlB,CACT,CAEA,MAAMmB,GAAe,QACnB,sBACA7G,MAAO8G,UACcA,EAAUC,WACjBzD,IAAI6B,KAId6B,GAAe,QACnB,sBACAhH,OAASmG,KAAIW,eAGX,GAFiB3C,EAAWgC,GAEd,CAeZ,aAdoBW,EAAUG,SAASd,IACV7C,KAAK8B,IAChC,GAAuB,IAAnBA,EAAKO,UAIT,OAAOR,EAAc,IAChBC,EACHe,KACAd,UAAWrB,EAAUyB,SACrBF,KAAMH,GACN,IAGkB8B,OAAO9C,QAC/B,CAaA,aAXmB0C,EAAUK,eAAehB,IACjB7C,KAAK8B,IAC9B,MAAMC,EAAYD,EAAKI,GAAKxB,EAAUyB,SAAWzB,EAAUsB,KAC3D,OAAOH,EAAc,IAChBC,EACHC,YACAc,KACAZ,KAAMH,GACN,GAGgB,IAIlBgC,GAAa,QACjB,oBACApH,OAASmG,KAAIW,cACJA,EAAUM,WAAWjB,KAI1BkB,EAAyB,CAC7BlB,GAAI,GACJ3B,WAAW,EACX/D,KAAM,GACN0B,WAAO,EACPwC,YAAa,GAGf,SAAS2C,EAAqBhC,EAAYiC,GAOxC,OANqBjC,EAAK7E,KAAK+G,OAAO,GAEDC,MAAMC,GAClCA,EAAI5B,kBAAoByB,EAAWzB,iBAI9C,CAgBA,MAAM0B,GAAQ,QAAY,CACxBG,KAAM,QACNrD,eACAsD,SAAU,CAERC,gBAAiB,CACfC,QAAS,CAACC,EAAOC,KACFA,EAAOC,QAEflE,SAASmE,IACZ,MAAQhC,YAAaC,GAAO+B,EAEvBH,EAAMtD,MAAM0B,KACf4B,EAAMtD,MAAM0B,GAAM,IAAKkB,IAGzB,MAAM/B,EAAOyC,EAAMtD,MAAM0B,GAEzBtC,OAAO6C,OAAOpB,EAAM,CAClBa,KAEAxB,YAAauD,EAAQvD,aAAe,IAGjC2C,EAAqBhC,EAAM4C,KAC9B5C,EAAK7E,KAAO6E,EAAK7E,KAAK0H,OAAOD,GAC/B,IAGFV,EAAMY,aAAaC,eAAeN,EAAM,EAE1CO,QAAU7H,IACD,CACLwH,QAASxH,EAAK6C,IAAI6B,MAKxBoD,aACER,EACAC,GAEA,MAAM,GAAE7B,EAAE,KAAEf,GAAS4C,EAAOC,QACfF,EAAMtD,MAAM0B,GAEpB1F,KAAK+H,KAAK,IACVpD,EACHG,KAAMH,EAAKG,KACXkD,OAAQ,YAGV,MAAMC,EAAUX,EAAMxD,KAAK9D,KAAKyG,QAAQ9B,GAASA,IAASe,IAC1DuC,EAAQC,QAAQxC,GAChB4B,EAAMxD,KAAK9D,KAAOiI,CACpB,EAEAE,gBACEb,EACAC,GAMA,MAAM,OAAEa,EAAM,OAAEC,EAAM,UAAEC,GAAcf,EAAOC,QAGvC7C,EAFO2C,EAAMtD,MAAMoE,GAEPpI,KAAKuI,MAAM5D,GAASA,EAAKU,kBAAoBgD,IAE3D1D,IACE2D,SACK3D,EAAKqD,OAEZrD,EAAKqD,OAAS,QAGpB,EACAJ,eAAeN,GACb,MAmBMkB,EAnBmBpF,OAAOC,KAAKiE,EAAMtD,OAAOyE,QAKhD,CAACC,EAAKhD,KACN,MAAMb,EAAOyC,EAAMtD,MAAM0B,GAGzB,IAAKb,EAAK7E,KAAK2I,OACb,OAAOD,EAGT,MAAME,EAAU/D,EAAK7E,KAAK6E,EAAK7E,KAAK2I,OAAS,GAG7C,OAFAD,EAAIX,KAAK,CAAErC,KAAIkD,YAERF,CAAG,GACT,IAE6BG,MAAK,CAACC,EAAGC,IAErC5D,KAAKX,MAAMuE,EAAEH,QAAQ1D,WAAaC,KAAKX,MAAMsE,EAAEF,QAAQ1D,aAI3DoC,EAAMxD,KAAK9D,KAAOwI,EAAO3F,KAAKmG,GAAMA,EAAEtD,IACxC,EACAuD,MAAK,IACIpF,EAGTqF,gBACE5B,EACAC,GAEA,MAAM4B,EAAuB,CAC3BzD,GAAI6B,EAAOC,QAAQ9B,GACnB0D,SAAU,GACVC,YAAalE,KAAKmE,MAClBC,MACEhC,EAAOC,QAAQ+B,OACf,gBAAgBjC,EAAMhD,IAAIC,QAAQoE,OAAS,KAE/CrB,EAAMhD,IAAIC,QAAQwD,KAAKoB,GACvB7B,EAAMhD,IAAIG,gBAAkB8C,EAAOC,QAAQ9B,GAC3CjI,aAAa+L,QAAQ,aAAczG,KAAKC,UAAUsE,EAAMhD,IAAIC,SAC9D,EAEAkF,gBAAgBnC,EAAOC,GACrBD,EAAMhD,IAAIG,gBAAkB8C,EAAOC,QAAQ9B,EAC7C,EAEAgE,sBACEpC,EACAC,GAEA,MAAMoC,EAASrC,EAAMhD,IAAIC,QAAQgE,MAC9BqB,GAAMA,EAAElE,KAAO6B,EAAOC,QAAQqC,WAE7BF,IACFA,EAAOP,SAASrB,KAAKR,EAAOC,QAAQC,SACpCkC,EAAON,YAAc9B,EAAOC,QAAQC,QAAQvC,UAC5CzH,aAAa+L,QAAQ,aAAczG,KAAKC,UAAUsE,EAAMhD,IAAIC,UAEhE,EAGAuF,8BACExC,EACAC,GAEA,MAAMoC,EAASrC,EAAMhD,IAAIC,QAAQgE,MAC9BqB,GAAMA,EAAElE,KAAO6B,EAAOC,QAAQqC,WAE7BF,GAAUA,EAAOP,SAAST,OAAS,IACrCgB,EAAOP,SAASO,EAAOP,SAAST,OAAS,GAAKpB,EAAOC,QAAQC,QAC7DhK,aAAa+L,QAAQ,aAAczG,KAAKC,UAAUsE,EAAMhD,IAAIC,UAEhE,EAEAwF,gBAAgBzC,EAAOC,GACrB,MAAMyC,EAAO1C,EAAMhD,IAAIC,QAAQkC,QAC5BkD,GAAWA,EAAOjE,KAAO6B,EAAOC,QAAQ9B,KAG3C,EAAQtD,IAAI,OAAQ4H,GAEpB1C,EAAMhD,IAAIC,QAAUyF,EAEhB1C,EAAMhD,IAAIG,kBAAoB8C,EAAOC,QAAQ9B,KAC/C4B,EAAMhD,IAAIG,gBAAkB,MAO9BhH,aAAa+L,QAAQ,aAAczG,KAAKC,UAAUsE,EAAMhD,IAAIC,SAC9D,EAEA0F,gBAAgB3C,GACdA,EAAMhD,IAAIC,QAAU,GACpB+C,EAAMhD,IAAIG,gBAAkB,KAC5BhH,aAAayM,WAAW,aAC1B,GAGFC,cAAgBC,IACdA,EAAQC,QAAQjE,EAAakE,SAAUhD,IACrCA,EAAMxD,KAAKC,WAAY,CAAI,IAG7BqG,EAAQC,QAAQjE,EAAamE,WAAW,CAACjD,EAAOC,KAC9CD,EAAMxD,KAAKC,WAAY,EAEvB,MAAMkE,EAAsC,GAE5CV,EAAOC,QAAQlE,SAASmE,IACtB,MAAQhC,YAAaC,GAAO+B,EAEvBH,EAAMtD,MAAM0B,KACf4B,EAAMtD,MAAM0B,GAAM,IAAKkB,IAGzB,MAAM/B,EAAOyC,EAAMtD,MAAM0B,GAEzBtC,OAAO6C,OAAOpB,EAAM,CAClBa,KAEAxB,YAAauD,EAAQvD,aAAe,IAGjC2C,EAAqBhC,EAAM4C,KAC9B5C,EAAK7E,KAAO6E,EAAK7E,KAAK0H,OAAOD,IAG/BQ,EAAQF,KAAKrC,EAAG,IAGlB4B,EAAMxD,KAAK9D,KAAOiI,CAAO,IAE3BmC,EAAQC,QAAQjE,EAAaoE,UAAU,CAAClD,EAAOC,KAC7C,EAAQ7F,MAAM6F,GAEdD,EAAMxD,KAAKC,WAAY,EACvBuD,EAAMxD,KAAKpC,MAAQ6F,EAAO7F,MAAM+F,OAAO,IAGzC2C,EAAQC,QAAQ9D,EAAa+D,SAAS,CAAChD,EAAOC,KAC5C,MAAM,GAAE7B,GAAO6B,EAAOzC,KAAKhC,IAEtBwE,EAAMtD,MAAM0B,KACf4B,EAAMtD,MAAM0B,GAAM,IAAKkB,IAIzBU,EAAMtD,MAAM0B,GAAI3B,WAAY,CAAI,IAGlCqG,EAAQC,QAAQ9D,EAAagE,WAAW,CAACjD,EAAOC,KAC9C,MAAM,GAAE7B,GAAO6B,EAAOzC,KAAKhC,IACrB+B,EAAOyC,EAAMtD,MAAM0B,GACzBb,EAAKd,WAAY,EAEjBc,EAAKa,GAAKA,EAEVb,EAAK7E,KAAOuH,EAAOC,OAAO,IAE5B4C,EAAQC,QAAQ9D,EAAaiE,UAAU,CAAClD,EAAOC,KAC7C,EAAQ7F,MAAM6F,GAEd,MAAM1C,EAAOyC,EAAMtD,MAAMuD,EAAOzC,KAAKhC,IAAI4C,IACzCb,EAAKd,WAAY,EACjBc,EAAKnD,MAAQ6F,EAAO7F,MAAM+F,OAAO,IAKnC2C,EAAQC,QAAQ1D,EAAW4D,WAAW,CAACjD,EAAOC,KAC5C,MAAM,GAAE7B,GAAO6B,EAAOzC,KAAKhC,IACrB+B,EAAOyC,EAAMtD,MAAM0B,GAEnBV,EAAWtB,EAAWgC,IAEtB,YAAExB,GAAgBW,EAExByC,EAAMrD,QAAQC,YAAYC,OAASD,EAC/Bc,EACFsC,EAAMrD,QAAQC,YAAYE,WAAaF,EAEvCoD,EAAMrD,QAAQC,YAAYG,SAAWH,EAGvCW,EAAKX,YAAc,CAAC,GACpB,KA8BO,aACX4D,EAAY,gBACZK,EAAe,gBACff,EACA6B,MAAK,kBACLC,EAAe,gBACfa,EAAe,gBACfN,EAAe,sBACfC,EAAqB,8BACrBI,EAA6B,gBAC7BG,KApCyB,SACxB3C,GAAqBA,EAAMmD,MAAMzG,QACjCA,IACC,IAAI0G,EAAsB,EACtBC,EAAoB,EAExBvH,OAAOwH,OAAO5G,GAAOV,SAAQ,EAAGoC,KAAIxB,kBACjBR,EAAWgC,GAG1BgF,GAAuBxG,EAEvByG,GAAqBzG,CACvB,IAKF,MAAO,CACLC,MAHYuG,EAAsBC,EAIlCvG,UAAWsG,EACXrG,QAASsG,EACV,IAeD5D,EAAM8D,SAOK9D,EAAa,QC7jBrB,MAAM,EACH,CACN+D,OAAQ,SACRC,eAAgB,iBCmBpB,MAAM,EAA2B,CAC/BC,UAAW,CACTC,M,SAAO,GAAOC,uBAAuBC,OAEvCC,eAAe,EACfC,eAAgB,CACdnE,KAAM,KACNoE,QAAS,MAEXC,SAAU,MAUZ,SAASC,EAAmBlE,GAC1B,MAAM,eAAE+D,EAAc,SAAEE,GAAajE,EAErC+D,GACE5N,aAAa+L,QACX,EAAwBsB,OACxB/H,KAAKC,UAAU,CACb,CAACqI,EAAenE,MAAOmE,EAAeC,WAG5CC,GACE9N,aAAa+L,QACX,EAAwBuB,eACxBhI,KAAKC,UAAUuI,GAErB,CAEA,MAAM,GAAQ,QAAY,CACxBrE,KAAM,SACNrD,aAAY,EACZsD,SAAU,CACRsE,kBAAmB,CACjBnE,GAEEE,SAAWN,OAAMoE,eAGnBhE,EAAM+D,eAAiB,CACrBnE,OACAoE,QAASA,GAAWhE,EAAMiE,WAAWrE,IAAS,MAGhDsE,EAAmBlE,EAAM,EAE3BoE,YAAa,CAACpE,GAASE,cACrBF,EAAMiE,SAAW/D,EAEjBgE,EAAmBlE,EAAM,EAE3BqE,eAAiBrE,IACfA,EAAM8D,eAAgB,CAAI,EAE5BQ,uBAAwB,CAACtE,GAASE,cAChCF,EAAM0D,UAAUC,MAAQzD,CAAO,EAIjCqE,cAAe,CAACvE,GAASE,cACnBF,EAAMiE,UACRnI,OAAOC,KAAKiE,EAAMiE,UAAUjI,SAASwI,IACnC1I,OAAOC,KAAKiE,EAAMiE,SAASO,IAAaxI,SAASyI,IAC/C,GAAIzE,EAAMiE,SAASO,GAAYC,GAAYC,SAAWxE,EAAS,CAO7D,UANOF,EAAMiE,SAASO,GAAYC,GAEqB,IAAnD3I,OAAOC,KAAKiE,EAAMiE,SAASO,IAAanD,eACnCrB,EAAMiE,SAASO,GAGpBxE,EAAM+D,gBAAgBC,SAASW,OAAOD,SAAWxE,EAAS,CAC5D,MAEM0E,EAFU9I,OAAO+I,QAAQ7E,EAAMiE,UAEVhD,MACzB,EAAE,CAAEtI,KAAWA,EAAMgM,OAAOD,SAI5B1E,EAAM+D,eADJa,EACqB,CACrBhF,KAAMgF,EAAW,GACjBZ,QAASY,EAAW,IAGC,CACrBhF,KAAM,KACNoE,QAAS,KAGf,CAEAE,EAAmBlE,EACrB,IACA,GAEN,MAQO,kBACXmE,EAAiB,YACjBC,EAAW,uBACXE,EAAsB,cACtBC,GACE,EAAMhB,QAEK,EAAa,QAGrB,MC5IMuB,EAAoB,oBCgEjC,MApDA,MAGEC,cACEC,KAAKC,QAAU,IAAIC,iBDjBc,wBCkBnC,CAEOC,kBACLvF,EACAc,EACAP,GAEA6E,KAAKC,QAAQ5L,YAAY,CACvBwC,KAAM,iBACNlD,MAAO,CAAEiH,OAAMc,SAAQP,YAE3B,CAEOiF,sBAAsBC,EAAsBrF,GAEjDgF,KAAKC,QAAQ5L,YAAY,CAAEwC,KAAM,aAAclD,MAAO,CAAE0M,QAAOrF,UACjE,CAEOsF,wBAAwBD,EAAerF,GAE5CgF,KAAKC,QAAQ5L,YAAY,CACvBwC,KAAM,gBACNlD,MAAO,CAAE0M,QAAOrF,UAEpB,CAEOuF,gBAAgBC,GAEjBA,EAAUnE,OAAS,GACrB2D,KAAKC,QAAQ5L,YAAYyG,EAAgB0F,GAE7C,CAEOC,sBAAsB7F,EAAcoE,GACzCgB,KAAKC,QAAQ5L,YACX8K,EAAkB,CAChBvE,OACAoE,YAGN,CAEA0B,KAAK/F,GACHqF,KAAKC,QAAQ5L,YAAYsG,EAC3B,G,sECtDK,MAAMgG,GAAkB,CAC7B/F,EACAgG,KAGO,CACLC,WAAY,CACVnF,EACAP,EACA2F,KAGAF,EAAWR,sBAAsBxF,EAAM,CACrCc,SACAP,UACA2F,WACA1N,KAAM,CAAC,SAAU,QAAS,UAAUsH,MAAMqG,GAAMA,IAAMrF,KACtD,IClBRzI,eAAe+N,GACbC,EACAC,EACAC,EAAY,IAEZ,IAAIC,EAAQ,GAEZ,gBAAiB/I,KAAQ4I,EACvBG,EAAM3F,KAAKpD,GACP+I,EAAM/E,SAAW8E,UACbD,EAAaE,GACnBA,EAAQ,IAIRA,EAAM/E,OAAS,SACX6E,EAAaE,EAEvB,CA4BOnO,eAAgBoO,GACrBC,EACAC,GAEA,IAAIC,EAAS,EACb,OAAa,CAEX,MAAMP,QAAcK,EAAc,IAAKC,EAAQC,WAE/C,GAAqB,IAAjBP,EAAM5E,OACR,YAGI4E,EAENO,GAAUP,EAAM5E,MAClB,CACF,CC7DO,MAUDoF,GAVkC,MACtC,MAAMxB,EAAU,IAAIC,iBAAiBJ,GAErC,MAAO,CACL4B,QAAU/G,IACRsF,EAAQ5L,YAAYsG,EAAI,EAE3B,EAGegH,GAELC,GAAgC3O,MAAO4O,IAClD,MAAMC,QAAuBC,GAA4BF,GAczD,OAZIC,GACFL,GAAUC,QAAQ,CAChB7K,KAAM,OACNnD,KAAM,CACJ0F,GAAIyI,EAAQG,IACZtO,KAAMoO,EACNG,QAAS9K,EAAiB+K,UAC1BC,SAAUrP,EAAcsP,YAKrBN,CAAc,E,gBCnClB,MAAMO,GAAY,iDAEZC,GAAa,iDCUbC,GAAyB,CAACF,GAAWC,I,gBCwBlD,MAaaP,GAA8B9O,MAAO4O,IAChD,MAAOW,EAAa9O,QAZIT,OAAO4O,IAC/B,MAAMW,EAAcX,GAASrJ,MAAMgK,aAAe,GAGlD,MAAoB,SAAhBA,EACK,CAACA,EAAaX,EAAQY,aAGxB,CAACD,OAAa,EAAU,EAIGE,CAAkBb,GAEpD,IAAIc,EAA8B,SAAhBH,KAA4B9O,EAM9C,OAJAiP,EACEA,KACEjP,EAAM4D,MAAM,SAAoB5D,EAAM4D,MAAM,QAEzCqL,EAAcjP,OAAO,GA0P9B,OAvPA,MA6BEqM,YAAY6C,GACV,GAjBF,KAAQC,UAAYlC,GAAgB,WAAY,IAAI,GAEpD,KAAQmC,YAAc,IAAIC,EAAA,EACxB,IAAIC,MAcCJ,EAAKK,uBACR,MAAM,IAAIC,MAAM,yCAGlBlD,KAAKiD,uBAAyBL,EAAKK,uBAEnCL,EAAKO,cAAchO,WAAWiO,IAC5BpD,KAAKoD,aAAeA,EAEhBpD,KAAKqD,MAAMC,KAAO,GACpBtD,KAAK8C,YAAYhS,KAAKkP,KAAKqD,MAC7B,IAGFT,EAAKW,YACFC,MACC,EAAAC,EAAA,IAAO9P,QAAoB,IAAVA,KAElBwB,WAAUlC,MAAOyQ,IAChB1D,KAAK0D,GAAKA,QACJ1D,KAAK2D,eAAe,IAG9B3D,KAAK4D,gBAAiB,EAAAC,EAAA,GAAc,CAClCjB,EAAKW,YACLX,EAAKkB,gBACJN,MACD,EAAAjN,EAAA,IAAI,EAAEwN,EAAYC,OAAoBA,KAAkBD,IAE5D,CApDYE,eACV,QAASjE,KAAKoD,YAChB,CAUWC,YACT,OAAOrD,KAAK8C,YAAYoB,UAC1B,CAIWC,YACT,OAAOnE,KAAKoE,MACd,CAkCA,0BAAkChL,EAAiB+I,GACjD,OAAOnC,KAAKiD,uBAAuB7J,EAAI+I,GACpC7M,MAAKrC,OAASyI,SAAQ3H,cACS,cAAX2H,IACC3H,WAId6N,GAA8B7N,IAC7B,KAERsQ,OAAM,KAAM,GACjB,CAEA,oBAA4BrC,EAAkBsC,GAC5C,IAGE,UAFsBtE,KAAK0D,GAAIa,eAAevC,GAEhC,CACZ,MAAMwC,QAAYxE,KAAKoD,aAAcqB,gBAAgBH,SAEhCtE,KAAK0D,GAAIgB,aAAa1C,EAAKwC,EAClD,CAEA,OAAO,CACT,CAAE,MAAOG,GAEP,OADA,GAAQvP,MAAM,wBAAwB4M,OAASsC,KAASK,EAAIC,aACrD,CACT,CACF,CAEA,uBAA+BC,GAG7B,MAAM1D,EAAY0D,EAAaxI,OAE/B2D,KAAK6C,UAAUhC,WACb,cACA,oBAAoBM,KAAaA,YAAoBnB,KAAKqD,MAAMC,mBAGlE,IAAI5G,EAAIyE,QACF7N,QAAQwR,IACZD,EAAatO,KAAItD,MAAOoF,IACtB,MAAM,GAAEe,EAAE,QAAE6I,EAAO,KAAEvO,GAAS2E,EAE9B,IAAI0M,EAAazR,QAAQC,SAAQ,GASjC,OAPI0O,IAAY9K,EAAiB+K,WAAaxO,EAC5CqR,EAAa/E,KAAKgF,cAAc5L,EAAI1F,GAC3BuO,IAAY9K,EAAiBuB,WACtCqM,EAAa/E,KAAKiF,oBAAoB7L,EAAItG,EAAcsP,SAInD2C,EAAWzP,MAAKrC,MAAOc,IACxBA,QACIiM,KAAK0D,GAAIwB,gBAAgB,CAAE9L,KAAI6I,kBAE/BjC,KAAK0D,GAAIyB,gBAAgB,CAC7B/L,KACA6I,UACAvG,OAAQxE,EAAgB9B,QAI5B,MAAMiO,EAAQrD,KAAK8C,YAAYnP,MAC/B0P,EAAM+B,OAAOhM,GACbsD,IACAsD,KAAK8C,YAAYhS,KAAKuS,GAEtBrD,KAAK6C,UAAUhC,WACb,cACA,oBAAoBM,EAAYzE,KAAKyE,YACnCnB,KAAKqD,MAAMC,kBAEd,GACD,IAGR,CAEA+B,QACE,MAAMC,EAAUtF,KAAK4D,eAAeJ,MAClC,EAAA+B,GAAA,IAAKC,GAAM,GAAQ1P,IAAI,8BAA8B0P,QACrD,EAAArL,GAAA,IAAQ2E,IAAoC,IAAlBA,KAC1B,EAAA2G,GAAA,IAAS,IAAMzF,KAAK8C,eAEpB,EAAA3I,GAAA,IAAQqL,GAAMA,EAAElC,KAAO,KACvB,EAAAmC,GAAA,IAAUpC,IACR,MAAM7L,EAAO,IAAI6L,EAAM/E,UAMjB6C,EAtLW,IAkLM3J,EAAK2C,QACzBuC,GAAMA,EAAEhB,SAAWxE,EAAgBwO,YACpCrJ,OAIIsJ,EAAiBjJ,GACrBA,EAAEuF,UAAY9K,EAAiBuB,UAC9BgE,EAAEuF,UAAY9K,EAAiB+K,WAAalC,KAAKiE,SAEpD,GAAI9C,EAAY,EAAG,CACjB,MAAM0D,EAAerN,EAClB2C,QACEuC,GAAMA,EAAEhB,SAAWxE,EAAgB8G,SAAW2H,EAAcjJ,KAE9DH,MAAK,CAACC,EAAGC,IACDD,EAAE2F,SAAW1F,EAAE0F,WAEvB1H,MAAM,EAAG0G,GAEZ,GAAI0D,EAAaxI,OAAS,EAWxB,OAVAwI,EAAa7N,SAAS0F,IACpB2G,EAAMvO,IAAI4H,EAAEtD,GAAI,IACXsD,EACHhB,OAAQxE,EAAgBwO,WACxB,IAGJ1F,KAAK8C,YAAYhS,KAAKuS,GAEtBrD,KAAK6C,UAAUhC,WAAW,cAAe,eAClCb,KAAK4F,iBAAiBf,EAEjC,CAEA,OAAO,IAAK,KAahB,OATA7E,KAAKoE,OAASkB,EAAQ9B,MAAK,EAAAqC,GAAA,MAE3B7F,KAAKoE,OAAOjP,UAAU,CACpBrE,KAAOiD,IACLiM,KAAK6C,UAAUhC,WAAW,SAAS,EAErCzL,MAAQuP,GAAQ3E,KAAK6C,UAAUhC,WAAW,QAAS8D,EAAIC,cAGlD5E,IACT,CAEA,mBACE8F,EACA7D,EACAE,GAEA,OAAOnB,GACL8E,GACCA,GACC9F,KAAK0B,QACHoE,EAAKvP,KAAKyL,IAAQ,CAChB5I,GAAI4I,EACJG,WACAF,gBD/QyB,ICoRnC,CAEA,cAAqBhB,GACnB,GAAqB,IAAjBA,EAAM5E,OACR,aAGmB2D,KAAK0D,GAAIqC,aAAa9E,GAA3C,MAEMoC,EAAQrD,KAAK8C,YAAYnP,MAE/BsN,EAAMjK,SAASqB,GACbgL,EAAMvO,IAAIuD,EAAKe,GAAI,IAAKf,EAAMqD,OAAQxE,EAAgB8G,YAExDgC,KAAK8C,YAAYhS,KAAKuS,EACxB,CAEA,sBACE,MAAMA,QAAcrD,KAAK0D,GAAIsC,aAAa,CACxCC,SAAU,CAAC/O,EAAgB8G,WAC1B1I,MAAM2L,GAAU,IAAI+B,IAAI/B,EAAM1K,KAAK8B,GAAS,CAACA,EAAKe,GAAIf,QAEzD2H,KAAK8C,YAAYhS,KAAK,IAAIkS,IAAI,IAAIK,KAAUrD,KAAKqD,QACnD,G,uEClTK,SAAS6C,GAAcC,EAAgB9J,EAAS,KACrD,OAAO8J,EAAO9J,OAASA,EAAS,GAAG8J,EAAO1L,MAAM,EAAG4B,QAAe8J,CACpE,CA0BA,MAAMC,GAAqB,uBClBpB,MA0BMC,GAAgC,CAC3CzM,EACA0M,KAEA,MAAM,iBACJrN,EAAgB,MAChBsN,EACAC,aAAa,KACXtN,EACAuN,OAAO,UAAE7N,EAAS,OAAE8N,GAAQ,QAC5BC,GACD,KACD9P,EAAI,MACJlD,GACE2S,EACJ,MAAO,CACLtN,KAAMC,EACNsN,QACA1P,OACA+B,WAAW,EAAAgO,GAAA,IAAgBhO,GAE3BM,OACAvF,QACAgT,UACA/M,SACAiN,YAAaH,EACd,EAoCUI,GAA0B,EACrCxN,OACAb,KACAmB,SACAhB,YACAK,uBACF,CACEK,OACAb,KACAmB,SACAhB,WAAW,EAAAgO,GAAA,IAAgBhO,GAC3BG,gBAAiBE,ICxDZ,SAAS8N,GACdC,EACAC,GAEA,OAAOhU,SAAUmD,KACf,GAAI6Q,EAAOC,QACT,MAAM,IAAIC,aAAa,6BAA8B,cAEvD,OAAOH,KAAQ5Q,EAAK,CAExB,C,eC6NO,IAy8LKgR,GAAL,CAAKA,IAEVA,EAAA,IAAM,MAENA,EAAA,cAAgB,kBAEhBA,EAAA,aAAe,iBAEfA,EAAA,KAAO,OAEPA,EAAA,eAAiB,mBAEjBA,EAAA,cAAgB,kBAZNA,GAAL,CAAKA,IAAA,IAo0QwB,KAAG;;;;;;;;;MAgCH,KAAG;;;;;;;;MAyCF,KAAG;;;;;;;;;;;;;;;MAiDV,KAAG;;;;;;;;;;;;;;MAkDK,KAAG;;;;;;;;MAyClC,MAAMC,GAA+B,KAAG;;;;;;;;;;MA+CxC,MAAMC,GAAkC,KAAG;;;;;;;;;;MA8CD,KAAG;;;;;;;;MA2C7C,MAAMC,GAAiC,KAAG;;;;;;;;;;;MA8C1C,MAAMC,GAAiC,KAAG;;;;;;;;;;;;;;;;;;;;;MA4D1C,MAAMC,GAAmC,KAAG;;;;;;;;;;;;;;;;;;;;;MAkDX,KAAG;;;;;;;;MAyCJ,KAAG;;;;;;MAwCD,KAAG;;;;;;;;;;;;;;;;;;;;;;;MA0DH,KAAG;;;;;;;;;;;;;;;;;;;;6CC7perC,MAAMC,GAA4B,8BAE5BC,GACX,mCCeWC,GAA+B,CAC1ChO,EACA7F,KAEA,MAAM,KAAEL,EAAI,OAAEmU,GAAW9T,EAEnBiF,EAAO6O,EAAO,WAAW,GACzBC,EAAkBD,EAAO,kBAAkB,GAAGpN,MAAM,GACpD7B,GAAY,WACZiO,EAAcgB,EAAO,aAAa,IAElC,KAAE3O,EAAO,YAAI4D,GAhCQ,CAACpJ,IAC5B,MAAMK,EAAS,GAAAgU,GAAGC,QAAO,KAAAC,YAAWvU,IAC9BwF,EAAOnF,EAAOmU,MAAMhP,KACpB4D,EAAW/I,EAAOmU,MAAMpL,SAC3BvG,KAAK4E,IACJ,MAAMgN,EAAUhN,EAAQiN,QAAQ3N,MAAM,GACtC,OAAI0N,IAAYT,GACP,GAAAW,QAAQL,OAAO7M,EAAQxH,OAG5BwU,IAAYR,GACP,GAAAW,aAAaN,OAAO7M,EAAQxH,YADrC,CAGO,IAERwG,QAAQgB,QAAwB,IAAZA,IAEvB,MAAO,CAAEjC,OAAM4D,WAAU,EAeOyL,CAAc7U,EAAKC,MAAM6U,SAASlC,IAE5DjN,EAAiC,GAevC,OAdAyD,EAAU9F,SAAQ,CAACmE,EAASoL,KAC1BlN,EAAaoC,KAAK,CAChBzC,OACAuN,QACA1P,KAAMiR,EACNlP,YACA+N,SAAS,EACThT,MAAOwH,EACPjC,OACAU,SACAiN,eACA,IAGGxN,CAAY,E,8DCtDrB,MAAMoP,GAAqB,IAAI,MAC7B,SAAa,CACXC,IAAK,KACLC,YAAcC,IAA6B,EAC3CC,cAAe,GACfC,UAAW7V,MAAO8V,IAChBC,YAAW,IAAM1V,QAAQC,WAAW0V,KAAKC,IAAI,IAAO,GAAKH,EAAS,KAAO,KAqBlEI,GAAuBC,GAClC,IAAI,MAAc,KAAa,CAC7BnC,OAAQmC,ICnBZ,MAAMC,GAAkBpW,OACtBqW,cACAC,gBACA/H,SAAS,EACT4H,wBAOkBD,GAAoBC,GAAaI,QAGjDnC,GAA8B,CAC9BoC,MChC2B,IDiC3BjI,SACAkI,QAAS,CAAC,CAAE9Q,UAAWwO,GAASuC,MAChCC,MAAO,CACLC,IAAK,CACH,CAAEC,YAAa,CAAEC,IAAKT,IACtB,CAAEU,cAAe,CAAED,IAAKT,KAE1B1Q,UAAW,CAAEqR,KAAK,SAAgBV,QAI3BW,WAqBPC,GAA0BlX,OAC9B2G,SACAwQ,gBACAb,gBACApI,YACAK,SAAS,EACT4H,kBASA,MAAMQ,EAAQ,CACZS,KAAM,CACJ,CACEzR,UAAW,CACTqR,KAAK,SAAgBV,KAGzB,CACE3P,OAAQ,CACNmQ,IAAKnQ,IAGT,CAAEoQ,cAAe,CAAEM,IAAKF,MAkB5B,aAdkBjB,GAAoBC,GAAaI,QAGjDnC,GAA8B,CAC9BoC,MAAOtI,EACPK,SACAkI,QAAS,CACP,CACE9Q,UAAWwO,GAASuC,MAGxBC,WAGSM,UAAU,EAGVK,GAAkCtX,MAC7C2G,EACAwQ,EACAb,EACApI,EACAiI,IAEA/H,GAAsB8I,GAAyB,CAC7CvQ,SACAwQ,gBACAb,gBACApI,YACAiI,gBE/GEoB,GAA+BC,GACnC,IACK,IAAIC,IAAI,IACND,EAAMlU,KAAKoU,GAASA,EAAKlS,QACzBgS,EAAMlU,KAAKoU,GAASA,EAAKrR,UAKrBsR,GAAqC3X,MAChD+O,EACA6I,EACAC,EACAC,EACA3B,KAEA,MAAM4B,EFkGwB,EAC9B1B,EACAC,EACAH,IAEA/H,GAAsBgI,GAAiB,CACrCC,cACAC,gBACAH,gBE1GyB6B,CACzBjJ,EACA6I,EACAzB,GAEIqB,EAAQ,GAEd,gBAAiBrJ,KAAS4J,EAAoB,CAC5CP,EAAMhP,QAAQ2F,GACd,MAAMtJ,EAAY0S,GAA4BpJ,GAC1CtJ,EAAUuE,OAAS,SACf2E,GACJlJ,GACCgO,GACCgF,EAAmBI,aACjBpF,EACA3O,EAAiBuB,SACjBqS,IXvC2B,GW4CrC,CAEA,OAAON,CAAK,EC5BP,MAAMU,GAAgC,EAC3CvR,SACA2P,gBACA/H,SAAS,EACT4J,QAAQ,GACRC,iBAAiB,OACjB5B,YACF,CACE/P,QAAS,IAAIE,KACb6P,QACA6B,gBAAgB,SAAgB/B,GAChC/H,SACA4J,MAAO,IAAIA,EAAM7U,KAAK+G,GAAM,IAAIA,OAAMiO,KAAK,SAC3CC,gBAAiBH,IAGbI,GAAoBxY,OACxB2G,SACA2P,gBACA/H,SAAS,EACT4J,QAAQ,GACRC,iBAAiB,OACjB5B,QACAL,kBAEA,MAAMsC,QAAYvC,GAAoBC,GAAaI,QAIjDhC,GACA2D,GAA8B,CAC5BvR,SACA2P,gBACA/H,SACA4J,QACAC,iBACA5B,QACAL,iBAIJ,OAAOsC,GAAKC,mBAAmB,ECrB3BC,GAAkB,CACtBlU,EACAmU,EACAvO,EACAwO,EACAC,KAEA,MAAMxT,EAAOb,EAAMxC,IAAI2W,GACjBxS,EAAed,GAAMc,cAAgB,GAS3C,OAPAA,EAAaoC,KAAK6B,GAClB5F,EAAM5C,IAAI+W,EAAM,CACdG,YAAaH,EACbI,kBAAmBF,EAAWzO,EAAE1E,UAAYL,GAAM0T,mBAAqB,EACvEC,KAAM,CAAEJ,SAAQ5S,KAAMoE,EAAEpE,KAAMiT,UAAWJ,EAAW,KAAO,QAC3D1S,iBAEK3B,CAAK,ECrDD0U,GAAcnZ,MACzByQ,EACA2I,EACA9C,EACAtC,EACAqF,GAAwB,KAExB,MAAMC,QAAkB7I,EAAG8I,eAAe,CACxCjT,QAAS8S,EACT/T,UAAWrB,EAAUsB,OAGjBkU,EAAe,IAAIzJ,IAAIuJ,GAAWhW,KAAKmG,GAAM,CAACA,EAAEtD,GAAIsD,MAOpDgQ,EDnByB,EAC/BL,EACAhT,KASA,GAAgC,KAN9BA,EAAcc,QACXmD,GACCA,EAAEzG,OAAS6Q,IACXpK,EAAEzG,OAAS8Q,MACV,IAEctL,OACnB,MAAO,GAET,MAAM3E,EAAQ,IAAIsL,IAmBlB,OAlBA3J,EAAarC,SAASsG,IACpB,IAAI0O,EAAc,GAClB,GAAI1O,EAAEzG,OAAS8Q,GAAiC,CAC9C,MAAM,OAAElO,EAAM,QAAEkT,GAAYrP,EAAE3J,MACxBoY,EAAWtS,EAAOwC,MAAMS,GAAMA,EAAEhD,UAAY2S,KAC7BN,EAAWY,EAAUlT,GAC7BzC,SAAS2D,GACpBiR,GAAgBlU,EAAOiD,EAAIjB,QAAS4D,EAAG3C,EAAIiS,MAAOb,IAEtD,MAAO,GAAIzO,EAAEzG,OAAS6Q,GAA2B,CAC/C,MAAM,YAAElO,EAAW,UAAEqT,EAAS,OAAEf,GAC9BxO,EAAE3J,MACEoY,EAAWvS,IAAgB6S,EACjCL,EAAcD,EAAWc,EAAYrT,EACrCoS,GAAgBlU,EAAOsU,EAAa1O,EAAGwO,EAAQC,EACjD,KAGKrU,CAAK,ECdIoV,CAAkBT,QALL3I,EAAGqJ,gBAAgBV,EAAW,CACzDW,MAAO,MACPzD,mBAKI0D,EAA2B,GAGjC,UAAW1U,KAAQmU,EAAQpO,SAAU,CACnC,MAAM4O,EAAWT,EAAavX,IAAIqD,EAAKyT,aACjCmB,EAAkB5U,EAAKc,aAAa+T,IAAI,IAEtCxU,UAAWyU,EAAoB,KAAErU,EAAI,MAAEuN,GAAU4G,EACnDG,EAAiB,CACrBhV,UAAWrB,EAAUsB,KACrBgB,QAAS8S,EACT7T,KAAM,CACJO,gBAAiBC,EACjBuN,UAKJ,GAAK2G,EAmBE,CACL,MAAM,GACJ9T,EAAE,cACFmU,EAAa,gBACb1C,EAAe,KACfrS,EACAZ,YAAa4V,GACXN,EAEEO,EAAoBxE,KAAKyE,IAC7BH,EACAhV,EAAK0T,oBAED,uBAAE0B,EAAyB,EAAC,oBAAEC,EAAsB,GAAMpV,EAC1DqV,EAAsB5E,KAAKyE,IAC/BnV,EAAK0T,kBACL2B,GAEIhW,EACJ4V,EACAjV,EAAKc,aAAac,QAAQmD,GAAMA,EAAE1E,UAAYiV,IAC3CxR,OAEL,GAAIwO,EAAkBwC,EAAsB,CAE1C,MAAMS,EAAyBxB,EAC3Be,EACAO,EAEEG,EAAoB,IACrBT,EACHlU,KACAxB,cACA2V,cAAeE,EAGf5C,gBAAiB5B,KAAKyE,IACpBL,EACAM,EACAG,GAGFtV,KAAM,IACD8U,EAAe9U,KAClBoV,oBAAqBE,EACrBH,iCAKE5G,GACJrD,EAAGsK,iBAAiBC,KAAKvK,GACzBuD,EAFIF,CAGJgH,GAEFd,EAAQxR,KAAK,IACRyR,KACAa,EACHvV,KAAM2U,GAEV,CACF,KAhFe,CACb,MAAMvV,EAAcW,EAAKc,aAAac,QACnCmD,GAAMA,EAAE1E,UAAYL,EAAK0T,oBAC1B5P,OAEI6R,EAAU,IACXZ,EACHlU,GAAIb,EAAKyT,YACTpU,cAEAiT,gBAAiByB,EAAwBe,EAAuB,EAChEE,cAAehV,EAAK0T,kBACpBkC,UAAU,SAINpH,GAAerD,EAAG0K,cAAcH,KAAKvK,GAAKuD,EAA1CF,CAAkDmH,GAExDjB,EAAQxR,KAAK,IAAKyS,EAAS1V,KAAM2U,GACnC,CA8DF,CACA,OAAOF,CAAO,E,wCCxHT,MAAMoB,GAqBXtO,YAAYuO,GApBZ,KAAQC,eAAkC,GAE1C,KAAQC,cAAgB,EAExB,KAAQC,kBAAoB,EAE5B,KAAQC,eAAiB,EAEzB,KAAQvN,UAAY,EAalBnB,KAAKsO,iBAAmBA,CAC1B,CAVWxN,eACT,MAAO,CACL6N,WAAY3O,KAAKwO,cACjBI,cAAe5O,KAAKyO,kBACpBC,cAAe1O,KAAK0O,cAExB,CAMOrJ,MAAMmJ,EAAuBrN,EAAY,GAO9C,OANAnB,KAAKwO,cAAgBA,EACrBxO,KAAKuO,eAAiB,GACtBvO,KAAKyO,kBAAoB,EACzBzO,KAAK0O,eAAiB,EACtB1O,KAAKmB,UAAYA,EAEVnB,KAAKc,QACd,CAEOtL,IAAIqZ,GAGT,OAFA7O,KAAKwO,eAAiBK,EAEf7O,KAAKc,QACd,CAEOgO,cAAcC,GAOnB,GANA/O,KAAKgP,iBAAiBD,GAElB/O,KAAKuO,eAAelS,OAtDL,IAuDjB2D,KAAKuO,eAAeU,QAGlBjP,KAAKuO,eAAelS,OAAS,EAAG,CAClC,MAGM6S,EAHqBlP,KAAKmP,gCACNnP,KAAKwO,cAAgBxO,KAAKyO,mBACAM,GAIpD/O,KAAKyO,mBAAqBM,EAC1B/O,KAAK0O,cAAgBzF,KAAKmG,MAAMF,GAChClP,KAAKsO,kBAAoBtO,KAAKsO,iBAAiBtO,KAAKc,SACtD,CAEA,OAAOd,KAAKc,QACd,CAEQkO,iBAAiBK,GACvBrP,KAAKuO,eAAe9S,KAAK,CAAE7C,UAAWC,KAAKmE,MAAOqS,aACpD,CAEQF,8BACN,IAAIG,EAAY,EACZC,EAAa,EAEjB,QAAS7S,EAAI,EAAGA,EAAIsD,KAAKuO,eAAelS,OAAQK,IAAK,CACnD,MAAM8S,EACJxP,KAAKuO,eAAe7R,GAAG9D,UAAYoH,KAAKuO,eAAe7R,EAAI,GAAG9D,WAC1D,UAAEyW,GAAcrP,KAAKuO,eAAe7R,GAE1C4S,GAAaE,EAAWH,EACxBE,GAAcF,CAChB,CAEA,OAAsB,IAAfE,EAAmB,EAAID,EAAYC,CAC5C,ECyBF,OAjGA,MAuBExP,YACEnF,EACAgI,EACAkI,GASA,GA5BF,KAAU2E,gBAAkB,IAAIpB,GAEhC,KAAUzN,WAAa,IAAI,EAM3B,KAAUW,OAA4B,CACpC8K,UAAW,MAYXrM,KAAKpF,KAAOA,EAEZoF,KAAK0P,gBAAkB,IAAIC,gBAE3B3P,KAAK6C,UAAYlC,GAAgB/F,EAAMoF,KAAKY,YAC5CZ,KAAK8K,kBAAoBA,EACzB9K,KAAK4P,UAAW,QAAoB,CAAEvS,OAAQ,OAAQwS,OAAQjV,KACzDgI,EAAKkN,QACR,MAAM,IAAI5M,MAAM,0BAGlBN,EAAKW,YAAYpO,WAAWuO,IAC1B1D,KAAK0D,GAAKA,CAAE,IAGd1D,KAAK8K,kBAAoBA,EAEzB9K,KAAK4D,eAAiB5D,KAAK+P,4BAA4BnN,GAEvD5C,KAAK4D,eAAezO,WAAW2J,IAC7BkB,KAAK4P,SAASI,KACZ,OAAOhQ,KAAKpF,UAAUkE,EAAgB,cAAgB,cAExDkB,KAAK6C,UAAUhC,WAAW/B,EAAgB,cAAgB,WAAW,IAGvEkB,KAAK4D,eACFJ,MAAK,EAAAyM,GAAA,IAAU,IAAMrN,EAAKkN,WAC1B3a,WAAWoM,IACVvB,KAAKuB,OAASA,EACdvB,KAAK4P,SAASI,KAAK,OAAOhQ,KAAKpF,wBAAyB,CACtDlH,KAAM6N,GACN,IAINvB,KAAK4D,eACFJ,MACC,EAAArJ,GAAA,IAAQ2E,KAAoBA,KAC5B,EAAAmR,GAAA,IAAU,IAAMjQ,KAAKkQ,sBAAsBtN,EAAKkN,YAEjD3a,WAAU,KACT6K,KAAKmQ,SAAS,GAEpB,CAEUC,sBACRpQ,KAAK0P,gBAAkB,IAAIC,eAC7B,CAOUO,sBAAsBJ,GAC9B,OAAOA,EAAQtM,MACb,EAAAjN,EAAA,IAAKgL,GAAWA,EAAO8K,aACvB,EAAAgE,GAAA,IAAqB,CAACC,EAAYC,IAAcD,IAAeC,KAC/D,EAAAha,EAAA,IAAKia,KAAQA,KACb,EAAArW,GAAA,IAAQqW,KAAQA,IAEpB,GCrGK,MAAMC,GAAwB,CACnC7M,EACA8M,EACAC,IAEA/M,EAAeJ,MACb,EAAA6M,GAAA,MACA,EAAA9K,GAAA,IAAKzG,GAAkB6R,IAAW7R,MAClC,EAAA3E,GAAA,IAAQyW,GAAgBA,KACxB,EAAAX,GAAA,IAAU,IAAMS,KAChB,EAAA7K,GAAA,M,gBC+DJ,OAzEA,cAAsC,GAKpC9F,YACEnF,EACAgI,EACAkI,GAEA+F,MAAMjW,EAAMgI,EAAMkI,GAPpB,KAAmBgG,eAAiB,IAAIC,EAAA,EAStC,MAAMzL,EAAUmL,GACdzQ,KAAK4D,eACL5D,KAAK8Q,eAAetN,MAClB,EAAAwN,GAAA,GAAU,OACV,EAAAzL,GAAA,IAAI,KAEFvF,KAAKoQ,qBAAqB,KAE5B,EAAAH,GAAA,IAAU,IACRjQ,KAAKiR,uBAAuBzN,MAC1B,EAAAyM,GAAA,IAAW1G,GACTvJ,KAAKkR,uBAAuB3H,GAAe/F,MACzC,EAAA+B,GAAA,IAAI,IAAMvF,KAAK6C,UAAUhC,WAAW,aACpC,EAAAoP,GAAA,IAAWvc,IAAS,EAAA4F,GAAA,GAAK0G,KAAKmR,SAASzd,EAAMsM,KAAKuB,mBAM3DzC,IACC,GAAQhJ,IAAI,OAAO8E,kBAAsBkE,GACzCkB,KAAK6C,UAAUhC,WAAW/B,EAAgB,cAAgB,WAAW,IAIzEwG,EAAQnQ,UAAU,CAChBrE,KAAM,KACJkP,KAAK6C,UAAUhC,WAAW,SAAS,EAErCzL,MAAQuP,IACN3E,KAAK6C,UAAUhC,WAAW,QAAS8D,EAAI,IAG3C3E,KAAKsF,QAAUA,CACjB,CAQO6K,UACLnQ,KAAK0P,iBAAiB0B,QACtBpR,KAAK8Q,eAAehgB,OACpB,GAAQgF,IAAI,OAAOkK,KAAKpF,sBAC1B,CAOOyK,QAIL,OAHArF,KAAKsF,QAAQnQ,WAAU,SAGhB6K,IACT,GCsRF,OA9SA,cAAmC,GACvB+P,4BAA4BnN,GAepC,OAduB,EAAAiB,EAAA,GAAc,CACnCjB,EAAKW,YACLX,EAAKkN,QAAStM,MACZ,EAAAjN,EAAA,IAAKgL,GAAWA,EAAO8K,aACvB,EAAAgE,GAAA,MAEFrQ,KAAK8K,kBAAmBlH,iBACvBJ,MACD,EAAAjN,EAAA,IACE,EAAEwN,EAAYsI,EAAWgF,OACrBtN,KAAgBsN,KAA0BhF,IAKpD,CAGU6E,uBACR3H,GAEA,MAAM,UAAE8C,GAAcrM,KAAKuB,OAC3BvB,KAAK4P,SAASI,KACZ,OAAOhQ,KAAKpF,kBAAkByR,WAAkB,SAC9C9C,MAIJ,MAAM+H,EAAYnG,GAA8B,CAC9CvR,OAAQyS,EACR9C,gBACA6B,MAAO,GACPC,eAAgB,OAChB5B,MAAO,MAGH8H,EXrDH,SACLC,EACAF,GAEA,MAKMG,EALS,IAAIC,GAAA,EAAa,CAC9B/G,KAAMlC,GACNkJ,MAAO,IAAI,OAGmBxc,UAAU,CAAEqc,QAAOF,cACnD,OAAO,IAAIvc,EAAA,GAAY6c,IACrB,MAAMrc,EAAekc,EAAiBtc,UAAU,CAC9CrE,KAAKiD,GACH6d,EAAW9gB,KAAKiD,EAAOL,KACzB,EACA0B,MAAMuP,GACJiN,EAAWxc,MAAMuP,EACnB,EACAtP,WACEuc,EAAWvc,UACb,IAIF,MAAO,IAAME,EAAaE,aAAa,GAE3C,CW4BMoc,CACEpK,GACA6J,GACA9N,MACA,EAAAjN,EAAA,IAAKub,IACI,CACLC,OAAQ,UACR1Y,aAAcyY,EAASnG,oBAAoBpV,KAAKmG,GAC9C2J,GAA8BgG,EAAY3P,UAM9CsV,ECpGH,SACLtY,EACA8X,EACA1b,GAEA,OAAO,IAAIf,EAAA,GAAY6c,IACrB,MAAMK,EAAK,IAAIC,UAAU,MA8BzB,OA5BAD,EAAGE,OAAS,KACVrc,EAAI,wBAAwB,aAAsB0b,KAClDS,EAAGG,KACD3b,KAAKC,UAAU,CACb2b,QAAS,MACTlc,OAAQ,YACRiD,GAAI,IACJmI,OAAQ,CAAEiQ,WAEb,EAGHS,EAAGze,UAAaC,IACd,MAAM0H,EAAU1E,KAAKyB,MAAMzE,EAAMC,MACjCoC,EAAI,WAAW4D,cAAqByB,GACpCyW,EAAW9gB,KAAKqK,EAAQpH,OAAO,EAGjCke,EAAGK,QAAW7e,IACZqC,EAAI,WAAW4D,UAAiB,CAAEtE,MAAO3B,IACzCme,EAAWxc,MAAM3B,EAAM,EAGzBwe,EAAGM,QAAU,KACXzc,EAAI,WAAW4D,YACfkY,EAAWvc,UAAU,EAGhB,KACL4c,EAAG3d,OAAO,CACX,GAEL,CD4D6Bke,CACvBnG,GCzGoC3S,ED0GV2S,ECzG9B,yCAAyC3S,OD0GrC,CAACyB,EAASsX,IAAQzS,KAAK4P,SAASI,KAAK7U,EAAS,CAAEuX,KAAM,aAAcD,MACpEjP,MACA,EAAArJ,GAAA,IAAQzG,KAAU,KAAAif,SAAQjf,MAC1B,EAAA6C,EAAA,IAAK7C,IACI,CACLqe,OAAQ,OACR1Y,aAAcuO,GAA6ByE,EAAY3Y,QCjHxB,IAACgG,EDsHtC,OAAO,EAAAkZ,GAAA,GACLrB,EACAS,EAEJ,CAEUf,uBACR,OAAO,EAAA4B,GAAA,IAAM,KAAM,EAAAvZ,GAAA,GAAK0G,KAAK8S,aAE/B,CAEA,iBACE,MAAM,UAAEzG,GAAcrM,KAAKuB,QACrB,OAAE0F,GAAWjH,KAAK0P,gBAClBxC,QAAiBlN,KAAK0D,GAAIqP,cAAc1G,EAAYA,GAEpD2G,QAAiChT,KAAKiT,iBAC1C5G,EACAA,EACAa,GAGFlN,KAAK6C,UAAUhC,WAAW,cAAe,iBACzC,MAAMqS,QAAwB9G,GAC5BpM,KAAK0D,GACL2I,EACAa,EAASrC,gBACT5D,GAMF,OAHAjH,KAAKY,WAAWL,gBAAgB2S,GAChClT,KAAK6C,UAAUhC,WAAW,UAEnBmS,CACT,CAEA,gBACE,OAAEjB,EAAM,aAAE1Y,GACVkI,GAEA,MAAM,UAAE8K,GAAc9K,GAChB,OAAE0F,GAAWjH,KAAK0P,gBACxB,GAA4B,IAAxBrW,EAAagD,OAEf,YADA2D,KAAK4P,SAASI,KAAK,OAAOhQ,KAAKpF,QAAQyR,wBAGzC,MAAMa,QAAiBlN,KAAK0D,GAAIqP,cAAc1G,EAAYA,SAEpDrM,KAAKmT,yBACT9G,EACAA,EACAhT,EACA6T,EACA6E,GAGF/R,KAAK6C,UAAUhC,WAAW,cAAe,iBACzC,MAAMqS,QAAwB9G,GAC5BpM,KAAK0D,GACL2I,EACAa,EAASrC,gBACT5D,EACW,SAAX8K,GAGF/R,KAAKY,WAAWL,gBAAgB2S,GAChClT,KAAK6C,UAAUhC,WAAW,SAC5B,CAEA,+BACEwL,EACA3S,EACAL,GACA,cAAEkU,EAAa,YAAE3V,EAAW,gBAAEiT,GAC9BkH,GAEA,MAAM,OAAE9K,GAAWjH,KAAK0P,gBAIlBpD,EAAmC,SAAXyF,EAE9B/R,KAAK4P,SAASI,KACZ,iCAAiCtW,KAAWqY,eAC1C1Y,EAAagD,iBACJhD,EAAa+T,GAAG,IAAIxU,kBAC7BS,EAAa+T,IAAI,IAAIxU,mBAKnBmO,GAAe/G,KAAK0D,GAAI0P,gBAAiBnM,EAAzCF,CAAiD1N,GAGvD2G,KAAKqT,UAAUha,EAAc4N,GAE7B,MAAM,KACJjO,EAAI,MACJuN,EAAK,UAEL3N,GACES,EAAa+T,IAAI,GAEfkG,EAAoB1a,EAGpB2a,EAAc,CAClBha,QAAS8S,EACT/T,UAAWrB,EAAUoC,aACrBD,GAAIM,EACJmR,gBAAiByB,EACbgH,EACAzI,EACJjT,YAAaA,EAAeyB,EAAagD,OACzCkR,cAAeA,GAAiB,EAChCY,UAAU,EACV3V,KAAM,CACJO,gBAAiBC,EACjBuN,UAMJ,aAFMQ,GAAe/G,KAAK0D,GAAI0K,cAAenH,EAAvCF,CAA+CwM,GAE9CD,CACT,CAEA,uBACEjH,EACA3S,EACAwT,GAEA,MAAM,YAAEtV,EAAW,gBAAEiT,GAAoBqC,EACnC3D,EAAgBsB,EAAkB,EAExC7K,KAAK6C,UAAUhC,WAAW,cAE1B,MAAM2S,OP7LmCvgB,OAC3CyG,EACA6P,EACAH,KAEA,MAAMsC,QAAYvC,GAAoBC,GAAaI,QAGjDjC,GAAgC,CAChC7N,QAAS,IAAIA,KACbd,WAAW,SAAgB2Q,KAG7B,OAAOmC,GAAK+H,8BAA8BC,WAAWC,KAAK,EOgLxBC,CAC9Bla,EACA6P,EACAvJ,KAAK0P,gBAAiBzI,QAOxB,GAJAjH,KAAK4P,SAASI,KACZ,gCAAgCtW,cAAoB8Z,YAA4BjK,KAGxD,IAAtBiK,EACF,OAAOjK,EAGTvJ,KAAK6C,UAAUhC,WACb,cACA,QAAQnH,OACRsG,KAAKyP,gBAAgBpK,MACnB4D,KAAK4K,KAAKL,ETtRe,OS0R7B,MAAMM,EPnM+B,GACvCla,SACA2P,gBACA6B,QACAC,iBACA5B,QACAL,iBAEA/H,GAAsBoK,GAAmB,CACvC7R,SACA2P,gBACA6B,QACAC,iBACA5B,QACAL,gBOqLkC2K,CAA0B,CAC1Dna,OAAQF,EACR6P,gBACA6B,MAAO,GACPC,eAAgB,MAChB5B,MT/R2B,ISgS3BL,YAAapJ,KAAK0P,iBAAiBzI,SAGrC,IAAI+M,EAAmB,EACnBV,EAAoB/J,EAGxB,gBAAiBnI,KAAS0S,EAA2B,CACnD9T,KAAK6C,UAAUhC,WACb,cACA,QAAQnH,OACRsG,KAAKyP,gBAAgBX,cAAc,IAGrCkF,GAAoB5S,EAAM/E,OAE1B,MAAMhD,EAAe+H,EAAM7K,KAAKmG,GAC9B2J,GAA8B3M,EAASgD,KAGzC4W,QAA0BtT,KAAKmT,yBAC7B9G,EACA3S,EACAL,EACA,IACK6T,EACHtV,YAAaA,EAAcoc,GAE7B,UAEJ,CAEA,OAAOV,CACT,CAEA,gBAAwBlS,EAAyB6F,GAC/C,MAAM,OAAEgN,EAAM,eAAEC,EAAc,MAAEzJ,GR1Q7B,SAAyCrJ,GAC9C,MAAM8I,EAAa9I,EAAMjH,QACtBga,GLvCsC,qCKuChCA,EAAEtd,OAELqd,EAAiB,IAAIxJ,IACrBD,EAAmB,GAuBzB,MAAO,CACLwJ,OAtB2C/J,EAAW/N,QAEtD,CAACC,GAAOzI,QAAOqF,OAAMJ,gBACpBjF,EAAyB8W,MAAMzT,SAAS2T,IACvCuJ,EAAe1e,IAAImV,EAAKlS,IACxByb,EAAe1e,IAAImV,EAAKrR,MACxB,MAAM8a,EAAS,IACVzJ,EACH/R,YACAgB,OAASjG,EAAyBiG,OAClCb,gBAAiBC,GAEnByR,EAAMhP,KAAK2Y,GAEPzJ,EAAKrR,OAAS+I,KAChBjG,EAAIgY,EAAO3b,IAAM2b,EACnB,IAEKhY,IACN,CAAC,GAIF8X,eAAgB,IAAIA,GACpBzJ,QAEJ,CQ0OM4J,CAAgCjT,GAC9BqJ,EAAMpO,OAAS,SACX2E,GACJyJ,GACCA,GAAU1D,GAAe/G,KAAK0D,GAAI4Q,cAAerN,EAAvCF,CAA+C0D,InBlU7B,KmBuUjC,MAAM8J,EAAiBzd,OAAOC,KAAKkd,GAE7BO,EAAoBN,EAAe/Z,QACtC6H,IAASuS,EAAeE,SAASzS,WAI9BhC,KAAK8K,kBAAmBI,aAC5BqJ,EACApd,EAAiBuB,SACjB5F,EAAc4hB,MAIZF,EAAkBnY,OAAS,SACvB2D,KAAK8K,kBAAmBI,aAC5BsJ,EACArd,EAAiBuB,SACjB5F,EAAc6hB,IAGpB,GEhWK,MAAMC,GAAgBC,GAC3BA,EAAIC,QAAQ,gBAAiBC,GAC3BA,EAAMC,cAAcF,QAAQ,IAAK,IAAIA,QAAQ,IAAK,MAQ/C,SAASG,GACdC,GAEA,IAAKA,GAAgC,iBAAbA,EACtB,OAAOA,EAET,MAAMC,EAA2B,CAAC,EAelC,OAdAre,OAAOC,KAAKme,GAAUle,SAASoe,IAC7B,GAAIte,OAAOue,UAAUC,eAAeC,KAAKL,EAAUE,GAAM,CACvD,MAAMI,EAAeZ,GAAaQ,GAClC,IAAIzhB,EAAQuhB,EAASE,GACjBK,MAAMC,QAAQR,EAASE,IACzBzhB,EAAQuhB,EAASE,GAAK7e,KAAK8B,GAAS4c,GAAY5c,KACd,iBAAlB6c,EAASE,GACzBzhB,EAAQshB,GAAYC,EAASE,IACK,iBAAlBF,EAASE,KACzBzhB,EAA0BA,EnBT7BmhB,QAAQ,OAAQ,MAChBA,QAAQ,OAAQ,MAChBA,QAAQ,OAAQ,KAChBA,QAAQ,QAAS,KACjBA,QAAQ,QAAS,MACjBA,QAAQ,QAAS,MmBMhBK,EAAIK,GAAgB7hB,CACtB,KAEKwhB,CACT,CCzBO,SAASQ,GACdlL,EACAlR,EACAqc,EAAoB,EACpBpI,EAAkB,GAElB,MAAMqI,EAAkBpL,EAAMtQ,QAC3BwQ,GAASA,EAAK/R,UAAYgd,IAEvBE,GAAkB,KAAAC,eACtBF,GACClL,GAASA,EAAK/Q,SAAWL,IAGtB3B,EACJke,EAAkB,EACdtI,EAAkBqI,EAAgBxZ,OAClCwZ,EAAgBxZ,OAASyZ,EAAkB,EAKjD,MAAO,CACLvI,cAHAuI,EAAkB,EAAIF,EAAoBnL,EAAMqL,GAAiBld,UAIjEhB,cAEJ,CAEO,SAASoe,GACdC,EACAxL,EACAlR,EACA+S,GAAwB,GAExB,MAAM,cAAEiB,EAAa,YAAE3V,GAAgB+d,GACrClL,EACAlR,EACA0c,EAAW1I,cACX0I,EAAWre,aAGPse,EAAWjB,GAAYxK,EAAMA,EAAMpO,OAAS,IAC5CwO,EAAkBqL,EAAStd,UACjC,MAAO,IACFqd,EACH1c,UACAjB,UAAWrB,EAAUyB,SACrByV,UAAU,EACVvW,cACAY,KAAM,IACD0d,EACHtd,UAAWiS,GAEb0C,gBACA1C,gBAAiByB,EACbzB,EACAoL,EAAWpL,gBAEnB,CAEA,MCtEasL,GAAoBC,GAC/BA,aAAajP,cAA2B,eAAXiP,EAAExb,K,4ECsFjC,OA3EA,cAAoC,GAKlCmF,YACEnF,EACAyb,EACAzT,EACAkI,GACA,SACEwL,GAGE,CAAEA,SAAU,IAEhBzF,MAAMjW,EAAMgI,EAAMkI,GAElB,MAAM,MAAE3G,EAAK,YAAEoS,GCPiB,EAClC3S,EACA8M,EACA8F,EAAiC,CAAC,KAElC,MAAM,WACJH,EAAU,SACVC,EAAW,EAAC,gBACZG,EAAe,QACfC,EAAO,aACPC,EAAe,EAAC,SAChBhG,GACE6F,EAEEI,EAAkB,IAAI7F,EAAA,EAEtB8F,EAAqBD,EAAgBpT,MACzC,EAAAwN,GAAA,GAAU,OACV,EAAAf,GAAA,IAAU,KAAM,EAAA6G,GAAA,GAAST,GAAY7S,MAAK,EAAAwN,GAAA,GAAU,IAAI,EAAA+F,GAAA,GAAMT,OAsBhE,MAAO,CACLnS,MApBcsM,GACd7M,EACAiT,EAAmBrT,MACjB,EAAA+B,GAAA,IAAI,IAAMkR,GAAmBA,OAC7B,EAAAO,GAAA,IAAW,IACTtG,EAAkBlN,MAChB,EAAAyT,GAAA,GAAM,CACJF,MAAQ3hB,IACN,GAAQU,IAAI,QAASV,GACrBshB,GAAWA,EAAQthB,IACZ,EAAA0hB,GAAA,GAASH,WAMzB7X,GAAkB6R,IAAW7R,KAK9ByX,YAAa,KAGXK,EAAgB9lB,MAAM,EAEzB,EDxCgComB,CAC7BlX,KAAK4D,gBAEL,EAAAiP,GAAA,IAAM,KAAM,EAAAvZ,GAAA,GAAK0G,KAAKmX,YACtB,CACEd,aACAC,WAEAI,QAAUthB,IACR4K,KAAK4P,SAASI,KAAK,OAAOpV,UAAcxF,EAAMwP,YAC9C5E,KAAK6C,UAAUhC,WAAW,QAASzL,EAAMwP,WAAW,EAEtD+L,SAAW7R,IACTkB,KAAK4P,SAASI,KAAK,OAAOpV,kBAAqBkE,KAC/CkB,KAAK6C,UAAUhC,WAAW/B,EAAgB,cAAgB,WAAW,IAK3EkB,KAAKmE,MAAQA,EACbnE,KAAKuW,YAAcA,CACrB,CAEOpG,UACLnQ,KAAK0P,iBAAiB0B,QACtBpR,KAAKuW,gBACLvW,KAAK4P,SAASI,KAAK,OAAOhQ,KAAKpF,oBACjC,CAEOyK,QAEL,OADArF,KAAKmE,MAAMhP,WAAU,IAAM6K,KAAK6C,UAAUhC,WAAW,YAC9Cb,IACT,CAEA,eACE,MAAMuB,GAAS,UAAMvB,KAAKuB,QAC1BvB,KAAKoQ,sBACL,UACQpQ,KAAKoX,KAAK7V,EAClB,CAAE,MAAO6U,GACP,MAAMiB,EAAYlB,GAAiBC,GAQnC,GAPApW,KAAK4P,SAASI,KACZ,OAAOhQ,KAAKpF,QAAQ2G,EAAO8K,8BAA8BgL,MACzD,CACEjiB,MAAOghB,KAINiB,EACH,MAAMjB,CAEV,CACF,GE2HF,OAlLA,cAAgC,GACpBrG,4BAA4BnN,GAmBpC,OAlBuB,EAAAiB,EAAA,GAAc,CACnCjB,EAAKW,YACLX,EAAKkB,cACLlB,EAAKkN,QAAStM,MACZ,EAAAjN,EAAA,IAAKgL,GAAWA,EAAO8K,aACvB,EAAAgE,GAAA,MAEFrQ,KAAK8K,kBAAmBlH,iBACvBJ,MACD,EAAAjN,EAAA,IACE,EAAEwN,EAAYC,EAAcqI,EAAWiL,QACnCtT,GACAD,GACAuT,GACAjL,KAKV,CAEA,WAAqB9K,GACnB,MAAM,UAAE8K,GAAc9K,GAChB,OAAE0F,GAAWjH,KAAK0P,gBACxB1P,KAAK6C,UAAUhC,WAAW,cAE1B,MAAM0W,QAA0BvX,KAAK0D,GAAI8I,eAAe,CACtDjT,QAAS8S,EACT/T,UAAWrB,EAAUyB,WAGjBmS,EAAkB0M,EAAkBnK,GAAG,IAAIvC,iBAAmB,EAG9D2M,OjBfmBvkB,OAC3ByG,EACA0Q,EACAb,EACAH,KAEA,MAAMsC,QAAYvC,GAAoBC,GAAaI,QAGjDlC,GAAiC,CACjC5N,UACA+d,eAAgBrN,EAChBxR,WAAW,SAAgB2Q,KAG7B,OAAOmC,EAAIgM,qBAAqBhE,WAAWC,KAAK,EiBAnBgE,CACzBtL,EACA,CAAChK,IACDwI,EACA5D,GAaF,GAVAjH,KAAK4P,SAASI,KACZ,uBAAuB3D,WAAmBmL,KAE5CxX,KAAKyP,gBAAgBpK,MAAMmS,EAAeD,EAAkBlb,QAC5D2D,KAAK6C,UAAUhC,WACb,cACA,eACAb,KAAKyP,gBAAgB3O,UAGnB0W,EAAe,EAAG,CAEpB,MAAMI,QAA6B5X,KAAK6X,eACtCxL,EACAxB,EACA5D,GAIFsQ,EAAkB9b,QAAQmc,EAC5B,OACM5X,KAAK8X,cAAczL,EAAYkL,EAAmBtQ,EAC1D,CAEA,qBACEoF,EACAxB,EACA5D,GAEA,MAAM8Q,QAA4BxN,GAChC8B,EACA,CAAChK,IACDwI,EhBrGyB,IgBuGzB7K,KAAK0P,iBAAiBzI,QAGlB+Q,EAA6B,GAC7BC,QAA0BjY,KAAK0D,GAAI8I,eAAe,CACtDjT,QAAS8S,EACT/T,UAAWrB,EAAUyB,WAEjBwf,EAAuB,IAAIlV,IAC/BiV,EAAkB1hB,KAAKmG,GAAM,CAACA,EAAEtD,GAAIsD,MAGtC,gBAAiByb,KAAeJ,EAAqB,CACnD/X,KAAK6C,UAAUhC,WACb,cACA,yBACAb,KAAKyP,gBAAgBX,cAAc,IAErC,MAAMsJ,EAAqBD,EAAY5hB,IAAI0e,IAAa1e,KAAK8B,IAC3D,MAAM,UAAEO,EAAS,GAAEH,GAAOJ,EACpBwS,GAAkB,EAAAjE,GAAA,IAAgBhO,GAGlCyf,EAAoBH,EAAqBhjB,IAAIuD,IAC/C,EAAAmO,GAAA,IAAgBhO,GAChB,EAGJ,MAAO,CACLW,QAAS8S,EACTjT,GAAIX,EACJH,UAAWrB,EAAUyB,SACrBmS,gBAAiBwN,EACjB9K,cAAe1C,EACfjT,YAAa,EACbuW,UAAU,EACV3V,KAAM,IAAKH,EAAMO,UAAWiS,GAC7B,IAGCuN,EAAmB/b,OAAS,UACxB0K,GACJ/G,KAAK0D,GAAI0K,cACTnH,EAFIF,CAGJqR,GACFJ,EAAUvc,QAAQ2c,GAEtB,CAEA,OAAOJ,CACT,CAEA,oBACE3L,EACAE,EACAtF,GAEA,MAAMqR,EAAoC,GAG1C,UAAWpL,KAAYX,EAAW,CAChC,MAAM,GAAEnT,EAAE,gBAAEyR,GAAoBqC,EAEhClN,KAAK6C,UAAUhC,WACb,cACA,4BACAb,KAAKyP,gBAAgBX,cAAc,IAGrC,MAAMyJ,QAAqB3N,GACzBxR,EACAyR,EACA7K,KAAK8K,kBACLhY,EAAcsP,OACdpC,KAAK0P,iBAAiBzI,QAGxB,GAAIsR,EAAalc,OAAS,EAAG,CAC3B,MAAMoO,EAAQ8N,EAAahiB,IAAIuQ,UAIzB9F,GACJyJ,GACCA,GAAU1D,GAAe/G,KAAK0D,GAAI4Q,cAAerN,EAAvCF,CAA+C0D,I1BrL/B,K0ByL7B,MAAMyD,EAAU8H,GAAyB9I,EAAUzC,EAAO4B,GAE1DiM,EAAiB7c,KAAKyS,EACxB,CACF,CAEIoK,EAAiBjc,OAAS,SACtB0K,GAAe/G,KAAK0D,GAAI0K,cAAenH,EAAvCF,CAA+CuR,GAEvDtY,KAAKY,WAAWL,gBAAgB+X,EAClC,GC0BF,OAlMA,cAAgC,GAG9BvY,YACEnF,EACAyb,EACAzT,EACAkI,GACA,SAAEwL,GAAmC,CAAEA,SAAU,IAEjD,IAAK1T,EAAK4V,YACR,MAAM,IAAItV,MAAM,2BAGlB2N,MAAMjW,EAAMyb,EAAYzT,EAAMkI,EAAmB,CAC/CwL,aAdJ,KAAUmC,WAA8B,EAgBxC,CAEU1I,4BAA4BnN,GACpC,MAAM8V,EAAyB,IAAI3V,EAAA,GAAyB,GAC5DH,EAAKkN,SACDtM,MACA,EAAAjN,EAAA,IAAKgL,GAAWA,EAAO8K,aACvB,EAAAgE,GAAA,MAEDlb,WAAU,KACTujB,EAAuB5nB,MAAK,EAAM,IAGtC8R,EAAK4V,YAAarjB,WAAWsjB,IAC3BzY,KAAKyY,WAAaA,EAClBC,EAAuB5nB,MAAK,GAE5BkP,KAAKmQ,SAAS,IAkBhB,OAfuB,EAAAtM,EAAA,GAAc,CACnCjB,EAAKW,YACLX,EAAKkN,QACL9P,KAAK8K,kBAAmBlH,eACxB8U,IACClV,MACD,EAAAjN,EAAA,IACE,EAAEwN,EAAYxC,EAAQ8P,EAAsBsH,OACxC5U,KACAxC,EAAO8K,aACPgF,GACFsH,IAKR,CAEA,WAAqBpX,GACnB,MAAM,OAAE0F,GAAWjH,KAAK0P,gBAExB1P,KAAK6C,UAAUhC,WAAW,cAAe,gBACzC,MAAM,UAAEwL,GAAc9K,GAEhB,WAAEkX,GAAezY,KAEvBA,KAAK6C,UAAUhC,WAAW,cAE1Bb,KAAK4P,SAASI,KACZ,qBAAqB3D,WAAmBoM,EAAWpc,SACnD,CACEqW,KAAM,eACNhf,KAAM+kB,IAIVzY,KAAKyP,gBAAgBpK,MAAMoT,EAAWpc,QACtC2D,KAAK6C,UAAUhC,WACb,cACA,UACAb,KAAKyP,gBAAgB3O,UAIvB,UAAW+K,KAAQ4M,QAEXzY,KAAKqT,UAAUhH,EAAYR,EAAM5E,EAE3C,CAEA,gBACEoF,EACA3S,EACAuN,GAEA,IAAI2R,EAAc,GAClB,IACE5Y,KAAK6C,UAAUhC,WACb,cACA,iBAAiBnH,OACjBsG,KAAKyP,gBAAgB3O,UAEvB,MAAM,cAAEyM,EAAa,YAAE3V,EAAW,KAAEY,SAAewH,KAAK0D,GAAIqP,cAC1D1G,EACA3S,IAGI,oBAAEkU,EAAsB,EAAC,uBAAED,EAAyB,GACxDnV,GAAQ,CAAC,EAEL+Q,EAAgBoE,EAAyB,EAEzCkL,QAA2BtO,GAC/B7Q,EACA6I,GACAgH,EjBjJuB,IiBmJvBtC,GAIF,gBAAiB6R,KAAcD,EAAoB,CACjD7Y,KAAK6C,UAAUhC,WACb,cACA,QAAQnH,OACRsG,KAAKyP,gBAAgBX,cAAc,IAGrC,MAAMrE,EAAQqO,EAAWviB,IAAIuQ,KAErByG,cAAewL,EAAkBnhB,YAAaohB,GACpDrD,GAAgBlL,EAAO4B,EAAWkB,EAAe3V,GAInD,GAAI6S,EAAMpO,OAAS,EAAG,CACpB,MAAM6Z,EAAWjB,GAAYxK,EAAM2C,IAAI,IACjC6L,EAA4B/C,EAAUtd,gBAEtCmO,GAAe/G,KAAK0D,GAAI4Q,cAAerN,EAAvCF,CAA+C0D,GAErD,MAAM3S,EAAY2S,EAAMlU,KAAK+G,GAAMA,EAAE7E,WAC/BuH,KAAK8K,kBAAmBI,aAC5BpT,EACAX,EAAiBuB,SACjB5F,EAAc4hB,MAGhB,MAAMnB,EAAc,CAClBha,QAAS8S,EACT/T,UAAWrB,EAAUsB,KACrBa,GAAIM,EACJmR,gBAAiB5B,KAAKyE,IACpBuL,EACArL,GAEFhW,YAAaohB,EACbzL,cAAewL,EACf5K,UAAU,EACV3V,KAAM,IACD0d,EACHvI,uBAAwBsL,EACxBrL,8BAIE7G,GAAe/G,KAAK0D,GAAI0K,cAAenH,EAAvCF,CAA+CwM,GAErDqF,EAAYnd,KAAK8X,EACnB,CACF,CACF,CAAE,MAAO5O,GAIP,GAHA3E,KAAK4P,SAASxa,MAAM,qBAAqBsE,UAAiB,CACxDtE,MAAOuP,IAEJwR,GAAiBxR,GAIpB,MADAiU,EAAc,GACRjU,EAHN3E,KAAK6C,UAAUhC,WAAW,QAAS8D,EAAIC,WAK3C,CAAE,QAEA5E,KAAKY,WAAWL,gBAAgBqY,EAClC,CACF,G,iCCvMF,MAAMhJ,IAAW,QAAoB,CACnCvS,OAAQ,OACRqV,KAAM,6BAIKwG,GAA4B,CACvCC,EACAzf,EACA0f,EACAnS,IAEO,IAAIlS,EAAA,GAAiC6c,IAC1CA,EAAW9gB,KAAK,CAAEmK,OAAQ,QAASgG,MAAO,KAE1C,WACE,MAAMoY,QAAwBF,EAAMG,aAAa5f,GAEjDkY,EAAW9gB,KAAK,CAAEmK,OAAQ,MAAOgG,MAAOoY,IAExC,MAAME,EAAsB,IAAIvW,IAC9BqW,EAAgB9iB,KAAKijB,GAAM,CAACA,EAAE9gB,SAAU8gB,MAGpCC,EAAwBzX,GAC5BuX,EAAoBrkB,IAAI8M,IAAQ,CAC9BzI,QAASG,EACTkB,KAAM,GACN8e,WAAW,EACXC,UAAU,GAGRC,OC5CmB3mB,OAC7ByG,EACAuN,KAEA,MAAM6K,QAAiB,SAAgB,CACrCjK,OAAQ,CACN,CACEuN,IAAK,mBACLzhB,MAAO+F,GAET,CACE0b,IAAK,yBACLzhB,MAAO2O,KAGXuX,WAAY,CACVpQ,MAAO,KAETqQ,OAAQ,CACN7S,YAIJ,OAAK6K,EAASiI,YAAY1d,OAInByV,EAASiI,YAAYxjB,KACzB8B,GAASA,GAAMiO,IAAI4B,KAAKpL,SAAS,GAAG2N,MAAM,GAAGhS,KAJvC,EAKR,EDe6B,CAAgBiB,EAASuN,GAC7C+S,OCZgB/mB,OAC1ByG,EACAuN,KAEA,MAAMgT,QAAoB,QAAYvgB,GAEhCoY,QAAiB,SAAgB,CACrCjK,OAAQ,CACN,CACEuN,IAAK,yBACLzhB,MAAO2O,IAET,CACE8S,IAAK,uBACLzhB,MAAOsmB,IAGXJ,WAAY,CACVpQ,MAAO,KAETqQ,OAAQ,CACN7S,YAIJ,OAAK6K,EAASiI,YAAY1d,OAInByV,EAASiI,YAAYxjB,KAAK8B,GAASA,GAAMiO,IAAI4B,KAAKpL,SAAS,GAAGlD,SAH5D,EAGmE,EDjBhD,CAAaF,EAASuN,GAExCiT,EAAkBN,EAAYzf,QACjC6H,IAASqX,EAAgB3e,MAAMgC,GAAMA,EAAEhE,WAAasJ,GAAOtF,EAAEgd,cAG1DS,EAAsBH,EAAU7f,QACnC0R,IAAUwN,EAAgB3e,MAAMgC,GAAMA,EAAE9C,SAAWiS,GAAQnP,EAAEid,aAGhE/J,GAASI,KACP,uBAAuBtW,wBAA8B2f,EAAgBhd,yBAAyB6d,EAAgB7d,yBAAyB8d,EAAoB9d,UAG7J,MAAM+d,QAA2B9mB,QAAQwR,IACvCqV,EAAoB5jB,KAAItD,MAAO2G,IAC7B,MAAMoI,QAAY,QAAYpI,GAExBygB,EAAgB,IACjBZ,EAAqBzX,GACxBtJ,SAAUsJ,EACVpI,SACA+f,UAAU,GAKZ,aAFMR,EAAMmB,aAAaD,GACzBd,EAAoBzkB,IAAIkN,EAAKqY,GACtBA,CAAa,KAIxBzI,EAAW9gB,KAAK,CAAEmK,OAAQ,MAAOgG,MAAOmZ,UAElC9mB,QAAQwR,IACZoV,EAAgB3jB,KAAItD,MAAO+O,IACzB,MAAMpI,SAAgBwf,EAAoBpX,EAAKlP,EAAcynB,UACzDxmB,QAAQ0O,YACZ,GAAI7I,GAAUA,EAAOtC,MAAM,OAAgB,CACzC,MAAM+iB,EAAgB,IACjBZ,EAAqBzX,GACxBpI,SACAlB,SAAUsJ,EACV0X,WAAW,SAGPP,EAAMmB,aAAaD,GACzBd,EAAoBzkB,IAAIkN,EAAKqY,GAC7BzI,EAAW9gB,KAAK,CAAEmK,OAAQ,MAAOgG,MAAO,CAACoZ,IAC3C,MAIJzK,GAASI,KAAK,uBAAuBtW,WAMrCkY,EAAW9gB,KAAK,CAAEmK,OAAQ,WAAYgG,MAAO,KAE7C2Q,EAAWvc,UACZ,EA/ED,GA+EKgP,OAAOM,IACViL,GAASxa,MAAM,uBAAuBsE,WAAkB,CAAEtE,MAAOuP,IACjEiN,EAAWxc,MAAMuP,EAAI,GACrB,I,gBEhBN,OApFA,MAOE5E,YACE+K,EACAvH,GARF,KAAQtD,QAAU,IAAIC,iBAAiBJ,GAUrCE,KAAK8K,kBAAoBA,EACzBvH,EAAYpO,WAAWqb,IACrBxQ,KAAKuD,YAAYzS,KAAK0f,EAAE,IAE1BxQ,KAAKuD,YAAc,IAAIR,EAAA,OAA+B,GAEtD/C,KAAKC,QAAQzM,UAAaC,GAAUuM,KAAKwa,UAAU/mB,GAEnDuM,KAAKC,QAAQwa,eAAkBhnB,GAC7B,GAAQ2B,MAAM,GAAG0K,UAA2BrM,EAChD,CAEA,wBACE,OAAO,IAAIH,SAASC,IAClB,MAAM4lB,EAAQnZ,KAAKuD,YAAYW,WAC3BiV,GACF5lB,EAAQ4lB,GAGVnZ,KAAKuD,YACFC,MACC,EAAAC,EAAA,IAAO9P,QAAoB,IAAVA,KAElBwB,WAAWxB,IACVJ,EAAQI,EAAe,GACvB,GAER,CAEA,gBAAwB8W,GACtB,MAAM0O,QAAcnZ,KAAK0a,wBACPvB,EAAM7E,cAAc7J,EAExC,CAEA,oBAA4B5I,GAC1B,IACE,MAAMsX,QAAcnZ,KAAK0a,kBACnBC,E3BrDuB,CAACjiB,IAClC,MAAM,IAAEsJ,EAAG,KAAExJ,EAAI,YAAEiK,GAAgB/J,GAC7B,KAAE4K,EAAI,KAAEsX,EAAI,KAAE/jB,EAAI,OAAEgkB,EAAM,UAAEC,GAActiB,EDR3C,IAAuB2N,ECe5B,MAAO,CACLnE,MACAsB,KAAMA,GAAQ,EACdsX,KAAMA,GAAQ,UACd/jB,OACAyN,KATW7B,GDXe0D,EA8BvB,SAAkC4U,GAEvC,IAAIzW,EAAOyW,EAASjG,QAAQ,eAAgB,IAiC5C,OA9BAxQ,EAAOA,EAAKwQ,QAAQ,oBAAqB,MACzCxQ,EAAOA,EAAKwQ,QAAQ,iBAAkB,MACtCxQ,EAAOA,EAAKwQ,QAAQ,eAAgB,MAGpCxQ,EAAOA,EAAKwQ,QAAQ,yBAA0B,MAC9CxQ,EAAOA,EAAKwQ,QAAQ,kBAAmB,IAGvCxQ,EAAOA,EAAKwQ,QAAQ,iBAAkB,IAGtCxQ,EAAOA,EAAKwQ,QAAQ,oBAAqB,MAGzCxQ,EAAOA,EAAKwQ,QAAQ,qBAAsB,MAG1CxQ,EAAOA,EAAKwQ,QAAQ,YAAa,IAGjCxQ,EAAOA,EAAKwQ,QAAQ,iBAAkB,IAGtCxQ,EAAOA,EAAKwQ,QAAQ,iBAAkB,IAGtCxQ,EAAOA,EAAKwQ,QAAQ,UAAW,QAC/BxQ,EAAOA,EAAKwQ,QAAQ,aAAc,IAE3BxQ,CACT,CCtDoB0W,CAAyBvY,GDXpC0D,EAAO2O,QAAQ,KAAM,MCYxB,GAQFmG,WAAYH,IAAc,EAC1BD,OAAQA,GAAU,EACnB,E2BoCkBK,CAAoBrZ,UACdsX,EAAMgC,aAAaR,IAC7BS,UACHxZ,GAA8BC,EAExC,CAAE,MAAOuU,GAOP,MANA,GAAQtgB,IACN,oBACA+L,EACAA,EAAQY,YACR2T,EAAExR,YAEEwR,CACR,CACF,CAEA,iBAAyB1iB,SAEjBsM,KAAK0a,kBAEX1a,KAAK8K,kBAAkBpJ,QAAQ+T,MAAMC,QAAQhiB,GAAQA,EAAO,CAACA,GAC/D,CAEQ8mB,UAAU7f,GAChB,MAAM,KAAE9D,EAAI,KAAEnD,GAASiH,EAAIjH,KACd,SAATmD,EACFmJ,KAAKqb,UAAU3nB,GACG,aAATmD,EACTmJ,KAAKsb,cAAc5nB,GACD,SAATmD,GACTmJ,KAAKub,WAAW7nB,EAEpB,GCvEF,MAAM,IAAW,QAAoB,CAAE2J,OAAQ,SAGxC,MAAMme,GAOXzb,YAAY6C,GAJZ,KAAQhC,WAAa,IAAI,EAEzB,KAAQ6a,MAAsD,CAAC,EAG7D,MAAM,YAAElY,EAAW,cAAEO,GAAkBlB,EAEjCkI,EAAoB,IAAI,GAAuBlI,GAAMyC,QAErC,IAAIqW,GACxB5Q,EACAvH,GAGFvD,KAAK4D,gBAAiB,EAAAC,EAAA,GAAc,CAACN,EAAaO,IAAgBN,MAChE,EAAAjN,EAAA,IAAI,EAAEwN,EAAYC,OAAoBD,KAAgBC,KAGxDhE,KAAK4D,eAAezO,UAAU,CAC5BrE,KAAOiD,GACEA,GAAUiM,KAAKY,WAAWT,kBAAkB,OAAQ,WAE7D/K,MAAQuP,GAAQ3E,KAAKY,WAAWT,kBAAkB,OAAQ,QAASwE,KAGrE,MAAMgX,ECpCK,SACb/Y,GAEA,MAAM,YAAEW,EAAW,cAAEO,EAAa,QAAEgM,GAAYlN,EAC1C3C,EAAU,IAAI,EAEpB,OAAO,EAAA4D,EAAA,GAAc,CACnBN,EACAuM,EAAStM,MACP,EAAAjN,EAAA,IAAKgL,GAAWA,EAAO8K,aACvB,EAAAgE,GAAA,MAEFvM,IACCN,MACD,EAAArJ,GAAA,IACE,EAAE4J,EAAYsI,EAAWrI,OACrBD,KAAgBC,KAAkBqI,KAExC,EAAA4D,GAAA,IAAU,EAAEkJ,EAAO9M,EAAWrI,MAC5B,MAAM,uBAAEf,GAA2BL,EACnC,IAAIgZ,EAA4B,GAChC,OAAO,IAAI7mB,EAAA,GAA4BC,IACrCA,EAASlE,KAAK,IAEdooB,GACEC,EACA9M,EACApJ,GACA9N,WAAU,EAAG8F,SAAQgG,YACrBhB,EAAQS,KAAK,CAAE7J,KAAM,iBAAkBlD,MAAO,CAAEsH,SAAQgG,WAEzC,UAAXhG,EACF2gB,EAAY,GACH,CAAC,MAAO,YAAYlhB,MAAMqG,GAAMA,IAAM9F,KAC/C2gB,EAAUngB,QAAQwF,GAGL,aAAXhG,IACFjG,EAASlE,KAAK8qB,GACd5mB,EAASK,WACX,GACA,GACF,IAGR,CDT2BwmB,CAAqBjZ,GAC5C+Y,EAAexmB,WAAWymB,IACxB,GAAS5L,KAAK,2BAAyB,CACrC0C,KAAM,YACNhf,KAAMkoB,GACN,IAGJ,MAAMpD,EAAcmD,EAAenY,MACjC,EAAAjN,EAAA,IAAKijB,GAAMA,EAAErf,QAAQuC,GAAMA,EAAEgd,eAC7B,EAAAnjB,EAAA,IAAKijB,GAAMA,EAAEjjB,KAAKmG,GAAMA,EAAE9C,YAK5B,IAAI,GAAqB,eAAgBgJ,EAAMkI,GAAmBzF,QAElE,IAAI,GACF,Y/BtEoC,I+BwEpCzC,EACAkI,GACAzF,QAEF,IAAI,GACF,a/B5EkC,I+B8ElC,IAAKzC,EAAM4V,eACX1N,GAEAzF,OACJ,CAEO8K,QAAQvV,GACboF,KAAKyb,MAAM7gB,IAAOuV,SACpB,E,gHErEK,MAAM2L,GAAwB7oB,MACnC8oB,IAEA,IAAKA,EACH,MAAO,UAGT,MAAMC,QAAiB,SAAmBD,GAE1C,OAAOC,GAAUpB,MAAQ,cC5B3B,MAAMlX,GAAK,I,SAAI,IAAM,mBACrBA,GAAGuY,QAAQ,GAAGC,OAAO,CACnBla,IAAK,MACL0X,UAAW,QAGb,UCkBA,OAHS,CAAElkB,IApBGvC,MAAO+O,EAAa+Z,KAG9B,UAFsB,GAAGI,MAAM,OAAOjnB,IAAI,CAAE8M,QAE9B,CACZ,MAAMoa,EAA6B,CACjCpa,MACAtO,KAAMqoB,GAER,GAAGI,MAAM,OAAO3mB,IAAI4mB,EACtB,GAWYlnB,IARFjC,MAAO+O,IAEjB,MAAMqa,QAAgB,GAAGF,MAAM,OAAOjnB,IAAI,CAAE8M,QAG5C,OAAOqa,GAAS3oB,MAAQ2oB,GAASxa,OAAO,G,YClBrC,MAAMya,GACX,iDAEWC,GAA2B,+CAA+CD,KAC1EE,GAA2B,mCAAmCF,KAI9DG,GAAoB,oCCajC,OAfqB,MACnB,MAAMC,EAAU,IAAI,MDDU,2BCY9B,MAAO,CAAElnB,IATGvC,MACV0pB,IAEA,MAAMC,EACY,iBAATD,EAAoB,IAAIE,KAAK,CAACF,GAAO,YAAcA,EAC5D,OAAOD,EAAQlnB,IAAIonB,EAAU,CAAEE,WAAY,EAAGC,WAAW,GAAQ,EAIrDrhB,OADCzI,MAAO+O,GAAgB0a,EAAQhhB,OAAOsG,GAC/B,EAGxB,G,kDCTA,SAASgb,GAAgBC,EAAqBpmB,GAC5C,MAAMqmB,EAAO,IAAIC,KAAK,CAACF,GAAU,CAAEpmB,SACnC,OAAOumB,IAAIJ,gBAAgBE,EAC7B,CAEA,SAASG,GAAcJ,EAAqBpmB,GAG1C,MADa,QAAQA,aADH,QAAwBomB,EAAS,WAGrD,CAGO,MAAMK,GACX1C,IAEA,GAAIA,EAAM,CACR,GAAIA,EAAKnG,SAAS,SAChB,MAAO,QAGT,GAAImG,EAAKnG,SAAS,SAChB,MAAO,QAGT,GAAImG,EAAKnG,SAAS,QAChB,MAAO,MAEX,CACO,EAGH8I,GAAQ,+DAQP,MAMMC,GACX5C,IAEA,IAAKA,EACH,MAAO,QAGT,MAAM6C,EAAcH,GAAyB1C,GAC7C,OAAI6C,KAK8B,IAAhC7C,EAAK8C,QAAQ,gBACwB,IAArC9C,EAAK8C,QAAQ,mBAEN,QAEsB,IAA3B9C,EAAK8C,QAAQ,SACR,SAEgC,IAArC9C,EAAK8C,QAAQ,mBACR,MAEF,UAIIC,GAA0B1qB,MACrC4O,EACAG,EACA4b,KAGA,IAAK/b,IAAYA,GAAS9N,OACxB,MAAO,CACL8pB,SAAS,EACTvZ,KAAMtC,EAAI4C,WACV5C,OAIJ,MAAM,OAAEjO,EAAM,KAAEyE,GAASqJ,GAEnB,KAAE+Y,EAAI,YAAEpY,GAAgBhK,EAE9B,IAAKoiB,EACH,MAAO,CACL5Y,MACA6b,SAAS,EACTvZ,KAAM,yBAAyBtC,EAAI4C,cAGvC,MAEMkN,EAA+B,CACnCnH,KAAM,SAAS3I,IACf6b,SAAS,EACT7b,IALiBH,EAAQG,IAMzBnL,KAAM2L,GAGR,GAAI8a,GAAyB1C,GAC3B,MAAO,IAAK9I,EAAU+L,SAAS,GAGjC,MAAMZ,EACc,iBAAXlpB,OL1BsBd,OAC/B6e,EACA8L,KAEA,IAAIE,EAAkB,EACtB,IACE,GAAIhM,aAAoBiM,WAEtB,OADAH,GAAcA,EAAW9L,EAASkM,YAC3BlM,EAET,MAAMmM,EAA4B,GAElC,GAAInM,aAAoBoM,eAAgB,CACtC,MAAMC,EAASrM,EAASsM,YAElBC,EAAaprB,OACjBG,OACAO,WAEIP,GACK,QAAiB6qB,IAG1BA,EAAOxiB,KAAK9H,GACZmqB,GAAmBnqB,EAAOqqB,WAC1BJ,GAAcA,EAAWE,GAClBK,EAAOG,OAAOhpB,KAAK+oB,IAK5B,aAFoCF,EAAOG,OAAOhpB,KAAK+oB,EAGzD,CAEA,GAAInrB,OAAOC,iBAAiB2e,EAAU,CACpC,MAAMqM,EAASrM,EAAS5e,OAAOC,iBAM/B,gBAAiBorB,KAASJ,EACpBI,aAAiBR,aACnBE,EAAOxiB,KAAK8iB,GACZT,GAAmBS,EAAMP,WACzBJ,GAAcA,EAAWE,IAI7B,OADe,QAAiBG,EAElC,CACA,MACF,CAAE,MAAO7oB,GAQP,YAPA,GAAQA,MACN,gEACAA,EAMJ,GKlCYopB,CAAkBzqB,EAAQ6pB,GAChC7pB,EAEA0qB,EAAkC,iBAAZxB,EAG5B,IAAKA,EACH,MAAO,IACFnL,EACH+L,SAAS,EACTvZ,KAAM,2BAA2BtC,EAAI4C,cAKzC,GAAsB,SAAlBkN,EAASjb,KAAiB,CAE5B,IAAK4nB,GAAgB,KAAMC,GAAOplB,KAAK2jB,IACrC,MAAO,IACFnL,EACHjb,KAAM,QACNgL,QAASwb,GAAcJ,EAAS,kBAIpC,MAAMpI,EAAM4J,EAAexB,GAAU,QAAwBA,GAE7D,OAAIpI,EAAIvd,MAAM,OACL,IACFwa,EACHjb,KAAM,MACNgL,QAASgT,GAGTA,EAAIvd,MAAM,OACL,IACFwa,EACHjb,KAAM,OACNgL,QAASgT,GAtHjB,SAAgB1O,GACd,MAAMwY,EAAYxY,EAAOyY,OAAOnkB,MAAM,EAAG,KACzC,OAAO8iB,GAAMsB,KAAKF,EACpB,CAsHQG,CAAOjK,GACF,IACF/C,EACHjb,KAAM,OACNgnB,SAAS,EACThc,QAASG,EAAI4C,YAMV,IACFkN,EACHnH,KAAMkK,EAAIxY,OAAS,GAAK,SAAS2F,IAAQ,WAAW6S,IACpDhe,KAAM,OACNyN,KAAM4B,GAAc2O,GACpBhT,QAASgT,EAEb,CAEA,IAAK4J,EAAc,CACjB,GAAsB,UAAlB3M,EAASjb,KACX,MAAO,IAAKib,EAAUjQ,QAASwb,GAAcJ,EAASrC,IAExD,GAAsB,QAAlB9I,EAASjb,KACX,MAAO,IACFib,EACHjQ,QAASmb,GAAgBC,EAASrC,GAClCiD,SAAS,EAGf,CAEA,OAAO/L,CAAQ,EAiBJiN,GAAoB,CAC/BC,EACAxc,EACAyc,EAAgB,OAEhB,GAAKD,EAGL,MAAqB,iBAAVA,EACFA,EAAMvkB,MAAM,EAAGwkB,GAEjBzc,GAA+B,SAAhBA,GAClB,QAAwBwc,GAAOvkB,MAAM,EAAGwkB,QACxC,G,gBC3MN,MAAMC,GAAwBjsB,MAC5B+O,IAKA,MAAMtO,QAAa,GAAYwB,IAAI8M,GACnC,GAAItO,GAAQA,EAAK2I,OAAQ,CAEvB,MAAMue,QAAakB,GAAsBpoB,GACnC8O,EAAcgb,GAAsB5C,GAEpCnY,EAAcsc,GAAkBrrB,EAAM8O,GAS5C,MAAO,CAAEzO,OAAQL,EAAMsO,MAAKxJ,KAPE,CAC5B3B,KAAM,OACNyM,KAAM5P,EAAK2I,OACXye,UAAWpnB,EAAK2I,OAChBue,OACApY,eAEgCuP,OAAQ,KAAMtP,cAClD,CAEO,EAGH0c,GAA4B,CAChCtoB,KAAM,OACNyM,UAAM,EACNwX,eAAW,EACXD,YAAQ,GAGJuE,GAAuBnsB,MAC3B+O,EACAqd,EACApY,KAEA,GAAIoY,EAAM,CAER,aADoBA,EAAKC,KAAKtd,EAAK,CAAEiF,UAEvC,CACA,OAAOkY,EAAU,EAGbI,GAA2BtsB,MAC/B+O,EACAqd,EACAG,KAEA,MAAMC,EAAmBD,GAAc,IAAI7P,iBACrC,OAAE1I,GAAWwY,EACnB,IAAIC,EAEJ,GAAKL,EAAL,CAKKG,IACHE,EAAQ1W,YAAW,KACjByW,EAAiBrO,OAAO,GACvB,MAIL,IAEE,MAAMuO,EAAY9mB,KAAKmE,MACjB4iB,QAAcR,GAAqBpd,EAAKqd,EAAMpY,GAC9CzO,EAAOonB,EACPC,EAAgBhnB,KAAKmE,MAC3BxE,EAAKsnB,UAAYD,EAAgBF,EACjC,MAAMI,IAAcH,EAAMtc,MAAOsc,EAAMtc,KH1FT,IG6F9B,GAFAoc,GAASM,aAAaN,GAGf,cADCE,EAAM/oB,KAGV,MAAO,CAAEmL,MAAKie,mBAAmB,EAAMlO,OAAQ,OAAQvZ,KAAMonB,GAEtD,CAEP,MAAQjsB,MAAOusB,SAAqBb,EACjCc,IAAIne,EAAK,CAAEiF,SAAQ5K,OAAQ,KAAMmF,OAAQ,IACzCtO,OAAOC,iBACPrC,OAEH0H,EAAKoiB,WAAakB,GAAsBoE,GACxC1nB,EAAKgK,YAAcgb,GAAsBhlB,EAAKoiB,MAC9C,MAAMwF,EACJR,EAAMtc,MAAQsc,EAAMtc,MAAQ,GAAK4c,EAAW7jB,QAAUujB,EAAMtc,KAExDb,EAAcsc,GAAkBmB,EAAY1nB,EAAKgK,aAEnD4d,SACI,GAAY5qB,IAAIwM,GAAK,QAAiB,CAACke,KAI/C,MAAMG,EAASD,EACXF,EACAH,EACAV,EAAKc,IAAIne,EAAK,CAAEiF,gBAChB,EAcJ,OAZAzO,EAAK8nB,QAAUznB,KAAKmE,MAAQ6iB,GAIvBrnB,EAAK+nB,OAASR,GACjBV,EAAKmB,IAAIxe,GAETxJ,EAAKioB,QAAU5nB,KAAKmE,MAAQxE,EAAK8nB,SAEjC9nB,EAAKioB,SAAW,EAGX,CACL1sB,OAAQssB,EACR5d,cACAT,MACAxJ,OACAuZ,OAAQ,OAGZ,CAEJ,CAAE,MAAO3c,GAEP,OADA,GAAQsrB,MAAM,iCAAkCtrB,GACzC,CACL4M,MACAie,mBAAmB,EACnBlO,OAAQ,OACRvZ,KAAM,IAAK2mB,IAEf,CA/EA,MAFE,GAAQrpB,IAAI,8DAiFd,EAGI6qB,GAA8B1tB,MAClC+O,EACAqd,EACAG,EACAoB,KAGA,MAAMC,EAAoC,aAAnBxB,GAAMyB,SAEvBlB,EAAQiB,QACJzB,GAAqBpd,EAAKqd,EAAMG,GAAYvY,QAClDkY,GAEE4B,EAAa,GAAGtE,WAA0Bza,IAC1C8P,QAAiBkP,MAAMD,EAAY,CACvC5qB,OAAQ,MACR8Q,OAAQuY,GAAYvY,OACpB2Z,YAEF,GAAI9O,GAAYA,EAAS5J,KAAM,CAe7B,MAAM+Y,EAAgBhD,GACnB4C,EAEGvtB,QAAQC,UADR,GAAYiC,IAAIwM,GAAK,QAAiBic,KAGtC,KAAErD,EAAI,OAAE7mB,EAAM,WAAEmsB,SN9KnBjtB,eACLotB,EACAa,GAEA,MAAOC,EAAkBC,GAAcf,EAAOgB,MACxCpD,EAA4B,GAG5BqD,EAAcH,EAAiB/C,aAC/B,MAAEzqB,SAAgB2tB,EAAYhD,OAC9B1D,EAAOjnB,QAAcmoB,GAAsBnoB,QAAS,EAEpD4tB,EAAaH,EAAWhD,YAExBoD,EAA2C,CAC/CvuB,OAAQC,OAAOC,iBACb,OAAa,CACX,MAAM,KAAEC,EAAMO,MAAAA,SAAgB4tB,EAAWjD,OACzC,GAAIlrB,EAEF,YADA8tB,GAASA,EAAMjD,EAAQrD,IAGzBsG,GAASjD,EAAOxiB,KAAK9H,SACfA,CACR,CACF,GAGF,MAAO,CAAEinB,OAAM7mB,OAAQytB,EAAetB,WAAYvsB,EACpD,CMiJ+C8tB,CACzC3P,EAAS5J,KACT+Y,GAGIze,EAAcgb,GAAsB5C,GAEpCnY,EAAcsc,GAAkBmB,EAAY1d,GAClD,MAAO,CACLR,MACAS,cACAjK,KAAM,IAAKonB,EAAOhF,OAAMpY,eACxBzO,SACAge,OAAQ,UACRgP,aAEJ,CAEO,EAiCT,MAAMW,GAAiBzuB,MACrB+O,EACAqd,EACAG,EACAmC,KAEA,MAAMC,QAAsB1C,GAAsBld,GAClD,QAAsB,IAAlB4f,EACF,OAAOA,EAGT,GAAIvC,EAAM,CACRsC,GAAsBA,EAAmB,6BAIzC,aAF0BpC,GAAyBvd,EAAKqd,EAAMG,EAGhE,CAEAmC,GAAsBA,EAAmB,+BAQzC,aAN6BhB,GAC3B3e,EACAqd,EACAG,EAGmB,EAqDjBqC,GAAkB5uB,MACtBosB,EACAxd,KAEA,IAAIG,EAQJ,OAPIqd,IACFrd,QAAYqd,EAAK7pB,IAAIqM,IAGvB,GAAarM,IAAIqM,GAEjBG,SAAc,GAAYxM,IAAIwM,OD7IG/O,OACjC4O,GAEO,IAAIkc,WACU,iBAAZlc,EACH6c,GAAOplB,KAAKuI,SACNA,EAAQigB,eCuIqBC,CAAoBlgB,IACtDG,CAAG,E,gBC5VL,MAAMggB,GAKXjiB,YAAYkiB,EAAyBjV,GACnChN,KAAKiiB,SAAWA,EAChBjiB,KAAKgN,MAAQA,CACf,CAEAkV,cAAcnQ,GACZ,MAAMxL,EAAQvG,KAAKgN,MAAM0Q,QAAQ3L,GACjC,OAAOxL,EAAQvG,KAAKgN,MAAM3Q,OAAS2D,KAAKgN,MAAMzG,EAAQ,QAAK,CAC7D,ECfK,MAAM4b,WAA8Bjf,MACzCnD,YAAYqiB,GACVvR,MAAM,iBAAiBuR,KACvBtrB,OAAOurB,eAAeriB,KAAMmiB,GAAsB9M,UACpD,ECHK,MAAMiN,GACC,eAGP,IAAKC,GAAL,CAAKA,IACVA,EAAA,aAAe,gBADLA,GAAL,CAAKA,IAAA,I,YC8CZ,SAASC,GAA0BnqB,GACjC,OAAQA,EAAK8J,UAAY,IAAM9J,EAAKoqB,kBAAoB,EAC1D,CAEA,MAIMC,GAAa,CACjBC,SAAU,IAAIX,GACZ,CACEte,GAAI,CAAEkf,QAAS,IAAMC,wBAAyB,KAC9CxD,KAAM,CAAEuD,QAAS,IAAWC,wBAAyB,IACrDhF,QAAS,CAAE+E,QAAS,IAAOC,wBAAyB,KAEtD,CAAC,KAAM,OAAQ,YAEjBC,SAAU,IAAId,GACZ,CACEte,GAAI,CAAEkf,QAAS,IAAMC,wBAAyB,KAC9CxD,KAAM,CAAEuD,QAAS,IAAWC,wBAAyB,IACrDhF,QAAS,CAAE+E,QAAS,KAAOC,wBAAyB,KAEtD,CAAC,KAAM,UAAW,SAEpBE,MAAO,IAAIf,GACT,CACEte,GAAI,CAAEkf,QAAS,IAAMC,wBAAyB,KAC9CxD,KAAM,CAAEuD,QAAS,IAAWC,wBAAyB,IACrDhF,QAAS,CAAE+E,QAAS,IAAOC,wBAAyB,KAEtD,CAAC,KAAM,OAAQ,aAganB,OA1ZA,MA+LE9iB,YACE+D,GACA,SACEkf,EAAQ,gBACRC,IAlMJ,KAAQC,OAAS,IAAIngB,EAAA,EAA0B,IAAIC,KAEnD,KAAQqc,UAAgC,EAMxC,KAAQ8D,iBAA2BtqB,KAAKmE,MAExC,KAAQiD,QAAU,IAAI,EAEtB,KAAQyF,UAAmD,CACzDhC,GAAI,IAAIgH,IACR2U,KAAM,IAAI3U,IACVmT,QAAS,IAAInT,KAyLb5G,EAAc3O,WAAWkqB,IACnBA,GACFrf,KAAKojB,QAAQ/D,EACf,IAGFrf,KAAKgjB,SAAWA,GAAYN,GAAWI,SACvC9iB,KAAKijB,gBAAkBA,GAxPD,IA4PtB,EAAAnM,GAAA,GA3P+B,MA4P5BtT,MACC,EAAArJ,GAAA,IACE,MACI6F,KAAKqf,QACL,IAAIrf,KAAKkjB,OAAOvvB,MAAM2K,UAAUrC,MAAMS,GAAmB,SAAbA,EAAEqV,YAGrD5c,WAAU,KAMT6K,KAAKqf,KAAMgE,kBAAiB,EAAK,IAGrC,MAAMzf,GAAiB,EAAAC,EAAA,GAAc,CAACC,IAAgBN,MACpD,EAAAjN,EAAA,IAAI,EAAEyN,OAAoBA,GAAgBA,GAAcsf,aACxD,EAAAjT,GAAA,MACA,EAAAxK,GAAA,MAGFjC,EAAezO,WAAW2J,IACxBA,GAAiB,GAAQhJ,IAAI,6BAC7BkK,KAAKqf,MAAMgE,kBAAiB,EAAK,IAGnCrjB,KAAKkjB,OACF1f,MACC,EAAA+f,GAAA,GAAe3f,IACf,EAAAzJ,GAAA,IAAO,EAAE,CAAE2E,KAAmBA,KAC9B,EAAA0kB,GAAA,GAAaxjB,KAAKijB,kBAClB,EAAA1sB,EAAA,IAAI,EAAE8M,KAAWrD,KAAKyjB,yBAAyBpgB,MAC/C,EAAAoC,GAAA,IAAUpC,IACR,MAAMqgB,EAAY1jB,KAAK2jB,2BAA2BtgB,GAElD,OAAIqgB,EAAUrnB,OAAS,GAErB2D,KAAKqf,MAAMgE,kBAAiB,IAErB,EAAAzQ,GAAA,MAAS8Q,EAAUntB,KAAK8B,GAAS2H,KAAK4jB,WAAWvrB,OAEnD,IAAK,KAGflD,WAAU,EAAGkD,OAAMqD,SAAQqW,SAAQhe,aAClC,MAAM,IAAEiO,GAAQ3J,EACVwrB,EAAY7jB,KAAKkjB,OAAOvvB,MAAMuB,IAAI8M,IAAM6hB,WAAa,GAc3D,GAVAA,EAAUttB,KAAKutB,GAAaA,EAAS9hB,EAAKtG,EAAQqW,EAAQhe,KAG3C,SAAXge,IACF/R,KAAKmjB,iBAAmBtqB,KAAKmE,OAG/BgD,KAAK0F,UAAUqM,GAAQ3M,OAAOpD,GAGf,cAAXtG,GAAqC,cAAXA,EAE5BsE,KAAK+jB,cAAc/hB,OACd,CAIL,MAAMgiB,EAAahkB,KAAKgjB,SAASd,cAAcnQ,GAE3CiS,EACFhkB,KAAKikB,oBAAoB5rB,EAAM2rB,IAE/BhkB,KAAK+jB,cAAc/hB,GAEnB6hB,EAAUttB,KAAKutB,GACbA,EAAS9hB,EAAK,YAAa+P,EAAQhe,KAGzC,CAEAiM,KAAKkkB,aAAa,GAExB,CArRQC,eAAenB,GACrBhjB,KAAKgjB,SAAWA,CAClB,CAEA,cAAqB3D,EAAmB+E,GACtC,GAAQtuB,IACN,sBAAsBkK,KAAKqf,MAAMyB,UAAY,eAC3CzB,EAAKyB,YAGT9gB,KAAKqf,KAAOA,EACZrf,KAAKmkB,eAAeC,GAAkB1B,GAAWrD,EAAKyB,UACxD,CAEQ6C,2BAA2BtgB,GACjC,MAAMwB,EAAe,IAAIxB,EAAM/E,UAAUnE,QACtCuC,GAAmB,YAAbA,EAAEhB,SAGL2oB,EAAkB,QAAW3nB,GAAMA,EAAEqV,QAAQlN,GAE7Cyf,EAA8B,GAEpC,UAAYC,EAAatjB,KAAUnK,OAAO+I,QAAQwkB,GAAkB,CAClE,MAEMG,EAFWxkB,KAAKgjB,SAASf,SAASsC,GAG7B1B,wBACT7iB,KAAK0F,UAAU6e,GAAkCjhB,KAC7CmhB,EAAkBxjB,EACrB1E,MACC,CAACC,EAAGC,IAAM+lB,GAA0B/lB,GAAK+lB,GAA0BhmB,KAEpE/B,MAAM,EAAG+pB,GAEZF,EAAe7oB,QAAQgpB,EACzB,CAEA,OAAOH,CACT,CAEQJ,cACN,MAAMvsB,EAAU,WAAWqI,KAAKkjB,OAAOvvB,MAAM2P,gBAAgBtD,KAAK0F,UAAUhC,GAAGJ,eAAetD,KAAK0F,UAAU2Z,KAAK/b,kBAAkBtD,KAAK0F,UAAUmY,QAAQva,QAC3JtD,KAAKC,QAAQE,kBAAkB,OAAQ,UAAWxI,EACpD,CAEQisB,WAAWvrB,GACjB,MAAM,IAAE2J,EAAG,OAAE+P,EAAM,UAAE8R,EAAS,WAAErE,GAAennB,EAEzC4pB,EAAWjiB,KAAKgjB,SAASf,SAASlQ,GACxC/R,KAAK0F,UAAUqM,GAAQvc,IAAIwM,GAC3BhC,KAAKkkB,cACL,MAAMQ,EAAY1kB,KAAKkjB,OAAOvvB,MAAMuB,IAAI8M,GAWxC,OATAhC,KAAKkjB,OAAOvvB,MAAMmB,IAAIkN,EAAK,IACtB0iB,EACHhpB,OAAQ,YACRipB,cAAe9rB,KAAKmE,MACpBwiB,WAAY,IAAI7P,kBAGlBkU,EAAUttB,KAAKutB,GAAaA,EAAS9hB,EAAK,YAAa+P,MCjKpB6S,EDmKR3xB,SJgE/BA,eACE+O,EACA+P,EACAyE,GAEA,MAAM,KAAE6I,EAAI,WAAEG,EAAU,QAAEoB,GAAYpK,EAEtC,IACE,OAAQzE,GACN,IAAK,KACH,OAAOmN,GAAsBld,GAC/B,IAAK,OACH,OAAOud,GAAyBvd,EAAKqd,EAAMG,GAC7C,IAAK,UACH,OAAOmB,GAA4B3e,EAAKqd,EAAMG,EAAYoB,GAC5D,QACE,OAEN,CAAE,MAAOxK,GAEP,YADA,GAAQtgB,IAAI,6BAA8BsgB,EAE5C,CACF,CIrFayO,CAAiB7iB,EAAK+P,EAAQ,CACnCyN,aACAH,KAAMrf,KAAKqf,KACXuB,QAAS,CACP,CAAC0B,IAA2BC,GAAiBuC,gBAE9CxvB,MAAMuM,IAEHA,GAAsB,OAAXkQ,G7C5IY,CAAClQ,IAClCJ,GAAUC,QAAQ,CAChB7K,KAAM,WAENnD,KAAM,IAAKmO,EAAS9N,YAAQ,IAGnB,E6CsIHgxB,CAAoBljB,GAGfA,KC/KN,IAAI9M,EAAA,GAAeC,IACxB4vB,IACGtvB,MAAMwc,IACL9c,EAASlE,KAAKghB,GACd9c,EAASK,UAAU,IAEpBgP,OAAOjP,IACN,GAAQsrB,MAAM,gCAAiCtrB,GAC/CJ,EAASI,MAAMA,EAAM,GACrB,KDwKDoO,MACD,EAAAof,GAAA,GAAQ,CACNoC,KAAM/C,EAASW,QACfqC,KAAM,KACJ,EAAAC,GAAA,IAAW,KACT1F,GAAYpO,MAAM,WAEX,IAAI+Q,GAAsBF,EAASW,eAGhD,EAAArsB,EAAA,IAAKxC,IACI,CACLsE,OACAqD,OAAQ3H,EAAS,YAAc,QAC/Bge,SACAhe,cAGJ,EAAAoxB,GAAA,IAAY/vB,GAENA,aAAiB+sB,IACZ,EAAAiD,GAAAA,IAAG,CACR/sB,OACAqD,OAAQ,UACRqW,WAIgB,eAAhB3c,GAAOwF,MACF,EAAAwqB,GAAAA,IAAG,CAAE/sB,OAAMqD,OAAQ,YAAaqW,YAElC,EAAAqT,GAAAA,IAAG,CAAE/sB,OAAMqD,OAAQ,QAASqW,cCjNpC,IAAgC6S,CDoNrC,CAQQS,gBAAgBrjB,EAAasjB,GACnC,MAAMjiB,EAAQrD,KAAKkjB,OAAOvvB,MACpB0E,EAAOgL,EAAMnO,IAAI8M,GAKvB,OAJI3J,GACFgL,EAAMvO,IAAIkN,EAAK,IAAK3J,KAASitB,IAGxBtlB,KAAKkjB,OAAOpyB,KAAKuS,EAC1B,CAEQ0gB,cAAc/hB,GACpB,MAAMqB,EAAQrD,KAAKkjB,OAAOvvB,MAC1B0P,EAAM+B,OAAOpD,GACbhC,KAAKkjB,OAAOpyB,KAAKuS,EACnB,CAGQ4gB,oBAAoB5rB,EAAiB2rB,GAC3C3rB,EAAKwrB,UAAUttB,KAAKutB,GAAaA,EAASzrB,EAAK2J,IAAK,UAAWgiB,KAE/DhkB,KAAKqlB,gBAAgBhtB,EAAK2J,IAAK,CAAEtG,OAAQ,UAAWqW,OAAQiS,GAC9D,CAEQP,yBAAyBpgB,GAmB/B,MAlBC,CAAC,OAAQ,WAAmCrM,SAAS+a,IACpD0D,MAAMnc,KAAK0G,KAAK0F,UAAUqM,IAAS/a,SAASgL,IAC1C,MAAM3J,EAAOgL,EAAMnO,IAAI8M,GACnB3J,GAAQmqB,GAA0BnqB,GAAQ,GAAKA,EAAKmnB,aAEtDnnB,EAAKmnB,WAAWpO,MAAM,aACtB/Y,EAAKwrB,UAAUttB,KAAKutB,GAClBA,EAASzrB,EAAK2J,IAAK,UAAW3J,EAAK0Z,UAGrC1O,EAAMvO,IAAIkN,EAAK,IAAK3J,EAAMqD,OAAQ,YAGlCsE,KAAK0F,UAAUqM,GAAQ3M,OAAOpD,GAChC,GACA,IAGGqB,CACT,CAEQkiB,iBAAiBvjB,GAEvBlL,OAAOC,KAAKiJ,KAAK0F,WAAW1O,SAASoe,GACnCpV,KAAK0F,UAAU0P,GAA0BhQ,OAAOpD,IAEpD,CA6GON,QACLM,EACA8hB,EACAtN,EAA4B,CAAC,GAE7B,MAAMnT,EAAQrD,KAAKkjB,OAAOvvB,MACpB6xB,EAAeniB,EAAMnO,IAAI8M,GAK/B,GAAIwjB,EACFxlB,KAAKqlB,gBAAgBrjB,EAAK,CACxB6hB,UAAW,IAAI2B,EAAa3B,UAAWC,SAEpC,CACL,MAAM/R,EAASyE,EAAQiP,eAAiBzlB,KAAKgjB,SAAShW,MAAM,GACtD3U,EAAkB,CACtB2J,MACA6hB,UAAW,CAACC,GACZ/R,SACArW,OAAQ,UACRgqB,gBAAgB,KACblP,GAGLsN,EAAS9hB,EAAK,UAAW+P,GAEzB1O,EAAMvO,IAAIkN,EAAK3J,GACf2H,KAAKkjB,OAAOpyB,KAAKuS,EACnB,CACF,CAEOsiB,eACL3jB,EACAwU,EAA4B,CAAC,GAE7B,OAAO,IAAIljB,SAASC,IAOlByM,KAAK0B,QAAQM,GANK,CAACA,EAAKtG,EAAQqW,EAAQhe,KACvB,cAAX2H,GAAqC,cAAXA,GAC5BnI,EAAQ,CAAEmI,SAAQqW,SAAQhe,UAC5B,GAG0ByiB,EAAQ,GAExC,CAEOoP,uBAAuB5jB,EAAaygB,GACzCziB,KAAKqlB,gBAAgBrjB,EAAK,CAAEygB,oBAC9B,CAEOoD,OAAO7jB,GACZ,MACM3J,EADQ2H,KAAKkjB,OAAOvvB,MACPuB,IAAI8M,GAEnB3J,IAGGA,EAAKmnB,WAGRnnB,EAAKmnB,WAAWpO,MAAM,aAFtBpR,KAAK+jB,cAAc/hB,GAKzB,CAEO8jB,eAAeC,GACpB,MAAM1iB,EAAQrD,KAAKkjB,OAAOvvB,MAE1B0P,EAAMrM,SAAQ,CAACqB,EAAM2J,KACf3J,EAAK0tB,SAAWA,IAClB/lB,KAAKulB,iBAAiBvjB,GACtB3J,EAAKmnB,YAAYpO,MAAM,aACvB/N,EAAM+B,OAAOpD,GACf,IAGFhC,KAAKkjB,OAAOpyB,KAAKuS,EACnB,CAEO2iB,QACL,MAAM3iB,EAAQrD,KAAKkjB,OAAOvvB,MAE1B0P,EAAMrM,SAAQ,CAACqB,EAAM2J,KACnBhC,KAAKulB,iBAAiBvjB,GACtB3J,EAAKmnB,YAAYpO,MAAM,aACvB/N,EAAM+B,OAAOpD,EAAI,IAGnBhC,KAAKkjB,OAAOpyB,KAAK,IAAIkS,IACvB,CAEOijB,cACL,OAAOjmB,KAAKkjB,OAAOvvB,KACrB,CAEOuyB,eACL,OAAOzQ,MAAMnc,KAAK0G,KAAKkjB,OAAOvvB,MAAM2K,SACtC,CAEO6nB,WAOL,OANW,OACT,OAAqB,OAAO,WAC5B,OACA,OAAM,OAAS,CAAC,SAAU,WAGrBC,CAAGpmB,KAAKkmB,eACjB,G,kCE7eK,MAAMG,GAAetlB,GAAc,MAAI7I,MAAM6I,GACvCulB,GAAoBvlB,GAAc,SAASA,I,gBC+GxD,OAnGA,oBACE,KAAS+f,SAAyB,WAIlC,KAAQyF,QAA6B,CAAC,EAMtC,KAAQC,YAAsB,EAJ1B1M,aACF,OAAO9Z,KAAKumB,OACd,CAIIjD,gBACF,OAAOtjB,KAAKwmB,UACd,CAEA,mBACE,MAAM1U,QAAiB9R,KAAKqf,KAAMvF,OAAO5kB,IAAI,qBAC7C,IAAK4c,EACH,MAAO,CAAE2U,WAAYhK,IAEvB,MAAM/iB,GAAU,SAAUoY,GAAoB4U,cAE9C,MAAO,CAAED,WAAY,UAAU/sB,EAAQA,WAAWA,EAAQ1G,OAC5D,CAEAC,WAAWujB,GACTxW,KAAKqf,MAAO,SAAiB7I,GAC7BxW,KAAKumB,cAAgBvmB,KAAK2mB,aAEJ,oBAAXC,SACTA,OAAOvH,KAAOrf,KAAKqf,KACnBuH,OAAOC,MAAQR,IAEjB,GAAQvwB,IACN,2BACOkK,KAAKqf,KAAKyH,MAAMC,cAAcxwB,KAAKiG,GAAMA,EAAEoI,cAEpD5E,KAAKwmB,YAAa,CACpB,CAEAvzB,WAAW+O,EAAawU,EAAwB,CAAC,GAC/C,OAAOxW,KAAKqf,KAAM2H,MAAM1H,KAAKgH,GAAiBtkB,GAAM,IAC/CwU,EACHyQ,WAAW,EACX3jB,MAAM,IACLhO,MAAMvB,IACP,MAAM,KAAE8C,EAAI,KAAEyM,EAAI,UAAEwX,EAAS,MAAEyF,EAAK,OAAE1F,GAAW9mB,EACjD,MAAO,CACL8C,OACAyM,KAAMA,IAAS,EACfwX,UAAWA,IAAc,EACzBD,SACD,GAEL,CAEAsF,IAAIne,EAAawU,EAAsB,CAAC,GACtC,OAAOxW,KAAKqf,KAAMc,IAAIkG,GAAYrkB,GAAMwU,EAC1C,CAEAvjB,UAAU4O,EAAwB2U,EAAwB,CAAC,GACzD,aAAcxW,KAAKqf,KAAM7pB,IAAIqM,EAAS2U,IAAUxU,IAAI4C,UACtD,CAEA3R,UAAU+O,EAAawU,EAAwB,CAAC,GAC9C,aAAcxW,KAAKqf,KAAMmB,IAAIhrB,IAAI6wB,GAAYrkB,GAAMwU,IAAU5R,UAC/D,CAEA3R,iBACE,aAAc+M,KAAKqf,KAAMyH,MAAMI,SAAS3wB,KAAKijB,GAAMA,EAAE2N,KAAKviB,YAC5D,CAEA3R,aAAc,CACdA,cAAe,CAEfA,kBAAkByG,GAChB,MAAMmS,GAAO,SAAUnS,GAIvB,aAHMsG,KAAKqf,KAAM+H,UAAU5xB,IAAIqW,SAEzB7L,KAAKqf,KAAMyH,MAAMO,QAAQxb,IACxB,CACT,CAEAyb,KACE,OAAOtnB,KAAKqf,KAAMmB,IAAI8G,IACxB,CAEAr0B,aACE,MAAM,SAAEs0B,SAAmBvnB,KAAKqf,KAAMO,MAAM4H,OAEtCC,QAAmBznB,KAAKqf,KAAMjmB,MAC9B,aAAEsuB,EAAY,GAAEtuB,GAAOquB,EAC7B,MAAO,CAAEruB,GAAIA,EAAGwL,WAAY8iB,eAAcH,WAC5C,G,wKCvEF,MA+DMI,GAAoC,CACxC7K,WAAY,EACZC,WAAW,GAiLb,OA9KA,oBACE,KAAS+D,SAAyB,QAMlC,KAAQ0F,YAAa,EAJjB1M,aACF,MAAO,CAAE2M,WAAYhK,GACvB,CAII6G,gBACF,OAAOtjB,KAAKwmB,UACd,CAMAvzB,aACE,MAAM20B,EAAa,IAAI,KAAc,kBAC/BA,EAAWC,OAEjB,MAAMC,EAAY,IAAI,KAAa,kBAC7BA,EAAUD,OAEhB,MAOME,OAnGY90B,OACpB60B,EACAE,EAA0B,WAEL,QAAa,CAChCF,YAOAG,WAAY,EACV,WACA,WACA,QAAO,CACLC,iBAAkB,CAChBC,WAAY,CACV,CACEC,KAAM,CACJ,+BACA,mCACA,yBACA,wCACA,6BAGJ,CACEC,WAAY,OACZC,SAAU,OACVF,KAAM,CAAC,yBAA0B,iCAKzC,WACA,QAAsB,CACpBG,eAAgB,KAGpBC,qBAAsB,EAAC,WACvBC,aAAc,EAAC,WACfC,gBAAiB,CACfC,kBAAmB,KACV,GAOXC,cAAe,EACb,QAAU,CACRpxB,KAAMwwB,KAGVa,SAAU,CACRC,UAAU,EAAAA,GAAA,SA0CSC,CAAcjB,EAPb,CACpB,kFACA,kFACA,kFACA,kFACA,+FAIF9nB,KAAKqf,WAAa,QAAY,CAAEuI,aAAYE,YAAWC,WAEvD/nB,KAAKgpB,IAAK,SAAOhpB,KAAKqf,MAEA,oBAAXuH,SACTA,OAAOmB,OAASA,EAChBnB,OAAOvH,KAAOrf,KAAKqf,KACnBuH,OAAOoC,GAAKhpB,KAAKgpB,GACjBpC,OAAOC,MAAQR,IAIjB0B,EAAOkB,iBAAiB,gBAAiBC,IACvC,MAAMC,EAASD,EAAIE,OAAOxkB,WACpBykB,EAAOtB,EAAOuB,eAAeH,IAAW,GACxCI,EAAmBzyB,OAAO0yB,YAC9BH,EAAK9yB,KAAKijB,GAAM,CACdA,EAAEiQ,WAAW7kB,WACb4U,EAAEiQ,WAAWC,aAAanzB,KAAKia,IAAM,SAAUA,IAAI5V,WAGvD,GAAQ8lB,MAAM,gBAAgByI,IAAUI,EAAiB,IAe3DxB,EAAOkB,iBAAiB,mBAAoBC,IAC1C,GAAQxI,MAAM,qBAAqBwI,EAAIE,OAAOxkB,aAAa,IAE7D,GAAQ9O,IACN,qBACAiyB,EAAO4B,gBAAgBpzB,KAAKiG,GAAMA,EAAEoI,cAStC5E,KAAKwmB,YAAa,CACpB,CAEAvzB,WAAW+O,EAAawU,EAAwB,CAAC,GAC/C,OAAOxW,KAAKgpB,GAAI1J,KAAK+G,GAAYrkB,GAAMwU,GAASlhB,MAAMvB,IACpD,MAAM,KAAE8C,EAAI,SAAE+yB,EAAQ,cAAEC,EAAa,OAAEhP,EAAM,QAAEiP,EAAO,MAAEC,GAAUh2B,EAClE,MAAO,CACL8C,OACAyM,KAAMsmB,IAAa,EACnB9O,UAAW+O,IAAkB,EAC7BhP,SACD,GAEL,CAEAsF,IAAIne,EAAawU,EAAsB,CAAC,GACtC,OAAOxW,KAAKgpB,GAAI7I,IAAIkG,GAAYrkB,GAAMwU,EACxC,CAEAvjB,UAAU4O,EAAwB2U,EAAwB,CAAC,GAEzD,MAAMwT,EAAY,IACbxT,KACAmR,IAGL,IAAI3lB,EAEJ,GAAIH,aAAmBgb,KAAM,CAC3B,MAAMoN,EAAWpoB,EAAQjH,KACnBknB,QAAoBjgB,EAAQigB,cAC5BpuB,EAAO,IAAIqqB,WAAW+D,GAC5B9f,QAAYhC,KAAKgpB,GAAIkB,QACnB,CAAEC,KAAMF,EAAUpoB,QAASnO,GAC3Bs2B,EAEJ,KAAO,CACL,MAAMt2B,GAAO,IAAI02B,aAAcC,OAAOxoB,GACtCG,QAAYhC,KAAKgpB,GAAIsB,SAAS52B,EAAMs2B,EACtC,CAGA,OADAhqB,KAAKwgB,IAAIxe,EAAI4C,WAAY4R,GAClBxU,EAAI4C,UACb,CAEA3R,UAAU+O,EAAawU,EAAwB,CAAC,GAC9C,MAAM+T,EAAOlE,GAAYrkB,GAEzB,UADuBhC,KAAKqf,MAAMmL,KAAKC,SAASF,EAAM/T,IACvC,QAELxW,KAAKqf,MAAMmL,KAAKh1B,IAAI+0B,EAAM/T,MAC/BxU,IAAI4C,UAET,CAGF,CAEA3R,iBACE,OAAO+M,KAAKqf,KAAM0I,OAAQuB,iBAAiB/yB,KAAKijB,GAC9CA,EAAEkR,WAAW9lB,YAEjB,CAEA3R,mBACQ+M,KAAKqf,MAAMsL,OACnB,CAEA13B,oBACQ+M,KAAKqf,MAAMha,QACnB,CAEApS,kBAAkByG,SACGsG,KAAKqf,KAAM0I,OAAQ6C,MAAK,SAAUlxB,IACrD,OAAO,CACT,CAEA4tB,KACE,MAAMvzB,EA/OVd,gBACE43B,GAGA,gBAAiBxyB,KAAQwyB,EAAU,CACjC,MAAM,IAAE7oB,EAAG,SAAE8oB,GAAazyB,OACpB,CAAE2J,IAAKA,EAAI+oB,OAAQD,WAAUj0B,KAAM,YAC3C,CACF,CAuOmBm0B,CAAchrB,KAAKqf,KAAMmL,KAAKlD,MAC7C,OAAOvzB,CACT,CAEAd,aAIE,MAAO,CAAEmG,GAHE4G,KAAKqf,KAAM0I,OAAOoB,OAAOvkB,WAGvB8iB,aAFQ1nB,KAAKqf,KAAM0I,OAAQc,SAAUC,SAAUmC,KACzDvD,aACwBH,UAAW,EACxC,G,wBC1LF,OAtFmB,MACjBliB,OAAO,EACPmiB,KAAM,qBACN0D,MAAO,CACLC,SAAS,EACTC,IAAK,CACHD,SAAS,IAGbE,QAAS,CACPF,SAAS,GAEXrR,OAAQ,CACNwR,IAAK,CACHC,YAAa,CACX,+BAAgC,CAAC,MAAO,QACxC,8BAA+B,CAC7B,wBACA,wBACA,wBACA,2BAINC,UAAW,CACTC,QAAS,0BACTC,MAAO,GAKPC,UAAW,IAMbC,UAAW,CACTC,KAAM,CACJC,SAAS,EACTC,SAAU,IAEZC,WAAY,CACVF,SAAS,IAGbG,UAAW,GAQXC,OAAQ,CACNJ,SAAS,GAEXJ,MAAO,CACLS,QAAS,CACPC,UAAW,IACXC,SAAU,IAEZC,mBAAmB,GAErBC,QAAS,CACPC,KAAM,cAGVzE,OAAQ,CACNE,WAAY,EAIV,QAAW,CACT9tB,OAAQsyB,GAAA,MAGZC,IAAK,CACHvB,SAAS,IAGbwB,aAAc,CACZC,YAAY,KCWhB,OAnFA,oBACE,KAAS9L,SAAyB,WAMlC,KAAQ0F,YAAa,EAJjB1M,aACF,MAAO,CAAE2M,WAAYhK,GACvB,CAII6G,gBACF,OAAOtjB,KAAKwmB,UACd,CAIAvzB,aACE+M,KAAKqf,WAAa,SAAmB,MACf,oBAAXuH,SACTA,OAAOvH,KAAOrf,KAAKqf,KACnBuH,OAAOC,MAAQR,IAGjBrmB,KAAKwmB,YAAa,CACpB,CAEAvzB,WAAW+O,EAAawU,EAAwB,CAAC,GAC/C,OAAOxW,KAAKqf,KAAM2H,MAAM1H,KAAKgH,GAAiBtkB,GAAM,IAC/CwU,EACHyQ,WAAW,EACX3jB,MAAM,IACLhO,MAAMvB,IACP,MAAM,KAAE8C,EAAI,KAAEyM,EAAI,UAAEwX,EAAS,MAAEyF,EAAK,OAAE1F,GAAW9mB,EACjD,MAAO,CACL8C,OACAyM,KAAMA,IAAS,EACfwX,UAAWA,IAAc,EACzBD,SACD,GAEL,CAEAsF,IAAIne,EAAawU,EAAsB,CAAC,GACtC,OAAOxW,KAAKqf,KAAMc,IAAIkG,GAAYrkB,GAAMwU,EAC1C,CAEAvjB,UAAU4O,EAAwB2U,EAAwB,CAAC,GACzD,aAAcxW,KAAKqf,KAAM7pB,IAAIqM,EAAS2U,IAAUxU,IAAI4C,UACtD,CAEA3R,UAAU+O,EAAawU,EAAwB,CAAC,GAC9C,aAAcxW,KAAKqf,KAAMmB,IAAIhrB,IAAI6wB,GAAYrkB,GAAMwU,IAAU5R,UAC/D,CAEA3R,iBACE,aAAc+M,KAAKqf,KAAMyH,MAAMI,SAAS3wB,KAAKijB,GAAMA,EAAE2N,KAAKviB,YAC5D,CAEA3R,aAAc,CACdA,cAAe,CAEfA,kBAAkByG,GAChB,MAAMmS,GAAO,SAAUnS,GAIvB,aAHMsG,KAAKqf,KAAM+H,UAAU5xB,IAAIqW,SAEzB7L,KAAKqf,KAAMyH,MAAMO,QAAQxb,IACxB,CACT,CAEAyb,KACE,OAAOtnB,KAAKqf,KAAMmB,IAAI8G,IACxB,CAEAr0B,aACE,MAAM6e,QAAiB9R,KAAKqf,KAAMO,MAAM4H,OAClCD,EAAWsF,OAAO/a,EAASyV,UAE3BE,QAAmBznB,KAAKqf,KAAMjmB,MAC9B,aAAEsuB,EAAY,GAAEtuB,GAAOquB,EAC7B,MAAO,CAAEruB,GAAIA,EAAGwL,WAAY8iB,eAAcH,WAC5C,G,YCjFF,MAAMuF,GAAyD,CAC7D/J,MAAO,GACPD,SAAU,GACVH,SAAU,IAIL1vB,eAAe85B,GACpBvW,GAEA,MAAM,aAAEwW,KAAiBC,GAAgBzW,EASnC0W,ECvBR,SACEC,EACA3W,GAEA,OAAO,cAA+B2W,EACpCl6B,uBACE+O,EACAorB,EACA1d,GAEA,MAAM7N,QAAgB6f,GAAe1f,EAAKhC,KAAM0P,GAE1C2d,QAAgB1P,GAAwB9b,EAASG,GACvD,OAAQorB,EAEJC,GAASx2B,OAASu2B,EAClBC,OACA,EAHAA,CAIN,CAEAp6B,iBAAiB4O,GACf,OAAOggB,GAAgB7hB,KAAM6B,EAC/B,CAEA5O,2BAEE,eADoB4d,MAAMyc,YACXrxB,MAAMktB,GAAWA,IAAW3S,EAAQ+W,aACrD,CAEAt6B,uBAAuBu6B,GAAS,SACGxtB,KAAKytB,uBACXD,GAMzB3c,MACG6c,YAAYlX,EAAQmX,kBACpBr4B,MAAK,KACJ,GAAQQ,IAAI,2BAA2B0gB,EAAQmX,qBACxC,KAERtpB,OAAOM,IACN,GAAQ7O,IACN,0BAA0B0gB,EAAQmX,qBAAqBhpB,EAAIxJ,YAEtD,IAGf,EAEJ,CD7BwByyB,CAAgBd,GAAaE,GAAe,CAChEO,YARkBjR,GASlBqR,iBANiB,aAAjBX,EACIxQ,GACAD,KAOAsR,EAAW,IAAIX,EAQrB,aANMW,EAASC,KAAK,CAAEplB,IAAKukB,EAAYc,gBAKjCF,EAASxK,mBACRwK,CACT,C,wDE5BA,OAAIG,kBAAmB,EAMvB,MAAMC,GAA4C,CAChDC,iBAAkB,CAChBtzB,KAAM,qBACNuzB,MAAO,4BAiGEC,GAAc,CACzB7qB,EACA8qB,KAEA,MAAMC,EAAoB,IAAIvd,EAAA,EACxB5N,EA9CoB,EAC1BI,EACA+qB,KAEA,MAAMC,EAAgB,IAAIC,GAAA,EAAc,GAiCxC,OA/BA,EAAA3qB,EAAA,GAAc,CAACN,EAAa+qB,IAAoBn5B,WAC9C,EAAE4O,EAAYmqB,MACZ,GAAInqB,GAAcmqB,EAAkB,CAClC,MAAMzpB,EAAkBxR,MAAOqR,UACR4pB,EAAiB5pB,EAAM,CAC1CmqB,QAAS,OACTC,WAAW,KAGCh7B,KAGVi7B,EAAoB17B,MAAOqR,EAAcqP,KAC7C,MAAMnP,QAAYC,EAAgBH,GAMlC,aAHmBP,EAAW4qB,kBAAkBnqB,EAAKmP,EAG1C,EAGPib,EAAM,CACVnqB,kBACAkqB,qBAEFJ,EAAcz9B,MAAK,QAAM89B,GAC3B,KAIGL,CAAa,EASEM,CAAoBtrB,EAAa+qB,GAEjDQ,EAAuB77B,MAAO87B,IAClC,MAAM,KAAEn0B,EAAI,MAAEuzB,GAAUF,GAAWc,GAE7BC,OA/FW,EACnBp0B,EACAuzB,EACAvQ,KAEO,UAAShjB,EAAMuzB,EAAO,CAC3Bc,kBAAoBC,IAClB,IACE,MAAM,OACJxzB,EAAM,SACNoF,EAAQ,OAERquB,EAAM,MACNt3B,GACEq3B,EAEE/zB,EAAUg0B,EAAS,GAAGhB,OAAWgB,KAAUt3B,UAAgBs2B,EAE3D/6B,EAAO,CAAC,QAAS,SAASsH,MAAMqG,GAAMA,IAAMrF,IAE5C0zB,EAAmB,CACvB1zB,SACAP,UACA/H,OACA0N,SAAUA,EAAWmI,KAAKmG,MAAMtO,GAAY,GAI9C8c,EAAWwR,EACb,CAAE,MAAOhZ,GACP,GAAQtgB,IAAI,yBAA0Bq4B,EAAO/X,EAAExR,WACjD,KAgEqByqB,CAAaz0B,EAAMuzB,GAAQz6B,GAChD26B,EAAa/tB,wBAAwByuB,EAAOr7B,KAEjC,uBAATkH,GACF0zB,EAAkBx9B,KAAKk+B,GAEzB,GAAQl5B,IAAI,GAAGi5B,aAAiB,EAG5BjB,EAAO76B,UACXo7B,EAAaluB,kBAAkB,KAAM,YACrC,GAAQmvB,KAAK,qBAENh8B,QAAQwR,IAAI,CACjBgqB,EAAqB,sBAIpBx5B,MAAMvB,IACLiV,YAAW,IAAMqlB,EAAaluB,kBAAkB,KAAM,YAAY,GAClE,GAAQovB,QAAQ,qBAETx7B,KAERsQ,OAAO+R,GACNiY,EAAaluB,kBAAkB,KAAM,QAASiW,EAAExR,eAMtD,OAFAkpB,IAEO,CAAE3qB,gBAAe2qB,OAAM,E,8oBC/GhC,MAAM0B,GAAgB,CACpBC,OAAQ,IACRC,cAAc,EACdC,cAAc,EACdnZ,QAAS,IAGLoZ,GAAwC,CAC5CC,UAAU,EACVC,SAAS,EACTC,SAAU,OACVC,WAAY,CAAC,EACbzuB,OAAQ,CAAC,EACT0uB,M,mvIACAC,OAAQ,IA0QV,MAAMC,GA9PN,WACE,IAAIC,EAA0C,CAAC,EAC3CC,EAAyB,CAAE9uB,OAAQ,CAAC,EAAG+uB,KAAM,CAAC,EAAGC,QAAS,CAAC,GAC/D,MAAM3sB,EAAiB,IAAIb,EAAA,GAAyB,GAC9CytB,EAAe,IAAIztB,EAAA,EAA4C,CAAC,GAEhE0tB,EAAkB,IAAIztB,IAEtB0tB,EAAqB,IAAIlC,GAAA,EAAc,IAC7C,EAAA3qB,EAAA,GAAc,CAACD,EAAgB4sB,IAC5BhtB,MACC,EAAAjN,EAAA,IACE,EAAEuI,EAAesxB,QACZtxB,IAAiBsxB,EAAY13B,aAEpC,EAAA2X,GAAA,MAEDlb,WAAWqb,IACVkgB,EAAmB5/B,KAAK0f,EAAE,IAG9BggB,EAAar7B,WAAWqb,IACtB4f,EAAc5f,CAAC,IAGjB,MAgCMmgB,EAAM19B,MACVi9B,EACAU,EACA9M,EACA+M,EAAkC,CAAC,KAEnC,MAAMC,GAAQ,UAASlsB,WAEvBkf,GAAY2M,EAAgB37B,IAAIg8B,EAAOhN,GACvC,MAIMiN,EAAiB,IAClBnB,MACAgB,EACHX,MAAOC,EACPW,QAAS,IAAKA,EAASG,QAAS,IAChCzvB,OATmB,CACnB0vB,IAAKZ,EACLS,UAUII,QAAmB,SAAQH,EAAgBvB,KAG3C,OAAEz7B,EAAM,MAAEqB,GAAU87B,EAE1B,IAGE,OAFAT,EAAgBrrB,OAAO0rB,GAEhB,IACF7b,GAAYic,GACf97B,QACArB,OAAQA,EACO,OAAXA,EACE,GACA0C,KAAKyB,OnD9IiBiO,EmD8ISpS,EnD7IpCoS,EAAO2O,QAAQ1O,GAAoB,MmD8IhC,CAAEnL,OAAQ,QAASE,QAAS,aAEpC,CAAE,MAAOib,GASP,OARAqa,EAAgBrrB,OAAO0rB,GAEvB,GAAQh7B,IACN,kBAAkBi7B,EAAehB,WACjC3Z,EACA8a,EACAH,GAEK,CACLI,kBAAmB,0BAA0B/a,OAC1C8a,EACHn9B,OAAQ,CAAEkH,OAAQ,QAASE,QAASib,GAAGxR,YAAc,iBAEzD,CnD/JG,IAA6BuB,CmD+JhC,EAGIirB,EAA4B,KAEhC,IAAKhB,EAAY13B,SACf,MAAO,CAAC,QAAS,IAGnB,MAAM,OAAEw3B,EAAM,QAAE/E,GAAYiF,EAAY13B,SAExC,OAAKyyB,EAIE,CAAC,SAAU+E,GAHT,CAAC,OAAQ,GAGO,EA+G3B,MAAO,CACLpC,KAnNW76B,UACX,GAAQq8B,KAAK,6BACP,WAEN,GAAQC,QAAQ,uBAChB3rB,EAAe9S,MAAK,EAAK,EA+MzB4/B,qBACAC,MAEAU,aAnDmBp+B,MACnB+O,EACAQ,EACAX,EACAiiB,KAEA,MAAOwN,EAAYpB,GAAUkB,IAC7B,GAAmB,UAAfE,EACF,MAAO,CACLr2B,OAAQ,QACRs2B,UAAW,CAAC,CAAC,CAAE16B,KAAM,OAAQyN,KAAM,6BAIvC,GAAmB,SAAfgtB,EACF,MAAO,CAAEr2B,OAAQ,OAAQs2B,UAAW,IAGtC,MAAMC,QAAeb,EACnBT,EACA,CACEH,SAAU,gBACVC,WAAY,CAAChuB,EAAKQ,EAAaX,IAEjCiiB,GAGF,MAA6B,UAAzB0N,EAAOz9B,OAAOkH,QAChB,GAAQ7F,MAAM,wBAAyBo8B,GAChC,CACLv2B,OAAQ,QACRs2B,UAAW,CAAC,CAAC,CAAE16B,KAAM,OAAQyN,KAAMktB,EAAOp8B,WAIvC,CAAE6F,OAAQ,SAAUs2B,UAAWC,EAAOz9B,OAAO8N,QAAS,EAiB7D4vB,kBAlHwBx+B,MACxBsO,IAEA,MAAO+vB,EAAYpB,GAAUkB,IAE7B,GAAmB,UAAfE,EACF,MAAO,CAAEr2B,OAAQ,QAASE,QAAS,0BAGrC,GAAmB,WAAfm2B,EACF,MAAO,CAAEr2B,OAAQ,QAGnB,MAAM,IAAE+G,EAAG,YAAEQ,EAAW,QAAEX,GAAYN,EAChCiwB,QAAeb,EAAIT,EAAQ,CAC/BH,SAAU,qBACVC,WAAY,CAAChuB,EAAKQ,EAAaX,MAE3B,OAAE5G,EAAQ4G,QAAS6vB,GAAkBF,EAAOz9B,OAUlD,MARe,UAAXkH,GACF,GAAQ7F,MACN,kCAAkCmM,EAAOS,MACzCT,EACAiwB,GAIAE,EACK,IAAKF,EAAOz9B,OAAQ8N,QAAS6vB,GAG/BF,EAAOz9B,MAAM,EAmFpB49B,eAjMsBC,IACtBxB,GAAc,WACX5f,IAAM,IAAMA,EAAG0f,QAAQ,SAAkB1f,EAAE0f,WAC5C0B,GAEFpB,EAAa1/B,KAAKs/B,EAAY,EA6L9ByB,YAlNkB,CAClBj3B,EACAjH,KAGA08B,EAAQz1B,GAAQjH,CAAK,EA8MrBm+B,WA3MkBC,IAClB,MAAMC,EAAa3B,EACnB0B,EAAM/6B,SAAS4D,IACbo3B,EAAWp3B,GAAQ,CAAC,CAAC,IAEvBy1B,EAAU2B,CAAU,EAuMpBC,gBAnFsBh/B,MACtBi9B,EACAH,EACAC,KAEA,GAAQl6B,IAAI,4BAA6Bi6B,EAAUC,GAOnD,aANqBW,EAAIT,EAAQ,CAC/BH,WACAC,aACAH,UAAU,KAGE97B,MAAM,EAwEpBm+B,gBAnBsBj/B,MAAO69B,EAAep9B,KAC5C,MAAMowB,EAAW2M,EAAgBv7B,IAAI47B,GAEjChN,SACIA,EAASpwB,EACjB,EAeAy+B,SAAU,KAAM,CACd9B,UACAD,gBAGN,CAEqBgC,GAIrB,U,WC1UO,MC2EDC,GA9D4B,MAChC,MAAMhE,EAAe,IAAI,EAEnB9qB,EAAc,IAAIwN,EAAA,EAIlBjB,EAAU,IAAI/M,EAAA,EAAmC,CACrDsJ,UAAW,QAGP,cAAElJ,GAAkBirB,GAAY7qB,EAAa8qB,IAE7C,aAAEiE,EAAY,KAAEC,GD1BK,EAC3BpvB,EACAI,EACA8qB,KAEA,MAAMiE,EAAgB1vB,GACpB4vB,GAAA,EAASF,aAAa1vB,GAyBxB,OAvBAO,EAAchO,WAAWiO,IACvBkvB,EAAa,CAAElvB,gBAAe,IAGhCG,EAAYpO,WAAWgkB,IACrBmZ,EAAa,CAAEnZ,SAAQ,IAGzB,GAAKuX,mBAAmBv7B,WAAWxB,IACjCA,EACIqV,YAAW,IAAMqlB,EAAaluB,kBAAkB,OAAQ,YAAY,GACpEkuB,EAAaluB,kBAAkB,OAAQ,WAAW,IAG3ClN,WACXo7B,EAAaluB,kBAAkB,OAAQ,kBAEjC,GAAK2tB,OACXwE,EAAa,CAAEC,KAAI,IAAG,EAGxBzE,GAEO,CAAEyE,KAAI,GAAED,eAAclhB,MAAOohB,GAAA,EAASphB,MAAO,ECLrBqhB,CAC7BtvB,EACAI,EACA8qB,IAGI,UACJqE,EAAS,cACT5uB,EACA8qB,IAAK+D,GCzBoB,EAC3BJ,EACAlE,KAEA,MAAMvqB,EAAgB,IAAIf,EAAA,OAAyC,GAC7D2vB,EAAY,IAAI,GAAa5uB,EAAe,CAChDyuB,SAsCI3D,EAAM,CACVvpB,MA3BgBpS,MAAO2/B,IACvB,IAEE,GADiB9uB,EAAcI,WAI7B,OADA8E,YAAW,IAAMqlB,EAAaluB,kBAAkB,OAAQ,YAAY,GAC7D7M,QAAQC,UAGjB86B,EAAaluB,kBAAkB,OAAQ,YACvC,GAAQmvB,KAAK,uBAEb,MAAMuD,QAAoB9F,GAAa6F,GAKvC,OAJA,GAAQrD,QAAQ,uBAEhBzrB,EAAchT,KAAK+hC,GACnB7pB,YAAW,IAAMqlB,EAAaluB,kBAAkB,OAAQ,YAAY,IAC7D,CACT,CAAE,MAAOwE,GACP,GAAQ7O,IAAI,4BAA6B6O,GACzC,MAAMhK,EAAMgK,aAAezB,MAAQyB,EAAIxJ,QAAWwJ,EAElD,MADA0pB,EAAaluB,kBAAkB,OAAQ,QAASxF,GAC1CuI,MAAMvI,EACd,GAKAgwB,KAtCe13B,UACf,MAAM6/B,EAAWhvB,EAAcI,WAE3B4uB,SACIA,EAASnI,OAEjB7mB,EAAchT,UAAK,GACnBu9B,EAAaluB,kBAAkB,OAAQ,WAAW,EAgClD2Z,OAAQ7mB,SAAY6Q,EAAcI,YAAY4V,OAC9C9J,KAAM/c,SAAY6Q,EAAcI,YAAY8L,OAC5C+iB,iBAAkB9/B,MAChB+O,EACAorB,EACA5N,KAEA,MAAMsT,EAAWhvB,EAAcI,WAC/B,IAAK4uB,EACH,MAAM,IAAI5vB,MAAM,6BAElB,OAAO4vB,EAASC,iBAAiB/wB,EAAKorB,EAAS5N,EAAW,EAE5D9d,QAASzO,MACP+O,EACA8hB,EACAtN,IACGkc,EAAUhxB,QAAQM,EAAK8hB,EAAUtN,GACtCmP,eAAgB1yB,MAAO+O,EAAawU,IAClCkc,EAAW/M,eAAe3jB,EAAKwU,GACjCwc,QAAS//B,MAAO+O,GAAgB0wB,EAAU7M,OAAO7jB,GACjDixB,gBAAiBhgC,MAAO8yB,GAAmB2M,EAAU5M,eAAeC,GACpEmN,WAAYjgC,SAAYy/B,EAAU1M,QAClCmN,WAAYlgC,MAAO4O,GACjBiC,EAAcI,YAAYivB,WAAWtxB,IAGzC,MAAO,CAAEiC,gBAAe4uB,YAAW9D,MAAK,EDhDpCwE,CAAcb,EAAMlE,GAOlBgF,EAAc,CAClBpwB,uBAN6B,CAC7BjB,EACAG,EAA0BrP,EAAcsP,SACrCswB,EAAU/M,eAAe3jB,EAAK,CAAE0jB,gBAAgB,EAAOvjB,aAI1DoB,cACAO,gBACAX,gBACA2M,WAIkB,IAAI0L,GAAY6X,GAKpC,OAFAf,EAAa,CAAEK,YAER,CACLW,SAxCgB5vB,GAAcH,EAAYzS,KAAK4S,GAyC/C6vB,kBAAmB,MAAQzvB,EAAcI,WAEzCyuB,SAAS,QAAMA,GACfJ,MAAM,QAAMA,GACZpvB,gBAEAuvB,WAAW,QAAMA,GACjBc,YACE5wB,GACG0vB,EAAa1vB,GAElB6wB,UAAYlyB,GACVuO,EAAQhf,KAAK,IAAKgf,EAAQnc,SAAU4N,IACvC,EAGsBmyB,GnE4DlB,IAA4B99B,GAAoBg5B,GAApBh5B,GmEvDnB3E,KnEuDuC29B,GmEvDjCyD,GnEwDpBx9B,SACgC,IAArBe,GAAO+9B,UAChB/9B,GAAO+9B,UAAavd,IAClB,MAAMpjB,EAAOojB,EAAEwd,MAAM,GACrBj+B,EAAgB3C,IAEhB,OAAO47B,GAAK57B,EAAK,GAInB,OAAO47B,G,qCqE1GJ,SAASiF,EAAkB9Y,GAChC,MAAM,OAAEmV,EAAQnV,SAAU+Y,EAAE,QAAEC,GA3BzB,SAA4BhZ,GAEjC,MAAMiZ,EAAY,2BAElB,IAAI18B,EACA28B,EAAa,GACbC,EAAmBnZ,EACnBgZ,GAAU,EAEd,KAA8C,QAAtCz8B,EAAQ08B,EAAUG,KAAKpZ,KAC7BgZ,GAAU,EAEVE,GAAc38B,EAAM,GAAK,KAGzB48B,EAAmBA,EAAiBpf,QAAQxd,EAAM,GAAI,IAIxD,MAAO,CACL44B,OAAQ+D,EAAWrV,OACnB7D,SAAUmZ,EACVH,UAEJ,CAG4CK,CAAmBrZ,GAE7D,OAAOgZ,EAAU7D,EAAS4D,CAC5B,C,4SC9C2B,qBAAyC,GAuCpE,I,qCCnCA,MAaM,EACJ,qEA6HIO,EAA6BphC,MAAOqhC,EAAQ9iB,KAChD,IACE,MAAMM,QC9IH7e,eAA2Bue,GAQhC,aAPuB,IAAMtc,IAC3B,GACEjD,EAAA,EAAgBC,QAAQb,qCACI,YAAmC,IAAAkjC,WAC/D,IAAAC,SAAQ/9B,KAAKC,UAAU8a,SAGX9d,KAAKA,IACvB,CDqI2B+gC,CAAYjjB,GAKnC,OAAOM,CACT,CAAE,MAAO1c,GAEP,OADA,EAAQU,IAAI,QAASV,GACd,IACT,G,4BE9JK,MAAMs/B,UAAgCxxB,MAG3CnD,YAAY+R,GACV,IAAI3W,EAAU,GACVw5B,GAAQ,EACR7iB,aAAoB2D,MACtBta,EAAU2W,EAASvG,KAAK,QACfuG,EAAS8iB,QAClBz5B,EAAU2W,EAAS8iB,OAAOhwB,WAC1B+vB,EAAO7iB,EAAS6iB,MAEhBx5B,EAAUA,GAAS/F,MAGrByb,MAAM1V,GACN05B,EAAOz/B,MAAM+F,EAAS,CAAE/F,MAAO0c,IAE/B9R,KAAK20B,KAAOA,CACd,E,2BCLF,MAAMG,EAAa,CACjBhpB,OAAQ,GACRipB,IAAK,KAAmBnwB,YAGbowB,EAAgB/hC,MAC3B2G,EACAN,EACAb,GAEEsB,WACAk7B,iBAKFC,EAAcJ,KAEd,MACM/gC,EDX4B,CAClC+d,IAGA,GADwBA,aAAoB2D,OAA2B,IAAlB3D,EAAS6iB,KAE5D,MAAM,IAAID,EAAwB5iB,GAEpC,OAAOA,CAAQ,ECIA,OADQmjB,EAAeE,UAAUv7B,EAAQN,EAAMb,EAAIy8B,KAG5D,gBAAEn8B,GAAoBhF,EACtB4W,EAAO,CACXrR,OACAb,KACAM,kBACAH,WAAW,UACXgB,UAOF,aAHMG,GAAUq7B,aAAazqB,UACvB5Q,GAAUs7B,kBAAkB1qB,IAE3B5R,CAAe,E,eChBxB,MAwJMy5B,EAxJiB,MACrB,MAAM8C,EAA0C,CAE9C3C,QAAS,IAAI5vB,EAAA,OAA0C,GACvDwvB,KAAM,IAAIxvB,EAAA,OAAuC,GACjDwyB,YAAa,IAAIxyB,EAAA,OAA8C,GAC/DK,aAAc,IAAIL,EAAA,OAAsC,GACxDkyB,cAAe,IAAIlyB,EAAA,OACjB,GAEFhJ,SAAU,IAAIgJ,EAAA,OAA2C,GACzDrJ,QAAS,IAAIqJ,EAAA,OAA0C,GACvDoW,MAAO,IAAIpW,EAAA,OAAwC,IAGrD,IAAI2M,EAEJ,MAAM8lB,EACJ56B,GAEO,IAAItH,SAASC,IAClB,MAAMkiC,EAAQH,EAAY16B,GAGtB66B,EAAMvxB,YACR3Q,EAAQkiC,EAAMvxB,YAGhBuxB,EACGjyB,MACC,EAAAC,EAAA,IAAO9P,KAAYA,KAGpBwB,WAAWxB,IACVJ,EAAQI,EAAM,GACd,IAIR,EAAA+hC,YAAYrO,QAAQ,MAAS/xB,MAAMg/B,IACjCgB,EAAYC,aAAazkC,KAAKwjC,EAAO,IAGvC,MAmBMqB,EAAoB1iC,MAAO+O,UACRwzB,EAAmB,YAC3BzC,iBAAiB/wB,EAAK,QA2CjC4zB,EAAS,CACbC,sBAV4B,KAC5BnmB,EAAkB,IAAIC,gBACfD,GASPomB,YAzDkB7iC,MAAOue,EAAeukB,EAAO,KAC/C,MAAMR,QAAqBC,EACzB,eAGIQ,OCsDoB/iC,OAAOue,GACnCA,EAAMla,MAAM,MAAqBka,GAAQ,OAAwBA,ECyJhCsD,QAAQ,OAAQ,MFhNrBmhB,CAAezkB,GAEzC,MCuDwBve,OAC1BqhC,EACAt7B,EACA+8B,KAEA,IAGE,aAFsBzB,EAAO4B,OAAOl9B,EAAM+8B,EAG5C,CAAE,MAAO3gC,GAGP,YADA,EAAQA,MAAMA,EAEhB,GDpES+gC,CAAaZ,EAAaS,EAAaD,EAAK,EAmDnDZ,UAAWliC,MAAOqG,EAAcb,KAC9B,MAAMiB,EAAU47B,EAAY57B,QAAQwK,WACpC,IAAKxK,EACH,MAAM,IAAIwJ,MAAM,6BAElB,MAAMnJ,QAAkBy7B,EAAmB,YACrCP,QAAuBO,EAC3B,iBAGF,OAAOR,EAAct7B,EAASJ,EAAMb,EAAI,CACtCsB,WACAk7B,iBACA,EAEJmB,sBAAuBnjC,MAAOojC,UACFb,EAAmB,eAA7C,MACMc,OJsFkBrjC,OAAOqhC,EAAQ+B,KAC3C,IACE,MAAM7kB,EAAQ,CACZ+kB,qBAAsB,CACpBF,aAIJ,aADuBhC,EAA2BC,EAAQ9iB,EAE5D,CAAE,MAAOpc,GAEP,OADA,EAAQU,IAAI,QAASV,GACd,IACT,GIlG2BghC,CAAsBb,EAAac,GAE1D,OAAOC,CAAQ,EAEjBE,iBAAkBvjC,MAAOqR,EAAcqP,EAAQ,MAC7C,MAAMvQ,QAAsBoyB,EAC1B,gBAIF,aAFMA,EAAmB,SAElBpyB,EAAaurB,kBAAkBrqB,EAAMqP,EAAM,EAEpD8iB,mBAxEyBxjC,MACzB+O,EACA+tB,EACAxuB,EAAS,CAAC,KAEV,IACE,MAAMxN,QAAe4hC,EAAkB3zB,GACvC,QAAwB,IAApBjO,GAAQ8N,QACV,MAAO,CAAE5G,OAAQ,QAASE,QAAS,sBAIrC,MAAMu7B,GAAa,QAAkB3iC,EAAO8N,SAI5C,aAFoB2zB,EAAmB,SAE3BvD,gBAAgByE,EAAY3G,EAAUxuB,EACpD,CAAE,MAAO6U,GACP,MAAO,CAAEnb,OAAQ,QAASE,QAASib,EAAExR,WACvC,GAsDA+wB,oBACA9T,gBAAiB5uB,MAAO4O,UACC2zB,EAAmB,YAE3BrC,WAAWtxB,GAE5B80B,sBAzD4B1jC,MAAO69B,EAAep9B,EAAO,CAAC,KAC1D,IAEE,aADoB8hC,EAAmB,SAC3BtD,gBAAgBpB,EAAOp9B,EACrC,CAAE,MAAO0iB,GACP,MAAO,CAAEnb,OAAQ,QAASE,QAASib,EAAExR,WACvC,IAsDF,MAAO,CAAE0tB,aA1GasE,IACpB9/B,OAAOC,KAAK6/B,GACTz8B,QAAQS,QAAuD,IAA9Cg8B,EAAah8B,KAC9B5D,SAAS4D,IACR,MAAMvC,EAAOu+B,EAAah8B,GAC1B06B,EAAY16B,GAA6B9J,KAAKuH,EAAK,GACnD,EAoGiBu9B,SAAQxkB,MA9CjB,KACZ1B,GAAiB0B,OAAO,EA6CY,EAGvBylB,GAMjB,O,0VG9KA,MAAMC,EAA6C,CACjD3I,MAAO,iBAGI4I,EAAmB9jC,MAC9B6J,EACAk6B,EACAz1B,EAAgC,CAAC,EACjC01B,EACAvnB,KAEA,MAAMxH,EAAOzR,KAAKC,UAAU,CAC1BoG,cACGg6B,KACAv1B,IAGCqf,EAAU,CACd,eAAgB,mBAChBsW,cAAe,UAAUF,KAGrBllB,QAAiBkP,MAAM,6CAA8C,CACzE7qB,OAAQ,OACR8Q,OAAQyI,GAAiBzI,OACzB2Z,UACA1Y,SAGF,IAAK3G,EAAO8e,OAAQ,CAGlB,aADmBvO,EAASqlB,QAChBC,QAAQ,GAAGj8B,QAAQ0G,OACjC,CAEA,MAAMsc,EAASrM,EAAS5J,MAAMkW,YACxBiZ,EAAU,IAAIC,YACpB,IAAIvjC,EAAS,GACTwjC,EAAS,GAEb,GAAIpZ,EAEF,OAAa,CAEX,MAAM,KAAE/qB,EAAI,MAAEO,SAAgBwqB,EAAOG,OACrC,GAAIlrB,GAAQsc,GAAiBzI,OAAOC,QAClC,MAGFqwB,GAAUF,EAAQrvB,OAAOrU,EAAO,CAAE0sB,QAAQ,IAC1C,MAAMmX,EAAQD,EAAOE,MAAM,MAG3BF,EAASC,EAAME,OAAS,GAGxB,UAAWC,KAAQH,EAAO,CACxB,MAAMr8B,EAAUw8B,EAAK7iB,QAAQ,UAAW,IACxC,GAAgB,WAAZ3Z,EACF,OAAOpH,EAET,IACE,MAAM6jC,EAASnhC,KAAKyB,MAAMiD,GAC1B,GAAIy8B,EAAOR,SAAWQ,EAAOR,QAAQ/6B,OAAS,EAAG,CAC/C,MAAM,QAAEwF,GAAY+1B,EAAOR,QAAQ,GAAGS,MACtC9jC,GAAU8N,EACNA,SAEIo1B,EAAGp1B,EAEb,CACF,CAAE,MAAOzM,GACPY,EAAQZ,MAAM,gCAAiC+F,EAAS/F,EAC1D,CACF,CACF,CAGF,OAAOrB,CAAM,E,eCnFRd,eAAe6kC,EAActmB,GAClC,OAAOghB,EAAA,EAASoD,OAAOE,YAAYtkB,EACrC,CAEOve,eAAe8kC,EAAYC,EAASnR,GACzC,OAAO2L,EAAA,EAASoD,OAAOT,UAAU6C,EAASnR,EAC5C,CAEO5zB,eAAeglC,EAAwB5B,GAC5C,OAAO7D,EAAA,EAASoD,OAAOQ,sBAAsBC,EAC/C,CAEOpjC,eAAeilC,EAAqBl2B,EAAK+tB,EAAUxuB,EAAS,CAAC,GAClE,OAAOixB,EAAA,EAASoD,OAAOa,mBAAmBz0B,EAAK+tB,EAAUxuB,EAC3D,CAEOtO,eAAeklC,EAAqBn2B,GACzC,OAAOwwB,EAAA,EAASmD,kBAAkB3zB,EACpC,CAEO/O,eAAemlC,EAAkBv2B,GACtC,OAAO2wB,EAAA,EAAS3Q,gBAAgBhgB,EAClC,CAEO5O,eAAeolC,EAAwBvH,EAAOp9B,GAEnD,OADA,EAAQoC,IAAI,qBAAsBg7B,GAC3B0B,EAAA,EAASoD,OAAOe,sBAAsB7F,EAAOp9B,EACtD,CAEOT,eAAeqlC,EAAoBx7B,EAAUk6B,EAAQz1B,EAAQuvB,GASlE,aAPqBiG,EACnBj6B,EACAk6B,EACAz1B,GAJetO,MAAOS,GAAS2kC,EAAwBvH,EAAOp9B,IAM9D8+B,EAAA,EAASoD,OAAOC,wBAGpB,CAEO5iC,eAAeslC,EAAoBj0B,EAAMqP,GAC9C,OAAO6e,EAAA,EAASoD,OAAOY,iBAAiBlyB,EAAMqP,EAChD,CAEO1gB,eAAeulC,EAAiBx2B,GAErC,aADqB,QAAYA,EAEnC,CAEO/O,eAAewlC,EAAez2B,GAEnC,aADqB,QAAUA,EAEjC,C,iLCjCO/O,eAAe8Z,GAAgB,OACpClF,EAAM,WACNgS,EAAa,CAAEpQ,MAAO,GAAIjI,OAAQ,GAAG,QACrCkI,EAAO,OACPoQ,IAEA,MAAM,OAAEtY,EAAM,MAAEiI,GAAUoQ,EACpB/H,QAAiB,IAAM5c,IAC3B,GAAG,6BACH,CACEqM,OAAQ,CACN,oBAAqBC,EACrB,mBAAoBiI,EACpBC,UACA7B,OAAQA,EAAOtR,KAAKmiC,GAAQ,GAAGA,EAAItjB,QAAQsjB,EAAI/kC,YAEjDglC,iBAAkB,CAChBC,QAAS,MAEX3xB,OAAQ6S,GAAQ7S,UAId,IAAE4xB,GAAQ/mB,EAASpe,KAKnBiF,EAAY,CAChBkgC,MACAhf,WAAY/H,EAASpe,KAAKmmB,YAAc,CACtChiB,MAAOia,EAASpe,KAAKmE,OAEvBkiB,YAAajI,EAASpe,KAAKolC,cAO7B,OAJKngC,EAAUkhB,YAAYhiB,QACzBc,EAAUkhB,WAAWhiB,MAAQc,EAAUohB,YAAY1d,QAG9C1D,CACT,CAEA,MAAMogC,EAAU9lC,MACd+O,EACAnL,EAAkB,IAAgByC,MAChCkI,SAAQiI,QAAOuD,QAAQ,KAAQgsB,kBAEjC,IAiBE,aAhBuBjsB,EAAgB,CACrClF,OAAQ,CACN,CACEuN,IAAK,sBACHve,IAAS,IAAgB4B,GAAK,KAAO,QAEvC9E,MAAOqO,IAGX6X,WAAY,CACVpQ,QACAjI,UAEFkI,QAASsD,GAIb,CAAE,MAAOoJ,GAEP,OADApgB,EAAQF,IAAIsgB,GACL,IACT,GAGW6iB,EAAchmC,MAAO+O,EAAKR,EAAQiI,IACtCsvB,EAAQ/2B,EAAK,IAAgB1I,KAAM,CAAEkI,SAAQiI,UAGzCyvB,EAAYjmC,MAAO+O,EAAKR,EAAQiI,IACpCsvB,EAAQ/2B,EAAK,IAAgBvJ,GAAI,CAAE+I,SAAQiI,S,qEC9G7C,IAAW0vB,EAAX,CAAWA,IAChBA,EAAA,QAAU,UACVA,EAAA,cAAgB,eAChBA,EAAA,YAAc,cACdA,EAAA,IAAM,MACNA,EAAA,KAAO,OACPA,EAAA,MAAQ,QACRA,EAAA,OAAS,cAPOA,GAAX,CAAWA,GAAA,G,sECAlB,MAiBM36B,EAAS,CACbI,uBAAwB,CACtBw6B,WAAY,YACZC,OAAQ,SACRx6B,MAAO,S,8ICnBJ,MAAMy6B,EAAmB1gC,GAC9B,IAAW,IAAIC,KAAKD,GAAY,2BAA2B,GAEhD2gC,EAAmBC,GAC9B3gC,KAAKX,MAAMshC,EAAUC,SAAS,KAAOD,EAAY,GAAGA,MAEzCE,EAAkB,IAAM7gC,KAAKmE,K,yICCnC,MAKM28B,EAAexzB,GAC1B,IAAI7S,SAAQ,CAACC,EAASqmC,KACpB,MAEMrC,EAFa,IAAI,IAAJ,CAAW,OAAQ7Y,EAAOplB,KAAK6M,IAExB0zB,UAC1B,EAAAC,QAAA,OAAevC,GAAQ,CAAC5yB,EAAKo1B,KACvBp1B,GACFi1B,EAAO,IAAI12B,MAAM,+BAGnB,WAAY62B,GAAS,CAAC3kC,EAAO4M,KAC3BzO,EAAQyO,EAAIg4B,sBAAsB,GAClC,GACF,G,qEC3BC,MAEMC,EAAgC,yB,iHCE7C,MAAMC,EAAqB,GAoJpB,MAAMC,EAAsB,CACjCC,EAAyC,CAAC,KAE1C,MAAMn6B,EAAU,IAAIC,iBAAiB,MAErC,SAASm6B,EACPC,EACAn/B,EACAk1B,GAEA,MAAM5d,EAAM,IAAK2nB,KAAmB/J,GAChCA,GAASj7B,QACXqd,EAAIrd,MAAQqB,KAAKC,UAAU25B,EAAQj7B,QAErC6K,EAAQ5L,YAAY,CAClBwC,KAAM,MACNlD,MAAO,CAAE2mC,QAAOn/B,UAASk1B,QAAS5d,IAEtC,CAkBA,MAAO,CAAEzC,KAhBT,SAAiB7U,EAAYk1B,GAC3B,OAAOgK,EAAiB,OAAQl/B,EAASk1B,EAC3C,EAcej7B,MAZf,SAAkB+F,EAAYk1B,GAC5B,OAAOgK,EAAiB,QAASl/B,EAASk1B,EAC5C,EAUsBp6B,KARtB,SAAiBkF,EAAYk1B,GAC3B,OAAOgK,EAAiB,OAAQl/B,EAASk1B,EAC3C,EAM4BkK,MAJ5B,SAAkBp/B,EAAYk1B,GAC5B,OAAOgK,EAAiB,OAAQl/B,EAASk1B,EAC3C,EAEmC,EAG/BwE,EAzLN,SAAyBuF,EAAyC,CAAC,GAQjE,IAAII,EAAmB,CAAC,EAyCxB,SAAS1kC,EACPwkC,EACAn/B,EACAk1B,EAA2B+J,GAE3B,IACE,MAAMK,EAAmBpK,GAASqK,UAC9BrK,GAASqK,UAAUv/B,GACnBA,GAxDR,SAAmBw/B,EAAkBC,GAAW,GAG9C,IAFAV,EAAQz+B,KAAKk/B,GAENC,GAAYV,EAAQ79B,OAAS,KAClC69B,EAAQjrB,OAEZ,CA4DI4rB,CARiB,CACfjiC,UAAW,IAAIC,KACfyhC,QACAn/B,QAASs/B,EACTK,WAAYzK,GAASyK,WACrBzK,QAAS,SAAOA,EAAS,CAAC,YAAa,iBAKlBv5B,OAAOC,KAAKyjC,GAAkBr+B,QACnD,CAACC,EAAcgZ,KACb,MAAM7T,EAASi5B,EAAiBplB,GAC1B2lB,EAAc1K,EAAQjb,GAC5B,OAAI7T,GAAUw5B,EAEV3+B,GACW,QAAXmF,GACkB,IAAlBA,EAAOlF,QACPkF,EAAO7G,MAAMsgC,GAAMA,IAAMD,IAGtB3+B,CAAG,IAEZ,IA/DN,SACEk+B,EACAn/B,EACAk1B,GAEA,MAAM5d,EAAM,SAAO4d,EAAS,CAC1B,YACA,SACA,SACA,OACA,UAEI,OAAEhzB,EAAS,UAAIwS,EAAS,QAAI6C,EAAO,QAAIhf,EAAO,IAAO28B,EACrD4K,GAAU,IAAAtoB,SAAQF,GAAO,GAAKA,EAEhCgD,MAAMC,QAAQva,GAChBnF,EAAQskC,MAAUn/B,EAAS8/B,GAIzB5K,GAASqK,UACX1kC,EAAQskC,GAAOjK,GAASqK,UAAUv/B,GAAU8/B,GAI9CjlC,EAAQskC,GAAO,IAAIj9B,KAAUwS,KAAU6C,MAASvX,IAAWzH,EAAMunC,EACnE,CAyCMC,CAAWZ,EAAOn/B,EAASk1B,EAE/B,CAAE,MAAOj7B,GACPY,EAAQF,IAAI,eAAgBV,EAC9B,CACF,CAyCA,OA3HgB,IAAI8K,iBAAiB,MAE7B1M,UAAaC,IACK,WAApBA,EAAMC,KAAKmD,OACb2jC,EAAmB,IAAKA,KAAqB/mC,EAAMC,KAAKC,OAC1D,EAsHK,CACLmC,MACAka,KAzCF,SAAiB7U,EAAYk1B,GAC3B,OAAOv6B,EAAI,OAAQqF,EAASk1B,EAC9B,EAwCEj7B,MAtCF,SAAkB+F,EAAYk1B,GAC5B,OAAOv6B,EAAI,QAASqF,EAASk1B,EAC/B,EAqCEp6B,KAnCF,SAAiBkF,EAAYk1B,GAC3B,OAAOv6B,EAAI,OAAQqF,EAASk1B,EAC9B,EAkCEkK,MAhCF,SAAkBp/B,EAAYk1B,GAC5B,OAAOv6B,EAAI,OAAQqF,EAASk1B,EAC9B,EA+BE6J,UACAiB,QAAS,IA7BFjB,EAAQ3jC,KAAKokC,IAClB,MAAM,QAAEtK,KAAYp8B,GAAS0mC,GACvB,KACJjoB,EAAO,UACP7C,EAAS,UACTxS,EAAS,QACT3J,EAAO,GACP0B,MAAAA,EAAQ,cACR0lC,EAAa,IACXzK,GAAW,CAAC,EAChB,MAAO,IACFp8B,EACHye,OACA7C,SACAxS,SACA3J,OACA0B,MAAAA,EACA0lC,aACD,IAYH9U,MAAO,IAAMkU,EAAQkB,OAAO,EAAGlB,EAAQ79B,QACvCg/B,oBA5H0B,IAAMb,EA8HpC,CAyCec,CAAa,CAAEj+B,OAAQ,SAMtC,K,gtBCpMIk+B,EAA2B,CAAC,EAGhC,SAASC,EAAoBC,GAE5B,IAAIC,EAAeH,EAAyBE,GAC5C,QAAqBE,IAAjBD,EACH,OAAOA,EAAaE,QAGrB,IAAI/rB,EAAS0rB,EAAyBE,GAAY,CACjDriC,GAAIqiC,EACJtM,QAAQ,EACRyM,QAAS,CAAC,GAUX,OANAC,EAAoBJ,GAAUlmB,KAAK1F,EAAO+rB,QAAS/rB,EAAQA,EAAO+rB,QAASJ,GAG3E3rB,EAAOsf,QAAS,EAGTtf,EAAO+rB,OACf,CAGAJ,EAAoBM,EAAID,EAGxBL,EAAoBO,EAAI,WAGvB,IAAIC,EAAsBR,EAAoBS,OAAEN,EAAW,CAAC,IAAI,IAAI,GAAG,IAAI,IAAI,IAAI,MAAM,WAAa,OAAOH,EAAoB,MAAQ,IAEzI,OADAQ,EAAsBR,EAAoBS,EAAED,EAE7C,ECrCAR,EAAoBU,KAAO,CAAC,EjGAxBvrC,EAAW,GACf6qC,EAAoBS,EAAI,SAASloC,EAAQooC,EAAU/V,EAAIjkB,GACtD,IAAGg6B,EAAH,CAMA,IAAIC,EAAeC,IACnB,IAAS3/B,EAAI,EAAGA,EAAI/L,EAAS0L,OAAQK,IAAK,CACrCy/B,EAAWxrC,EAAS+L,GAAG,GACvB0pB,EAAKz1B,EAAS+L,GAAG,GACjByF,EAAWxR,EAAS+L,GAAG,GAE3B,IAJA,IAGIuB,GAAY,EACPq+B,EAAI,EAAGA,EAAIH,EAAS9/B,OAAQigC,MACpB,EAAXn6B,GAAsBi6B,GAAgBj6B,IAAarL,OAAOC,KAAKykC,EAAoBS,GAAGM,OAAM,SAASnnB,GAAO,OAAOomB,EAAoBS,EAAE7mB,GAAK+mB,EAASG,GAAK,IAChKH,EAASf,OAAOkB,IAAK,IAErBr+B,GAAY,EACTkE,EAAWi6B,IAAcA,EAAej6B,IAG7C,GAAGlE,EAAW,CACbtN,EAASyqC,OAAO1+B,IAAK,GACrB,IAAI8/B,EAAIpW,SACEuV,IAANa,IAAiBzoC,EAASyoC,EAC/B,CACD,CACA,OAAOzoC,CArBP,CAJCoO,EAAWA,GAAY,EACvB,IAAI,IAAIzF,EAAI/L,EAAS0L,OAAQK,EAAI,GAAK/L,EAAS+L,EAAI,GAAG,GAAKyF,EAAUzF,IAAK/L,EAAS+L,GAAK/L,EAAS+L,EAAI,GACrG/L,EAAS+L,GAAK,CAACy/B,EAAU/V,EAAIjkB,EAwB/B,EkG5BAq5B,EAAoBiB,EAAI,SAAS5sB,GAChC,IAAI6sB,EAAS7sB,GAAUA,EAAO8sB,WAC7B,WAAa,OAAO9sB,EAAgB,OAAG,EACvC,WAAa,OAAOA,CAAQ,EAE7B,OADA2rB,EAAoBoB,EAAEF,EAAQ,CAAElgC,EAAGkgC,IAC5BA,CACR,EjGPI7rC,EAAWiG,OAAO+lC,eAAiB,SAAS/oC,GAAO,OAAOgD,OAAO+lC,eAAe/oC,EAAM,EAAI,SAASA,GAAO,OAAOA,EAAIgpC,SAAW,EAQpItB,EAAoBl+B,EAAI,SAAS3J,EAAOopC,GAEvC,GADU,EAAPA,IAAUppC,EAAQqM,KAAKrM,IAChB,EAAPopC,EAAU,OAAOppC,EACpB,GAAoB,iBAAVA,GAAsBA,EAAO,CACtC,GAAW,EAAPopC,GAAappC,EAAMgpC,WAAY,OAAOhpC,EAC1C,GAAW,GAAPopC,GAAoC,mBAAfppC,EAAM2B,KAAqB,OAAO3B,CAC5D,CACA,IAAIqpC,EAAKlmC,OAAOmmC,OAAO,MACvBzB,EAAoBgB,EAAEQ,GACtB,IAAIE,EAAM,CAAC,EACXtsC,EAAiBA,GAAkB,CAAC,KAAMC,EAAS,CAAC,GAAIA,EAAS,IAAKA,EAASA,IAC/E,IAAI,IAAIssC,EAAiB,EAAPJ,GAAYppC,EAAyB,iBAAXwpC,KAAyBvsC,EAAe8sB,QAAQyf,GAAUA,EAAUtsC,EAASssC,GACxHrmC,OAAOsmC,oBAAoBD,GAASnmC,SAAQ,SAASoe,GAAO8nB,EAAI9nB,GAAO,WAAa,OAAOzhB,EAAMyhB,EAAM,CAAG,IAI3G,OAFA8nB,EAAa,QAAI,WAAa,OAAOvpC,CAAO,EAC5C6nC,EAAoBoB,EAAEI,EAAIE,GACnBF,CACR,EkGxBAxB,EAAoBoB,EAAI,SAAShB,EAASyB,GACzC,IAAI,IAAIjoB,KAAOioB,EACX7B,EAAoB8B,EAAED,EAAYjoB,KAASomB,EAAoB8B,EAAE1B,EAASxmB,IAC5Ete,OAAOymC,eAAe3B,EAASxmB,EAAK,CAAEooB,YAAY,EAAMtoC,IAAKmoC,EAAWjoB,IAG3E,ECPAomB,EAAoBiC,EAAI,CAAC,EAGzBjC,EAAoBplB,EAAI,SAASsnB,GAChC,OAAOpqC,QAAQwR,IAAIhO,OAAOC,KAAKykC,EAAoBiC,GAAGthC,QAAO,SAASwhC,EAAUvoB,GAE/E,OADAomB,EAAoBiC,EAAEroB,GAAKsoB,EAASC,GAC7BA,CACR,GAAG,IACJ,ECPAnC,EAAoBoC,EAAI,SAASF,GAEhC,OAAgB,MAAZA,EAAwB,kBACZ,MAAZA,EAAwB,kBACZ,KAAZA,EAAuB,iBAEfA,EAAU,IAAM,CAAC,IAAM,WAAW,IAAM,WAAW,IAAM,WAAW,IAAM,WAAW,IAAM,YAAYA,GAAW,WAC/H,ECPAlC,EAAoBqC,SAAW,SAASH,GAGxC,ECJAlC,EAAoBsC,EAAI,WACvB,GAA0B,iBAAfC,WAAyB,OAAOA,WAC3C,IACC,OAAO/9B,MAAQ,IAAIg+B,SAAS,cAAb,EAChB,CAAE,MAAO5nB,GACR,GAAsB,iBAAXwQ,OAAqB,OAAOA,MACxC,CACA,CAPuB,GCAxB4U,EAAoByC,IAAM,SAASpuB,GASlC,OARAA,EAAS/Y,OAAOmmC,OAAOptB,IACXquB,WAAUruB,EAAOquB,SAAW,IACxCpnC,OAAOymC,eAAe1tB,EAAQ,UAAW,CACxC2tB,YAAY,EACZ1oC,IAAK,WACJ,MAAM,IAAIoO,MAAM,0FAA4F2M,EAAOzW,GACpH,IAEMyW,CACR,ECVA2rB,EAAoB8B,EAAI,SAASxpC,EAAKqqC,GAAQ,OAAOrnC,OAAOue,UAAUC,eAAeC,KAAKzhB,EAAKqqC,EAAO,ECCtG3C,EAAoBgB,EAAI,SAASZ,GACX,oBAAX1oC,QAA0BA,OAAOkrC,aAC1CtnC,OAAOymC,eAAe3B,EAAS1oC,OAAOkrC,YAAa,CAAEzqC,MAAO,WAE7DmD,OAAOymC,eAAe3B,EAAS,aAAc,CAAEjoC,OAAO,GACvD,ECNA6nC,EAAoB6C,IAAM,SAASxuB,GAGlC,OAFAA,EAAOyuB,MAAQ,GACVzuB,EAAOquB,WAAUruB,EAAOquB,SAAW,IACjCruB,CACR,ECJA2rB,EAAoBR,EAAI,I,WCAxBQ,EAAoB/+B,EAAIxL,KAAKstC,SAAW,GAIxC,IAAIC,EAAkB,CACrB,IAAK,GAkBNhD,EAAoBiC,EAAE/gC,EAAI,SAASghC,EAASC,GAEvCa,EAAgBd,IAElBe,cAAcjD,EAAoBR,EAAIQ,EAAoBoC,EAAEF,GAG/D,EAEA,IAAIgB,EAAqBztC,KAAsB,gBAAIA,KAAsB,iBAAK,GAC1E0tC,EAA6BD,EAAmBjjC,KAAKwS,KAAKywB,GAC9DA,EAAmBjjC,KAzBA,SAAS/H,GAC3B,IAAIyoC,EAAWzoC,EAAK,GAChBkrC,EAAclrC,EAAK,GACnBs9B,EAAUt9B,EAAK,GACnB,IAAI,IAAI+nC,KAAYmD,EAChBpD,EAAoB8B,EAAEsB,EAAanD,KACrCD,EAAoBM,EAAEL,GAAYmD,EAAYnD,IAIhD,IADGzK,GAASA,EAAQwK,GACdW,EAAS9/B,QACdmiC,EAAgBrC,EAASzE,OAAS,EACnCiH,EAA2BjrC,EAC5B,C,I3GtBI5C,EAAO0qC,EAAoBO,EAC/BP,EAAoBO,EAAI,WACvB,OAAOzoC,QAAQwR,IAAI,CAAC,IAAI,IAAI,GAAG,IAAI,IAAI,IAAI,KAAKvO,IAAIilC,EAAoBplB,EAAGolB,IAAsBlmC,KAAKxE,EACvG,E4GF0B0qC,EAAoBO,G","sources":["webpack://cyb/webpack/runtime/chunk loaded","webpack://cyb/webpack/runtime/create fake namespace object","webpack://cyb/webpack/runtime/startup chunk dependencies","webpack://cyb/./src/constants/config.ts","webpack://cyb/./src/constants/defaultNetworks.ts","webpack://cyb/./src/constants/patterns.ts","webpack://cyb/./src/containers/Search/types.ts","webpack://cyb/./src/services/QueueManager/types.ts","webpack://cyb/./src/services/backend/workers/serializers.ts","webpack://cyb/./src/services/backend/workers/factoryMethods.ts","webpack://cyb/./src/services/CozoDb/types/entities.ts","webpack://cyb/./src/features/particle/utils.tsx","webpack://cyb/./src/features/sense/redux/sense.redux.ts","webpack://cyb/./src/constants/localStorageKeys.ts","webpack://cyb/./src/redux/features/pocket.ts","webpack://cyb/./src/services/backend/channels/consts.ts","webpack://cyb/./src/services/backend/channels/BroadcastChannelSender.ts","webpack://cyb/./src/services/backend/channels/broadcastStatus.ts","webpack://cyb/./src/utils/async/iterable.ts","webpack://cyb/./src/services/backend/channels/BackendQueueChannel/backendQueueSenders.ts","webpack://cyb/./src/constants/app.ts","webpack://cyb/./src/services/backend/services/sync/services/consts.ts","webpack://cyb/./src/services/backend/services/sync/services/ParticlesResolverQueue/ParticlesResolverQueue.ts","webpack://cyb/./src/utils/string.ts","webpack://cyb/./src/services/CozoDb/mapping.ts","webpack://cyb/./src/utils/async/promise.ts","webpack://cyb/./src/generated/graphql.ts","webpack://cyb/./src/services/backend/services/indexer/types.ts","webpack://cyb/./src/services/lcd/utils/mapping.ts","webpack://cyb/./src/services/backend/services/indexer/utils/graphqlClient.ts","webpack://cyb/./src/services/backend/services/indexer/cyberlinks.ts","webpack://cyb/./src/services/backend/services/indexer/consts.ts","webpack://cyb/./src/services/backend/services/sync/services/utils/links.ts","webpack://cyb/./src/services/backend/services/indexer/transactions.ts","webpack://cyb/./src/services/backend/services/sync/services/utils/sense.ts","webpack://cyb/./src/services/backend/services/sync/services/SyncTransactionsLoop/services/chat.ts","webpack://cyb/./src/services/backend/services/sync/services/ProgressTracker/ProgressTracker.ts","webpack://cyb/./src/services/backend/services/sync/services/BaseSyncLoop/BaseSync.ts","webpack://cyb/./src/services/backend/services/sync/services/utils/rxjs/withInitializer.ts","webpack://cyb/./src/services/backend/services/sync/services/BaseSyncLoop/BaseSyncClient.ts","webpack://cyb/./src/services/backend/services/sync/services/SyncTransactionsLoop/SyncTransactionsLoop.ts","webpack://cyb/./src/services/lcd/websocket.ts","webpack://cyb/./src/utils/dto.ts","webpack://cyb/./src/services/backend/services/sync/utils.ts","webpack://cyb/./src/utils/exceptions/helpers.ts","webpack://cyb/./src/services/backend/services/sync/services/BaseSyncLoop/BaseSyncLoop.ts","webpack://cyb/./src/services/backend/services/sync/services/utils/rxjs/loop.ts","webpack://cyb/./src/services/backend/services/sync/services/SyncParticlesLoop/SyncParticlesLoop.ts","webpack://cyb/./src/services/backend/services/sync/services/SyncMyFriendsLoop/SyncMyFriendsLoop.ts","webpack://cyb/./src/services/community/community.ts","webpack://cyb/./src/services/community/lcd.ts","webpack://cyb/./src/services/backend/channels/BackendQueueChannel/BackendQueueChannel.ts","webpack://cyb/./src/services/backend/services/sync/sync.ts","webpack://cyb/./src/services/backend/services/sync/services/CommunitySync/CommunitySync.ts","webpack://cyb/./src/services/ipfs/utils/stream.ts","webpack://cyb/./src/db.js","webpack://cyb/./src/services/ipfs/utils/ipfsCacheDb.ts","webpack://cyb/./src/services/ipfs/config.ts","webpack://cyb/./src/services/ipfs/utils/cluster.ts","webpack://cyb/./src/services/ipfs/utils/content.ts","webpack://cyb/./src/services/ipfs/utils/utils-ipfs.ts","webpack://cyb/./src/services/QueueManager/QueueStrategy.ts","webpack://cyb/./src/services/QueueManager/QueueItemTimeoutError.ts","webpack://cyb/./src/services/QueueManager/constants.ts","webpack://cyb/./src/services/QueueManager/QueueManager.ts","webpack://cyb/./src/utils/rxjs/helpers.ts","webpack://cyb/./src/services/ipfs/utils/cid.ts","webpack://cyb/./src/services/ipfs/node/impl/kubo.ts","webpack://cyb/./src/services/ipfs/node/impl/helia.ts","webpack://cyb/./src/services/ipfs/node/impl/configs/jsIpfsConfig.ts","webpack://cyb/./src/services/ipfs/node/impl/js-ipfs.ts","webpack://cyb/./src/services/ipfs/node/factory.ts","webpack://cyb/./src/services/ipfs/node/mixins/withCybFeatures.ts","webpack://cyb/./src/services/backend/workers/background/api/mlApi.ts","webpack://cyb/./src/services/scripting/engine.ts","webpack://cyb/./src/services/backend/workers/background/api/runeApi.ts","webpack://cyb/./src/services/backend/workers/background/worker.ts","webpack://cyb/./src/services/backend/workers/background/api/ipfsApi.ts","webpack://cyb/./src/services/scripting/helpers.ts","webpack://cyb/./src/contexts/queryClient.tsx","webpack://cyb/./src/containers/portal/utils.ts","webpack://cyb/./src/services/passports/lcd.ts","webpack://cyb/./src/services/neuron/errors.ts","webpack://cyb/./src/services/neuron/neuronApi.ts","webpack://cyb/./src/services/scripting/runeDeps.ts","webpack://cyb/./src/utils/search/utils.ts","webpack://cyb/./src/utils/utils.ts","webpack://cyb/./src/services/scripting/services/llmRequests/openai.ts","webpack://cyb/./src/services/scripting/wasmBindings.js","webpack://cyb/./src/services/transactions/lcd.tsx","webpack://cyb/./src/types/networks.ts","webpack://cyb/./src/utils/config.ts","webpack://cyb/./src/utils/date.ts","webpack://cyb/./src/utils/ipfs/helpers.ts","webpack://cyb/./src/utils/logging/constants.ts","webpack://cyb/./src/utils/logging/cyblog.ts","webpack://cyb/webpack/bootstrap","webpack://cyb/webpack/runtime/amd options","webpack://cyb/webpack/runtime/compat get default export","webpack://cyb/webpack/runtime/define property getters","webpack://cyb/webpack/runtime/ensure chunk","webpack://cyb/webpack/runtime/get javascript chunk filename","webpack://cyb/webpack/runtime/get mini-css chunk filename","webpack://cyb/webpack/runtime/global","webpack://cyb/webpack/runtime/harmony module decorator","webpack://cyb/webpack/runtime/hasOwnProperty shorthand","webpack://cyb/webpack/runtime/make namespace object","webpack://cyb/webpack/runtime/node module decorator","webpack://cyb/webpack/runtime/publicPath","webpack://cyb/webpack/runtime/importScripts chunk loading","webpack://cyb/webpack/startup"],"sourcesContent":["var deferred = [];\n__webpack_require__.O = function(result, chunkIds, fn, priority) {\n\tif(chunkIds) {\n\t\tpriority = priority || 0;\n\t\tfor(var i = deferred.length; i > 0 && deferred[i - 1][2] > priority; i--) deferred[i] = deferred[i - 1];\n\t\tdeferred[i] = [chunkIds, fn, priority];\n\t\treturn;\n\t}\n\tvar notFulfilled = Infinity;\n\tfor (var i = 0; i < deferred.length; i++) {\n\t\tvar chunkIds = deferred[i][0];\n\t\tvar fn = deferred[i][1];\n\t\tvar priority = deferred[i][2];\n\t\tvar fulfilled = true;\n\t\tfor (var j = 0; j < chunkIds.length; j++) {\n\t\t\tif ((priority & 1 === 0 || notFulfilled >= priority) && Object.keys(__webpack_require__.O).every(function(key) { return __webpack_require__.O[key](chunkIds[j]); })) {\n\t\t\t\tchunkIds.splice(j--, 1);\n\t\t\t} else {\n\t\t\t\tfulfilled = false;\n\t\t\t\tif(priority < notFulfilled) notFulfilled = priority;\n\t\t\t}\n\t\t}\n\t\tif(fulfilled) {\n\t\t\tdeferred.splice(i--, 1)\n\t\t\tvar r = fn();\n\t\t\tif (r !== undefined) result = r;\n\t\t}\n\t}\n\treturn result;\n};","var getProto = Object.getPrototypeOf ? function(obj) { return Object.getPrototypeOf(obj); } : function(obj) { return obj.__proto__; };\nvar leafPrototypes;\n// create a fake namespace object\n// mode & 1: value is a module id, require it\n// mode & 2: merge all properties of value into the ns\n// mode & 4: return value when already ns object\n// mode & 16: return value when it's Promise-like\n// mode & 8|1: behave like require\n__webpack_require__.t = function(value, mode) {\n\tif(mode & 1) value = this(value);\n\tif(mode & 8) return value;\n\tif(typeof value === 'object' && value) {\n\t\tif((mode & 4) && value.__esModule) return value;\n\t\tif((mode & 16) && typeof value.then === 'function') return value;\n\t}\n\tvar ns = Object.create(null);\n\t__webpack_require__.r(ns);\n\tvar def = {};\n\tleafPrototypes = leafPrototypes || [null, getProto({}), getProto([]), getProto(getProto)];\n\tfor(var current = mode & 2 && value; typeof current == 'object' && !~leafPrototypes.indexOf(current); current = getProto(current)) {\n\t\tObject.getOwnPropertyNames(current).forEach(function(key) { def[key] = function() { return value[key]; }; });\n\t}\n\tdef['default'] = function() { return value; };\n\t__webpack_require__.d(ns, def);\n\treturn ns;\n};","var next = __webpack_require__.x;\n__webpack_require__.x = function() {\n\treturn Promise.all([667,742,45,168,112,235,187].map(__webpack_require__.e, __webpack_require__)).then(next);\n};","import { Networks } from 'src/types/networks';\nimport defaultNetworks from './defaultNetworks';\n\n// FIXME: seems temp\nexport function isWorker() {\n  return (\n    typeof WorkerGlobalScope !== 'undefined' &&\n    self instanceof WorkerGlobalScope\n  );\n}\n\nconst LOCALSTORAGE_CHAIN_ID = !isWorker() && localStorage.getItem('chainId');\n\nconst DEFAULT_CHAIN_ID: Networks.BOSTROM | Networks.SPACE_PUSSY =\n  LOCALSTORAGE_CHAIN_ID || process.env.CHAIN_ID || Networks.BOSTROM;\n\nexport const CHAIN_ID = DEFAULT_CHAIN_ID;\n\nexport const LCD_URL =\n  process.env.LCD_URL || defaultNetworks[DEFAULT_CHAIN_ID].LCD_URL;\n\nexport const RPC_URL =\n  process.env.RPC_URL || defaultNetworks[DEFAULT_CHAIN_ID].RPC_URL;\n\nexport const WEBSOCKET_URL =\n  process.env.WEBSOCKET_URL || defaultNetworks[DEFAULT_CHAIN_ID].WEBSOCKET_URL;\n\nexport const INDEX_HTTPS =\n  process.env.INDEX_HTTPS || defaultNetworks[DEFAULT_CHAIN_ID].INDEX_HTTPS;\n\nexport const INDEX_WEBSOCKET =\n  process.env.INDEX_WEBSOCKET ||\n  defaultNetworks[DEFAULT_CHAIN_ID].INDEX_WEBSOCKET;\n\nexport const BECH32_PREFIX =\n  process.env.BECH32_PREFIX || defaultNetworks[DEFAULT_CHAIN_ID].BECH32_PREFIX;\n\nconst BECH32_PREFIX_VAL = `${BECH32_PREFIX}val`;\n\nexport const BECH32_PREFIX_VALOPER = `${BECH32_PREFIX_VAL}oper`;\n\nexport const BECH32_PREFIX_VAL_CONS = `${BECH32_PREFIX_VAL}cons`;\n\nexport const BASE_DENOM =\n  process.env.BASE_DENOM || defaultNetworks[DEFAULT_CHAIN_ID].BASE_DENOM;\n\nexport const DENOM_LIQUID =\n  process.env.DENOM_LIQUID || defaultNetworks[DEFAULT_CHAIN_ID].DENOM_LIQUID;\n\nexport const CYBER_GATEWAY =\n  process.env.CYBER_GATEWAY || 'https://gateway.ipfs.cybernode.ai';\n\nexport const DIVISOR_CYBER_G = 10 ** 9;\n\nexport const DEFAULT_GAS_LIMITS = 200000;\n\nexport const COIN_DECIMALS_RESOURCE = 3;\n\nexport const { MEMO_KEPLR } = defaultNetworks[DEFAULT_CHAIN_ID];\n","import { NetworkConfig, Networks } from 'src/types/networks';\n\ntype NetworksList = {\n  [key in Networks]: NetworkConfig;\n};\n\nconst defaultNetworks: NetworksList = {\n  bostrom: {\n    CHAIN_ID: Networks.BOSTROM,\n    BASE_DENOM: 'boot',\n    DENOM_LIQUID: 'hydrogen',\n    RPC_URL: 'https://rpc.bostrom.cybernode.ai',\n    LCD_URL: 'https://lcd.bostrom.cybernode.ai',\n    WEBSOCKET_URL: 'wss://rpc.bostrom.cybernode.ai/websocket',\n    INDEX_HTTPS: 'https://index.bostrom.cybernode.ai/v1/graphql',\n    INDEX_WEBSOCKET: 'wss://index.bostrom.cybernode.ai/v1/graphql',\n    BECH32_PREFIX: 'bostrom',\n    MEMO_KEPLR: '[bostrom] cyb.ai, using keplr',\n  },\n  localbostrom: {\n    CHAIN_ID: Networks.LOCAL_BOSTROM,\n    BASE_DENOM: 'boot',\n    DENOM_LIQUID: 'hydrogen',\n    RPC_URL: 'https://rpc.bostrom.moon.cybernode.ai',\n    LCD_URL: 'https://lcd.bostrom.moon.cybernode.ai',\n    WEBSOCKET_URL: 'wss://rpc.bostrom.moon.cybernode.ai/websocket',\n    INDEX_HTTPS: 'https://index.bostrom.moon.cybernode.ai/v1/graphql',\n    INDEX_WEBSOCKET: 'wss://index.bostrom.moon.cybernode.ai/v1/graphql',\n    BECH32_PREFIX: 'bostrom',\n    MEMO_KEPLR: '[bostrom] cyb.ai, using keplr',\n  },\n\n  'space-pussy': {\n    CHAIN_ID: Networks.SPACE_PUSSY,\n    BASE_DENOM: 'pussy',\n    DENOM_LIQUID: 'liquidpussy',\n    RPC_URL: 'https://rpc.space-pussy.cybernode.ai/',\n    LCD_URL: 'https://lcd.space-pussy.cybernode.ai',\n    WEBSOCKET_URL: 'wss://rpc.space-pussy.cybernode.ai/websocket',\n    INDEX_HTTPS: 'https://index.space-pussy.cybernode.ai/v1/graphql',\n    INDEX_WEBSOCKET: 'wss://index.space-pussy.cybernode.ai/v1/graphql',\n    BECH32_PREFIX: 'pussy',\n    MEMO_KEPLR: '[space-pussy] cyb.ai, using keplr',\n  },\n};\n\nexport default defaultNetworks;\n","import { BECH32_PREFIX, BECH32_PREFIX_VALOPER } from './config';\n\nexport const PATTERN_CYBER = new RegExp(\n  `^${BECH32_PREFIX}[a-zA-Z0-9]{39}$`,\n  'g'\n);\n\nexport const PATTERN_SPACE_PUSSY = /^pussy[a-zA-Z0-9]{39}$/g;\n\nexport const PATTERN_IPFS_HASH = /^Qm[a-zA-Z0-9]{44}$/g;\n\nexport const PATTERN_CYBER_CONTRACT = new RegExp(\n  `^${BECH32_PREFIX}[a-zA-Z0-9]{59}$`,\n  'g'\n);\n\nexport const PATTERN_CYBER_VALOPER = new RegExp(\n  `^${BECH32_PREFIX_VALOPER}valoper[a-zA-Z0-9]{39}$`,\n  'g'\n);\n\nexport const PATTERN_COSMOS = /^cosmos[a-zA-Z0-9]{39}$/g;\n\nexport const PATTERN_OSMOS = /^osmo[a-zA-Z0-9]{39}$/g;\n\nexport const PATTERN_TERRA = /^terra[a-zA-Z0-9]{39}$/g;\n\nexport const PATTERN_ETH = /^0x[a-fA-F0-9]{40}$/g;\n\nexport const PATTERN_TX = /[0-9a-fA-F]{64}$/g;\n\nexport const PATTERN_BLOCK = /^[0-9]+$/g;\n\nexport const PATTERN_HTTP = /^https:\\/\\/|^http:\\/\\//g;\n\nexport const PATTERN_HTML = /<\\/?[\\w\\d]+>/gi;\n","export enum LinksTypeFilter {\n  to = 'to',\n  from = 'from',\n  all = 'all',\n}\n\nexport type LinksType = Exclude<LinksTypeFilter, LinksTypeFilter.all>;\n\nexport type SearchItem = {\n  cid: string;\n  rank?: string;\n  grade?: string;\n  timestamp?: string;\n  type?: LinksTypeFilter;\n};\n\nexport enum SortBy {\n  rank = 'rank',\n  date = 'date',\n  // not ready\n  // popular = 'popular',\n  // mine = 'mine',\n}\n","import { Option } from 'src/types';\nimport { IPFSContent, IpfsContentSource } from '../ipfs/types';\n\n/* eslint-disable import/no-unused-modules */\nexport type QueueItemStatus =\n  | 'pending'\n  | 'executing'\n  | 'timeout'\n  | 'completed'\n  | 'cancelled'\n  | 'error'\n  | 'not_found';\n\nexport type QueueSourceSettings = {\n  timeout: number;\n  maxConcurrentExecutions: number;\n};\n\nexport type QueueSource = IpfsContentSource;\n\nexport type QueueSettings = Record<QueueSource, QueueSourceSettings>;\n\nexport interface IQueueStrategy {\n  settings: QueueSettings;\n  order: QueueSource[];\n  getNextSource(source: QueueSource): QueueSource | undefined;\n}\n\nexport type QueueStats = {\n  status: QueueItemStatus;\n  count: number;\n};\n\nexport enum QueuePriority {\n  ZERO = 0,\n  LOW = 0.1,\n  MEDIUM = 0.5,\n  HIGH = 0.9,\n  URGENT = 1,\n}\nexport type QueueItemOptions = {\n  parent?: string;\n  priority?: QueuePriority | number;\n  viewPortPriority?: number;\n  initialSource?: QueueSource;\n  postProcessing?: boolean;\n};\n\nexport type QueueItemCallback = (\n  cid: string,\n  status: QueueItemStatus,\n  source: QueueSource,\n  result?: Option<IPFSContent>\n) => void;\n\nexport type QueueItem = {\n  cid: string;\n  source: QueueSource;\n  status: QueueItemStatus;\n  callbacks: QueueItemCallback[];\n  controller?: AbortController;\n  executionTime?: number;\n} & Omit<QueueItemOptions, 'initialSource'>;\n\nexport type QueueItemResult = {\n  item: QueueItem;\n  status: QueueItemStatus;\n  source: QueueSource;\n  result?: Option<IPFSContent>;\n};\n\nexport type QueueItemAsyncResult = Omit<QueueItemResult, 'item'>;\n\nexport type QueueItemPostProcessor = (\n  content: Option<IPFSContent>\n) => Promise<Option<IPFSContent>>;\n\nexport type FetchParticleAsync = (\n  cid: string,\n  options?: QueueItemOptions\n) => Promise<QueueItemAsyncResult>;\n","import type { TransferHandler } from 'comlink';\nimport { IPFSContent } from 'src/services/ipfs/types';\n\nexport type IPFSContentTransferable = Omit<IPFSContent, 'result'> & {\n  port: MessagePort;\n};\n\nfunction createAsyncIterable(port: MessagePort): AsyncIterable<Uint8Array> {\n  return {\n    async *[Symbol.asyncIterator](): AsyncGenerator<\n      Uint8Array,\n      void,\n      undefined\n    > {\n      let done = false;\n      while (!done) {\n        // eslint-disable-next-line no-loop-func\n        const promise = new Promise<Uint8Array | null>((resolve) => {\n          // resolve = res;\n          port.onmessage = (event: MessageEvent) => {\n            if (event.data === null) {\n              done = true;\n              resolve(null);\n            } else {\n              resolve(event.data);\n            }\n          };\n        });\n        // eslint-disable-next-line no-await-in-loop\n        const value = await promise;\n        // eslint-disable-next-line no-await-in-loop\n        if (value !== null) {\n          yield value;\n        }\n      }\n    },\n  };\n}\n\nconst IPFSContentTransferHandler: TransferHandler<\n  IPFSContent | undefined,\n  IPFSContentTransferable | null\n> = {\n  canHandle: (obj: IPFSContent | undefined) =>\n    obj && obj.result && typeof obj.result[Symbol.asyncIterator] === 'function',\n  serialize(obj: IPFSContent) {\n    if (obj === undefined) {\n      return [null, []];\n    }\n    const { result, ...rest } = obj;\n    const { port1, port2 } = new MessageChannel();\n    if (result) {\n      (async () => {\n        // eslint-disable-next-line no-restricted-syntax\n        for await (const value of result) {\n          port1.postMessage(value);\n        }\n        port1.postMessage(null); // Send  \"end\" message\n\n        port1.close();\n      })();\n    }\n    return [{ ...rest, port: port2 }, [port2]];\n  },\n  deserialize(serializedObj: IPFSContentTransferable | null) {\n    if (!serializedObj) {\n      return undefined;\n    }\n    const { port, ...rest } = serializedObj;\n\n    return {\n      ...rest,\n      result: createAsyncIterable(port),\n    };\n  },\n};\n\nexport {\n  IPFSContentTransferHandler,\n  // serializeIPFSContent,\n  // deserializeIPFSContent,\n};\n","import {\n  wrap,\n  Remote,\n  proxy,\n  releaseProxy,\n  expose,\n  transferHandlers,\n} from 'comlink';\nimport { IPFSContentTransferHandler } from './serializers';\nimport { Observable, Observer, Subscribable, Subscription } from 'rxjs'; // v7.8.0\ntype WorkerType = SharedWorker | Worker;\n\nconst isSharedWorkersSupported = typeof SharedWorker !== 'undefined';\n\nconst isSharedWorkerUsed = isSharedWorkersSupported && !process.env.IS_DEV;\n\n// apply serializers for custom types\nfunction installTransferHandlers() {\n  transferHandlers.set('IPFSContent', IPFSContentTransferHandler);\n  transferHandlers.set('observable', {\n    canHandle: (value: unknown): value is Observable<unknown> => {\n      return value instanceof Observable;\n    },\n    deserialize: (value: MessagePort) => {\n      return new Observable<unknown>((observer) => {\n        const remote = transferHandlers\n          .get('proxy')!\n          .deserialize(value) as Remote<Subscribable<unknown>>;\n\n        remote\n          .subscribe(\n            proxy({\n              next: (next: unknown) => observer.next(next),\n              error: (error: unknown) => observer.error(error),\n              complete: () => observer.complete(),\n            })\n          )\n          .then((subscription) =>\n            observer.add(() => {\n              subscription.unsubscribe();\n              remote[releaseProxy]();\n            })\n          );\n      });\n    },\n    serialize: (value: Observable<unknown>) => {\n      return transferHandlers.get('proxy')!.serialize({\n        subscribe: (observer: Remote<Observer<unknown>>) =>\n          value.subscribe({\n            next: (next: unknown) => observer.next(next).then(),\n            error: (error: unknown) => observer.error(error).then(),\n            complete: () => observer.complete().then(),\n          }),\n      });\n    },\n  });\n\n  transferHandlers.set('subscription', {\n    canHandle: (value: unknown): value is Subscription => {\n      return value instanceof Subscription;\n    },\n    deserialize: (value: MessagePort) => {\n      return new Subscription(() => {\n        const remote = transferHandlers\n          .get('proxy')!\n          .deserialize(value) as Remote<Subscription>;\n\n        remote.unsubscribe().then(() => {\n          remote[releaseProxy]();\n        });\n      });\n    },\n    serialize: (value: Subscription) => {\n      return transferHandlers.get('proxy')!.serialize({\n        unsubscribe: () => value.unsubscribe(),\n      });\n    },\n  });\n}\n\nfunction safeStringify(obj: any): string {\n  try {\n    return JSON.stringify(obj);\n  } catch (error) {\n    return String(obj);\n  }\n}\n\n// Override console.log to send logs to main thread\nfunction overrideLogging(worker: Worker | MessagePort) {\n  const consoleLogMap = {\n    log: { original: console.log },\n    error: { original: console.error },\n    warn: { original: console.warn },\n  };\n  const replaceConsoleLog = (method: keyof typeof consoleLogMap) => {\n    const { original } = consoleLogMap[method];\n\n    consoleLogMap[method].original = console[method];\n\n    console[method] = (...args) => {\n      original.apply(console, args);\n      const serializableArgs = args.map((arg) => safeStringify(arg));\n\n      worker.postMessage({ type: 'console', method, args: serializableArgs });\n    };\n  };\n\n  Object.keys(consoleLogMap).forEach((method) =>\n    replaceConsoleLog(method as keyof typeof consoleLogMap)\n  );\n}\n\n// Install handlers for logging from worker\nfunction installLoggingHandler(worker: Worker | MessagePort, name: string) {\n  // Add event listener\n  worker.addEventListener('message', (event) => {\n    if (event.data.type === 'console') {\n      const { method, args } = event.data;\n\n      console[method](name, ...args);\n    }\n  });\n}\n\n// Create Shared Worker with fallback to usual Worker(in case of DEV too)\nexport function createWorkerApi<T>(\n  workerUrl: URL,\n  workerName: string\n): { worker: WorkerType; workerApiProxy: Remote<T> } {\n  installTransferHandlers();\n  //&& !process.env.IS_DEV\n  if (isSharedWorkerUsed) {\n    const worker = new SharedWorker(workerUrl, { name: workerName });\n    installLoggingHandler(worker.port, workerName);\n    return { worker, workerApiProxy: wrap<T>(worker.port) };\n  }\n\n  const worker = new Worker(workerUrl);\n  // installLoggingHandler(worker, workerName);\n  return { worker, workerApiProxy: wrap<T>(worker) };\n}\n\nexport function exposeWorkerApi<T>(worker: WorkerType, api: T) {\n  installTransferHandlers();\n  if (typeof worker.onconnect !== 'undefined') {\n    worker.onconnect = (e) => {\n      const port = e.ports[0];\n      overrideLogging(port);\n\n      expose(api, port);\n    };\n  } else {\n    // overrideLogging(worker);\n    expose(api);\n  }\n}\n","import { PinType } from 'ipfs-core-types/src/pin';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { Transaction } from 'src/services/backend/services/indexer/types';\nimport {\n  SenseChatExtension,\n  SenseLinkMeta,\n  SenseListItemtMeta,\n  SenseTransactionMeta,\n} from 'src/services/backend/types/sense';\nimport { IpfsContentType } from 'src/services/ipfs/types';\nimport { NeuronAddress, ParticleCid, TransactionHash } from 'src/types/base';\nimport { DtoToEntity } from 'src/types/dto';\n\ntype PinEntryType = Exclude<PinType, 'all'>;\n// example of db optimization for classifiers\n\nexport const PinTypeMap: Record<PinEntryType, number> = {\n  indirect: -1,\n  direct: 0,\n  recursive: 1,\n};\n\nexport enum EntryType {\n  transactions = 1,\n  particle = 2,\n  chat = 3,\n}\n\n// Transaction if formed by frontend\n// Should be replaced after sync\n\nexport type PinDbEntity = {\n  cid: string;\n  type: keyof typeof PinTypeMap;\n};\n\nexport type TransactionDbEntity = {\n  hash: string;\n  index: number;\n  type: string;\n  timestamp: number;\n  block_height: number;\n  value: Transaction['value'];\n  success: boolean;\n  memo: string;\n  neuron: NeuronAddress;\n};\n\ntype SyncItemMeta = DtoToEntity<\n  (SenseLinkMeta | SenseTransactionMeta) & SenseChatExtension\n>;\n\nexport type SyncStatusDbEntity = {\n  entry_type: EntryType;\n  id: NeuronAddress | ParticleCid;\n  owner_id: NeuronAddress;\n  timestamp_update: number;\n  timestamp_read: number;\n  disabled: boolean;\n  unread_count: number;\n  meta: SyncItemMeta;\n};\n\nexport type ParticleDbEntity = {\n  id: ParticleCid;\n  size: number;\n  size_local: number;\n  blocks: number;\n  mime: string;\n  type: IpfsContentType;\n  text: string;\n};\n\nexport type LinkDbEntity = {\n  from: ParticleCid;\n  to: ParticleCid;\n  neuron: NeuronAddress;\n  timestamp: number;\n  transaction_hash: string;\n};\n\nexport type ConfigDbEntity = {\n  key: string;\n  group_key: string;\n  value: NonNullable<unknown>;\n};\n\nexport enum SyncQueueStatus {\n  pending = 0,\n  executing = 1,\n  done = 2,\n  error = -1,\n}\n\nexport enum SyncQueueJobType {\n  particle = 0,\n  embedding = 1,\n}\n\nexport type SyncQueueKey = {\n  id: string;\n  job_type: SyncQueueJobType;\n};\n\nexport type SyncQueueDbEntity = SyncQueueKey & {\n  data: string;\n  status: SyncQueueStatus;\n  priority: QueuePriority | number;\n};\n\nexport type CommunityDbEntity = {\n  ownerId: NeuronAddress;\n  particle: ParticleCid;\n  neuron: NeuronAddress;\n  name: string;\n  following: boolean;\n  follower: boolean;\n};\n\nexport type EmbeddinsDbEntity = {\n  cid: ParticleCid;\n  vec: number[];\n};\n\nexport type DbEntity =\n  | TransactionDbEntity\n  | ParticleDbEntity\n  | SyncStatusDbEntity\n  | ConfigDbEntity\n  | SyncQueueDbEntity\n  | EmbeddinsDbEntity;\n","export function isParticle(value: string) {\n  // copied from src/utils/config.ts , to prevent crash in worker, need refactor\n  // import { PATTERN_IPFS_HASH } from 'src/utils/config';\n  return Boolean(value.match(/^Qm[a-zA-Z0-9]{44}$/g));\n}\n","import {\n  createAsyncThunk,\n  createSelector,\n  createSlice,\n  PayloadAction,\n} from '@reduxjs/toolkit';\nimport { SenseApi } from 'src/contexts/backend/services/senseApi';\nimport {\n  SenseItemLinkMeta,\n  SenseListItem,\n  SenseListItemTransactionMeta,\n} from 'src/services/backend/types/sense';\nimport { EntryType } from 'src/services/CozoDb/types/entities';\nimport {\n  MsgMultiSendValue,\n  MsgSendValue,\n} from 'src/services/backend/services/indexer/types';\nimport { RootState } from 'src/redux/store';\n// Add this import for generating unique thread IDs\nimport { SenseItemId } from '../types/sense';\nimport { isParticle } from '../../particle/utils';\nimport { isWorker } from 'src/constants/config';\n\n// similar to blockchain/tx/message type\nexport type SenseItem = {\n  id: SenseItemId;\n  transactionHash: string;\n\n  // add normal type\n  type: string;\n\n  meta: SenseListItem['meta'];\n  timestamp: string;\n  memo: string | undefined;\n  from: string;\n\n  // for optimistic update\n  status?: 'pending' | 'error';\n  fromLog?: boolean;\n};\n\ntype Chat = {\n  id: SenseItemId;\n  isLoading: boolean;\n  error: string | undefined;\n  data: SenseItem[];\n  unreadCount: number;\n};\n\ntype SliceState = {\n  list: {\n    isLoading: boolean;\n    data: string[];\n    error: string | undefined;\n  };\n  chats: {\n    [key in SenseItemId]?: Chat;\n  };\n  summary: {\n    unreadCount: {\n      total: number;\n      particles: number;\n      neurons: number;\n    };\n  };\n  llm: {\n    // Change from messages array to threads array\n    threads: LLMThread[];\n    currentThreadId: string | null; // Keep track of the currently selected thread\n  };\n};\n\nconst initialState: SliceState = {\n  list: {\n    isLoading: false,\n    data: [],\n    error: undefined,\n  },\n  chats: {},\n  summary: {\n    unreadCount: {\n      total: 0,\n      particles: 0,\n      neurons: 0,\n    },\n  },\n  llm: {\n    // Change from messages array to threads array\n    threads: !isWorker()\n      ? (JSON.parse(localStorage.getItem('llmThreads') || '[]') as LLMThread[])\n      : [],\n    currentThreadId: null, // Keep track of the currently selected thread\n  },\n};\n\nfunction formatApiData(item: SenseListItem): SenseItem {\n  if (item.entryType === EntryType.chat && item.meta.to) {\n    item.entryType = EntryType.particle;\n  }\n\n  const { meta } = item;\n\n  const formatted: SenseItem = {\n    timestamp: new Date(meta.timestamp).toISOString(),\n\n    // lol\n    transactionHash:\n      item.transactionHash ||\n      item.hash ||\n      item.meta.transaction_hash ||\n      item.meta.hash ||\n      item.meta.transactionHash,\n\n    memo: item.memo || meta.memo,\n\n    senseChatId: item.id,\n    // not good\n    unreadCount: item.unreadCount || 0,\n  };\n\n  switch (item.entryType) {\n    case EntryType.chat:\n    case EntryType.transactions: {\n      const meta = item.meta as SenseListItemTransactionMeta;\n      const { type } = meta;\n\n      let from = item.ownerId;\n\n      if (type === 'cosmos.bank.v1beta1.MsgSend') {\n        const value = meta.value as MsgSendValue;\n        from = value.fromAddress;\n      } else if (type === 'cosmos.bank.v1beta1.MsgMultiSend') {\n        const value = meta.value as MsgMultiSendValue;\n\n        from = value.inputs[0].address;\n      }\n\n      Object.assign(formatted, {\n        type,\n        from,\n        meta: item.meta.value,\n      });\n\n      break;\n    }\n\n    case EntryType.particle: {\n      const meta = item.meta as SenseItemLinkMeta;\n\n      Object.assign(formatted, {\n        type: 'cyber.graph.v1beta1.MsgCyberlink',\n        from: meta.neuron,\n        meta,\n        fromLog: true,\n      });\n\n      break;\n    }\n\n    default:\n      // sholdn't be\n      debugger;\n      return {};\n  }\n\n  return formatted;\n}\n\nconst getSenseList = createAsyncThunk(\n  'sense/getSenseList',\n  async (senseApi: SenseApi) => {\n    const data = await senseApi!.getList();\n    return data.map(formatApiData);\n  }\n);\n\nconst getSenseChat = createAsyncThunk(\n  'sense/getSenseChat',\n  async ({ id, senseApi }: { id: SenseItemId; senseApi: SenseApi }) => {\n    const particle = isParticle(id);\n\n    if (particle) {\n      const links = await senseApi!.getLinks(id);\n      const formattedLinks = links.map((item) => {\n        if (item.timestamp === 0) {\n          // FIXME:\n          return;\n        }\n        return formatApiData({\n          ...item,\n          id,\n          entryType: EntryType.particle,\n          meta: item,\n        });\n      });\n\n      return formattedLinks.filter(Boolean);\n    }\n\n    const data = await senseApi!.getFriendItems(id);\n    const formattedData = data.map((item) => {\n      const entryType = item.to ? EntryType.particle : EntryType.chat;\n      return formatApiData({\n        ...item,\n        entryType,\n        id,\n        meta: item,\n      });\n    });\n\n    return formattedData;\n  }\n);\n\nconst markAsRead = createAsyncThunk(\n  'sense/markAsRead',\n  async ({ id, senseApi }: { id: SenseItemId; senseApi: SenseApi }) => {\n    return senseApi!.markAsRead(id);\n  }\n);\n\nconst newChatStructure: Chat = {\n  id: '',\n  isLoading: false,\n  data: [],\n  error: undefined,\n  unreadCount: 0,\n};\n\nfunction checkIfMessageExists(chat: Chat, newMessage: SenseItem) {\n  const lastMessages = chat.data.slice(-5);\n\n  const isMessageExists = lastMessages.some((msg) => {\n    return msg.transactionHash === newMessage.transactionHash;\n  });\n\n  return isMessageExists;\n}\n\n// Add LLM types\nexport interface LLMMessage {\n  text: string;\n  sender: 'user' | 'llm';\n  timestamp: number;\n}\n\nexport interface LLMThread {\n  id: string;\n  title?: string;\n  dateUpdated: number;\n  messages: LLMMessage[];\n}\n\nconst slice = createSlice({\n  name: 'sense',\n  initialState,\n  reducers: {\n    // backend may push this action\n    updateSenseList: {\n      reducer: (state, action: PayloadAction<SenseItem[]>) => {\n        const data = action.payload;\n\n        data.forEach((message) => {\n          const { senseChatId: id } = message;\n\n          if (!state.chats[id]) {\n            state.chats[id] = { ...newChatStructure };\n          }\n\n          const chat = state.chats[id]!;\n\n          Object.assign(chat, {\n            id,\n            // fix ts\n            unreadCount: message.unreadCount || 0,\n          });\n\n          if (!checkIfMessageExists(chat, message)) {\n            chat.data = chat.data.concat(message);\n          }\n        });\n\n        slice.caseReducers.orderSenseList(state);\n      },\n      prepare: (data: SenseListItem[]) => {\n        return {\n          payload: data.map(formatApiData),\n        };\n      },\n    },\n    // optimistic update\n    addSenseItem(\n      state,\n      action: PayloadAction<{ id: SenseItemId; item: SenseItem }>\n    ) {\n      const { id, item } = action.payload;\n      const chat = state.chats[id]!;\n\n      chat.data.push({\n        ...item,\n        meta: item.meta,\n        status: 'pending',\n      });\n\n      const newList = state.list.data.filter((item) => item !== id);\n      newList.unshift(id);\n      state.list.data = newList;\n    },\n    // optimistic confirm/error\n    updateSenseItem(\n      state,\n      action: PayloadAction<{\n        chatId: SenseItemId;\n        txHash: string;\n        isSuccess: boolean;\n      }>\n    ) {\n      const { chatId, txHash, isSuccess } = action.payload;\n      const chat = state.chats[chatId]!;\n\n      const item = chat.data.find((item) => item.transactionHash === txHash);\n\n      if (item) {\n        if (isSuccess) {\n          delete item.status;\n        } else {\n          item.status = 'error';\n        }\n      }\n    },\n    orderSenseList(state) {\n      const chatsLastMessage = Object.keys(state.chats).reduce<\n        {\n          id: string;\n          lastMsg: SenseItem;\n        }[]\n      >((acc, id) => {\n        const chat = state.chats[id]!;\n\n        // may be loading this moment, no data\n        if (!chat.data.length) {\n          return acc;\n        }\n\n        const lastMsg = chat.data[chat.data.length - 1];\n        acc.push({ id, lastMsg });\n\n        return acc;\n      }, []);\n\n      const sorted = chatsLastMessage.sort((a, b) => {\n        return (\n          Date.parse(b.lastMsg.timestamp) - Date.parse(a.lastMsg.timestamp)\n        );\n      });\n\n      state.list.data = sorted.map((i) => i.id);\n    },\n    reset() {\n      return initialState;\n    },\n    // LLM reducers\n    createLLMThread(\n      state,\n      action: PayloadAction<{ id: string; title?: string }>\n    ) {\n      const newThread: LLMThread = {\n        id: action.payload.id,\n        messages: [],\n        dateUpdated: Date.now(),\n        title:\n          action.payload.title ||\n          `Conversation ${state.llm.threads.length + 1}`,\n      };\n      state.llm.threads.push(newThread);\n      state.llm.currentThreadId = action.payload.id;\n      localStorage.setItem('llmThreads', JSON.stringify(state.llm.threads));\n    },\n\n    selectLLMThread(state, action: PayloadAction<{ id: string }>) {\n      state.llm.currentThreadId = action.payload.id;\n    },\n\n    addLLMMessageToThread(\n      state,\n      action: PayloadAction<{ threadId: string; message: LLMMessage }>\n    ) {\n      const thread = state.llm.threads.find(\n        (t) => t.id === action.payload.threadId\n      );\n      if (thread) {\n        thread.messages.push(action.payload.message);\n        thread.dateUpdated = action.payload.message.timestamp;\n        localStorage.setItem('llmThreads', JSON.stringify(state.llm.threads));\n      }\n    },\n\n    // Add action to replace the last message (for updating \"waiting...\" message)\n    replaceLastLLMMessageInThread(\n      state,\n      action: PayloadAction<{ threadId: string; message: LLMMessage }>\n    ) {\n      const thread = state.llm.threads.find(\n        (t) => t.id === action.payload.threadId\n      );\n      if (thread && thread.messages.length > 0) {\n        thread.messages[thread.messages.length - 1] = action.payload.message;\n        localStorage.setItem('llmThreads', JSON.stringify(state.llm.threads));\n      }\n    },\n\n    deleteLLMThread(state, action: PayloadAction<{ id: string }>) {\n      const newT = state.llm.threads.filter(\n        (thread) => thread.id !== action.payload.id\n      );\n\n      console.log('newT', newT);\n\n      state.llm.threads = newT;\n\n      if (state.llm.currentThreadId === action.payload.id) {\n        state.llm.currentThreadId = null;\n      }\n\n      // Object.assign(state.llm, {\n      //   threads: newT,\n      // });\n\n      localStorage.setItem('llmThreads', JSON.stringify(state.llm.threads));\n    },\n\n    clearLLMThreads(state) {\n      state.llm.threads = [];\n      state.llm.currentThreadId = null;\n      localStorage.removeItem('llmThreads');\n    },\n  },\n\n  extraReducers: (builder) => {\n    builder.addCase(getSenseList.pending, (state) => {\n      state.list.isLoading = true;\n    });\n\n    builder.addCase(getSenseList.fulfilled, (state, action) => {\n      state.list.isLoading = false;\n\n      const newList: SliceState['list']['data'] = [];\n\n      action.payload.forEach((message) => {\n        const { senseChatId: id } = message;\n\n        if (!state.chats[id]) {\n          state.chats[id] = { ...newChatStructure };\n        }\n\n        const chat = state.chats[id]!;\n\n        Object.assign(chat, {\n          id,\n          // fix\n          unreadCount: message.unreadCount || 0,\n        });\n\n        if (!checkIfMessageExists(chat, message)) {\n          chat.data = chat.data.concat(message);\n        }\n\n        newList.push(id);\n      });\n\n      state.list.data = newList;\n    });\n    builder.addCase(getSenseList.rejected, (state, action) => {\n      console.error(action);\n\n      state.list.isLoading = false;\n      state.list.error = action.error.message;\n    });\n\n    builder.addCase(getSenseChat.pending, (state, action) => {\n      const { id } = action.meta.arg;\n\n      if (!state.chats[id]) {\n        state.chats[id] = { ...newChatStructure };\n      }\n\n      // don't understand why ts warning\n      state.chats[id].isLoading = true;\n    });\n\n    builder.addCase(getSenseChat.fulfilled, (state, action) => {\n      const { id } = action.meta.arg;\n      const chat = state.chats[id]!;\n      chat.isLoading = false;\n\n      chat.id = id;\n\n      chat.data = action.payload;\n    });\n    builder.addCase(getSenseChat.rejected, (state, action) => {\n      console.error(action);\n\n      const chat = state.chats[action.meta.arg.id]!;\n      chat.isLoading = false;\n      chat.error = action.error.message;\n    });\n\n    // maybe add .pending, .rejected\n    // can be optimistic\n    builder.addCase(markAsRead.fulfilled, (state, action) => {\n      const { id } = action.meta.arg;\n      const chat = state.chats[id]!;\n\n      const particle = isParticle(id);\n\n      const { unreadCount } = chat;\n\n      state.summary.unreadCount.total -= unreadCount;\n      if (particle) {\n        state.summary.unreadCount.particles -= unreadCount;\n      } else {\n        state.summary.unreadCount.neurons -= unreadCount;\n      }\n\n      chat.unreadCount = 0;\n    });\n  },\n});\n\nconst selectUnreadCounts = createSelector(\n  (state: RootState) => state.sense.chats,\n  (chats) => {\n    let unreadCountParticle = 0;\n    let unreadCountNeuron = 0;\n\n    Object.values(chats).forEach(({ id, unreadCount }) => {\n      const particle = isParticle(id);\n\n      if (particle) {\n        unreadCountParticle += unreadCount;\n      } else {\n        unreadCountNeuron += unreadCount;\n      }\n    });\n\n    const total = unreadCountParticle + unreadCountNeuron;\n\n    return {\n      total,\n      particles: unreadCountParticle,\n      neurons: unreadCountNeuron,\n    };\n  }\n);\n\nexport const {\n  addSenseItem,\n  updateSenseItem,\n  updateSenseList,\n  reset,\n  createLLMThread,\n  deleteLLMThread,\n  selectLLMThread,\n  addLLMMessageToThread,\n  replaceLastLLMMessageInThread,\n  clearLLMThreads,\n} = slice.actions;\n\nexport { getSenseList, getSenseChat, markAsRead };\n\n// selectors\nexport { selectUnreadCounts };\n\nexport default slice.reducer;\n","export const localStorageKeys = {\n  pocket: {\n    POCKET: 'pocket',\n    POCKET_ACCOUNT: 'pocketAccount',\n  },\n  MENU_SHOW: 'menuShow',\n  settings: {\n    adviserAudio: 'adviserAudio',\n    adviserVoice: 'adviserVoice',\n  },\n};\n","import { Dispatch } from 'redux';\nimport { localStorageKeys } from 'src/constants/localStorageKeys';\n\nimport {\n  Account,\n  AccountValue,\n  Accounts,\n  DefaultAccount,\n} from 'src/types/defaultAccount';\nimport { PayloadAction, createSlice } from '@reduxjs/toolkit';\nimport { POCKET } from '../../utils/config';\nimport { RootState } from '../store';\n\ntype SliceState = {\n  actionBar: {\n    tweet: string;\n  };\n  defaultAccount: DefaultAccount;\n  accounts: null | Accounts;\n  isInitialized: boolean;\n};\n\nconst initialState: SliceState = {\n  actionBar: {\n    tweet: POCKET.STAGE_TWEET_ACTION_BAR.TWEET, // stage for tweet ActionBar: 'addAvatar' 'follow' 'tweet'\n  },\n  isInitialized: false,\n  defaultAccount: {\n    name: null,\n    account: null,\n  },\n  accounts: null,\n};\n\nconst checkAddress = (obj, network, address) =>\n  Object.keys(obj).some((k) => {\n    if (obj[k][network]) {\n      return obj[k][network].bech32 === address;\n    }\n  });\n\nfunction saveToLocalStorage(state: SliceState) {\n  const { defaultAccount, accounts } = state;\n\n  defaultAccount &&\n    localStorage.setItem(\n      localStorageKeys.pocket.POCKET,\n      JSON.stringify({\n        [defaultAccount.name]: defaultAccount.account,\n      })\n    );\n  accounts &&\n    localStorage.setItem(\n      localStorageKeys.pocket.POCKET_ACCOUNT,\n      JSON.stringify(accounts)\n    );\n}\n\nconst slice = createSlice({\n  name: 'pocket',\n  initialState,\n  reducers: {\n    setDefaultAccount: (\n      state,\n      {\n        payload: { name, account },\n      }: PayloadAction<{ name: string; account?: Account }>\n    ) => {\n      state.defaultAccount = {\n        name,\n        account: account || state.accounts?.[name] || null,\n      };\n\n      saveToLocalStorage(state);\n    },\n    setAccounts: (state, { payload }: PayloadAction<Accounts>) => {\n      state.accounts = payload;\n\n      saveToLocalStorage(state);\n    },\n    setInitialized: (state) => {\n      state.isInitialized = true;\n    },\n    setStageTweetActionBar: (state, { payload }: PayloadAction<string>) => {\n      state.actionBar.tweet = payload;\n    },\n\n    // bullshit\n    deleteAddress: (state, { payload }: PayloadAction<string>) => {\n      if (state.accounts) {\n        Object.keys(state.accounts).forEach((accountKey) => {\n          Object.keys(state.accounts[accountKey]).forEach((networkKey) => {\n            if (state.accounts[accountKey][networkKey].bech32 === payload) {\n              delete state.accounts[accountKey][networkKey];\n\n              if (Object.keys(state.accounts[accountKey]).length === 0) {\n                delete state.accounts[accountKey];\n              }\n\n              if (state.defaultAccount?.account?.cyber?.bech32 === payload) {\n                const entries = Object.entries(state.accounts);\n\n                const entryCyber = entries.find(\n                  ([, value]) => value.cyber?.bech32\n                );\n\n                if (entryCyber) {\n                  state.defaultAccount = {\n                    name: entryCyber[0],\n                    account: entryCyber[1],\n                  };\n                } else {\n                  state.defaultAccount = {\n                    name: null,\n                    account: null,\n                  };\n                }\n              }\n\n              saveToLocalStorage(state);\n            }\n          });\n        });\n      }\n    },\n  },\n});\n\nexport const selectCurrentAddress = (store: RootState) =>\n  store.pocket.defaultAccount.account?.cyber?.bech32;\n\nexport const {\n  setDefaultAccount,\n  setAccounts,\n  setStageTweetActionBar,\n  deleteAddress,\n} = slice.actions;\n\nexport default slice.reducer;\n\n// refactor this\nexport const initPocket = () => (dispatch: Dispatch) => {\n  let defaultAccounts = null;\n  let defaultAccountsKeys = null;\n  let accountsTemp: Accounts | null = null;\n\n  const localStoragePocketAccount = localStorage.getItem(\n    localStorageKeys.pocket.POCKET_ACCOUNT\n  );\n  const localStoragePocket = localStorage.getItem(\n    localStorageKeys.pocket.POCKET\n  );\n  if (localStoragePocket !== null) {\n    const localStoragePocketData = JSON.parse(localStoragePocket);\n    const keyPocket = Object.keys(localStoragePocketData)[0];\n    const accountPocket = Object.values(localStoragePocketData)[0];\n    defaultAccounts = accountPocket;\n    defaultAccountsKeys = keyPocket;\n  }\n  if (localStoragePocketAccount !== null) {\n    const localStoragePocketAccountData = JSON.parse(localStoragePocketAccount);\n    if (localStoragePocket === null) {\n      const keys0 = Object.keys(localStoragePocketAccountData)[0];\n      localStorage.setItem(\n        localStorageKeys.pocket.POCKET,\n        JSON.stringify({ [keys0]: localStoragePocketAccountData[keys0] })\n      );\n      defaultAccounts = localStoragePocketAccountData[keys0];\n      defaultAccountsKeys = keys0;\n    } else if (defaultAccountsKeys !== null) {\n      accountsTemp = {\n        [defaultAccountsKeys]:\n          localStoragePocketAccountData[defaultAccountsKeys] || undefined,\n        ...localStoragePocketAccountData,\n      };\n    }\n  } else {\n    localStorage.removeItem(localStorageKeys.pocket.POCKET);\n    localStorage.removeItem(localStorageKeys.pocket.POCKET_ACCOUNT);\n  }\n\n  defaultAccountsKeys &&\n    defaultAccounts &&\n    dispatch(\n      setDefaultAccount({\n        name: defaultAccountsKeys,\n        account: defaultAccounts,\n      })\n    );\n\n  accountsTemp &&\n    Object.keys(accountsTemp).forEach((key) => {\n      if (!accountsTemp[key] || Object.keys(accountsTemp[key]).length === 0) {\n        delete accountsTemp[key];\n      }\n    });\n\n  accountsTemp && dispatch(setAccounts(accountsTemp));\n\n  dispatch(slice.actions.setInitialized());\n};\n\nconst defaultNameAccount = () => {\n  let key = 'Account 1';\n  let count = 1;\n\n  const localStorageCount = localStorage.getItem('count');\n\n  if (localStorageCount !== null) {\n    const dataCount = JSON.parse(localStorageCount);\n    count = parseFloat(dataCount);\n    key = `Account ${count}`;\n  }\n\n  localStorage.setItem('count', JSON.stringify(count + 1));\n\n  return key;\n};\n\nexport const addAddressPocket =\n  (accounts: AccountValue) => (dispatch: Dispatch) => {\n    const key = accounts.name || defaultNameAccount();\n\n    let dataPocketAccount = null;\n    let valueObj = {};\n    let pocketAccount: Accounts = {};\n\n    const localStorageStory = localStorage.getItem(\n      localStorageKeys.pocket.POCKET_ACCOUNT\n    );\n\n    if (localStorageStory !== null) {\n      dataPocketAccount = JSON.parse(localStorageStory);\n      valueObj = Object.values(dataPocketAccount);\n    }\n\n    const isAdded = !checkAddress(valueObj, 'cyber', accounts.bech32);\n\n    if (!isAdded) {\n      return;\n    }\n\n    const cyberAccounts: Account = {\n      cyber: accounts,\n    };\n\n    if (localStorageStory !== null) {\n      pocketAccount = { [key]: cyberAccounts, ...dataPocketAccount };\n    } else {\n      pocketAccount = { [key]: cyberAccounts };\n    }\n\n    if (Object.keys(pocketAccount).length > 0) {\n      dispatch(setAccounts(pocketAccount));\n      if (accounts.keys !== 'read-only') {\n        dispatch(setDefaultAccount({ name: key, account: cyberAccounts }));\n      }\n    }\n  };\n","export const CYB_BROADCAST_CHANNEL = 'cyb-broadcast-channel';\nexport const CYB_QUEUE_CHANNEL = 'cyb-queue-channel';\n","import { updateSenseList } from 'src/features/sense/redux/sense.redux';\nimport { setDefaultAccount } from 'src/redux/features/pocket';\nimport { Account } from 'src/types/defaultAccount';\nimport { SenseListItem } from '../types/sense';\nimport {\n  BroadcastChannelMessage,\n  ServiceName,\n  ServiceStatus,\n  SyncEntryName,\n  SyncProgress,\n} from '../types/services';\nimport { CYB_BROADCAST_CHANNEL } from './consts';\n\nclass BroadcastChannelSender {\n  private channel: BroadcastChannel;\n\n  constructor() {\n    this.channel = new BroadcastChannel(CYB_BROADCAST_CHANNEL);\n  }\n\n  public postServiceStatus(\n    name: ServiceName,\n    status: ServiceStatus,\n    message?: string\n  ) {\n    this.channel.postMessage({\n      type: 'service_status',\n      value: { name, status, message },\n    });\n  }\n\n  public postSyncEntryProgress(entry: SyncEntryName, state: SyncProgress) {\n    // console.log('postSyncEntryProgress', entry, state);\n    this.channel.postMessage({ type: 'sync_entry', value: { entry, state } });\n  }\n\n  public postMlSyncEntryProgress(entry: string, state: SyncProgress) {\n    // console.log('postMlSyncEntryProgress', entry, state);\n    this.channel.postMessage({\n      type: 'sync_ml_entry',\n      value: { entry, state },\n    });\n  }\n\n  public postSenseUpdate(senseList: SenseListItem[]) {\n    // console.log('postSenseUpdate', senseList);\n    if (senseList.length > 0) {\n      this.channel.postMessage(updateSenseList(senseList));\n    }\n  }\n\n  public postSetDefaultAccount(name: string, account?: Account) {\n    this.channel.postMessage(\n      setDefaultAccount({\n        name,\n        account,\n      })\n    );\n  }\n\n  post(msg: BroadcastChannelMessage) {\n    this.channel.postMessage(msg);\n  }\n}\n\nexport default BroadcastChannelSender;\n","import { createCyblogChannel } from 'src/utils/logging/cyblog';\nimport {\n  ProgressTracking,\n  SyncEntryName,\n  SyncProgress,\n} from '../types/services';\nimport BroadcastChannelSender from './BroadcastChannelSender';\n\nexport const broadcastStatus = (\n  name: SyncEntryName,\n  channelApi: BroadcastChannelSender\n) => {\n  // const cyblogCh = createCyblogChannel({ thread: 'bckd', module: name });\n  return {\n    sendStatus: (\n      status: SyncProgress['status'],\n      message?: string,\n      progress?: ProgressTracking\n    ) => {\n      // cyblogCh.info(`>>>$ sync ${name} status: ${status} message: ${message}`);\n      channelApi.postSyncEntryProgress(name, {\n        status,\n        message,\n        progress,\n        done: ['active', 'error', 'listen'].some((s) => s === status),\n      });\n    },\n  };\n};\n","async function* arrayToAsyncIterable<T>(array: T[]): AsyncIterable<T> {\n  // eslint-disable-next-line no-restricted-syntax\n  for (const item of array) {\n    yield item;\n  }\n}\n\nasync function asyncIterableBatchProcessor<T, K>(\n  items: AsyncIterable<T> | Iterable<T>,\n  batchProcess: (arg: T[]) => Promise<K>,\n  batchSize = 10\n): Promise<void> {\n  let batch = [];\n  // eslint-disable-next-line no-restricted-syntax\n  for await (const item of items) {\n    batch.push(item);\n    if (batch.length === batchSize) {\n      await batchProcess(batch);\n      batch = [];\n    }\n  }\n  // process the rest\n  if (batch.length > 0) {\n    await batchProcess(batch);\n  }\n}\n\nasync function asyncIterableToArray<T>(asyncIterable: AsyncIterable<T>) {\n  const resultArray = [];\n  // eslint-disable-next-line no-restricted-syntax\n  for await (const item of asyncIterable) {\n    resultArray.push(item);\n  }\n  return resultArray;\n}\n// Create a helper function to create AsyncIterable from a list and iterate one by one\nfunction createAsyncIterable<T>(data: T[]): AsyncIterable<T> {\n  let index = 0;\n  return {\n    [Symbol.asyncIterator]() {\n      return {\n        next(): Promise<IteratorResult<T>> {\n          if (index < data.length) {\n            return Promise.resolve({ done: false, value: data[index++] });\n          }\n          return Promise.resolve({ done: true, value: undefined as any });\n        },\n      };\n    },\n  };\n}\n\n// eslint-disable-next-line import/prefer-default-export\nexport async function* fetchIterableByOffset<T, P>(\n  fetchFunction: (params: P & { offset: number }) => Promise<T[]>,\n  params: P\n): AsyncGenerator<T[], void, undefined> {\n  let offset = 0;\n  while (true) {\n    // eslint-disable-next-line no-await-in-loop\n    const items = await fetchFunction({ ...params, offset });\n\n    if (items.length === 0) {\n      break;\n    }\n\n    yield items;\n\n    offset += items.length;\n  }\n}\n\nexport {\n  arrayToAsyncIterable,\n  asyncIterableBatchProcessor,\n  asyncIterableToArray,\n  createAsyncIterable,\n};\n","import { SyncQueueJobType } from 'src/services/CozoDb/types/entities';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { IPFSContent } from 'src/services/ipfs/types';\nimport { LinkDto } from 'src/services/CozoDb/types/dto';\n\nimport { getTextContentIfShouldEmbed } from '../../services/sync/services/ParticlesResolverQueue/ParticlesResolverQueue';\nimport { CYB_QUEUE_CHANNEL } from '../consts';\nimport { QueueChannelMessage } from './types';\n\nexport const createBackendQueueSender = () => {\n  const channel = new BroadcastChannel(CYB_QUEUE_CHANNEL);\n\n  return {\n    enqueue: (msg: QueueChannelMessage) => {\n      channel.postMessage(msg);\n    },\n  };\n};\n\nconst busSender = createBackendQueueSender();\n\nexport const enqueueParticleEmbeddingMaybe = async (content: IPFSContent) => {\n  const contentToEmbed = await getTextContentIfShouldEmbed(content);\n\n  if (contentToEmbed) {\n    busSender.enqueue({\n      type: 'sync',\n      data: {\n        id: content.cid,\n        data: contentToEmbed,\n        jobType: SyncQueueJobType.embedding,\n        priority: QueuePriority.MEDIUM,\n      },\n    });\n  }\n\n  return !!contentToEmbed;\n};\n\nexport const enqueueParticleSave = (content: IPFSContent) => {\n  busSender.enqueue({\n    type: 'particle',\n    // TODO: add AsyncIterator serializer\n    data: { ...content, result: undefined } as IPFSContent,\n  });\n\n  return true;\n};\n\nexport const enqueueLinksSave = (links: LinkDto[]) => {\n  busSender.enqueue({\n    type: 'link',\n    data: links,\n  });\n};\n","// export const CID_AVATAR = 'Qmf89bXkJH9jw4uaLkHmZkxQ51qGKfUPtAMxA8rTwBrmTs';\nexport const CID_TWEET = 'QmbdH2WBamyKLPE5zu4mJ9v49qvY8BFfoumoVPMR5V4Rvx';\n\nexport const CID_FOLLOW = 'QmPLSA5oPqYxgc8F7EwrM8WS9vKrr1zPoDniSRFh8HSrxx';\n\nexport const INFINITY = '';\n\nexport const WP =\n  'https://ipfs.io/ipfs/QmQ1Vong13MDNxixDyUdjniqqEj8sjuNEBYMyhQU4gQgq3';\n\nexport const CYBER_CONGRESS_ADDRESS =\n  'bostrom1xszmhkfjs3s00z2nvtn7evqxw3dtus6yr8e4pw';\n","import { CID_FOLLOW, CID_TWEET } from 'src/constants/app';\nimport { SyncEntryName } from 'src/services/backend/types/services';\n\nexport const MY_PARTICLES_SYNC_INTERVAL = 5 * 60 * 1000; // 60 sec\nexport const MY_FRIENDS_SYNC_INTERVAL = 5 * 60 * 1000; // 60 sec\nexport const IPFS_SYNC_INTERVAL = 15 * 60 * 1000; // 15 minutes\n\nexport const MAX_DATABASE_PUT_SIZE = 500;\n\nexport const MAX_LINKS_RESOLVE_BATCH = 20;\n\nexport const DAY_IN_MS = 24 * 60 * 60 * 1000;\n\nexport const SENSE_FRIEND_PARTICLES = [CID_TWEET, CID_FOLLOW];\n\nexport const SYNC_ENTRIES_TO_TRACK_PROGRESS = [\n  'my-friends',\n  'particles',\n  'transactions',\n] as SyncEntryName[];\n","import {\n  BehaviorSubject,\n  Observable,\n  filter,\n  mergeMap,\n  tap,\n  map,\n  combineLatest,\n  share,\n  EMPTY,\n  Subject,\n  first,\n} from 'rxjs';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport { broadcastStatus } from 'src/services/backend/channels/broadcastStatus';\nimport { ParticleCid } from 'src/types/base';\nimport {\n  SyncQueueJobType,\n  SyncQueueStatus,\n} from 'src/services/CozoDb/types/entities';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { asyncIterableBatchProcessor } from 'src/utils/async/iterable';\n\nimport { enqueueParticleEmbeddingMaybe } from 'src/services/backend/channels/BackendQueueChannel/backendQueueSenders';\n\nimport { PATTERN_COSMOS, PATTERN_CYBER } from 'src/constants/patterns';\nimport { EmbeddingApi } from 'src/services/backend/workers/background/api/mlApi';\nimport { Option } from 'src/types';\n\nimport { IPFSContent } from 'src/services/ipfs/types';\nimport { FetchIpfsFunc } from '../../types';\nimport { ServiceDeps } from '../types';\nimport { SyncQueueItem } from './types';\nimport { MAX_DATABASE_PUT_SIZE } from '../consts';\n\nimport DbApi from '../../../DbApi/DbApi';\n\nconst QUEUE_BATCH_SIZE = 100;\n\nconst getContentToEmbed = async (content: IPFSContent) => {\n  const contentType = content?.meta?.contentType || '';\n\n  // create embedding for allowed content\n  if (contentType === 'text') {\n    return [contentType, content.textPreview];\n  }\n\n  return [contentType, undefined];\n};\n\nexport const getTextContentIfShouldEmbed = async (content: IPFSContent) => {\n  const [contentType, data] = await getContentToEmbed(content);\n\n  let shouldEmbed = contentType === 'text' && !!data;\n\n  shouldEmbed =\n    shouldEmbed &&\n    (!data!.match(PATTERN_COSMOS) || !data!.match(PATTERN_CYBER));\n\n  return shouldEmbed ? data : undefined;\n};\n\nclass ParticlesResolverQueue {\n  public isInitialized$: Observable<boolean>;\n\n  private db: Option<DbApi>;\n\n  private embeddingApi: Option<EmbeddingApi>;\n\n  private get canEmbed() {\n    return !!this.embeddingApi;\n  }\n\n  private waitForParticleResolve: FetchIpfsFunc;\n\n  private statusApi = broadcastStatus('resolver', new BroadcastChannelSender());\n\n  private _syncQueue$ = new BehaviorSubject<Map<ParticleCid, SyncQueueItem>>(\n    new Map()\n  );\n\n  public get queue(): Map<ParticleCid, SyncQueueItem> {\n    return this._syncQueue$.getValue();\n  }\n\n  private _loop$: Observable<any> | undefined;\n\n  public get loop$(): Observable<any> | undefined {\n    return this._loop$;\n  }\n\n  constructor(deps: ServiceDeps) {\n    if (!deps.waitForParticleResolve) {\n      throw new Error('waitForParticleResolve is not defined');\n    }\n\n    this.waitForParticleResolve = deps.waitForParticleResolve;\n\n    deps.embeddingApi$.subscribe((embeddingApi) => {\n      this.embeddingApi = embeddingApi;\n      // if embedding function is provided, retriger the queue\n      if (this.queue.size > 0) {\n        this._syncQueue$.next(this.queue);\n      }\n    });\n\n    deps.dbInstance$\n      .pipe(\n        first((value) => value !== undefined) // Automatically unsubscribes after the first valid value\n      )\n      .subscribe(async (db) => {\n        this.db = db;\n        await this.loadSyncQueue();\n      });\n\n    this.isInitialized$ = combineLatest([\n      deps.dbInstance$,\n      deps.ipfsInstance$,\n    ]).pipe(\n      map(([dbInstance, ipfsInstance]) => !!ipfsInstance && !!dbInstance)\n    );\n  }\n\n  private async resolveIpfsParticle(id: ParticleCid, priority: QueuePriority) {\n    return this.waitForParticleResolve(id, priority)\n      .then(async ({ status, result }) => {\n        const isResolved = status !== 'not_found';\n        if (!isResolved || !result) {\n          return false;\n        }\n\n        await enqueueParticleEmbeddingMaybe(result);\n        return true;\n      })\n      .catch(() => false);\n  }\n\n  private async saveEmbedding(cid: ParticleCid, text: string) {\n    try {\n      const hasItem = await this.db!.existEmbedding(cid);\n\n      if (!hasItem) {\n        const vec = await this.embeddingApi!.createEmbedding(text);\n\n        const result = await this.db!.putEmbedding(cid, vec);\n      }\n\n      return true;\n    } catch (err) {\n      console.error(`saveEmbedding error: ${cid} - ${text} `, err.toString());\n      return false;\n    }\n  }\n\n  private async processSyncQueue(pendingItems: SyncQueueItem[]) {\n    // console.log('------processSyncQueue ', pendingItems);\n\n    const batchSize = pendingItems.length;\n\n    this.statusApi.sendStatus(\n      'in-progress',\n      `processing batch ${batchSize}/${batchSize} batch. ${this.queue.size} pending...`\n    );\n\n    let i = batchSize;\n    await Promise.all(\n      pendingItems.map(async (item) => {\n        const { id, jobType, data } = item;\n\n        let jobPromise = Promise.resolve(false);\n\n        if (jobType === SyncQueueJobType.embedding && data) {\n          jobPromise = this.saveEmbedding(id, data as string);\n        } else if (jobType === SyncQueueJobType.particle) {\n          jobPromise = this.resolveIpfsParticle(id, QueuePriority.MEDIUM);\n        }\n\n        // eslint-disable-next-line no-await-in-loop\n        return jobPromise.then(async (result) => {\n          if (result) {\n            await this.db!.removeSyncQueue({ id, jobType });\n          } else {\n            await this.db!.updateSyncQueue({\n              id,\n              jobType,\n              status: SyncQueueStatus.error,\n            });\n          }\n\n          const queue = this._syncQueue$.value;\n          queue.delete(id);\n          i--;\n          this._syncQueue$.next(queue);\n\n          this.statusApi.sendStatus(\n            'in-progress',\n            `processing batch ${batchSize - i}/${batchSize} batch. ${\n              this.queue.size\n            } pending...`\n          );\n        });\n      })\n    );\n  }\n\n  start() {\n    const source$ = this.isInitialized$.pipe(\n      tap((q) => console.log(`sync queue isInitialized - ${q}`)),\n      filter((isInitialized) => isInitialized === true),\n      mergeMap(() => this._syncQueue$), // Merge the queue$ stream here.\n      // tap((q) => console.log(`sync queue - ${q.size}`)),\n      filter((q) => q.size > 0),\n      mergeMap((queue) => {\n        const list = [...queue.values()];\n\n        const executingCount = list.filter(\n          (i) => i.status === SyncQueueStatus.executing\n        ).length;\n\n        const batchSize = QUEUE_BATCH_SIZE - executingCount;\n\n        const jobTypeFilter = (i: SyncQueueItem) =>\n          i.jobType === SyncQueueJobType.particle ||\n          (i.jobType === SyncQueueJobType.embedding && this.canEmbed);\n\n        if (batchSize > 0) {\n          const pendingItems = list\n            .filter(\n              (i) => i.status === SyncQueueStatus.pending && jobTypeFilter(i)\n            )\n            .sort((a, b) => {\n              return a.priority - b.priority;\n            })\n            .slice(0, batchSize);\n\n          if (pendingItems.length > 0) {\n            pendingItems.forEach((i) => {\n              queue.set(i.id, {\n                ...i,\n                status: SyncQueueStatus.executing,\n              });\n            });\n\n            this._syncQueue$.next(queue);\n\n            this.statusApi.sendStatus('in-progress', `starting...`);\n            return this.processSyncQueue(pendingItems);\n          }\n        }\n\n        return EMPTY;\n      })\n    );\n\n    this._loop$ = source$.pipe(share());\n\n    this._loop$.subscribe({\n      next: (result) => {\n        this.statusApi.sendStatus('active');\n      },\n      error: (err) => this.statusApi.sendStatus('error', err.toString()),\n    });\n\n    return this;\n  }\n\n  public async enqueueBatch(\n    cids: ParticleCid[],\n    jobType: SyncQueueJobType,\n    priority: QueuePriority\n  ) {\n    return asyncIterableBatchProcessor(\n      cids,\n      (cids) =>\n        this.enqueue(\n          cids.map((cid) => ({\n            id: cid /* from is tweet */,\n            priority,\n            jobType,\n          }))\n        ),\n      MAX_DATABASE_PUT_SIZE\n    );\n  }\n\n  public async enqueue(items: SyncQueueItem[]) {\n    if (items.length === 0) {\n      return;\n    }\n\n    const result = await this.db!.putSyncQueue(items);\n\n    const queue = this._syncQueue$.value;\n\n    items.forEach((item) =>\n      queue.set(item.id, { ...item, status: SyncQueueStatus.pending })\n    );\n    this._syncQueue$.next(queue);\n  }\n\n  private async loadSyncQueue() {\n    const queue = await this.db!.getSyncQueue({\n      statuses: [SyncQueueStatus.pending],\n    }).then((items) => new Map(items.map((item) => [item.id, item])));\n\n    this._syncQueue$.next(new Map([...queue, ...this.queue]));\n  }\n}\n\nexport default ParticlesResolverQueue;\n","export function shortenString(string: string, length = 300) {\n  return string.length > length ? `${string.slice(0, length)}...` : string;\n}\n\nexport function replaceQuotes(string: string) {\n  return string.replace(/\"/g, \"'\");\n}\n\nexport function serializeString(input: string): string {\n  return input\n    .replace(/\\\\/g, '\\\\\\\\') // Escape backslashes\n    .replace(/\"/g, \"\\\\''\") // Escape double quotes\n    .replace(/'/g, \"\\\\'\") // Escape single quotes\n    .replace(/\\n/g, '\\\\n') // Escape newlines\n    .replace(/\\r/g, '\\\\r') // Escape carriage returns\n    .replace(/#/g, '\\\\!!'); // Escape  - that's comment in cozo\n}\n\nexport function deserializeString(serialized: string): string {\n  return serialized\n    .replace(/\\\\r/g, '\\r') // Unescape carriage returns\n    .replace(/\\\\n/g, '\\n') // Unescape newlines\n    .replace(/\\\\'/g, \"'\") // Unescape single quotes\n    .replace(/\\\\''/g, '\"') // Unescape double quotes\n    .replace(/\\\\\\\\/g, '\\\\') // Unescape backslashes\n    .replace(/\\\\!!/g, '#'); // Unescape # cozo comment\n}\n\nconst specialCharsRegexe = /\\\\u\\{[a-fA-F0-9]+\\}/g;\n\nexport function removeBrokenUnicode(string: string): string {\n  return string.replace(specialCharsRegexe, '');\n}\n\nexport function removeMarkdownFormatting(markdown: string): string {\n  // Remove headers\n  let text = markdown.replace(/^#{1,6}\\s+/gm, '');\n\n  // Remove emphasis (bold, italic, strikethrough)\n  text = text.replace(/(\\*\\*|__)(.*?)\\1/g, '$2');\n  text = text.replace(/(\\*|_)(.*?)\\1/g, '$2');\n  text = text.replace(/(~~)(.*?)\\1/g, '$2');\n\n  // Remove inline code and code blocks\n  text = text.replace(/`{1,3}[^`](.*?)`{1,3}/g, '$1');\n  text = text.replace(/```[\\s\\S]*?```/g, '');\n\n  // Remove blockquotes\n  text = text.replace(/^\\s{0,3}>\\s?/gm, '');\n\n  // Remove links\n  text = text.replace(/\\[(.*?)\\]\\(.*?\\)/g, '$1');\n\n  // Remove images\n  text = text.replace(/!\\[(.*?)\\]\\(.*?\\)/g, '$1');\n\n  // Remove horizontal rules\n  text = text.replace(/^-{3,}$/gm, '');\n\n  // Remove unordered lists\n  text = text.replace(/^\\s*[-+*]\\s+/gm, '');\n\n  // Remove ordered lists\n  text = text.replace(/^\\s*\\d+\\.\\s+/gm, '');\n\n  // Remove extra spaces and new lines\n  text = text.replace(/\\n{2,}/g, '\\n\\n');\n  text = text.replace(/^\\s+|\\s+$/g, '');\n\n  return text;\n}\n","import { LsResult } from 'ipfs-core-types/src/pin';\nimport { dateToUtcNumber } from 'src/utils/date';\nimport { NeuronAddress, ParticleCid, TransactionHash } from 'src/types/base';\nimport { IPFSContent } from '../ipfs/types';\nimport { LinkDbEntity, PinTypeMap } from './types/entities';\nimport { Transaction } from '../backend/services/indexer/types';\nimport { LinkDto, ParticleDto, PinDto, TransactionDto } from './types/dto';\nimport { CyberlinksByParticleQuery } from 'src/generated/graphql';\nimport { removeMarkdownFormatting, replaceQuotes } from 'src/utils/string';\n\nexport const mapParticleToEntity = (particle: IPFSContent): ParticleDto => {\n  const { cid, meta, textPreview } = particle;\n  const { size, mime, type, blocks, sizeLocal } = meta;\n\n  // hack to fix string command\n  const text = textPreview\n    ? replaceQuotes(removeMarkdownFormatting(textPreview))\n    : '';\n\n  return {\n    cid,\n    size: size || 0,\n    mime: mime || 'unknown',\n    type,\n    text,\n    size_local: sizeLocal || -1,\n    blocks: blocks || 0,\n  };\n};\n\n//TODO: REFACTOR\nexport const mapPinToEntity = (pin: LsResult): PinDto => ({\n  cid: pin.cid.toString(),\n  type: PinTypeMap[pin.type],\n});\n\nexport const mapIndexerTransactionToEntity = (\n  neuron: string,\n  tx: Transaction\n): TransactionDto => {\n  const {\n    transaction_hash,\n    index,\n    transaction: {\n      memo,\n      block: { timestamp, height },\n      success,\n    },\n    type,\n    value,\n  } = tx;\n  return {\n    hash: transaction_hash,\n    index,\n    type,\n    timestamp: dateToUtcNumber(timestamp),\n    // value: JSON.stringify(value),\n    memo,\n    value,\n    success,\n    neuron,\n    blockHeight: height,\n  };\n};\n\n// export const mapSyncStatusToEntity = (\n//   id: NeuronAddress | ParticleCid,\n//   entryType: EntryType,\n//   unreadCount: number,\n//   timestampUpdate: number,\n//   lastId: TransactionHash | ParticleCid = '',\n//   timestampRead: number = timestampUpdate,\n//   meta: Object = {}\n// ): SyncStatusDbEntity => {\n//   return {\n//     entry_type: entryType,\n//     id,\n//     timestamp_update: timestampUpdate,\n//     timestamp_read: timestampRead,\n//     unread_count: unreadCount,\n//     disabled: false,\n//     last_id: lastId,\n//     meta,\n//   };\n// };\n\nexport const mapLinkToLinkDto = (\n  from: ParticleCid,\n  to: ParticleCid,\n  neuron: NeuronAddress = '',\n  timestamp: number = 0\n): LinkDto => ({\n  from,\n  to,\n  neuron,\n  timestamp,\n});\n\nexport const mapLinkFromIndexerToDto = ({\n  from,\n  to,\n  neuron,\n  timestamp,\n  transaction_hash,\n}: CyberlinksByParticleQuery['cyberlinks'][0]): LinkDto => ({\n  from,\n  to,\n  neuron,\n  timestamp: dateToUtcNumber(timestamp),\n  transactionHash: transaction_hash,\n});\n","export async function waitUntil(cond: () => boolean, timeoutDuration = 60000) {\n  if (cond()) {\n    return true;\n  }\n\n  const waitPromise = new Promise((resolve) => {\n    const interval = setInterval(() => {\n      if (cond()) {\n        clearInterval(interval);\n        resolve(true);\n      }\n    }, 10);\n  });\n\n  const timeoutPromise = new Promise((_, reject) => {\n    setTimeout(() => {\n      reject(new Error('waitUntil timed out!'));\n    }, timeoutDuration);\n  });\n\n  return Promise.race([waitPromise, timeoutPromise]);\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport function makeCancellable<T extends (...args: any[]) => Promise<any>>(\n  func: T,\n  signal: AbortSignal\n): (...funcArgs: Parameters<T>) => Promise<ReturnType<T>> {\n  return async (...args: Parameters<T>): Promise<ReturnType<T>> => {\n    // Promise that listens for the abort signal\n    const abortPromise = new Promise<ReturnType<T>>((_, reject) => {\n      const abortHandler = () => {\n        signal.removeEventListener('abort', abortHandler); // Clean up the event listener\n        reject(new DOMException('The operation was aborted.', 'AbortError'));\n      };\n      signal.addEventListener('abort', abortHandler, { once: true });\n    });\n\n    // Wrapping the original function in a promise\n    const taskPromise = new Promise<ReturnType<T>>(async (resolve, reject) => {\n      try {\n        const result = await func(...args);\n        resolve(result);\n      } catch (error) {\n        reject(error);\n      }\n    });\n\n    // Using Promise.race to handle cancellation\n    return Promise.race([taskPromise, abortPromise]);\n  };\n}\n\nexport function throwIfAborted<T extends (...args: any[]) => Promise<any>>(\n  func: T,\n  signal: AbortSignal\n): (...funcArgs: Parameters<T>) => Promise<ReturnType<T>> {\n  return async (...args: Parameters<T>): Promise<ReturnType<T>> => {\n    if (signal.aborted) {\n      throw new DOMException('The operation was aborted.', 'AbortError');\n    }\n    return func(...args);\n  };\n}\n\n/**\n * Promise will be rejected after timeout.\n *\n * @param promise\n * @param timeout ms\n * @param abortController trigger abort\n * @returns\n */\n// eslint-disable-next-line import/no-unused-modules\nexport async function withTimeout<T>(\n  promise: Promise<T>,\n  timeout: number,\n  abortController?: AbortController\n): Promise<T> {\n  return Promise.race([\n    promise,\n    new Promise<T>((_, reject) => {\n      const timer = setTimeout(() => {\n        abortController?.abort('timeout');\n        clearTimeout(timer);\n        reject(new DOMException('timeout', 'AbortError'));\n      }, timeout);\n    }),\n  ]);\n}\n","import { gql } from '@apollo/client';\nimport * as Apollo from '@apollo/client';\nexport type Maybe<T> = T | null;\nexport type InputMaybe<T> = Maybe<T>;\nexport type Exact<T extends { [key: string]: unknown }> = { [K in keyof T]: T[K] };\nexport type MakeOptional<T, K extends keyof T> = Omit<T, K> & { [SubKey in K]?: Maybe<T[SubKey]> };\nexport type MakeMaybe<T, K extends keyof T> = Omit<T, K> & { [SubKey in K]: Maybe<T[SubKey]> };\nexport type MakeEmpty<T extends { [key: string]: unknown }, K extends keyof T> = { [_ in K]?: never };\nexport type Incremental<T> = T | { [P in keyof T]?: P extends ' $fragmentName' | '__typename' ? T[P] : never };\nconst defaultOptions = {} as const;\n/** All built-in and custom scalars, mapped to their actual values */\nexport type Scalars = {\n  ID: { input: string; output: string; }\n  String: { input: string; output: string; }\n  Boolean: { input: boolean; output: boolean; }\n  Int: { input: number; output: number; }\n  Float: { input: number; output: number; }\n  _text: { input: any; output: any; }\n  bigint: { input: any; output: any; }\n  coin: { input: any; output: any; }\n  coin_scalar: { input: any; output: any; }\n  date: { input: any; output: any; }\n  float8: { input: any; output: any; }\n  json: { input: any; output: any; }\n  jsonb: { input: any; output: any; }\n  numeric: { input: any; output: any; }\n  timestamp: { input: any; output: any; }\n};\n\n/** Boolean expression to compare columns of type \"Boolean\". All fields are combined with logical 'AND'. */\nexport type Boolean_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['Boolean']['input']>;\n  _gt?: InputMaybe<Scalars['Boolean']['input']>;\n  _gte?: InputMaybe<Scalars['Boolean']['input']>;\n  _in?: InputMaybe<Array<Scalars['Boolean']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['Boolean']['input']>;\n  _lte?: InputMaybe<Scalars['Boolean']['input']>;\n  _neq?: InputMaybe<Scalars['Boolean']['input']>;\n  _nin?: InputMaybe<Array<Scalars['Boolean']['input']>>;\n};\n\n/** Boolean expression to compare columns of type \"Int\". All fields are combined with logical 'AND'. */\nexport type Int_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['Int']['input']>;\n  _gt?: InputMaybe<Scalars['Int']['input']>;\n  _gte?: InputMaybe<Scalars['Int']['input']>;\n  _in?: InputMaybe<Array<Scalars['Int']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['Int']['input']>;\n  _lte?: InputMaybe<Scalars['Int']['input']>;\n  _neq?: InputMaybe<Scalars['Int']['input']>;\n  _nin?: InputMaybe<Array<Scalars['Int']['input']>>;\n};\n\n/** Boolean expression to compare columns of type \"String\". All fields are combined with logical 'AND'. */\nexport type String_Array_Comparison_Exp = {\n  /** is the array contained in the given array value */\n  _contained_in?: InputMaybe<Array<Scalars['String']['input']>>;\n  /** does the array contain the given value */\n  _contains?: InputMaybe<Array<Scalars['String']['input']>>;\n  _eq?: InputMaybe<Array<Scalars['String']['input']>>;\n  _gt?: InputMaybe<Array<Scalars['String']['input']>>;\n  _gte?: InputMaybe<Array<Scalars['String']['input']>>;\n  _in?: InputMaybe<Array<Array<Scalars['String']['input']>>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Array<Scalars['String']['input']>>;\n  _lte?: InputMaybe<Array<Scalars['String']['input']>>;\n  _neq?: InputMaybe<Array<Scalars['String']['input']>>;\n  _nin?: InputMaybe<Array<Array<Scalars['String']['input']>>>;\n};\n\n/** Boolean expression to compare columns of type \"String\". All fields are combined with logical 'AND'. */\nexport type String_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['String']['input']>;\n  _gt?: InputMaybe<Scalars['String']['input']>;\n  _gte?: InputMaybe<Scalars['String']['input']>;\n  /** does the column match the given case-insensitive pattern */\n  _ilike?: InputMaybe<Scalars['String']['input']>;\n  _in?: InputMaybe<Array<Scalars['String']['input']>>;\n  /** does the column match the given POSIX regular expression, case insensitive */\n  _iregex?: InputMaybe<Scalars['String']['input']>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  /** does the column match the given pattern */\n  _like?: InputMaybe<Scalars['String']['input']>;\n  _lt?: InputMaybe<Scalars['String']['input']>;\n  _lte?: InputMaybe<Scalars['String']['input']>;\n  _neq?: InputMaybe<Scalars['String']['input']>;\n  /** does the column NOT match the given case-insensitive pattern */\n  _nilike?: InputMaybe<Scalars['String']['input']>;\n  _nin?: InputMaybe<Array<Scalars['String']['input']>>;\n  /** does the column NOT match the given POSIX regular expression, case insensitive */\n  _niregex?: InputMaybe<Scalars['String']['input']>;\n  /** does the column NOT match the given pattern */\n  _nlike?: InputMaybe<Scalars['String']['input']>;\n  /** does the column NOT match the given POSIX regular expression, case sensitive */\n  _nregex?: InputMaybe<Scalars['String']['input']>;\n  /** does the column NOT match the given SQL regular expression */\n  _nsimilar?: InputMaybe<Scalars['String']['input']>;\n  /** does the column match the given POSIX regular expression, case sensitive */\n  _regex?: InputMaybe<Scalars['String']['input']>;\n  /** does the column match the given SQL regular expression */\n  _similar?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** columns and relationships of \"_transaction\" */\nexport type _Transaction = {\n  fee?: Maybe<Scalars['jsonb']['output']>;\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  logs?: Maybe<Scalars['jsonb']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  messages?: Maybe<Scalars['json']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n  signer_infos?: Maybe<Scalars['jsonb']['output']>;\n  subject1?: Maybe<Scalars['String']['output']>;\n  subject2?: Maybe<Scalars['String']['output']>;\n  success?: Maybe<Scalars['Boolean']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n  value?: Maybe<Scalars['json']['output']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionFeeArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionLogsArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionMessagesArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionSigner_InfosArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"_transaction\" */\nexport type _TransactionValueArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregated selection of \"_transaction\" */\nexport type _Transaction_Aggregate = {\n  aggregate?: Maybe<_Transaction_Aggregate_Fields>;\n  nodes: Array<_Transaction>;\n};\n\n/** aggregate fields of \"_transaction\" */\nexport type _Transaction_Aggregate_Fields = {\n  avg?: Maybe<_Transaction_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<_Transaction_Max_Fields>;\n  min?: Maybe<_Transaction_Min_Fields>;\n  stddev?: Maybe<_Transaction_Stddev_Fields>;\n  stddev_pop?: Maybe<_Transaction_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<_Transaction_Stddev_Samp_Fields>;\n  sum?: Maybe<_Transaction_Sum_Fields>;\n  var_pop?: Maybe<_Transaction_Var_Pop_Fields>;\n  var_samp?: Maybe<_Transaction_Var_Samp_Fields>;\n  variance?: Maybe<_Transaction_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"_transaction\" */\nexport type _Transaction_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<_Transaction_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type _Transaction_Avg_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"_transaction\". All fields are combined with a logical 'AND'. */\nexport type _Transaction_Bool_Exp = {\n  _and?: InputMaybe<Array<_Transaction_Bool_Exp>>;\n  _not?: InputMaybe<_Transaction_Bool_Exp>;\n  _or?: InputMaybe<Array<_Transaction_Bool_Exp>>;\n  fee?: InputMaybe<Jsonb_Comparison_Exp>;\n  gas_used?: InputMaybe<Bigint_Comparison_Exp>;\n  gas_wanted?: InputMaybe<Bigint_Comparison_Exp>;\n  hash?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  index?: InputMaybe<Bigint_Comparison_Exp>;\n  involved_accounts_addresses?: InputMaybe<String_Array_Comparison_Exp>;\n  logs?: InputMaybe<Jsonb_Comparison_Exp>;\n  memo?: InputMaybe<String_Comparison_Exp>;\n  messages?: InputMaybe<Json_Comparison_Exp>;\n  partition_id?: InputMaybe<Bigint_Comparison_Exp>;\n  raw_log?: InputMaybe<String_Comparison_Exp>;\n  signatures?: InputMaybe<String_Array_Comparison_Exp>;\n  signer_infos?: InputMaybe<Jsonb_Comparison_Exp>;\n  subject1?: InputMaybe<String_Comparison_Exp>;\n  subject2?: InputMaybe<String_Comparison_Exp>;\n  success?: InputMaybe<Boolean_Comparison_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  value?: InputMaybe<Json_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type _Transaction_Max_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  memo?: Maybe<Scalars['String']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n  subject1?: Maybe<Scalars['String']['output']>;\n  subject2?: Maybe<Scalars['String']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type _Transaction_Min_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  memo?: Maybe<Scalars['String']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n  subject1?: Maybe<Scalars['String']['output']>;\n  subject2?: Maybe<Scalars['String']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"_transaction\". */\nexport type _Transaction_Order_By = {\n  fee?: InputMaybe<Order_By>;\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  logs?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  messages?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n  signer_infos?: InputMaybe<Order_By>;\n  subject1?: InputMaybe<Order_By>;\n  subject2?: InputMaybe<Order_By>;\n  success?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"_transaction\" */\nexport enum _Transaction_Select_Column {\n  /** column name */\n  Fee = 'fee',\n  /** column name */\n  GasUsed = 'gas_used',\n  /** column name */\n  GasWanted = 'gas_wanted',\n  /** column name */\n  Hash = 'hash',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Index = 'index',\n  /** column name */\n  InvolvedAccountsAddresses = 'involved_accounts_addresses',\n  /** column name */\n  Logs = 'logs',\n  /** column name */\n  Memo = 'memo',\n  /** column name */\n  Messages = 'messages',\n  /** column name */\n  PartitionId = 'partition_id',\n  /** column name */\n  RawLog = 'raw_log',\n  /** column name */\n  Signatures = 'signatures',\n  /** column name */\n  SignerInfos = 'signer_infos',\n  /** column name */\n  Subject1 = 'subject1',\n  /** column name */\n  Subject2 = 'subject2',\n  /** column name */\n  Success = 'success',\n  /** column name */\n  TransactionHash = 'transaction_hash',\n  /** column name */\n  Type = 'type',\n  /** column name */\n  Value = 'value'\n}\n\n/** aggregate stddev on columns */\nexport type _Transaction_Stddev_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type _Transaction_Stddev_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type _Transaction_Stddev_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"_transaction\" */\nexport type _Transaction_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: _Transaction_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type _Transaction_Stream_Cursor_Value_Input = {\n  fee?: InputMaybe<Scalars['jsonb']['input']>;\n  gas_used?: InputMaybe<Scalars['bigint']['input']>;\n  gas_wanted?: InputMaybe<Scalars['bigint']['input']>;\n  hash?: InputMaybe<Scalars['String']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  index?: InputMaybe<Scalars['bigint']['input']>;\n  involved_accounts_addresses?: InputMaybe<Array<Scalars['String']['input']>>;\n  logs?: InputMaybe<Scalars['jsonb']['input']>;\n  memo?: InputMaybe<Scalars['String']['input']>;\n  messages?: InputMaybe<Scalars['json']['input']>;\n  partition_id?: InputMaybe<Scalars['bigint']['input']>;\n  raw_log?: InputMaybe<Scalars['String']['input']>;\n  signatures?: InputMaybe<Array<Scalars['String']['input']>>;\n  signer_infos?: InputMaybe<Scalars['jsonb']['input']>;\n  subject1?: InputMaybe<Scalars['String']['input']>;\n  subject2?: InputMaybe<Scalars['String']['input']>;\n  success?: InputMaybe<Scalars['Boolean']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n  value?: InputMaybe<Scalars['json']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type _Transaction_Sum_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type _Transaction_Var_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type _Transaction_Var_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type _Transaction_Variance_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"_uptime_temp\" */\nexport type _Uptime_Temp = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"_uptime_temp\" */\nexport type _Uptime_Temp_Aggregate = {\n  aggregate?: Maybe<_Uptime_Temp_Aggregate_Fields>;\n  nodes: Array<_Uptime_Temp>;\n};\n\n/** aggregate fields of \"_uptime_temp\" */\nexport type _Uptime_Temp_Aggregate_Fields = {\n  avg?: Maybe<_Uptime_Temp_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<_Uptime_Temp_Max_Fields>;\n  min?: Maybe<_Uptime_Temp_Min_Fields>;\n  stddev?: Maybe<_Uptime_Temp_Stddev_Fields>;\n  stddev_pop?: Maybe<_Uptime_Temp_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<_Uptime_Temp_Stddev_Samp_Fields>;\n  sum?: Maybe<_Uptime_Temp_Sum_Fields>;\n  var_pop?: Maybe<_Uptime_Temp_Var_Pop_Fields>;\n  var_samp?: Maybe<_Uptime_Temp_Var_Samp_Fields>;\n  variance?: Maybe<_Uptime_Temp_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"_uptime_temp\" */\nexport type _Uptime_Temp_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type _Uptime_Temp_Avg_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"_uptime_temp\". All fields are combined with a logical 'AND'. */\nexport type _Uptime_Temp_Bool_Exp = {\n  _and?: InputMaybe<Array<_Uptime_Temp_Bool_Exp>>;\n  _not?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n  _or?: InputMaybe<Array<_Uptime_Temp_Bool_Exp>>;\n  pre_commits?: InputMaybe<Bigint_Comparison_Exp>;\n  validator_address?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type _Uptime_Temp_Max_Fields = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type _Uptime_Temp_Min_Fields = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"_uptime_temp\". */\nexport type _Uptime_Temp_Order_By = {\n  pre_commits?: InputMaybe<Order_By>;\n  validator_address?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"_uptime_temp\" */\nexport enum _Uptime_Temp_Select_Column {\n  /** column name */\n  PreCommits = 'pre_commits',\n  /** column name */\n  ValidatorAddress = 'validator_address'\n}\n\n/** aggregate stddev on columns */\nexport type _Uptime_Temp_Stddev_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type _Uptime_Temp_Stddev_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type _Uptime_Temp_Stddev_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"_uptime_temp\" */\nexport type _Uptime_Temp_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: _Uptime_Temp_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type _Uptime_Temp_Stream_Cursor_Value_Input = {\n  pre_commits?: InputMaybe<Scalars['bigint']['input']>;\n  validator_address?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type _Uptime_Temp_Sum_Fields = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type _Uptime_Temp_Var_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type _Uptime_Temp_Var_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type _Uptime_Temp_Variance_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"account\" */\nexport type Account = {\n  /** An object relationship */\n  account_balance?: Maybe<Account_Balance>;\n  address: Scalars['String']['output'];\n  /** An array relationship */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An array relationship */\n  routesBySource: Array<Routes>;\n  /** An aggregate relationship */\n  routesBySource_aggregate: Routes_Aggregate;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n  /** An array relationship */\n  vesting_accounts: Array<Vesting_Account>;\n  /** An aggregate relationship */\n  vesting_accounts_aggregate: Vesting_Account_Aggregate;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountRoutesBySourceArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountRoutesBySource_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountVesting_AccountsArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Account_Order_By>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"account\" */\nexport type AccountVesting_Accounts_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Account_Order_By>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n/** aggregated selection of \"account\" */\nexport type Account_Aggregate = {\n  aggregate?: Maybe<Account_Aggregate_Fields>;\n  nodes: Array<Account>;\n};\n\n/** aggregate fields of \"account\" */\nexport type Account_Aggregate_Fields = {\n  count: Scalars['Int']['output'];\n  max?: Maybe<Account_Max_Fields>;\n  min?: Maybe<Account_Min_Fields>;\n};\n\n\n/** aggregate fields of \"account\" */\nexport type Account_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Account_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** columns and relationships of \"account_balance\" */\nexport type Account_Balance = {\n  /** An object relationship */\n  account: Account;\n  address: Scalars['String']['output'];\n  coins: Array<Scalars['coin']['output']>;\n  height: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"account_balance\" */\nexport type Account_Balance_Aggregate = {\n  aggregate?: Maybe<Account_Balance_Aggregate_Fields>;\n  nodes: Array<Account_Balance>;\n};\n\n/** aggregate fields of \"account_balance\" */\nexport type Account_Balance_Aggregate_Fields = {\n  avg?: Maybe<Account_Balance_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Account_Balance_Max_Fields>;\n  min?: Maybe<Account_Balance_Min_Fields>;\n  stddev?: Maybe<Account_Balance_Stddev_Fields>;\n  stddev_pop?: Maybe<Account_Balance_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Account_Balance_Stddev_Samp_Fields>;\n  sum?: Maybe<Account_Balance_Sum_Fields>;\n  var_pop?: Maybe<Account_Balance_Var_Pop_Fields>;\n  var_samp?: Maybe<Account_Balance_Var_Samp_Fields>;\n  variance?: Maybe<Account_Balance_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"account_balance\" */\nexport type Account_Balance_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Account_Balance_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"account_balance\". All fields are combined with a logical 'AND'. */\nexport type Account_Balance_Bool_Exp = {\n  _and?: InputMaybe<Array<Account_Balance_Bool_Exp>>;\n  _not?: InputMaybe<Account_Balance_Bool_Exp>;\n  _or?: InputMaybe<Array<Account_Balance_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  coins?: InputMaybe<Coin_Array_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Account_Balance_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  coins?: Maybe<Array<Scalars['coin']['output']>>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Account_Balance_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  coins?: Maybe<Array<Scalars['coin']['output']>>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"account_balance\". */\nexport type Account_Balance_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  address?: InputMaybe<Order_By>;\n  coins?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"account_balance\" */\nexport enum Account_Balance_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Coins = 'coins',\n  /** column name */\n  Height = 'height'\n}\n\n/** aggregate stddev on columns */\nexport type Account_Balance_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Account_Balance_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Account_Balance_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"account_balance\" */\nexport type Account_Balance_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Account_Balance_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Account_Balance_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n  coins?: InputMaybe<Array<Scalars['coin']['input']>>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Account_Balance_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Account_Balance_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Account_Balance_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Account_Balance_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"account\". All fields are combined with a logical 'AND'. */\nexport type Account_Bool_Exp = {\n  _and?: InputMaybe<Array<Account_Bool_Exp>>;\n  _not?: InputMaybe<Account_Bool_Exp>;\n  _or?: InputMaybe<Array<Account_Bool_Exp>>;\n  account_balance?: InputMaybe<Account_Balance_Bool_Exp>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  cyberlinks?: InputMaybe<Cyberlinks_Bool_Exp>;\n  cyberlinks_aggregate?: InputMaybe<Cyberlinks_Aggregate_Bool_Exp>;\n  investmints?: InputMaybe<Investmints_Bool_Exp>;\n  investmints_aggregate?: InputMaybe<Investmints_Aggregate_Bool_Exp>;\n  particles?: InputMaybe<Particles_Bool_Exp>;\n  particles_aggregate?: InputMaybe<Particles_Aggregate_Bool_Exp>;\n  routes?: InputMaybe<Routes_Bool_Exp>;\n  routesBySource?: InputMaybe<Routes_Bool_Exp>;\n  routesBySource_aggregate?: InputMaybe<Routes_Aggregate_Bool_Exp>;\n  routes_aggregate?: InputMaybe<Routes_Aggregate_Bool_Exp>;\n  vesting_accounts?: InputMaybe<Vesting_Account_Bool_Exp>;\n  vesting_accounts_aggregate?: InputMaybe<Vesting_Account_Aggregate_Bool_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Account_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Account_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"account\". */\nexport type Account_Order_By = {\n  account_balance?: InputMaybe<Account_Balance_Order_By>;\n  address?: InputMaybe<Order_By>;\n  cyberlinks_aggregate?: InputMaybe<Cyberlinks_Aggregate_Order_By>;\n  investmints_aggregate?: InputMaybe<Investmints_Aggregate_Order_By>;\n  particles_aggregate?: InputMaybe<Particles_Aggregate_Order_By>;\n  routesBySource_aggregate?: InputMaybe<Routes_Aggregate_Order_By>;\n  routes_aggregate?: InputMaybe<Routes_Aggregate_Order_By>;\n  vesting_accounts_aggregate?: InputMaybe<Vesting_Account_Aggregate_Order_By>;\n};\n\n/** select columns of table \"account\" */\nexport enum Account_Select_Column {\n  /** column name */\n  Address = 'address'\n}\n\n/** Streaming cursor of the table \"account\" */\nexport type Account_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Account_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Account_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** columns and relationships of \"average_block_time_from_genesis\" */\nexport type Average_Block_Time_From_Genesis = {\n  average_time: Scalars['numeric']['output'];\n  height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n};\n\n/** aggregated selection of \"average_block_time_from_genesis\" */\nexport type Average_Block_Time_From_Genesis_Aggregate = {\n  aggregate?: Maybe<Average_Block_Time_From_Genesis_Aggregate_Fields>;\n  nodes: Array<Average_Block_Time_From_Genesis>;\n};\n\n/** aggregate fields of \"average_block_time_from_genesis\" */\nexport type Average_Block_Time_From_Genesis_Aggregate_Fields = {\n  avg?: Maybe<Average_Block_Time_From_Genesis_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Average_Block_Time_From_Genesis_Max_Fields>;\n  min?: Maybe<Average_Block_Time_From_Genesis_Min_Fields>;\n  stddev?: Maybe<Average_Block_Time_From_Genesis_Stddev_Fields>;\n  stddev_pop?: Maybe<Average_Block_Time_From_Genesis_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Average_Block_Time_From_Genesis_Stddev_Samp_Fields>;\n  sum?: Maybe<Average_Block_Time_From_Genesis_Sum_Fields>;\n  var_pop?: Maybe<Average_Block_Time_From_Genesis_Var_Pop_Fields>;\n  var_samp?: Maybe<Average_Block_Time_From_Genesis_Var_Samp_Fields>;\n  variance?: Maybe<Average_Block_Time_From_Genesis_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"average_block_time_from_genesis\" */\nexport type Average_Block_Time_From_Genesis_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Average_Block_Time_From_Genesis_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Average_Block_Time_From_Genesis_Avg_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"average_block_time_from_genesis\". All fields are combined with a logical 'AND'. */\nexport type Average_Block_Time_From_Genesis_Bool_Exp = {\n  _and?: InputMaybe<Array<Average_Block_Time_From_Genesis_Bool_Exp>>;\n  _not?: InputMaybe<Average_Block_Time_From_Genesis_Bool_Exp>;\n  _or?: InputMaybe<Array<Average_Block_Time_From_Genesis_Bool_Exp>>;\n  average_time?: InputMaybe<Numeric_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Average_Block_Time_From_Genesis_Max_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Average_Block_Time_From_Genesis_Min_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"average_block_time_from_genesis\". */\nexport type Average_Block_Time_From_Genesis_Order_By = {\n  average_time?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"average_block_time_from_genesis\" */\nexport enum Average_Block_Time_From_Genesis_Select_Column {\n  /** column name */\n  AverageTime = 'average_time',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  OneRowId = 'one_row_id'\n}\n\n/** aggregate stddev on columns */\nexport type Average_Block_Time_From_Genesis_Stddev_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Average_Block_Time_From_Genesis_Stddev_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Average_Block_Time_From_Genesis_Stddev_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"average_block_time_from_genesis\" */\nexport type Average_Block_Time_From_Genesis_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Average_Block_Time_From_Genesis_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Average_Block_Time_From_Genesis_Stream_Cursor_Value_Input = {\n  average_time?: InputMaybe<Scalars['numeric']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  one_row_id?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Average_Block_Time_From_Genesis_Sum_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Average_Block_Time_From_Genesis_Var_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Average_Block_Time_From_Genesis_Var_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Average_Block_Time_From_Genesis_Variance_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"average_block_time_per_day\" */\nexport type Average_Block_Time_Per_Day = {\n  average_time: Scalars['numeric']['output'];\n  height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n};\n\n/** aggregated selection of \"average_block_time_per_day\" */\nexport type Average_Block_Time_Per_Day_Aggregate = {\n  aggregate?: Maybe<Average_Block_Time_Per_Day_Aggregate_Fields>;\n  nodes: Array<Average_Block_Time_Per_Day>;\n};\n\n/** aggregate fields of \"average_block_time_per_day\" */\nexport type Average_Block_Time_Per_Day_Aggregate_Fields = {\n  avg?: Maybe<Average_Block_Time_Per_Day_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Average_Block_Time_Per_Day_Max_Fields>;\n  min?: Maybe<Average_Block_Time_Per_Day_Min_Fields>;\n  stddev?: Maybe<Average_Block_Time_Per_Day_Stddev_Fields>;\n  stddev_pop?: Maybe<Average_Block_Time_Per_Day_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Average_Block_Time_Per_Day_Stddev_Samp_Fields>;\n  sum?: Maybe<Average_Block_Time_Per_Day_Sum_Fields>;\n  var_pop?: Maybe<Average_Block_Time_Per_Day_Var_Pop_Fields>;\n  var_samp?: Maybe<Average_Block_Time_Per_Day_Var_Samp_Fields>;\n  variance?: Maybe<Average_Block_Time_Per_Day_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"average_block_time_per_day\" */\nexport type Average_Block_Time_Per_Day_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Average_Block_Time_Per_Day_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Average_Block_Time_Per_Day_Avg_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"average_block_time_per_day\". All fields are combined with a logical 'AND'. */\nexport type Average_Block_Time_Per_Day_Bool_Exp = {\n  _and?: InputMaybe<Array<Average_Block_Time_Per_Day_Bool_Exp>>;\n  _not?: InputMaybe<Average_Block_Time_Per_Day_Bool_Exp>;\n  _or?: InputMaybe<Array<Average_Block_Time_Per_Day_Bool_Exp>>;\n  average_time?: InputMaybe<Numeric_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Average_Block_Time_Per_Day_Max_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Average_Block_Time_Per_Day_Min_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"average_block_time_per_day\". */\nexport type Average_Block_Time_Per_Day_Order_By = {\n  average_time?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"average_block_time_per_day\" */\nexport enum Average_Block_Time_Per_Day_Select_Column {\n  /** column name */\n  AverageTime = 'average_time',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  OneRowId = 'one_row_id'\n}\n\n/** aggregate stddev on columns */\nexport type Average_Block_Time_Per_Day_Stddev_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Average_Block_Time_Per_Day_Stddev_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Average_Block_Time_Per_Day_Stddev_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"average_block_time_per_day\" */\nexport type Average_Block_Time_Per_Day_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Average_Block_Time_Per_Day_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Average_Block_Time_Per_Day_Stream_Cursor_Value_Input = {\n  average_time?: InputMaybe<Scalars['numeric']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  one_row_id?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Average_Block_Time_Per_Day_Sum_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Average_Block_Time_Per_Day_Var_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Average_Block_Time_Per_Day_Var_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Average_Block_Time_Per_Day_Variance_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"average_block_time_per_hour\" */\nexport type Average_Block_Time_Per_Hour = {\n  average_time: Scalars['numeric']['output'];\n  height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n};\n\n/** aggregated selection of \"average_block_time_per_hour\" */\nexport type Average_Block_Time_Per_Hour_Aggregate = {\n  aggregate?: Maybe<Average_Block_Time_Per_Hour_Aggregate_Fields>;\n  nodes: Array<Average_Block_Time_Per_Hour>;\n};\n\n/** aggregate fields of \"average_block_time_per_hour\" */\nexport type Average_Block_Time_Per_Hour_Aggregate_Fields = {\n  avg?: Maybe<Average_Block_Time_Per_Hour_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Average_Block_Time_Per_Hour_Max_Fields>;\n  min?: Maybe<Average_Block_Time_Per_Hour_Min_Fields>;\n  stddev?: Maybe<Average_Block_Time_Per_Hour_Stddev_Fields>;\n  stddev_pop?: Maybe<Average_Block_Time_Per_Hour_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Average_Block_Time_Per_Hour_Stddev_Samp_Fields>;\n  sum?: Maybe<Average_Block_Time_Per_Hour_Sum_Fields>;\n  var_pop?: Maybe<Average_Block_Time_Per_Hour_Var_Pop_Fields>;\n  var_samp?: Maybe<Average_Block_Time_Per_Hour_Var_Samp_Fields>;\n  variance?: Maybe<Average_Block_Time_Per_Hour_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"average_block_time_per_hour\" */\nexport type Average_Block_Time_Per_Hour_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Average_Block_Time_Per_Hour_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Average_Block_Time_Per_Hour_Avg_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"average_block_time_per_hour\". All fields are combined with a logical 'AND'. */\nexport type Average_Block_Time_Per_Hour_Bool_Exp = {\n  _and?: InputMaybe<Array<Average_Block_Time_Per_Hour_Bool_Exp>>;\n  _not?: InputMaybe<Average_Block_Time_Per_Hour_Bool_Exp>;\n  _or?: InputMaybe<Array<Average_Block_Time_Per_Hour_Bool_Exp>>;\n  average_time?: InputMaybe<Numeric_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Average_Block_Time_Per_Hour_Max_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Average_Block_Time_Per_Hour_Min_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"average_block_time_per_hour\". */\nexport type Average_Block_Time_Per_Hour_Order_By = {\n  average_time?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"average_block_time_per_hour\" */\nexport enum Average_Block_Time_Per_Hour_Select_Column {\n  /** column name */\n  AverageTime = 'average_time',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  OneRowId = 'one_row_id'\n}\n\n/** aggregate stddev on columns */\nexport type Average_Block_Time_Per_Hour_Stddev_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Average_Block_Time_Per_Hour_Stddev_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Average_Block_Time_Per_Hour_Stddev_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"average_block_time_per_hour\" */\nexport type Average_Block_Time_Per_Hour_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Average_Block_Time_Per_Hour_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Average_Block_Time_Per_Hour_Stream_Cursor_Value_Input = {\n  average_time?: InputMaybe<Scalars['numeric']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  one_row_id?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Average_Block_Time_Per_Hour_Sum_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Average_Block_Time_Per_Hour_Var_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Average_Block_Time_Per_Hour_Var_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Average_Block_Time_Per_Hour_Variance_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"average_block_time_per_minute\" */\nexport type Average_Block_Time_Per_Minute = {\n  average_time: Scalars['numeric']['output'];\n  height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n};\n\n/** aggregated selection of \"average_block_time_per_minute\" */\nexport type Average_Block_Time_Per_Minute_Aggregate = {\n  aggregate?: Maybe<Average_Block_Time_Per_Minute_Aggregate_Fields>;\n  nodes: Array<Average_Block_Time_Per_Minute>;\n};\n\n/** aggregate fields of \"average_block_time_per_minute\" */\nexport type Average_Block_Time_Per_Minute_Aggregate_Fields = {\n  avg?: Maybe<Average_Block_Time_Per_Minute_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Average_Block_Time_Per_Minute_Max_Fields>;\n  min?: Maybe<Average_Block_Time_Per_Minute_Min_Fields>;\n  stddev?: Maybe<Average_Block_Time_Per_Minute_Stddev_Fields>;\n  stddev_pop?: Maybe<Average_Block_Time_Per_Minute_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Average_Block_Time_Per_Minute_Stddev_Samp_Fields>;\n  sum?: Maybe<Average_Block_Time_Per_Minute_Sum_Fields>;\n  var_pop?: Maybe<Average_Block_Time_Per_Minute_Var_Pop_Fields>;\n  var_samp?: Maybe<Average_Block_Time_Per_Minute_Var_Samp_Fields>;\n  variance?: Maybe<Average_Block_Time_Per_Minute_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"average_block_time_per_minute\" */\nexport type Average_Block_Time_Per_Minute_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Average_Block_Time_Per_Minute_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Average_Block_Time_Per_Minute_Avg_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"average_block_time_per_minute\". All fields are combined with a logical 'AND'. */\nexport type Average_Block_Time_Per_Minute_Bool_Exp = {\n  _and?: InputMaybe<Array<Average_Block_Time_Per_Minute_Bool_Exp>>;\n  _not?: InputMaybe<Average_Block_Time_Per_Minute_Bool_Exp>;\n  _or?: InputMaybe<Array<Average_Block_Time_Per_Minute_Bool_Exp>>;\n  average_time?: InputMaybe<Numeric_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Average_Block_Time_Per_Minute_Max_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Average_Block_Time_Per_Minute_Min_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"average_block_time_per_minute\". */\nexport type Average_Block_Time_Per_Minute_Order_By = {\n  average_time?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"average_block_time_per_minute\" */\nexport enum Average_Block_Time_Per_Minute_Select_Column {\n  /** column name */\n  AverageTime = 'average_time',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  OneRowId = 'one_row_id'\n}\n\n/** aggregate stddev on columns */\nexport type Average_Block_Time_Per_Minute_Stddev_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Average_Block_Time_Per_Minute_Stddev_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Average_Block_Time_Per_Minute_Stddev_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"average_block_time_per_minute\" */\nexport type Average_Block_Time_Per_Minute_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Average_Block_Time_Per_Minute_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Average_Block_Time_Per_Minute_Stream_Cursor_Value_Input = {\n  average_time?: InputMaybe<Scalars['numeric']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  one_row_id?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Average_Block_Time_Per_Minute_Sum_Fields = {\n  average_time?: Maybe<Scalars['numeric']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Average_Block_Time_Per_Minute_Var_Pop_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Average_Block_Time_Per_Minute_Var_Samp_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Average_Block_Time_Per_Minute_Variance_Fields = {\n  average_time?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to compare columns of type \"bigint\". All fields are combined with logical 'AND'. */\nexport type Bigint_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['bigint']['input']>;\n  _gt?: InputMaybe<Scalars['bigint']['input']>;\n  _gte?: InputMaybe<Scalars['bigint']['input']>;\n  _in?: InputMaybe<Array<Scalars['bigint']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['bigint']['input']>;\n  _lte?: InputMaybe<Scalars['bigint']['input']>;\n  _neq?: InputMaybe<Scalars['bigint']['input']>;\n  _nin?: InputMaybe<Array<Scalars['bigint']['input']>>;\n};\n\n/** columns and relationships of \"block\" */\nexport type Block = {\n  /** An array relationship */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  hash: Scalars['String']['output'];\n  height: Scalars['bigint']['output'];\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  num_txs?: Maybe<Scalars['Int']['output']>;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  proposer_address?: Maybe<Scalars['String']['output']>;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n  /** An array relationship */\n  swaps: Array<Swaps>;\n  /** An aggregate relationship */\n  swaps_aggregate: Swaps_Aggregate;\n  timestamp: Scalars['timestamp']['output'];\n  total_gas?: Maybe<Scalars['bigint']['output']>;\n  /** An array relationship */\n  transaction_155s: Array<Transaction_155>;\n  /** An aggregate relationship */\n  transaction_155s_aggregate: Transaction_155_Aggregate;\n  /** An array relationship */\n  transactions: Array<Transaction>;\n  /** An aggregate relationship */\n  transactions_aggregate: Transaction_Aggregate;\n  /** An object relationship */\n  validator?: Maybe<Validator>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockSwapsArgs = {\n  distinct_on?: InputMaybe<Array<Swaps_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Swaps_Order_By>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockSwaps_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Swaps_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Swaps_Order_By>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockTransaction_155sArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_155_Order_By>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockTransaction_155s_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_155_Order_By>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockTransactionsArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"block\" */\nexport type BlockTransactions_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n/** aggregated selection of \"block\" */\nexport type Block_Aggregate = {\n  aggregate?: Maybe<Block_Aggregate_Fields>;\n  nodes: Array<Block>;\n};\n\nexport type Block_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Block_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Block_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Block_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Block_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"block\" */\nexport type Block_Aggregate_Fields = {\n  avg?: Maybe<Block_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Block_Max_Fields>;\n  min?: Maybe<Block_Min_Fields>;\n  stddev?: Maybe<Block_Stddev_Fields>;\n  stddev_pop?: Maybe<Block_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Block_Stddev_Samp_Fields>;\n  sum?: Maybe<Block_Sum_Fields>;\n  var_pop?: Maybe<Block_Var_Pop_Fields>;\n  var_samp?: Maybe<Block_Var_Samp_Fields>;\n  variance?: Maybe<Block_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"block\" */\nexport type Block_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Block_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"block\" */\nexport type Block_Aggregate_Order_By = {\n  avg?: InputMaybe<Block_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Block_Max_Order_By>;\n  min?: InputMaybe<Block_Min_Order_By>;\n  stddev?: InputMaybe<Block_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Block_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Block_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Block_Sum_Order_By>;\n  var_pop?: InputMaybe<Block_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Block_Var_Samp_Order_By>;\n  variance?: InputMaybe<Block_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Block_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"block\" */\nexport type Block_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"block\". All fields are combined with a logical 'AND'. */\nexport type Block_Bool_Exp = {\n  _and?: InputMaybe<Array<Block_Bool_Exp>>;\n  _not?: InputMaybe<Block_Bool_Exp>;\n  _or?: InputMaybe<Array<Block_Bool_Exp>>;\n  cyberlinks?: InputMaybe<Cyberlinks_Bool_Exp>;\n  cyberlinks_aggregate?: InputMaybe<Cyberlinks_Aggregate_Bool_Exp>;\n  hash?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  investmints?: InputMaybe<Investmints_Bool_Exp>;\n  investmints_aggregate?: InputMaybe<Investmints_Aggregate_Bool_Exp>;\n  num_txs?: InputMaybe<Int_Comparison_Exp>;\n  particles?: InputMaybe<Particles_Bool_Exp>;\n  particles_aggregate?: InputMaybe<Particles_Aggregate_Bool_Exp>;\n  proposer_address?: InputMaybe<String_Comparison_Exp>;\n  routes?: InputMaybe<Routes_Bool_Exp>;\n  routes_aggregate?: InputMaybe<Routes_Aggregate_Bool_Exp>;\n  swaps?: InputMaybe<Swaps_Bool_Exp>;\n  swaps_aggregate?: InputMaybe<Swaps_Aggregate_Bool_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  total_gas?: InputMaybe<Bigint_Comparison_Exp>;\n  transaction_155s?: InputMaybe<Transaction_155_Bool_Exp>;\n  transaction_155s_aggregate?: InputMaybe<Transaction_155_Aggregate_Bool_Exp>;\n  transactions?: InputMaybe<Transaction_Bool_Exp>;\n  transactions_aggregate?: InputMaybe<Transaction_Aggregate_Bool_Exp>;\n  validator?: InputMaybe<Validator_Bool_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Block_Max_Fields = {\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  num_txs?: Maybe<Scalars['Int']['output']>;\n  proposer_address?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  total_gas?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by max() on columns of table \"block\" */\nexport type Block_Max_Order_By = {\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  proposer_address?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Block_Min_Fields = {\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  num_txs?: Maybe<Scalars['Int']['output']>;\n  proposer_address?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  total_gas?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by min() on columns of table \"block\" */\nexport type Block_Min_Order_By = {\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  proposer_address?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"block\". */\nexport type Block_Order_By = {\n  cyberlinks_aggregate?: InputMaybe<Cyberlinks_Aggregate_Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  investmints_aggregate?: InputMaybe<Investmints_Aggregate_Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  particles_aggregate?: InputMaybe<Particles_Aggregate_Order_By>;\n  proposer_address?: InputMaybe<Order_By>;\n  routes_aggregate?: InputMaybe<Routes_Aggregate_Order_By>;\n  swaps_aggregate?: InputMaybe<Swaps_Aggregate_Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n  transaction_155s_aggregate?: InputMaybe<Transaction_155_Aggregate_Order_By>;\n  transactions_aggregate?: InputMaybe<Transaction_Aggregate_Order_By>;\n  validator?: InputMaybe<Validator_Order_By>;\n};\n\n/** select columns of table \"block\" */\nexport enum Block_Select_Column {\n  /** column name */\n  Hash = 'hash',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  NumTxs = 'num_txs',\n  /** column name */\n  ProposerAddress = 'proposer_address',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TotalGas = 'total_gas'\n}\n\n/** aggregate stddev on columns */\nexport type Block_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"block\" */\nexport type Block_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Block_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"block\" */\nexport type Block_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Block_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"block\" */\nexport type Block_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"block\" */\nexport type Block_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Block_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Block_Stream_Cursor_Value_Input = {\n  hash?: InputMaybe<Scalars['String']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  num_txs?: InputMaybe<Scalars['Int']['input']>;\n  proposer_address?: InputMaybe<Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  total_gas?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Block_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  num_txs?: Maybe<Scalars['Int']['output']>;\n  total_gas?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"block\" */\nexport type Block_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Block_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"block\" */\nexport type Block_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Block_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"block\" */\nexport type Block_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Block_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  num_txs?: Maybe<Scalars['Float']['output']>;\n  total_gas?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"block\" */\nexport type Block_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  num_txs?: InputMaybe<Order_By>;\n  total_gas?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to compare columns of type \"coin\". All fields are combined with logical 'AND'. */\nexport type Coin_Array_Comparison_Exp = {\n  /** is the array contained in the given array value */\n  _contained_in?: InputMaybe<Array<Scalars['coin']['input']>>;\n  /** does the array contain the given value */\n  _contains?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _eq?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _gt?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _gte?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _in?: InputMaybe<Array<Array<Scalars['coin']['input']>>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _lte?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _neq?: InputMaybe<Array<Scalars['coin']['input']>>;\n  _nin?: InputMaybe<Array<Array<Scalars['coin']['input']>>>;\n};\n\n/** Boolean expression to compare columns of type \"coin_scalar\". All fields are combined with logical 'AND'. */\nexport type Coin_Scalar_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['coin_scalar']['input']>;\n  _gt?: InputMaybe<Scalars['coin_scalar']['input']>;\n  _gte?: InputMaybe<Scalars['coin_scalar']['input']>;\n  _in?: InputMaybe<Array<Scalars['coin_scalar']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['coin_scalar']['input']>;\n  _lte?: InputMaybe<Scalars['coin_scalar']['input']>;\n  _neq?: InputMaybe<Scalars['coin_scalar']['input']>;\n  _nin?: InputMaybe<Array<Scalars['coin_scalar']['input']>>;\n};\n\n/** columns and relationships of \"contracts\" */\nexport type Contracts = {\n  address: Scalars['String']['output'];\n  admin: Scalars['String']['output'];\n  code_id: Scalars['bigint']['output'];\n  creation_time: Scalars['String']['output'];\n  creator: Scalars['String']['output'];\n  fees: Scalars['bigint']['output'];\n  gas: Scalars['bigint']['output'];\n  height: Scalars['bigint']['output'];\n  label: Scalars['String']['output'];\n  tx: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"contracts\" */\nexport type Contracts_Aggregate = {\n  aggregate?: Maybe<Contracts_Aggregate_Fields>;\n  nodes: Array<Contracts>;\n};\n\n/** aggregate fields of \"contracts\" */\nexport type Contracts_Aggregate_Fields = {\n  avg?: Maybe<Contracts_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Contracts_Max_Fields>;\n  min?: Maybe<Contracts_Min_Fields>;\n  stddev?: Maybe<Contracts_Stddev_Fields>;\n  stddev_pop?: Maybe<Contracts_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Contracts_Stddev_Samp_Fields>;\n  sum?: Maybe<Contracts_Sum_Fields>;\n  var_pop?: Maybe<Contracts_Var_Pop_Fields>;\n  var_samp?: Maybe<Contracts_Var_Samp_Fields>;\n  variance?: Maybe<Contracts_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"contracts\" */\nexport type Contracts_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Contracts_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Contracts_Avg_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"contracts\". All fields are combined with a logical 'AND'. */\nexport type Contracts_Bool_Exp = {\n  _and?: InputMaybe<Array<Contracts_Bool_Exp>>;\n  _not?: InputMaybe<Contracts_Bool_Exp>;\n  _or?: InputMaybe<Array<Contracts_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  admin?: InputMaybe<String_Comparison_Exp>;\n  code_id?: InputMaybe<Bigint_Comparison_Exp>;\n  creation_time?: InputMaybe<String_Comparison_Exp>;\n  creator?: InputMaybe<String_Comparison_Exp>;\n  fees?: InputMaybe<Bigint_Comparison_Exp>;\n  gas?: InputMaybe<Bigint_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  label?: InputMaybe<String_Comparison_Exp>;\n  tx?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Contracts_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  admin?: Maybe<Scalars['String']['output']>;\n  code_id?: Maybe<Scalars['bigint']['output']>;\n  creation_time?: Maybe<Scalars['String']['output']>;\n  creator?: Maybe<Scalars['String']['output']>;\n  fees?: Maybe<Scalars['bigint']['output']>;\n  gas?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  label?: Maybe<Scalars['String']['output']>;\n  tx?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Contracts_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  admin?: Maybe<Scalars['String']['output']>;\n  code_id?: Maybe<Scalars['bigint']['output']>;\n  creation_time?: Maybe<Scalars['String']['output']>;\n  creator?: Maybe<Scalars['String']['output']>;\n  fees?: Maybe<Scalars['bigint']['output']>;\n  gas?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  label?: Maybe<Scalars['String']['output']>;\n  tx?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"contracts\". */\nexport type Contracts_Order_By = {\n  address?: InputMaybe<Order_By>;\n  admin?: InputMaybe<Order_By>;\n  code_id?: InputMaybe<Order_By>;\n  creation_time?: InputMaybe<Order_By>;\n  creator?: InputMaybe<Order_By>;\n  fees?: InputMaybe<Order_By>;\n  gas?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  label?: InputMaybe<Order_By>;\n  tx?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"contracts\" */\nexport enum Contracts_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Admin = 'admin',\n  /** column name */\n  CodeId = 'code_id',\n  /** column name */\n  CreationTime = 'creation_time',\n  /** column name */\n  Creator = 'creator',\n  /** column name */\n  Fees = 'fees',\n  /** column name */\n  Gas = 'gas',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Label = 'label',\n  /** column name */\n  Tx = 'tx'\n}\n\n/** aggregate stddev on columns */\nexport type Contracts_Stddev_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Contracts_Stddev_Pop_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Contracts_Stddev_Samp_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"contracts\" */\nexport type Contracts_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Contracts_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Contracts_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n  admin?: InputMaybe<Scalars['String']['input']>;\n  code_id?: InputMaybe<Scalars['bigint']['input']>;\n  creation_time?: InputMaybe<Scalars['String']['input']>;\n  creator?: InputMaybe<Scalars['String']['input']>;\n  fees?: InputMaybe<Scalars['bigint']['input']>;\n  gas?: InputMaybe<Scalars['bigint']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  label?: InputMaybe<Scalars['String']['input']>;\n  tx?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Contracts_Sum_Fields = {\n  code_id?: Maybe<Scalars['bigint']['output']>;\n  fees?: Maybe<Scalars['bigint']['output']>;\n  gas?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  tx?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Contracts_Var_Pop_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Contracts_Var_Samp_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Contracts_Variance_Fields = {\n  code_id?: Maybe<Scalars['Float']['output']>;\n  fees?: Maybe<Scalars['Float']['output']>;\n  gas?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  tx?: Maybe<Scalars['Float']['output']>;\n};\n\n/** ordering argument of a cursor */\nexport enum Cursor_Ordering {\n  /** ascending ordering of the cursor */\n  Asc = 'ASC',\n  /** descending ordering of the cursor */\n  Desc = 'DESC'\n}\n\n/** columns and relationships of \"cyb_cohort\" */\nexport type Cyb_Cohort = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neurons_activated?: Maybe<Scalars['bigint']['output']>;\n  redelegation_percent?: Maybe<Scalars['float8']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  undelegation_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"cyb_cohort\" */\nexport type Cyb_Cohort_Aggregate = {\n  aggregate?: Maybe<Cyb_Cohort_Aggregate_Fields>;\n  nodes: Array<Cyb_Cohort>;\n};\n\n/** aggregate fields of \"cyb_cohort\" */\nexport type Cyb_Cohort_Aggregate_Fields = {\n  avg?: Maybe<Cyb_Cohort_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyb_Cohort_Max_Fields>;\n  min?: Maybe<Cyb_Cohort_Min_Fields>;\n  stddev?: Maybe<Cyb_Cohort_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyb_Cohort_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyb_Cohort_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyb_Cohort_Sum_Fields>;\n  var_pop?: Maybe<Cyb_Cohort_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyb_Cohort_Var_Samp_Fields>;\n  variance?: Maybe<Cyb_Cohort_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyb_cohort\" */\nexport type Cyb_Cohort_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Cyb_Cohort_Avg_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"cyb_cohort\". All fields are combined with a logical 'AND'. */\nexport type Cyb_Cohort_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyb_Cohort_Bool_Exp>>;\n  _not?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyb_Cohort_Bool_Exp>>;\n  cyberlink_10_percent?: InputMaybe<Float8_Comparison_Exp>;\n  cyberlink_100_percent?: InputMaybe<Float8_Comparison_Exp>;\n  cyberlink_percent?: InputMaybe<Float8_Comparison_Exp>;\n  hero_hired_percent?: InputMaybe<Float8_Comparison_Exp>;\n  investmint_percent?: InputMaybe<Float8_Comparison_Exp>;\n  neurons_activated?: InputMaybe<Bigint_Comparison_Exp>;\n  redelegation_percent?: InputMaybe<Float8_Comparison_Exp>;\n  swap_percent?: InputMaybe<Float8_Comparison_Exp>;\n  undelegation_percent?: InputMaybe<Float8_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyb_Cohort_Max_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neurons_activated?: Maybe<Scalars['bigint']['output']>;\n  redelegation_percent?: Maybe<Scalars['float8']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  undelegation_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Cyb_Cohort_Min_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neurons_activated?: Maybe<Scalars['bigint']['output']>;\n  redelegation_percent?: Maybe<Scalars['float8']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  undelegation_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"cyb_cohort\". */\nexport type Cyb_Cohort_Order_By = {\n  cyberlink_10_percent?: InputMaybe<Order_By>;\n  cyberlink_100_percent?: InputMaybe<Order_By>;\n  cyberlink_percent?: InputMaybe<Order_By>;\n  hero_hired_percent?: InputMaybe<Order_By>;\n  investmint_percent?: InputMaybe<Order_By>;\n  neurons_activated?: InputMaybe<Order_By>;\n  redelegation_percent?: InputMaybe<Order_By>;\n  swap_percent?: InputMaybe<Order_By>;\n  undelegation_percent?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyb_cohort\" */\nexport enum Cyb_Cohort_Select_Column {\n  /** column name */\n  Cyberlink_10Percent = 'cyberlink_10_percent',\n  /** column name */\n  Cyberlink_100Percent = 'cyberlink_100_percent',\n  /** column name */\n  CyberlinkPercent = 'cyberlink_percent',\n  /** column name */\n  HeroHiredPercent = 'hero_hired_percent',\n  /** column name */\n  InvestmintPercent = 'investmint_percent',\n  /** column name */\n  NeuronsActivated = 'neurons_activated',\n  /** column name */\n  RedelegationPercent = 'redelegation_percent',\n  /** column name */\n  SwapPercent = 'swap_percent',\n  /** column name */\n  UndelegationPercent = 'undelegation_percent',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Cyb_Cohort_Stddev_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyb_Cohort_Stddev_Pop_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyb_Cohort_Stddev_Samp_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"cyb_cohort\" */\nexport type Cyb_Cohort_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Cyb_Cohort_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Cyb_Cohort_Stream_Cursor_Value_Input = {\n  cyberlink_10_percent?: InputMaybe<Scalars['float8']['input']>;\n  cyberlink_100_percent?: InputMaybe<Scalars['float8']['input']>;\n  cyberlink_percent?: InputMaybe<Scalars['float8']['input']>;\n  hero_hired_percent?: InputMaybe<Scalars['float8']['input']>;\n  investmint_percent?: InputMaybe<Scalars['float8']['input']>;\n  neurons_activated?: InputMaybe<Scalars['bigint']['input']>;\n  redelegation_percent?: InputMaybe<Scalars['float8']['input']>;\n  swap_percent?: InputMaybe<Scalars['float8']['input']>;\n  undelegation_percent?: InputMaybe<Scalars['float8']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Cyb_Cohort_Sum_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['float8']['output']>;\n  cyberlink_percent?: Maybe<Scalars['float8']['output']>;\n  hero_hired_percent?: Maybe<Scalars['float8']['output']>;\n  investmint_percent?: Maybe<Scalars['float8']['output']>;\n  neurons_activated?: Maybe<Scalars['bigint']['output']>;\n  redelegation_percent?: Maybe<Scalars['float8']['output']>;\n  swap_percent?: Maybe<Scalars['float8']['output']>;\n  undelegation_percent?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyb_Cohort_Var_Pop_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyb_Cohort_Var_Samp_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Cyb_Cohort_Variance_Fields = {\n  cyberlink_10_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_100_percent?: Maybe<Scalars['Float']['output']>;\n  cyberlink_percent?: Maybe<Scalars['Float']['output']>;\n  hero_hired_percent?: Maybe<Scalars['Float']['output']>;\n  investmint_percent?: Maybe<Scalars['Float']['output']>;\n  neurons_activated?: Maybe<Scalars['Float']['output']>;\n  redelegation_percent?: Maybe<Scalars['Float']['output']>;\n  swap_percent?: Maybe<Scalars['Float']['output']>;\n  undelegation_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs = {\n  address: Scalars['String']['output'];\n  amount: Scalars['bigint']['output'];\n  details: Array<Scalars['jsonb']['output']>;\n  proof: Array<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs_Aggregate = {\n  aggregate?: Maybe<Cyber_Gift_Proofs_Aggregate_Fields>;\n  nodes: Array<Cyber_Gift_Proofs>;\n};\n\n/** aggregate fields of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs_Aggregate_Fields = {\n  avg?: Maybe<Cyber_Gift_Proofs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyber_Gift_Proofs_Max_Fields>;\n  min?: Maybe<Cyber_Gift_Proofs_Min_Fields>;\n  stddev?: Maybe<Cyber_Gift_Proofs_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyber_Gift_Proofs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyber_Gift_Proofs_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyber_Gift_Proofs_Sum_Fields>;\n  var_pop?: Maybe<Cyber_Gift_Proofs_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyber_Gift_Proofs_Var_Samp_Fields>;\n  variance?: Maybe<Cyber_Gift_Proofs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Cyber_Gift_Proofs_Avg_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"cyber_gift_proofs\". All fields are combined with a logical 'AND'. */\nexport type Cyber_Gift_Proofs_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyber_Gift_Proofs_Bool_Exp>>;\n  _not?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyber_Gift_Proofs_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  amount?: InputMaybe<Bigint_Comparison_Exp>;\n  details?: InputMaybe<Jsonb_Array_Comparison_Exp>;\n  proof?: InputMaybe<String_Array_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyber_Gift_Proofs_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  details?: Maybe<Array<Scalars['jsonb']['output']>>;\n  proof?: Maybe<Array<Scalars['String']['output']>>;\n};\n\n/** aggregate min on columns */\nexport type Cyber_Gift_Proofs_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  amount?: Maybe<Scalars['bigint']['output']>;\n  details?: Maybe<Array<Scalars['jsonb']['output']>>;\n  proof?: Maybe<Array<Scalars['String']['output']>>;\n};\n\n/** Ordering options when selecting data from \"cyber_gift_proofs\". */\nexport type Cyber_Gift_Proofs_Order_By = {\n  address?: InputMaybe<Order_By>;\n  amount?: InputMaybe<Order_By>;\n  details?: InputMaybe<Order_By>;\n  proof?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyber_gift_proofs\" */\nexport enum Cyber_Gift_Proofs_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Amount = 'amount',\n  /** column name */\n  Details = 'details',\n  /** column name */\n  Proof = 'proof'\n}\n\n/** aggregate stddev on columns */\nexport type Cyber_Gift_Proofs_Stddev_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyber_Gift_Proofs_Stddev_Pop_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyber_Gift_Proofs_Stddev_Samp_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"cyber_gift_proofs\" */\nexport type Cyber_Gift_Proofs_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Cyber_Gift_Proofs_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Cyber_Gift_Proofs_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n  amount?: InputMaybe<Scalars['bigint']['input']>;\n  details?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  proof?: InputMaybe<Array<Scalars['String']['input']>>;\n};\n\n/** aggregate sum on columns */\nexport type Cyber_Gift_Proofs_Sum_Fields = {\n  amount?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyber_Gift_Proofs_Var_Pop_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyber_Gift_Proofs_Var_Samp_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Cyber_Gift_Proofs_Variance_Fields = {\n  amount?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"cyberlinks\" */\nexport type Cyberlinks = {\n  /** An object relationship */\n  account: Account;\n  /** An object relationship */\n  block: Block;\n  height: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n  neuron: Scalars['String']['output'];\n  particle_from: Scalars['String']['output'];\n  particle_to: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  transaction_hash: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"cyberlinks\" */\nexport type Cyberlinks_Aggregate = {\n  aggregate?: Maybe<Cyberlinks_Aggregate_Fields>;\n  nodes: Array<Cyberlinks>;\n};\n\nexport type Cyberlinks_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Cyberlinks_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Cyberlinks_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Cyberlinks_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"cyberlinks\" */\nexport type Cyberlinks_Aggregate_Fields = {\n  avg?: Maybe<Cyberlinks_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyberlinks_Max_Fields>;\n  min?: Maybe<Cyberlinks_Min_Fields>;\n  stddev?: Maybe<Cyberlinks_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyberlinks_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyberlinks_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyberlinks_Sum_Fields>;\n  var_pop?: Maybe<Cyberlinks_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyberlinks_Var_Samp_Fields>;\n  variance?: Maybe<Cyberlinks_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyberlinks\" */\nexport type Cyberlinks_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"cyberlinks\" */\nexport type Cyberlinks_Aggregate_Order_By = {\n  avg?: InputMaybe<Cyberlinks_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Cyberlinks_Max_Order_By>;\n  min?: InputMaybe<Cyberlinks_Min_Order_By>;\n  stddev?: InputMaybe<Cyberlinks_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Cyberlinks_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Cyberlinks_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Cyberlinks_Sum_Order_By>;\n  var_pop?: InputMaybe<Cyberlinks_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Cyberlinks_Var_Samp_Order_By>;\n  variance?: InputMaybe<Cyberlinks_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Cyberlinks_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"cyberlinks\". All fields are combined with a logical 'AND'. */\nexport type Cyberlinks_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyberlinks_Bool_Exp>>;\n  _not?: InputMaybe<Cyberlinks_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyberlinks_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n  particle_from?: InputMaybe<String_Comparison_Exp>;\n  particle_to?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyberlinks_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  particle_from?: Maybe<Scalars['String']['output']>;\n  particle_to?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle_from?: InputMaybe<Order_By>;\n  particle_to?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Cyberlinks_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  particle_from?: Maybe<Scalars['String']['output']>;\n  particle_to?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle_from?: InputMaybe<Order_By>;\n  particle_to?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"cyberlinks\". */\nexport type Cyberlinks_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle_from?: InputMaybe<Order_By>;\n  particle_to?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyberlinks\" */\nexport enum Cyberlinks_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  Neuron = 'neuron',\n  /** column name */\n  ParticleFrom = 'particle_from',\n  /** column name */\n  ParticleTo = 'particle_to',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TransactionHash = 'transaction_hash'\n}\n\n/** columns and relationships of \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats = {\n  cyberlinks?: Maybe<Scalars['numeric']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats_Aggregate = {\n  aggregate?: Maybe<Cyberlinks_Stats_Aggregate_Fields>;\n  nodes: Array<Cyberlinks_Stats>;\n};\n\n/** aggregate fields of \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats_Aggregate_Fields = {\n  avg?: Maybe<Cyberlinks_Stats_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Cyberlinks_Stats_Max_Fields>;\n  min?: Maybe<Cyberlinks_Stats_Min_Fields>;\n  stddev?: Maybe<Cyberlinks_Stats_Stddev_Fields>;\n  stddev_pop?: Maybe<Cyberlinks_Stats_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Cyberlinks_Stats_Stddev_Samp_Fields>;\n  sum?: Maybe<Cyberlinks_Stats_Sum_Fields>;\n  var_pop?: Maybe<Cyberlinks_Stats_Var_Pop_Fields>;\n  var_samp?: Maybe<Cyberlinks_Stats_Var_Samp_Fields>;\n  variance?: Maybe<Cyberlinks_Stats_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Cyberlinks_Stats_Avg_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"cyberlinks_stats\". All fields are combined with a logical 'AND'. */\nexport type Cyberlinks_Stats_Bool_Exp = {\n  _and?: InputMaybe<Array<Cyberlinks_Stats_Bool_Exp>>;\n  _not?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n  _or?: InputMaybe<Array<Cyberlinks_Stats_Bool_Exp>>;\n  cyberlinks?: InputMaybe<Numeric_Comparison_Exp>;\n  cyberlinks_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Cyberlinks_Stats_Max_Fields = {\n  cyberlinks?: Maybe<Scalars['numeric']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Cyberlinks_Stats_Min_Fields = {\n  cyberlinks?: Maybe<Scalars['numeric']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"cyberlinks_stats\". */\nexport type Cyberlinks_Stats_Order_By = {\n  cyberlinks?: InputMaybe<Order_By>;\n  cyberlinks_per_day?: InputMaybe<Order_By>;\n  date?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"cyberlinks_stats\" */\nexport enum Cyberlinks_Stats_Select_Column {\n  /** column name */\n  Cyberlinks = 'cyberlinks',\n  /** column name */\n  CyberlinksPerDay = 'cyberlinks_per_day',\n  /** column name */\n  Date = 'date'\n}\n\n/** aggregate stddev on columns */\nexport type Cyberlinks_Stats_Stddev_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyberlinks_Stats_Stddev_Pop_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyberlinks_Stats_Stddev_Samp_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"cyberlinks_stats\" */\nexport type Cyberlinks_Stats_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Cyberlinks_Stats_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Cyberlinks_Stats_Stream_Cursor_Value_Input = {\n  cyberlinks?: InputMaybe<Scalars['numeric']['input']>;\n  cyberlinks_per_day?: InputMaybe<Scalars['bigint']['input']>;\n  date?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Cyberlinks_Stats_Sum_Fields = {\n  cyberlinks?: Maybe<Scalars['numeric']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyberlinks_Stats_Var_Pop_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyberlinks_Stats_Var_Samp_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Cyberlinks_Stats_Variance_Fields = {\n  cyberlinks?: Maybe<Scalars['Float']['output']>;\n  cyberlinks_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev on columns */\nexport type Cyberlinks_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Cyberlinks_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Cyberlinks_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"cyberlinks\" */\nexport type Cyberlinks_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Cyberlinks_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Cyberlinks_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  id?: InputMaybe<Scalars['Int']['input']>;\n  neuron?: InputMaybe<Scalars['String']['input']>;\n  particle_from?: InputMaybe<Scalars['String']['input']>;\n  particle_to?: InputMaybe<Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Cyberlinks_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** order by sum() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Cyberlinks_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Cyberlinks_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Cyberlinks_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"cyberlinks\" */\nexport type Cyberlinks_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons_Aggregate = {\n  aggregate?: Maybe<Daily_Amount_Of_Active_Neurons_Aggregate_Fields>;\n  nodes: Array<Daily_Amount_Of_Active_Neurons>;\n};\n\n/** aggregate fields of \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons_Aggregate_Fields = {\n  avg?: Maybe<Daily_Amount_Of_Active_Neurons_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Daily_Amount_Of_Active_Neurons_Max_Fields>;\n  min?: Maybe<Daily_Amount_Of_Active_Neurons_Min_Fields>;\n  stddev?: Maybe<Daily_Amount_Of_Active_Neurons_Stddev_Fields>;\n  stddev_pop?: Maybe<Daily_Amount_Of_Active_Neurons_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Daily_Amount_Of_Active_Neurons_Stddev_Samp_Fields>;\n  sum?: Maybe<Daily_Amount_Of_Active_Neurons_Sum_Fields>;\n  var_pop?: Maybe<Daily_Amount_Of_Active_Neurons_Var_Pop_Fields>;\n  var_samp?: Maybe<Daily_Amount_Of_Active_Neurons_Var_Samp_Fields>;\n  variance?: Maybe<Daily_Amount_Of_Active_Neurons_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Daily_Amount_Of_Active_Neurons_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"daily_amount_of_active_neurons\". All fields are combined with a logical 'AND'. */\nexport type Daily_Amount_Of_Active_Neurons_Bool_Exp = {\n  _and?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Bool_Exp>>;\n  _not?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n  _or?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Daily_Amount_Of_Active_Neurons_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Daily_Amount_Of_Active_Neurons_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"daily_amount_of_active_neurons\". */\nexport type Daily_Amount_Of_Active_Neurons_Order_By = {\n  count?: InputMaybe<Order_By>;\n  date?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"daily_amount_of_active_neurons\" */\nexport enum Daily_Amount_Of_Active_Neurons_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Date = 'date'\n}\n\n/** aggregate stddev on columns */\nexport type Daily_Amount_Of_Active_Neurons_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Daily_Amount_Of_Active_Neurons_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Daily_Amount_Of_Active_Neurons_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"daily_amount_of_active_neurons\" */\nexport type Daily_Amount_Of_Active_Neurons_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Daily_Amount_Of_Active_Neurons_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Daily_Amount_Of_Active_Neurons_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['bigint']['input']>;\n  date?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Daily_Amount_Of_Active_Neurons_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Daily_Amount_Of_Active_Neurons_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Daily_Amount_Of_Active_Neurons_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Daily_Amount_Of_Active_Neurons_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas = {\n  daily_gas?: Maybe<Scalars['numeric']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n  gas_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas_Aggregate = {\n  aggregate?: Maybe<Daily_Amount_Of_Used_Gas_Aggregate_Fields>;\n  nodes: Array<Daily_Amount_Of_Used_Gas>;\n};\n\n/** aggregate fields of \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas_Aggregate_Fields = {\n  avg?: Maybe<Daily_Amount_Of_Used_Gas_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Daily_Amount_Of_Used_Gas_Max_Fields>;\n  min?: Maybe<Daily_Amount_Of_Used_Gas_Min_Fields>;\n  stddev?: Maybe<Daily_Amount_Of_Used_Gas_Stddev_Fields>;\n  stddev_pop?: Maybe<Daily_Amount_Of_Used_Gas_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Daily_Amount_Of_Used_Gas_Stddev_Samp_Fields>;\n  sum?: Maybe<Daily_Amount_Of_Used_Gas_Sum_Fields>;\n  var_pop?: Maybe<Daily_Amount_Of_Used_Gas_Var_Pop_Fields>;\n  var_samp?: Maybe<Daily_Amount_Of_Used_Gas_Var_Samp_Fields>;\n  variance?: Maybe<Daily_Amount_Of_Used_Gas_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Daily_Amount_Of_Used_Gas_Avg_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"daily_amount_of_used_gas\". All fields are combined with a logical 'AND'. */\nexport type Daily_Amount_Of_Used_Gas_Bool_Exp = {\n  _and?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Bool_Exp>>;\n  _not?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n  _or?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Bool_Exp>>;\n  daily_gas?: InputMaybe<Numeric_Comparison_Exp>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  gas_total?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Daily_Amount_Of_Used_Gas_Max_Fields = {\n  daily_gas?: Maybe<Scalars['numeric']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n  gas_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Daily_Amount_Of_Used_Gas_Min_Fields = {\n  daily_gas?: Maybe<Scalars['numeric']['output']>;\n  date?: Maybe<Scalars['date']['output']>;\n  gas_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"daily_amount_of_used_gas\". */\nexport type Daily_Amount_Of_Used_Gas_Order_By = {\n  daily_gas?: InputMaybe<Order_By>;\n  date?: InputMaybe<Order_By>;\n  gas_total?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"daily_amount_of_used_gas\" */\nexport enum Daily_Amount_Of_Used_Gas_Select_Column {\n  /** column name */\n  DailyGas = 'daily_gas',\n  /** column name */\n  Date = 'date',\n  /** column name */\n  GasTotal = 'gas_total'\n}\n\n/** aggregate stddev on columns */\nexport type Daily_Amount_Of_Used_Gas_Stddev_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Daily_Amount_Of_Used_Gas_Stddev_Pop_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Daily_Amount_Of_Used_Gas_Stddev_Samp_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"daily_amount_of_used_gas\" */\nexport type Daily_Amount_Of_Used_Gas_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Daily_Amount_Of_Used_Gas_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Daily_Amount_Of_Used_Gas_Stream_Cursor_Value_Input = {\n  daily_gas?: InputMaybe<Scalars['numeric']['input']>;\n  date?: InputMaybe<Scalars['date']['input']>;\n  gas_total?: InputMaybe<Scalars['numeric']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Daily_Amount_Of_Used_Gas_Sum_Fields = {\n  daily_gas?: Maybe<Scalars['numeric']['output']>;\n  gas_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Daily_Amount_Of_Used_Gas_Var_Pop_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Daily_Amount_Of_Used_Gas_Var_Samp_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Daily_Amount_Of_Used_Gas_Variance_Fields = {\n  daily_gas?: Maybe<Scalars['Float']['output']>;\n  gas_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions = {\n  date?: Maybe<Scalars['date']['output']>;\n  txs_per_day?: Maybe<Scalars['bigint']['output']>;\n  txs_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions_Aggregate = {\n  aggregate?: Maybe<Daily_Number_Of_Transactions_Aggregate_Fields>;\n  nodes: Array<Daily_Number_Of_Transactions>;\n};\n\n/** aggregate fields of \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions_Aggregate_Fields = {\n  avg?: Maybe<Daily_Number_Of_Transactions_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Daily_Number_Of_Transactions_Max_Fields>;\n  min?: Maybe<Daily_Number_Of_Transactions_Min_Fields>;\n  stddev?: Maybe<Daily_Number_Of_Transactions_Stddev_Fields>;\n  stddev_pop?: Maybe<Daily_Number_Of_Transactions_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Daily_Number_Of_Transactions_Stddev_Samp_Fields>;\n  sum?: Maybe<Daily_Number_Of_Transactions_Sum_Fields>;\n  var_pop?: Maybe<Daily_Number_Of_Transactions_Var_Pop_Fields>;\n  var_samp?: Maybe<Daily_Number_Of_Transactions_Var_Samp_Fields>;\n  variance?: Maybe<Daily_Number_Of_Transactions_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Daily_Number_Of_Transactions_Avg_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"daily_number_of_transactions\". All fields are combined with a logical 'AND'. */\nexport type Daily_Number_Of_Transactions_Bool_Exp = {\n  _and?: InputMaybe<Array<Daily_Number_Of_Transactions_Bool_Exp>>;\n  _not?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n  _or?: InputMaybe<Array<Daily_Number_Of_Transactions_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  txs_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n  txs_total?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Daily_Number_Of_Transactions_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  txs_per_day?: Maybe<Scalars['bigint']['output']>;\n  txs_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Daily_Number_Of_Transactions_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  txs_per_day?: Maybe<Scalars['bigint']['output']>;\n  txs_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"daily_number_of_transactions\". */\nexport type Daily_Number_Of_Transactions_Order_By = {\n  date?: InputMaybe<Order_By>;\n  txs_per_day?: InputMaybe<Order_By>;\n  txs_total?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"daily_number_of_transactions\" */\nexport enum Daily_Number_Of_Transactions_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  TxsPerDay = 'txs_per_day',\n  /** column name */\n  TxsTotal = 'txs_total'\n}\n\n/** aggregate stddev on columns */\nexport type Daily_Number_Of_Transactions_Stddev_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Daily_Number_Of_Transactions_Stddev_Pop_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Daily_Number_Of_Transactions_Stddev_Samp_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"daily_number_of_transactions\" */\nexport type Daily_Number_Of_Transactions_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Daily_Number_Of_Transactions_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Daily_Number_Of_Transactions_Stream_Cursor_Value_Input = {\n  date?: InputMaybe<Scalars['date']['input']>;\n  txs_per_day?: InputMaybe<Scalars['bigint']['input']>;\n  txs_total?: InputMaybe<Scalars['numeric']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Daily_Number_Of_Transactions_Sum_Fields = {\n  txs_per_day?: Maybe<Scalars['bigint']['output']>;\n  txs_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Daily_Number_Of_Transactions_Var_Pop_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Daily_Number_Of_Transactions_Var_Samp_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Daily_Number_Of_Transactions_Variance_Fields = {\n  txs_per_day?: Maybe<Scalars['Float']['output']>;\n  txs_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to compare columns of type \"date\". All fields are combined with logical 'AND'. */\nexport type Date_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['date']['input']>;\n  _gt?: InputMaybe<Scalars['date']['input']>;\n  _gte?: InputMaybe<Scalars['date']['input']>;\n  _in?: InputMaybe<Array<Scalars['date']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['date']['input']>;\n  _lte?: InputMaybe<Scalars['date']['input']>;\n  _neq?: InputMaybe<Scalars['date']['input']>;\n  _nin?: InputMaybe<Array<Scalars['date']['input']>>;\n};\n\n/** columns and relationships of \"first_10_cyberlink\" */\nexport type First_10_Cyberlink = {\n  cyberlink_10?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_10_cyberlink\" */\nexport type First_10_Cyberlink_Aggregate = {\n  aggregate?: Maybe<First_10_Cyberlink_Aggregate_Fields>;\n  nodes: Array<First_10_Cyberlink>;\n};\n\n/** aggregate fields of \"first_10_cyberlink\" */\nexport type First_10_Cyberlink_Aggregate_Fields = {\n  avg?: Maybe<First_10_Cyberlink_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_10_Cyberlink_Max_Fields>;\n  min?: Maybe<First_10_Cyberlink_Min_Fields>;\n  stddev?: Maybe<First_10_Cyberlink_Stddev_Fields>;\n  stddev_pop?: Maybe<First_10_Cyberlink_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_10_Cyberlink_Stddev_Samp_Fields>;\n  sum?: Maybe<First_10_Cyberlink_Sum_Fields>;\n  var_pop?: Maybe<First_10_Cyberlink_Var_Pop_Fields>;\n  var_samp?: Maybe<First_10_Cyberlink_Var_Samp_Fields>;\n  variance?: Maybe<First_10_Cyberlink_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_10_cyberlink\" */\nexport type First_10_Cyberlink_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_10_Cyberlink_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_10_Cyberlink_Avg_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_10_cyberlink\". All fields are combined with a logical 'AND'. */\nexport type First_10_Cyberlink_Bool_Exp = {\n  _and?: InputMaybe<Array<First_10_Cyberlink_Bool_Exp>>;\n  _not?: InputMaybe<First_10_Cyberlink_Bool_Exp>;\n  _or?: InputMaybe<Array<First_10_Cyberlink_Bool_Exp>>;\n  cyberlink_10?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_10_Cyberlink_Max_Fields = {\n  cyberlink_10?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_10_Cyberlink_Min_Fields = {\n  cyberlink_10?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_10_cyberlink\". */\nexport type First_10_Cyberlink_Order_By = {\n  cyberlink_10?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_10_cyberlink\" */\nexport enum First_10_Cyberlink_Select_Column {\n  /** column name */\n  Cyberlink_10 = 'cyberlink_10',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_10_Cyberlink_Stddev_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_10_Cyberlink_Stddev_Pop_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_10_Cyberlink_Stddev_Samp_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_10_cyberlink\" */\nexport type First_10_Cyberlink_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_10_Cyberlink_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_10_Cyberlink_Stream_Cursor_Value_Input = {\n  cyberlink_10?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_10_Cyberlink_Sum_Fields = {\n  cyberlink_10?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_10_Cyberlink_Var_Pop_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_10_Cyberlink_Var_Samp_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_10_Cyberlink_Variance_Fields = {\n  cyberlink_10?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"first_100_cyberlink\" */\nexport type First_100_Cyberlink = {\n  cyberlink_100?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_100_cyberlink\" */\nexport type First_100_Cyberlink_Aggregate = {\n  aggregate?: Maybe<First_100_Cyberlink_Aggregate_Fields>;\n  nodes: Array<First_100_Cyberlink>;\n};\n\n/** aggregate fields of \"first_100_cyberlink\" */\nexport type First_100_Cyberlink_Aggregate_Fields = {\n  avg?: Maybe<First_100_Cyberlink_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_100_Cyberlink_Max_Fields>;\n  min?: Maybe<First_100_Cyberlink_Min_Fields>;\n  stddev?: Maybe<First_100_Cyberlink_Stddev_Fields>;\n  stddev_pop?: Maybe<First_100_Cyberlink_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_100_Cyberlink_Stddev_Samp_Fields>;\n  sum?: Maybe<First_100_Cyberlink_Sum_Fields>;\n  var_pop?: Maybe<First_100_Cyberlink_Var_Pop_Fields>;\n  var_samp?: Maybe<First_100_Cyberlink_Var_Samp_Fields>;\n  variance?: Maybe<First_100_Cyberlink_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_100_cyberlink\" */\nexport type First_100_Cyberlink_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_100_Cyberlink_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_100_Cyberlink_Avg_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_100_cyberlink\". All fields are combined with a logical 'AND'. */\nexport type First_100_Cyberlink_Bool_Exp = {\n  _and?: InputMaybe<Array<First_100_Cyberlink_Bool_Exp>>;\n  _not?: InputMaybe<First_100_Cyberlink_Bool_Exp>;\n  _or?: InputMaybe<Array<First_100_Cyberlink_Bool_Exp>>;\n  cyberlink_100?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_100_Cyberlink_Max_Fields = {\n  cyberlink_100?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_100_Cyberlink_Min_Fields = {\n  cyberlink_100?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_100_cyberlink\". */\nexport type First_100_Cyberlink_Order_By = {\n  cyberlink_100?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_100_cyberlink\" */\nexport enum First_100_Cyberlink_Select_Column {\n  /** column name */\n  Cyberlink_100 = 'cyberlink_100',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_100_Cyberlink_Stddev_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_100_Cyberlink_Stddev_Pop_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_100_Cyberlink_Stddev_Samp_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_100_cyberlink\" */\nexport type First_100_Cyberlink_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_100_Cyberlink_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_100_Cyberlink_Stream_Cursor_Value_Input = {\n  cyberlink_100?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_100_Cyberlink_Sum_Fields = {\n  cyberlink_100?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_100_Cyberlink_Var_Pop_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_100_Cyberlink_Var_Samp_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_100_Cyberlink_Variance_Fields = {\n  cyberlink_100?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"first_cyberlink\" */\nexport type First_Cyberlink = {\n  cyberlink?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_cyberlink\" */\nexport type First_Cyberlink_Aggregate = {\n  aggregate?: Maybe<First_Cyberlink_Aggregate_Fields>;\n  nodes: Array<First_Cyberlink>;\n};\n\n/** aggregate fields of \"first_cyberlink\" */\nexport type First_Cyberlink_Aggregate_Fields = {\n  avg?: Maybe<First_Cyberlink_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_Cyberlink_Max_Fields>;\n  min?: Maybe<First_Cyberlink_Min_Fields>;\n  stddev?: Maybe<First_Cyberlink_Stddev_Fields>;\n  stddev_pop?: Maybe<First_Cyberlink_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_Cyberlink_Stddev_Samp_Fields>;\n  sum?: Maybe<First_Cyberlink_Sum_Fields>;\n  var_pop?: Maybe<First_Cyberlink_Var_Pop_Fields>;\n  var_samp?: Maybe<First_Cyberlink_Var_Samp_Fields>;\n  variance?: Maybe<First_Cyberlink_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_cyberlink\" */\nexport type First_Cyberlink_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_Cyberlink_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_Cyberlink_Avg_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_cyberlink\". All fields are combined with a logical 'AND'. */\nexport type First_Cyberlink_Bool_Exp = {\n  _and?: InputMaybe<Array<First_Cyberlink_Bool_Exp>>;\n  _not?: InputMaybe<First_Cyberlink_Bool_Exp>;\n  _or?: InputMaybe<Array<First_Cyberlink_Bool_Exp>>;\n  cyberlink?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_Cyberlink_Max_Fields = {\n  cyberlink?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_Cyberlink_Min_Fields = {\n  cyberlink?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_cyberlink\". */\nexport type First_Cyberlink_Order_By = {\n  cyberlink?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_cyberlink\" */\nexport enum First_Cyberlink_Select_Column {\n  /** column name */\n  Cyberlink = 'cyberlink',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_Cyberlink_Stddev_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_Cyberlink_Stddev_Pop_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_Cyberlink_Stddev_Samp_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_cyberlink\" */\nexport type First_Cyberlink_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_Cyberlink_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_Cyberlink_Stream_Cursor_Value_Input = {\n  cyberlink?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_Cyberlink_Sum_Fields = {\n  cyberlink?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_Cyberlink_Var_Pop_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_Cyberlink_Var_Samp_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_Cyberlink_Variance_Fields = {\n  cyberlink?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"first_hero_hired\" */\nexport type First_Hero_Hired = {\n  hero_hired?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_hero_hired\" */\nexport type First_Hero_Hired_Aggregate = {\n  aggregate?: Maybe<First_Hero_Hired_Aggregate_Fields>;\n  nodes: Array<First_Hero_Hired>;\n};\n\n/** aggregate fields of \"first_hero_hired\" */\nexport type First_Hero_Hired_Aggregate_Fields = {\n  avg?: Maybe<First_Hero_Hired_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_Hero_Hired_Max_Fields>;\n  min?: Maybe<First_Hero_Hired_Min_Fields>;\n  stddev?: Maybe<First_Hero_Hired_Stddev_Fields>;\n  stddev_pop?: Maybe<First_Hero_Hired_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_Hero_Hired_Stddev_Samp_Fields>;\n  sum?: Maybe<First_Hero_Hired_Sum_Fields>;\n  var_pop?: Maybe<First_Hero_Hired_Var_Pop_Fields>;\n  var_samp?: Maybe<First_Hero_Hired_Var_Samp_Fields>;\n  variance?: Maybe<First_Hero_Hired_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_hero_hired\" */\nexport type First_Hero_Hired_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_Hero_Hired_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_Hero_Hired_Avg_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_hero_hired\". All fields are combined with a logical 'AND'. */\nexport type First_Hero_Hired_Bool_Exp = {\n  _and?: InputMaybe<Array<First_Hero_Hired_Bool_Exp>>;\n  _not?: InputMaybe<First_Hero_Hired_Bool_Exp>;\n  _or?: InputMaybe<Array<First_Hero_Hired_Bool_Exp>>;\n  hero_hired?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_Hero_Hired_Max_Fields = {\n  hero_hired?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_Hero_Hired_Min_Fields = {\n  hero_hired?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_hero_hired\". */\nexport type First_Hero_Hired_Order_By = {\n  hero_hired?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_hero_hired\" */\nexport enum First_Hero_Hired_Select_Column {\n  /** column name */\n  HeroHired = 'hero_hired',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_Hero_Hired_Stddev_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_Hero_Hired_Stddev_Pop_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_Hero_Hired_Stddev_Samp_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_hero_hired\" */\nexport type First_Hero_Hired_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_Hero_Hired_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_Hero_Hired_Stream_Cursor_Value_Input = {\n  hero_hired?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_Hero_Hired_Sum_Fields = {\n  hero_hired?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_Hero_Hired_Var_Pop_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_Hero_Hired_Var_Samp_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_Hero_Hired_Variance_Fields = {\n  hero_hired?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"first_investmint\" */\nexport type First_Investmint = {\n  investmint?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_investmint\" */\nexport type First_Investmint_Aggregate = {\n  aggregate?: Maybe<First_Investmint_Aggregate_Fields>;\n  nodes: Array<First_Investmint>;\n};\n\n/** aggregate fields of \"first_investmint\" */\nexport type First_Investmint_Aggregate_Fields = {\n  avg?: Maybe<First_Investmint_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_Investmint_Max_Fields>;\n  min?: Maybe<First_Investmint_Min_Fields>;\n  stddev?: Maybe<First_Investmint_Stddev_Fields>;\n  stddev_pop?: Maybe<First_Investmint_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_Investmint_Stddev_Samp_Fields>;\n  sum?: Maybe<First_Investmint_Sum_Fields>;\n  var_pop?: Maybe<First_Investmint_Var_Pop_Fields>;\n  var_samp?: Maybe<First_Investmint_Var_Samp_Fields>;\n  variance?: Maybe<First_Investmint_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_investmint\" */\nexport type First_Investmint_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_Investmint_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_Investmint_Avg_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_investmint\". All fields are combined with a logical 'AND'. */\nexport type First_Investmint_Bool_Exp = {\n  _and?: InputMaybe<Array<First_Investmint_Bool_Exp>>;\n  _not?: InputMaybe<First_Investmint_Bool_Exp>;\n  _or?: InputMaybe<Array<First_Investmint_Bool_Exp>>;\n  investmint?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_Investmint_Max_Fields = {\n  investmint?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_Investmint_Min_Fields = {\n  investmint?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_investmint\". */\nexport type First_Investmint_Order_By = {\n  investmint?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_investmint\" */\nexport enum First_Investmint_Select_Column {\n  /** column name */\n  Investmint = 'investmint',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_Investmint_Stddev_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_Investmint_Stddev_Pop_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_Investmint_Stddev_Samp_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_investmint\" */\nexport type First_Investmint_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_Investmint_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_Investmint_Stream_Cursor_Value_Input = {\n  investmint?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_Investmint_Sum_Fields = {\n  investmint?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_Investmint_Var_Pop_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_Investmint_Var_Samp_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_Investmint_Variance_Fields = {\n  investmint?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"first_neuron_activation\" */\nexport type First_Neuron_Activation = {\n  neuron_activation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_neuron_activation\" */\nexport type First_Neuron_Activation_Aggregate = {\n  aggregate?: Maybe<First_Neuron_Activation_Aggregate_Fields>;\n  nodes: Array<First_Neuron_Activation>;\n};\n\n/** aggregate fields of \"first_neuron_activation\" */\nexport type First_Neuron_Activation_Aggregate_Fields = {\n  avg?: Maybe<First_Neuron_Activation_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_Neuron_Activation_Max_Fields>;\n  min?: Maybe<First_Neuron_Activation_Min_Fields>;\n  stddev?: Maybe<First_Neuron_Activation_Stddev_Fields>;\n  stddev_pop?: Maybe<First_Neuron_Activation_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_Neuron_Activation_Stddev_Samp_Fields>;\n  sum?: Maybe<First_Neuron_Activation_Sum_Fields>;\n  var_pop?: Maybe<First_Neuron_Activation_Var_Pop_Fields>;\n  var_samp?: Maybe<First_Neuron_Activation_Var_Samp_Fields>;\n  variance?: Maybe<First_Neuron_Activation_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_neuron_activation\" */\nexport type First_Neuron_Activation_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_Neuron_Activation_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_Neuron_Activation_Avg_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_neuron_activation\". All fields are combined with a logical 'AND'. */\nexport type First_Neuron_Activation_Bool_Exp = {\n  _and?: InputMaybe<Array<First_Neuron_Activation_Bool_Exp>>;\n  _not?: InputMaybe<First_Neuron_Activation_Bool_Exp>;\n  _or?: InputMaybe<Array<First_Neuron_Activation_Bool_Exp>>;\n  neuron_activation?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_Neuron_Activation_Max_Fields = {\n  neuron_activation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_Neuron_Activation_Min_Fields = {\n  neuron_activation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_neuron_activation\". */\nexport type First_Neuron_Activation_Order_By = {\n  neuron_activation?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_neuron_activation\" */\nexport enum First_Neuron_Activation_Select_Column {\n  /** column name */\n  NeuronActivation = 'neuron_activation',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_Neuron_Activation_Stddev_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_Neuron_Activation_Stddev_Pop_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_Neuron_Activation_Stddev_Samp_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_neuron_activation\" */\nexport type First_Neuron_Activation_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_Neuron_Activation_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_Neuron_Activation_Stream_Cursor_Value_Input = {\n  neuron_activation?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_Neuron_Activation_Sum_Fields = {\n  neuron_activation?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_Neuron_Activation_Var_Pop_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_Neuron_Activation_Var_Samp_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_Neuron_Activation_Variance_Fields = {\n  neuron_activation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"first_swap\" */\nexport type First_Swap = {\n  swap?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"first_swap\" */\nexport type First_Swap_Aggregate = {\n  aggregate?: Maybe<First_Swap_Aggregate_Fields>;\n  nodes: Array<First_Swap>;\n};\n\n/** aggregate fields of \"first_swap\" */\nexport type First_Swap_Aggregate_Fields = {\n  avg?: Maybe<First_Swap_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<First_Swap_Max_Fields>;\n  min?: Maybe<First_Swap_Min_Fields>;\n  stddev?: Maybe<First_Swap_Stddev_Fields>;\n  stddev_pop?: Maybe<First_Swap_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<First_Swap_Stddev_Samp_Fields>;\n  sum?: Maybe<First_Swap_Sum_Fields>;\n  var_pop?: Maybe<First_Swap_Var_Pop_Fields>;\n  var_samp?: Maybe<First_Swap_Var_Samp_Fields>;\n  variance?: Maybe<First_Swap_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"first_swap\" */\nexport type First_Swap_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<First_Swap_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type First_Swap_Avg_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"first_swap\". All fields are combined with a logical 'AND'. */\nexport type First_Swap_Bool_Exp = {\n  _and?: InputMaybe<Array<First_Swap_Bool_Exp>>;\n  _not?: InputMaybe<First_Swap_Bool_Exp>;\n  _or?: InputMaybe<Array<First_Swap_Bool_Exp>>;\n  swap?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type First_Swap_Max_Fields = {\n  swap?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type First_Swap_Min_Fields = {\n  swap?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"first_swap\". */\nexport type First_Swap_Order_By = {\n  swap?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"first_swap\" */\nexport enum First_Swap_Select_Column {\n  /** column name */\n  Swap = 'swap',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type First_Swap_Stddev_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type First_Swap_Stddev_Pop_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type First_Swap_Stddev_Samp_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"first_swap\" */\nexport type First_Swap_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: First_Swap_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type First_Swap_Stream_Cursor_Value_Input = {\n  swap?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type First_Swap_Sum_Fields = {\n  swap?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type First_Swap_Var_Pop_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type First_Swap_Var_Samp_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type First_Swap_Variance_Fields = {\n  swap?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to compare columns of type \"float8\". All fields are combined with logical 'AND'. */\nexport type Float8_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['float8']['input']>;\n  _gt?: InputMaybe<Scalars['float8']['input']>;\n  _gte?: InputMaybe<Scalars['float8']['input']>;\n  _in?: InputMaybe<Array<Scalars['float8']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['float8']['input']>;\n  _lte?: InputMaybe<Scalars['float8']['input']>;\n  _neq?: InputMaybe<Scalars['float8']['input']>;\n  _nin?: InputMaybe<Array<Scalars['float8']['input']>>;\n};\n\n/** columns and relationships of \"follow_stats\" */\nexport type Follow_Stats = {\n  date?: Maybe<Scalars['date']['output']>;\n  follow_total?: Maybe<Scalars['numeric']['output']>;\n  follows_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregated selection of \"follow_stats\" */\nexport type Follow_Stats_Aggregate = {\n  aggregate?: Maybe<Follow_Stats_Aggregate_Fields>;\n  nodes: Array<Follow_Stats>;\n};\n\n/** aggregate fields of \"follow_stats\" */\nexport type Follow_Stats_Aggregate_Fields = {\n  avg?: Maybe<Follow_Stats_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Follow_Stats_Max_Fields>;\n  min?: Maybe<Follow_Stats_Min_Fields>;\n  stddev?: Maybe<Follow_Stats_Stddev_Fields>;\n  stddev_pop?: Maybe<Follow_Stats_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Follow_Stats_Stddev_Samp_Fields>;\n  sum?: Maybe<Follow_Stats_Sum_Fields>;\n  var_pop?: Maybe<Follow_Stats_Var_Pop_Fields>;\n  var_samp?: Maybe<Follow_Stats_Var_Samp_Fields>;\n  variance?: Maybe<Follow_Stats_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"follow_stats\" */\nexport type Follow_Stats_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Follow_Stats_Avg_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"follow_stats\". All fields are combined with a logical 'AND'. */\nexport type Follow_Stats_Bool_Exp = {\n  _and?: InputMaybe<Array<Follow_Stats_Bool_Exp>>;\n  _not?: InputMaybe<Follow_Stats_Bool_Exp>;\n  _or?: InputMaybe<Array<Follow_Stats_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  follow_total?: InputMaybe<Numeric_Comparison_Exp>;\n  follows_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Follow_Stats_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  follow_total?: Maybe<Scalars['numeric']['output']>;\n  follows_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Follow_Stats_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  follow_total?: Maybe<Scalars['numeric']['output']>;\n  follows_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"follow_stats\". */\nexport type Follow_Stats_Order_By = {\n  date?: InputMaybe<Order_By>;\n  follow_total?: InputMaybe<Order_By>;\n  follows_per_day?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"follow_stats\" */\nexport enum Follow_Stats_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  FollowTotal = 'follow_total',\n  /** column name */\n  FollowsPerDay = 'follows_per_day'\n}\n\n/** aggregate stddev on columns */\nexport type Follow_Stats_Stddev_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Follow_Stats_Stddev_Pop_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Follow_Stats_Stddev_Samp_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"follow_stats\" */\nexport type Follow_Stats_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Follow_Stats_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Follow_Stats_Stream_Cursor_Value_Input = {\n  date?: InputMaybe<Scalars['date']['input']>;\n  follow_total?: InputMaybe<Scalars['numeric']['input']>;\n  follows_per_day?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Follow_Stats_Sum_Fields = {\n  follow_total?: Maybe<Scalars['numeric']['output']>;\n  follows_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Follow_Stats_Var_Pop_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Follow_Stats_Var_Samp_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Follow_Stats_Variance_Fields = {\n  follow_total?: Maybe<Scalars['Float']['output']>;\n  follows_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"genesis\" */\nexport type Genesis = {\n  chain_id: Scalars['String']['output'];\n  initial_height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n  time: Scalars['timestamp']['output'];\n};\n\n/** columns and relationships of \"genesis_accounts\" */\nexport type Genesis_Accounts = {\n  address: Scalars['String']['output'];\n  balance: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n};\n\n/** aggregated selection of \"genesis_accounts\" */\nexport type Genesis_Accounts_Aggregate = {\n  aggregate?: Maybe<Genesis_Accounts_Aggregate_Fields>;\n  nodes: Array<Genesis_Accounts>;\n};\n\n/** aggregate fields of \"genesis_accounts\" */\nexport type Genesis_Accounts_Aggregate_Fields = {\n  avg?: Maybe<Genesis_Accounts_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Genesis_Accounts_Max_Fields>;\n  min?: Maybe<Genesis_Accounts_Min_Fields>;\n  stddev?: Maybe<Genesis_Accounts_Stddev_Fields>;\n  stddev_pop?: Maybe<Genesis_Accounts_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Genesis_Accounts_Stddev_Samp_Fields>;\n  sum?: Maybe<Genesis_Accounts_Sum_Fields>;\n  var_pop?: Maybe<Genesis_Accounts_Var_Pop_Fields>;\n  var_samp?: Maybe<Genesis_Accounts_Var_Samp_Fields>;\n  variance?: Maybe<Genesis_Accounts_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"genesis_accounts\" */\nexport type Genesis_Accounts_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Genesis_Accounts_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Genesis_Accounts_Avg_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"genesis_accounts\". All fields are combined with a logical 'AND'. */\nexport type Genesis_Accounts_Bool_Exp = {\n  _and?: InputMaybe<Array<Genesis_Accounts_Bool_Exp>>;\n  _not?: InputMaybe<Genesis_Accounts_Bool_Exp>;\n  _or?: InputMaybe<Array<Genesis_Accounts_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  balance?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Genesis_Accounts_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  balance?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Genesis_Accounts_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  balance?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** Ordering options when selecting data from \"genesis_accounts\". */\nexport type Genesis_Accounts_Order_By = {\n  address?: InputMaybe<Order_By>;\n  balance?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"genesis_accounts\" */\nexport enum Genesis_Accounts_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  Balance = 'balance',\n  /** column name */\n  Id = 'id'\n}\n\n/** aggregate stddev on columns */\nexport type Genesis_Accounts_Stddev_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Genesis_Accounts_Stddev_Pop_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Genesis_Accounts_Stddev_Samp_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"genesis_accounts\" */\nexport type Genesis_Accounts_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Genesis_Accounts_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Genesis_Accounts_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n  balance?: InputMaybe<Scalars['bigint']['input']>;\n  id?: InputMaybe<Scalars['Int']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Genesis_Accounts_Sum_Fields = {\n  balance?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Genesis_Accounts_Var_Pop_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Genesis_Accounts_Var_Samp_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Genesis_Accounts_Variance_Fields = {\n  balance?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregated selection of \"genesis\" */\nexport type Genesis_Aggregate = {\n  aggregate?: Maybe<Genesis_Aggregate_Fields>;\n  nodes: Array<Genesis>;\n};\n\n/** aggregate fields of \"genesis\" */\nexport type Genesis_Aggregate_Fields = {\n  avg?: Maybe<Genesis_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Genesis_Max_Fields>;\n  min?: Maybe<Genesis_Min_Fields>;\n  stddev?: Maybe<Genesis_Stddev_Fields>;\n  stddev_pop?: Maybe<Genesis_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Genesis_Stddev_Samp_Fields>;\n  sum?: Maybe<Genesis_Sum_Fields>;\n  var_pop?: Maybe<Genesis_Var_Pop_Fields>;\n  var_samp?: Maybe<Genesis_Var_Samp_Fields>;\n  variance?: Maybe<Genesis_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"genesis\" */\nexport type Genesis_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Genesis_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Genesis_Avg_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"genesis\". All fields are combined with a logical 'AND'. */\nexport type Genesis_Bool_Exp = {\n  _and?: InputMaybe<Array<Genesis_Bool_Exp>>;\n  _not?: InputMaybe<Genesis_Bool_Exp>;\n  _or?: InputMaybe<Array<Genesis_Bool_Exp>>;\n  chain_id?: InputMaybe<String_Comparison_Exp>;\n  initial_height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n  time?: InputMaybe<Timestamp_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Genesis_Max_Fields = {\n  chain_id?: Maybe<Scalars['String']['output']>;\n  initial_height?: Maybe<Scalars['bigint']['output']>;\n  time?: Maybe<Scalars['timestamp']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Genesis_Min_Fields = {\n  chain_id?: Maybe<Scalars['String']['output']>;\n  initial_height?: Maybe<Scalars['bigint']['output']>;\n  time?: Maybe<Scalars['timestamp']['output']>;\n};\n\n/** columns and relationships of \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation = {\n  count?: Maybe<Scalars['float8']['output']>;\n  neurons?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation_Aggregate = {\n  aggregate?: Maybe<Genesis_Neurons_Activation_Aggregate_Fields>;\n  nodes: Array<Genesis_Neurons_Activation>;\n};\n\n/** aggregate fields of \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation_Aggregate_Fields = {\n  avg?: Maybe<Genesis_Neurons_Activation_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Genesis_Neurons_Activation_Max_Fields>;\n  min?: Maybe<Genesis_Neurons_Activation_Min_Fields>;\n  stddev?: Maybe<Genesis_Neurons_Activation_Stddev_Fields>;\n  stddev_pop?: Maybe<Genesis_Neurons_Activation_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Genesis_Neurons_Activation_Stddev_Samp_Fields>;\n  sum?: Maybe<Genesis_Neurons_Activation_Sum_Fields>;\n  var_pop?: Maybe<Genesis_Neurons_Activation_Var_Pop_Fields>;\n  var_samp?: Maybe<Genesis_Neurons_Activation_Var_Samp_Fields>;\n  variance?: Maybe<Genesis_Neurons_Activation_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Genesis_Neurons_Activation_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"genesis_neurons_activation\". All fields are combined with a logical 'AND'. */\nexport type Genesis_Neurons_Activation_Bool_Exp = {\n  _and?: InputMaybe<Array<Genesis_Neurons_Activation_Bool_Exp>>;\n  _not?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n  _or?: InputMaybe<Array<Genesis_Neurons_Activation_Bool_Exp>>;\n  count?: InputMaybe<Float8_Comparison_Exp>;\n  neurons?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Genesis_Neurons_Activation_Max_Fields = {\n  count?: Maybe<Scalars['float8']['output']>;\n  neurons?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Genesis_Neurons_Activation_Min_Fields = {\n  count?: Maybe<Scalars['float8']['output']>;\n  neurons?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"genesis_neurons_activation\". */\nexport type Genesis_Neurons_Activation_Order_By = {\n  count?: InputMaybe<Order_By>;\n  neurons?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"genesis_neurons_activation\" */\nexport enum Genesis_Neurons_Activation_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Neurons = 'neurons'\n}\n\n/** aggregate stddev on columns */\nexport type Genesis_Neurons_Activation_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Genesis_Neurons_Activation_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Genesis_Neurons_Activation_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"genesis_neurons_activation\" */\nexport type Genesis_Neurons_Activation_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Genesis_Neurons_Activation_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Genesis_Neurons_Activation_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['float8']['input']>;\n  neurons?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Genesis_Neurons_Activation_Sum_Fields = {\n  count?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Genesis_Neurons_Activation_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Genesis_Neurons_Activation_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Genesis_Neurons_Activation_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Ordering options when selecting data from \"genesis\". */\nexport type Genesis_Order_By = {\n  chain_id?: InputMaybe<Order_By>;\n  initial_height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n  time?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"genesis\" */\nexport enum Genesis_Select_Column {\n  /** column name */\n  ChainId = 'chain_id',\n  /** column name */\n  InitialHeight = 'initial_height',\n  /** column name */\n  OneRowId = 'one_row_id',\n  /** column name */\n  Time = 'time'\n}\n\n/** aggregate stddev on columns */\nexport type Genesis_Stddev_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Genesis_Stddev_Pop_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Genesis_Stddev_Samp_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"genesis\" */\nexport type Genesis_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Genesis_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Genesis_Stream_Cursor_Value_Input = {\n  chain_id?: InputMaybe<Scalars['String']['input']>;\n  initial_height?: InputMaybe<Scalars['bigint']['input']>;\n  one_row_id?: InputMaybe<Scalars['Boolean']['input']>;\n  time?: InputMaybe<Scalars['timestamp']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Genesis_Sum_Fields = {\n  initial_height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Genesis_Var_Pop_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Genesis_Var_Samp_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Genesis_Variance_Fields = {\n  initial_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"investmints\" */\nexport type Investmints = {\n  /** An object relationship */\n  account: Account;\n  amount: Scalars['coin_scalar']['output'];\n  /** An object relationship */\n  block: Block;\n  height: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n  length: Scalars['bigint']['output'];\n  neuron: Scalars['String']['output'];\n  resource: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  transaction_hash: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"investmints\" */\nexport type Investmints_Aggregate = {\n  aggregate?: Maybe<Investmints_Aggregate_Fields>;\n  nodes: Array<Investmints>;\n};\n\nexport type Investmints_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Investmints_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Investmints_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Investmints_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Investmints_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"investmints\" */\nexport type Investmints_Aggregate_Fields = {\n  avg?: Maybe<Investmints_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Investmints_Max_Fields>;\n  min?: Maybe<Investmints_Min_Fields>;\n  stddev?: Maybe<Investmints_Stddev_Fields>;\n  stddev_pop?: Maybe<Investmints_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Investmints_Stddev_Samp_Fields>;\n  sum?: Maybe<Investmints_Sum_Fields>;\n  var_pop?: Maybe<Investmints_Var_Pop_Fields>;\n  var_samp?: Maybe<Investmints_Var_Samp_Fields>;\n  variance?: Maybe<Investmints_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"investmints\" */\nexport type Investmints_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Investmints_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"investmints\" */\nexport type Investmints_Aggregate_Order_By = {\n  avg?: InputMaybe<Investmints_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Investmints_Max_Order_By>;\n  min?: InputMaybe<Investmints_Min_Order_By>;\n  stddev?: InputMaybe<Investmints_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Investmints_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Investmints_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Investmints_Sum_Order_By>;\n  var_pop?: InputMaybe<Investmints_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Investmints_Var_Samp_Order_By>;\n  variance?: InputMaybe<Investmints_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Investmints_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"investmints\" */\nexport type Investmints_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"investmints\". All fields are combined with a logical 'AND'. */\nexport type Investmints_Bool_Exp = {\n  _and?: InputMaybe<Array<Investmints_Bool_Exp>>;\n  _not?: InputMaybe<Investmints_Bool_Exp>;\n  _or?: InputMaybe<Array<Investmints_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  amount?: InputMaybe<Coin_Scalar_Comparison_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  length?: InputMaybe<Bigint_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n  resource?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Investmints_Max_Fields = {\n  amount?: Maybe<Scalars['coin_scalar']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  length?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  resource?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"investmints\" */\nexport type Investmints_Max_Order_By = {\n  amount?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  resource?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Investmints_Min_Fields = {\n  amount?: Maybe<Scalars['coin_scalar']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  length?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  resource?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"investmints\" */\nexport type Investmints_Min_Order_By = {\n  amount?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  resource?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"investmints\". */\nexport type Investmints_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  amount?: InputMaybe<Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  resource?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"investmints\" */\nexport enum Investmints_Select_Column {\n  /** column name */\n  Amount = 'amount',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  Length = 'length',\n  /** column name */\n  Neuron = 'neuron',\n  /** column name */\n  Resource = 'resource',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TransactionHash = 'transaction_hash'\n}\n\n/** aggregate stddev on columns */\nexport type Investmints_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"investmints\" */\nexport type Investmints_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Investmints_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"investmints\" */\nexport type Investmints_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Investmints_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"investmints\" */\nexport type Investmints_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"investmints\" */\nexport type Investmints_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Investmints_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Investmints_Stream_Cursor_Value_Input = {\n  amount?: InputMaybe<Scalars['coin_scalar']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  id?: InputMaybe<Scalars['Int']['input']>;\n  length?: InputMaybe<Scalars['bigint']['input']>;\n  neuron?: InputMaybe<Scalars['String']['input']>;\n  resource?: InputMaybe<Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Investmints_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  length?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"investmints\" */\nexport type Investmints_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Investmints_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"investmints\" */\nexport type Investmints_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Investmints_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"investmints\" */\nexport type Investmints_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Investmints_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n  length?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"investmints\" */\nexport type Investmints_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to compare columns of type \"json\". All fields are combined with logical 'AND'. */\nexport type Json_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['json']['input']>;\n  _gt?: InputMaybe<Scalars['json']['input']>;\n  _gte?: InputMaybe<Scalars['json']['input']>;\n  _in?: InputMaybe<Array<Scalars['json']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['json']['input']>;\n  _lte?: InputMaybe<Scalars['json']['input']>;\n  _neq?: InputMaybe<Scalars['json']['input']>;\n  _nin?: InputMaybe<Array<Scalars['json']['input']>>;\n};\n\n/** Boolean expression to compare columns of type \"jsonb\". All fields are combined with logical 'AND'. */\nexport type Jsonb_Array_Comparison_Exp = {\n  /** is the array contained in the given array value */\n  _contained_in?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  /** does the array contain the given value */\n  _contains?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _eq?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _gt?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _gte?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _in?: InputMaybe<Array<Array<Scalars['jsonb']['input']>>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _lte?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _neq?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _nin?: InputMaybe<Array<Array<Scalars['jsonb']['input']>>>;\n};\n\nexport type Jsonb_Cast_Exp = {\n  String?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** Boolean expression to compare columns of type \"jsonb\". All fields are combined with logical 'AND'. */\nexport type Jsonb_Comparison_Exp = {\n  _cast?: InputMaybe<Jsonb_Cast_Exp>;\n  /** is the column contained in the given json value */\n  _contained_in?: InputMaybe<Scalars['jsonb']['input']>;\n  /** does the column contain the given json value at the top level */\n  _contains?: InputMaybe<Scalars['jsonb']['input']>;\n  _eq?: InputMaybe<Scalars['jsonb']['input']>;\n  _gt?: InputMaybe<Scalars['jsonb']['input']>;\n  _gte?: InputMaybe<Scalars['jsonb']['input']>;\n  /** does the string exist as a top-level key in the column */\n  _has_key?: InputMaybe<Scalars['String']['input']>;\n  /** do all of these strings exist as top-level keys in the column */\n  _has_keys_all?: InputMaybe<Array<Scalars['String']['input']>>;\n  /** do any of these strings exist as top-level keys in the column */\n  _has_keys_any?: InputMaybe<Array<Scalars['String']['input']>>;\n  _in?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['jsonb']['input']>;\n  _lte?: InputMaybe<Scalars['jsonb']['input']>;\n  _neq?: InputMaybe<Scalars['jsonb']['input']>;\n  _nin?: InputMaybe<Array<Scalars['jsonb']['input']>>;\n};\n\n/** columns and relationships of \"message\" */\nexport type Message = {\n  height: Scalars['bigint']['output'];\n  index: Scalars['bigint']['output'];\n  involved_accounts_addresses: Array<Scalars['String']['output']>;\n  /** An object relationship */\n  message_type: Message_Type;\n  partition_id: Scalars['bigint']['output'];\n  /** An object relationship */\n  transaction?: Maybe<Transaction>;\n  /** An object relationship */\n  transaction_155?: Maybe<Transaction_155>;\n  transaction_hash: Scalars['String']['output'];\n  type: Scalars['String']['output'];\n  value: Scalars['json']['output'];\n};\n\n\n/** columns and relationships of \"message\" */\nexport type MessageValueArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** columns and relationships of \"message_155\" */\nexport type Message_155 = {\n  height: Scalars['bigint']['output'];\n  index: Scalars['bigint']['output'];\n  involved_accounts_addresses: Array<Scalars['String']['output']>;\n  /** An object relationship */\n  message_type: Message_Type;\n  partition_id: Scalars['bigint']['output'];\n  /** An object relationship */\n  transaction?: Maybe<Transaction>;\n  transaction_hash: Scalars['String']['output'];\n  type: Scalars['String']['output'];\n  value: Scalars['json']['output'];\n};\n\n\n/** columns and relationships of \"message_155\" */\nexport type Message_155ValueArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregated selection of \"message_155\" */\nexport type Message_155_Aggregate = {\n  aggregate?: Maybe<Message_155_Aggregate_Fields>;\n  nodes: Array<Message_155>;\n};\n\nexport type Message_155_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Message_155_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Message_155_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Message_155_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Message_155_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"message_155\" */\nexport type Message_155_Aggregate_Fields = {\n  avg?: Maybe<Message_155_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Message_155_Max_Fields>;\n  min?: Maybe<Message_155_Min_Fields>;\n  stddev?: Maybe<Message_155_Stddev_Fields>;\n  stddev_pop?: Maybe<Message_155_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Message_155_Stddev_Samp_Fields>;\n  sum?: Maybe<Message_155_Sum_Fields>;\n  var_pop?: Maybe<Message_155_Var_Pop_Fields>;\n  var_samp?: Maybe<Message_155_Var_Samp_Fields>;\n  variance?: Maybe<Message_155_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"message_155\" */\nexport type Message_155_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Message_155_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"message_155\" */\nexport type Message_155_Aggregate_Order_By = {\n  avg?: InputMaybe<Message_155_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Message_155_Max_Order_By>;\n  min?: InputMaybe<Message_155_Min_Order_By>;\n  stddev?: InputMaybe<Message_155_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Message_155_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Message_155_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Message_155_Sum_Order_By>;\n  var_pop?: InputMaybe<Message_155_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Message_155_Var_Samp_Order_By>;\n  variance?: InputMaybe<Message_155_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Message_155_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"message_155\" */\nexport type Message_155_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"message_155\". All fields are combined with a logical 'AND'. */\nexport type Message_155_Bool_Exp = {\n  _and?: InputMaybe<Array<Message_155_Bool_Exp>>;\n  _not?: InputMaybe<Message_155_Bool_Exp>;\n  _or?: InputMaybe<Array<Message_155_Bool_Exp>>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  index?: InputMaybe<Bigint_Comparison_Exp>;\n  involved_accounts_addresses?: InputMaybe<String_Array_Comparison_Exp>;\n  message_type?: InputMaybe<Message_Type_Bool_Exp>;\n  partition_id?: InputMaybe<Bigint_Comparison_Exp>;\n  transaction?: InputMaybe<Transaction_Bool_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  value?: InputMaybe<Json_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Message_155_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"message_155\" */\nexport type Message_155_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Message_155_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"message_155\" */\nexport type Message_155_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"message_155\". */\nexport type Message_155_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  message_type?: InputMaybe<Message_Type_Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  transaction?: InputMaybe<Transaction_Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"message_155\" */\nexport enum Message_155_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Index = 'index',\n  /** column name */\n  InvolvedAccountsAddresses = 'involved_accounts_addresses',\n  /** column name */\n  PartitionId = 'partition_id',\n  /** column name */\n  TransactionHash = 'transaction_hash',\n  /** column name */\n  Type = 'type',\n  /** column name */\n  Value = 'value'\n}\n\n/** aggregate stddev on columns */\nexport type Message_155_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"message_155\" */\nexport type Message_155_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Message_155_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"message_155\" */\nexport type Message_155_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Message_155_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"message_155\" */\nexport type Message_155_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"message_155\" */\nexport type Message_155_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Message_155_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Message_155_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  index?: InputMaybe<Scalars['bigint']['input']>;\n  involved_accounts_addresses?: InputMaybe<Array<Scalars['String']['input']>>;\n  partition_id?: InputMaybe<Scalars['bigint']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n  value?: InputMaybe<Scalars['json']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Message_155_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"message_155\" */\nexport type Message_155_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Message_155_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"message_155\" */\nexport type Message_155_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Message_155_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"message_155\" */\nexport type Message_155_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Message_155_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"message_155\" */\nexport type Message_155_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregated selection of \"message\" */\nexport type Message_Aggregate = {\n  aggregate?: Maybe<Message_Aggregate_Fields>;\n  nodes: Array<Message>;\n};\n\nexport type Message_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Message_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Message_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Message_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Message_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"message\" */\nexport type Message_Aggregate_Fields = {\n  avg?: Maybe<Message_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Message_Max_Fields>;\n  min?: Maybe<Message_Min_Fields>;\n  stddev?: Maybe<Message_Stddev_Fields>;\n  stddev_pop?: Maybe<Message_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Message_Stddev_Samp_Fields>;\n  sum?: Maybe<Message_Sum_Fields>;\n  var_pop?: Maybe<Message_Var_Pop_Fields>;\n  var_samp?: Maybe<Message_Var_Samp_Fields>;\n  variance?: Maybe<Message_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"message\" */\nexport type Message_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Message_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"message\" */\nexport type Message_Aggregate_Order_By = {\n  avg?: InputMaybe<Message_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Message_Max_Order_By>;\n  min?: InputMaybe<Message_Min_Order_By>;\n  stddev?: InputMaybe<Message_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Message_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Message_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Message_Sum_Order_By>;\n  var_pop?: InputMaybe<Message_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Message_Var_Samp_Order_By>;\n  variance?: InputMaybe<Message_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Message_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"message\" */\nexport type Message_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"message\". All fields are combined with a logical 'AND'. */\nexport type Message_Bool_Exp = {\n  _and?: InputMaybe<Array<Message_Bool_Exp>>;\n  _not?: InputMaybe<Message_Bool_Exp>;\n  _or?: InputMaybe<Array<Message_Bool_Exp>>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  index?: InputMaybe<Bigint_Comparison_Exp>;\n  involved_accounts_addresses?: InputMaybe<String_Array_Comparison_Exp>;\n  message_type?: InputMaybe<Message_Type_Bool_Exp>;\n  partition_id?: InputMaybe<Bigint_Comparison_Exp>;\n  transaction?: InputMaybe<Transaction_Bool_Exp>;\n  transaction_155?: InputMaybe<Transaction_155_Bool_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  value?: InputMaybe<Json_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Message_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"message\" */\nexport type Message_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Message_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  involved_accounts_addresses?: Maybe<Array<Scalars['String']['output']>>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"message\" */\nexport type Message_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"message\". */\nexport type Message_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  involved_accounts_addresses?: InputMaybe<Order_By>;\n  message_type?: InputMaybe<Message_Type_Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  transaction?: InputMaybe<Transaction_Order_By>;\n  transaction_155?: InputMaybe<Transaction_155_Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"message\" */\nexport enum Message_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Index = 'index',\n  /** column name */\n  InvolvedAccountsAddresses = 'involved_accounts_addresses',\n  /** column name */\n  PartitionId = 'partition_id',\n  /** column name */\n  TransactionHash = 'transaction_hash',\n  /** column name */\n  Type = 'type',\n  /** column name */\n  Value = 'value'\n}\n\n/** aggregate stddev on columns */\nexport type Message_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"message\" */\nexport type Message_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Message_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"message\" */\nexport type Message_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Message_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"message\" */\nexport type Message_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"message\" */\nexport type Message_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Message_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Message_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  index?: InputMaybe<Scalars['bigint']['input']>;\n  involved_accounts_addresses?: InputMaybe<Array<Scalars['String']['input']>>;\n  partition_id?: InputMaybe<Scalars['bigint']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n  value?: InputMaybe<Scalars['json']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Message_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  index?: Maybe<Scalars['bigint']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"message\" */\nexport type Message_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"message_type\" */\nexport type Message_Type = {\n  height: Scalars['bigint']['output'];\n  label: Scalars['String']['output'];\n  /** An array relationship */\n  message_155s: Array<Message_155>;\n  /** An aggregate relationship */\n  message_155s_aggregate: Message_155_Aggregate;\n  /** An array relationship */\n  messages: Array<Message>;\n  /** An aggregate relationship */\n  messages_aggregate: Message_Aggregate;\n  module: Scalars['String']['output'];\n  type: Scalars['String']['output'];\n};\n\n\n/** columns and relationships of \"message_type\" */\nexport type Message_TypeMessage_155sArgs = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"message_type\" */\nexport type Message_TypeMessage_155s_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"message_type\" */\nexport type Message_TypeMessagesArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"message_type\" */\nexport type Message_TypeMessages_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n/** aggregated selection of \"message_type\" */\nexport type Message_Type_Aggregate = {\n  aggregate?: Maybe<Message_Type_Aggregate_Fields>;\n  nodes: Array<Message_Type>;\n};\n\n/** aggregate fields of \"message_type\" */\nexport type Message_Type_Aggregate_Fields = {\n  avg?: Maybe<Message_Type_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Message_Type_Max_Fields>;\n  min?: Maybe<Message_Type_Min_Fields>;\n  stddev?: Maybe<Message_Type_Stddev_Fields>;\n  stddev_pop?: Maybe<Message_Type_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Message_Type_Stddev_Samp_Fields>;\n  sum?: Maybe<Message_Type_Sum_Fields>;\n  var_pop?: Maybe<Message_Type_Var_Pop_Fields>;\n  var_samp?: Maybe<Message_Type_Var_Samp_Fields>;\n  variance?: Maybe<Message_Type_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"message_type\" */\nexport type Message_Type_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Message_Type_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Message_Type_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"message_type\". All fields are combined with a logical 'AND'. */\nexport type Message_Type_Bool_Exp = {\n  _and?: InputMaybe<Array<Message_Type_Bool_Exp>>;\n  _not?: InputMaybe<Message_Type_Bool_Exp>;\n  _or?: InputMaybe<Array<Message_Type_Bool_Exp>>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  label?: InputMaybe<String_Comparison_Exp>;\n  message_155s?: InputMaybe<Message_155_Bool_Exp>;\n  message_155s_aggregate?: InputMaybe<Message_155_Aggregate_Bool_Exp>;\n  messages?: InputMaybe<Message_Bool_Exp>;\n  messages_aggregate?: InputMaybe<Message_Aggregate_Bool_Exp>;\n  module?: InputMaybe<String_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Message_Type_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  label?: Maybe<Scalars['String']['output']>;\n  module?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Message_Type_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  label?: Maybe<Scalars['String']['output']>;\n  module?: Maybe<Scalars['String']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"message_type\". */\nexport type Message_Type_Order_By = {\n  height?: InputMaybe<Order_By>;\n  label?: InputMaybe<Order_By>;\n  message_155s_aggregate?: InputMaybe<Message_155_Aggregate_Order_By>;\n  messages_aggregate?: InputMaybe<Message_Aggregate_Order_By>;\n  module?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"message_type\" */\nexport enum Message_Type_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Label = 'label',\n  /** column name */\n  Module = 'module',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Message_Type_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Message_Type_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Message_Type_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"message_type\" */\nexport type Message_Type_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Message_Type_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Message_Type_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  label?: InputMaybe<Scalars['String']['input']>;\n  module?: InputMaybe<Scalars['String']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Message_Type_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Message_Type_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Message_Type_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Message_Type_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Message_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"message\" */\nexport type Message_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Message_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"message\" */\nexport type Message_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Message_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  index?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"message\" */\nexport type Message_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  index?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\nexport type Messages_By_Address_Args = {\n  addresses?: InputMaybe<Scalars['_text']['input']>;\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n};\n\nexport type Messages_By_Type_Args = {\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n};\n\n/** columns and relationships of \"modules\" */\nexport type Modules = {\n  module_name: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"modules\" */\nexport type Modules_Aggregate = {\n  aggregate?: Maybe<Modules_Aggregate_Fields>;\n  nodes: Array<Modules>;\n};\n\n/** aggregate fields of \"modules\" */\nexport type Modules_Aggregate_Fields = {\n  count: Scalars['Int']['output'];\n  max?: Maybe<Modules_Max_Fields>;\n  min?: Maybe<Modules_Min_Fields>;\n};\n\n\n/** aggregate fields of \"modules\" */\nexport type Modules_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Modules_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** Boolean expression to filter rows from the table \"modules\". All fields are combined with a logical 'AND'. */\nexport type Modules_Bool_Exp = {\n  _and?: InputMaybe<Array<Modules_Bool_Exp>>;\n  _not?: InputMaybe<Modules_Bool_Exp>;\n  _or?: InputMaybe<Array<Modules_Bool_Exp>>;\n  module_name?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Modules_Max_Fields = {\n  module_name?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Modules_Min_Fields = {\n  module_name?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"modules\". */\nexport type Modules_Order_By = {\n  module_name?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"modules\" */\nexport enum Modules_Select_Column {\n  /** column name */\n  ModuleName = 'module_name'\n}\n\n/** Streaming cursor of the table \"modules\" */\nexport type Modules_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Modules_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Modules_Stream_Cursor_Value_Input = {\n  module_name?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** columns and relationships of \"neuron_activation_source\" */\nexport type Neuron_Activation_Source = {\n  genesis_percent?: Maybe<Scalars['float8']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activated?: Maybe<Scalars['bigint']['output']>;\n  recieve_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"neuron_activation_source\" */\nexport type Neuron_Activation_Source_Aggregate = {\n  aggregate?: Maybe<Neuron_Activation_Source_Aggregate_Fields>;\n  nodes: Array<Neuron_Activation_Source>;\n};\n\n/** aggregate fields of \"neuron_activation_source\" */\nexport type Neuron_Activation_Source_Aggregate_Fields = {\n  avg?: Maybe<Neuron_Activation_Source_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Neuron_Activation_Source_Max_Fields>;\n  min?: Maybe<Neuron_Activation_Source_Min_Fields>;\n  stddev?: Maybe<Neuron_Activation_Source_Stddev_Fields>;\n  stddev_pop?: Maybe<Neuron_Activation_Source_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Neuron_Activation_Source_Stddev_Samp_Fields>;\n  sum?: Maybe<Neuron_Activation_Source_Sum_Fields>;\n  var_pop?: Maybe<Neuron_Activation_Source_Var_Pop_Fields>;\n  var_samp?: Maybe<Neuron_Activation_Source_Var_Samp_Fields>;\n  variance?: Maybe<Neuron_Activation_Source_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"neuron_activation_source\" */\nexport type Neuron_Activation_Source_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Neuron_Activation_Source_Avg_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"neuron_activation_source\". All fields are combined with a logical 'AND'. */\nexport type Neuron_Activation_Source_Bool_Exp = {\n  _and?: InputMaybe<Array<Neuron_Activation_Source_Bool_Exp>>;\n  _not?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n  _or?: InputMaybe<Array<Neuron_Activation_Source_Bool_Exp>>;\n  genesis_percent?: InputMaybe<Float8_Comparison_Exp>;\n  ibc_receive_percent?: InputMaybe<Float8_Comparison_Exp>;\n  neuron_activated?: InputMaybe<Bigint_Comparison_Exp>;\n  recieve_percent?: InputMaybe<Float8_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Neuron_Activation_Source_Max_Fields = {\n  genesis_percent?: Maybe<Scalars['float8']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activated?: Maybe<Scalars['bigint']['output']>;\n  recieve_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Neuron_Activation_Source_Min_Fields = {\n  genesis_percent?: Maybe<Scalars['float8']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activated?: Maybe<Scalars['bigint']['output']>;\n  recieve_percent?: Maybe<Scalars['float8']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"neuron_activation_source\". */\nexport type Neuron_Activation_Source_Order_By = {\n  genesis_percent?: InputMaybe<Order_By>;\n  ibc_receive_percent?: InputMaybe<Order_By>;\n  neuron_activated?: InputMaybe<Order_By>;\n  recieve_percent?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"neuron_activation_source\" */\nexport enum Neuron_Activation_Source_Select_Column {\n  /** column name */\n  GenesisPercent = 'genesis_percent',\n  /** column name */\n  IbcReceivePercent = 'ibc_receive_percent',\n  /** column name */\n  NeuronActivated = 'neuron_activated',\n  /** column name */\n  RecievePercent = 'recieve_percent',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Neuron_Activation_Source_Stddev_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Neuron_Activation_Source_Stddev_Pop_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Neuron_Activation_Source_Stddev_Samp_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"neuron_activation_source\" */\nexport type Neuron_Activation_Source_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Neuron_Activation_Source_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Neuron_Activation_Source_Stream_Cursor_Value_Input = {\n  genesis_percent?: InputMaybe<Scalars['float8']['input']>;\n  ibc_receive_percent?: InputMaybe<Scalars['float8']['input']>;\n  neuron_activated?: InputMaybe<Scalars['bigint']['input']>;\n  recieve_percent?: InputMaybe<Scalars['float8']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Neuron_Activation_Source_Sum_Fields = {\n  genesis_percent?: Maybe<Scalars['float8']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['float8']['output']>;\n  neuron_activated?: Maybe<Scalars['bigint']['output']>;\n  recieve_percent?: Maybe<Scalars['float8']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Neuron_Activation_Source_Var_Pop_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Neuron_Activation_Source_Var_Samp_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Neuron_Activation_Source_Variance_Fields = {\n  genesis_percent?: Maybe<Scalars['Float']['output']>;\n  ibc_receive_percent?: Maybe<Scalars['Float']['output']>;\n  neuron_activated?: Maybe<Scalars['Float']['output']>;\n  recieve_percent?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons = {\n  date?: Maybe<Scalars['date']['output']>;\n  new_neurons_daily?: Maybe<Scalars['bigint']['output']>;\n  new_neurons_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons_Aggregate = {\n  aggregate?: Maybe<Number_Of_New_Neurons_Aggregate_Fields>;\n  nodes: Array<Number_Of_New_Neurons>;\n};\n\n/** aggregate fields of \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons_Aggregate_Fields = {\n  avg?: Maybe<Number_Of_New_Neurons_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Number_Of_New_Neurons_Max_Fields>;\n  min?: Maybe<Number_Of_New_Neurons_Min_Fields>;\n  stddev?: Maybe<Number_Of_New_Neurons_Stddev_Fields>;\n  stddev_pop?: Maybe<Number_Of_New_Neurons_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Number_Of_New_Neurons_Stddev_Samp_Fields>;\n  sum?: Maybe<Number_Of_New_Neurons_Sum_Fields>;\n  var_pop?: Maybe<Number_Of_New_Neurons_Var_Pop_Fields>;\n  var_samp?: Maybe<Number_Of_New_Neurons_Var_Samp_Fields>;\n  variance?: Maybe<Number_Of_New_Neurons_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Number_Of_New_Neurons_Avg_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"number_of_new_neurons\". All fields are combined with a logical 'AND'. */\nexport type Number_Of_New_Neurons_Bool_Exp = {\n  _and?: InputMaybe<Array<Number_Of_New_Neurons_Bool_Exp>>;\n  _not?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n  _or?: InputMaybe<Array<Number_Of_New_Neurons_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  new_neurons_daily?: InputMaybe<Bigint_Comparison_Exp>;\n  new_neurons_total?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Number_Of_New_Neurons_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  new_neurons_daily?: Maybe<Scalars['bigint']['output']>;\n  new_neurons_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Number_Of_New_Neurons_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  new_neurons_daily?: Maybe<Scalars['bigint']['output']>;\n  new_neurons_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"number_of_new_neurons\". */\nexport type Number_Of_New_Neurons_Order_By = {\n  date?: InputMaybe<Order_By>;\n  new_neurons_daily?: InputMaybe<Order_By>;\n  new_neurons_total?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"number_of_new_neurons\" */\nexport enum Number_Of_New_Neurons_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  NewNeuronsDaily = 'new_neurons_daily',\n  /** column name */\n  NewNeuronsTotal = 'new_neurons_total'\n}\n\n/** aggregate stddev on columns */\nexport type Number_Of_New_Neurons_Stddev_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Number_Of_New_Neurons_Stddev_Pop_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Number_Of_New_Neurons_Stddev_Samp_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"number_of_new_neurons\" */\nexport type Number_Of_New_Neurons_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Number_Of_New_Neurons_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Number_Of_New_Neurons_Stream_Cursor_Value_Input = {\n  date?: InputMaybe<Scalars['date']['input']>;\n  new_neurons_daily?: InputMaybe<Scalars['bigint']['input']>;\n  new_neurons_total?: InputMaybe<Scalars['numeric']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Number_Of_New_Neurons_Sum_Fields = {\n  new_neurons_daily?: Maybe<Scalars['bigint']['output']>;\n  new_neurons_total?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Number_Of_New_Neurons_Var_Pop_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Number_Of_New_Neurons_Var_Samp_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Number_Of_New_Neurons_Variance_Fields = {\n  new_neurons_daily?: Maybe<Scalars['Float']['output']>;\n  new_neurons_total?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to compare columns of type \"numeric\". All fields are combined with logical 'AND'. */\nexport type Numeric_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['numeric']['input']>;\n  _gt?: InputMaybe<Scalars['numeric']['input']>;\n  _gte?: InputMaybe<Scalars['numeric']['input']>;\n  _in?: InputMaybe<Array<Scalars['numeric']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['numeric']['input']>;\n  _lte?: InputMaybe<Scalars['numeric']['input']>;\n  _neq?: InputMaybe<Scalars['numeric']['input']>;\n  _nin?: InputMaybe<Array<Scalars['numeric']['input']>>;\n};\n\n/** column ordering options */\nexport enum Order_By {\n  /** in ascending order, nulls last */\n  Asc = 'asc',\n  /** in ascending order, nulls first */\n  AscNullsFirst = 'asc_nulls_first',\n  /** in ascending order, nulls last */\n  AscNullsLast = 'asc_nulls_last',\n  /** in descending order, nulls first */\n  Desc = 'desc',\n  /** in descending order, nulls first */\n  DescNullsFirst = 'desc_nulls_first',\n  /** in descending order, nulls last */\n  DescNullsLast = 'desc_nulls_last'\n}\n\n/** columns and relationships of \"particles\" */\nexport type Particles = {\n  /** An object relationship */\n  account: Account;\n  /** An object relationship */\n  block: Block;\n  height: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n  neuron: Scalars['String']['output'];\n  particle: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  transaction_hash: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"particles\" */\nexport type Particles_Aggregate = {\n  aggregate?: Maybe<Particles_Aggregate_Fields>;\n  nodes: Array<Particles>;\n};\n\nexport type Particles_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Particles_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Particles_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Particles_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Particles_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"particles\" */\nexport type Particles_Aggregate_Fields = {\n  avg?: Maybe<Particles_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Particles_Max_Fields>;\n  min?: Maybe<Particles_Min_Fields>;\n  stddev?: Maybe<Particles_Stddev_Fields>;\n  stddev_pop?: Maybe<Particles_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Particles_Stddev_Samp_Fields>;\n  sum?: Maybe<Particles_Sum_Fields>;\n  var_pop?: Maybe<Particles_Var_Pop_Fields>;\n  var_samp?: Maybe<Particles_Var_Samp_Fields>;\n  variance?: Maybe<Particles_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"particles\" */\nexport type Particles_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Particles_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"particles\" */\nexport type Particles_Aggregate_Order_By = {\n  avg?: InputMaybe<Particles_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Particles_Max_Order_By>;\n  min?: InputMaybe<Particles_Min_Order_By>;\n  stddev?: InputMaybe<Particles_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Particles_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Particles_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Particles_Sum_Order_By>;\n  var_pop?: InputMaybe<Particles_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Particles_Var_Samp_Order_By>;\n  variance?: InputMaybe<Particles_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Particles_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"particles\" */\nexport type Particles_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"particles\". All fields are combined with a logical 'AND'. */\nexport type Particles_Bool_Exp = {\n  _and?: InputMaybe<Array<Particles_Bool_Exp>>;\n  _not?: InputMaybe<Particles_Bool_Exp>;\n  _or?: InputMaybe<Array<Particles_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n  particle?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Particles_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  particle?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"particles\" */\nexport type Particles_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Particles_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  particle?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"particles\" */\nexport type Particles_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"particles\". */\nexport type Particles_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  particle?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"particles\" */\nexport enum Particles_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  Neuron = 'neuron',\n  /** column name */\n  Particle = 'particle',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TransactionHash = 'transaction_hash'\n}\n\n/** aggregate stddev on columns */\nexport type Particles_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"particles\" */\nexport type Particles_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Particles_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"particles\" */\nexport type Particles_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Particles_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"particles\" */\nexport type Particles_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"particles\" */\nexport type Particles_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Particles_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Particles_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  id?: InputMaybe<Scalars['Int']['input']>;\n  neuron?: InputMaybe<Scalars['String']['input']>;\n  particle?: InputMaybe<Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Particles_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** order by sum() on columns of table \"particles\" */\nexport type Particles_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Particles_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"particles\" */\nexport type Particles_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Particles_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"particles\" */\nexport type Particles_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Particles_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"particles\" */\nexport type Particles_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"pools\" */\nexport type Pools = {\n  a_denom: Scalars['String']['output'];\n  address: Scalars['String']['output'];\n  b_denom: Scalars['String']['output'];\n  pool_denom: Scalars['String']['output'];\n  pool_id: Scalars['bigint']['output'];\n  pool_name: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"pools\" */\nexport type Pools_Aggregate = {\n  aggregate?: Maybe<Pools_Aggregate_Fields>;\n  nodes: Array<Pools>;\n};\n\n/** aggregate fields of \"pools\" */\nexport type Pools_Aggregate_Fields = {\n  avg?: Maybe<Pools_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pools_Max_Fields>;\n  min?: Maybe<Pools_Min_Fields>;\n  stddev?: Maybe<Pools_Stddev_Fields>;\n  stddev_pop?: Maybe<Pools_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pools_Stddev_Samp_Fields>;\n  sum?: Maybe<Pools_Sum_Fields>;\n  var_pop?: Maybe<Pools_Var_Pop_Fields>;\n  var_samp?: Maybe<Pools_Var_Samp_Fields>;\n  variance?: Maybe<Pools_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pools\" */\nexport type Pools_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pools_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pools_Avg_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pools\". All fields are combined with a logical 'AND'. */\nexport type Pools_Bool_Exp = {\n  _and?: InputMaybe<Array<Pools_Bool_Exp>>;\n  _not?: InputMaybe<Pools_Bool_Exp>;\n  _or?: InputMaybe<Array<Pools_Bool_Exp>>;\n  a_denom?: InputMaybe<String_Comparison_Exp>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  b_denom?: InputMaybe<String_Comparison_Exp>;\n  pool_denom?: InputMaybe<String_Comparison_Exp>;\n  pool_id?: InputMaybe<Bigint_Comparison_Exp>;\n  pool_name?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** columns and relationships of \"pools_liquidity\" */\nexport type Pools_Liquidity = {\n  liquidity_a: Scalars['bigint']['output'];\n  liquidity_b: Scalars['bigint']['output'];\n  pool_id: Scalars['bigint']['output'];\n  timestamp: Scalars['timestamp']['output'];\n};\n\n/** aggregated selection of \"pools_liquidity\" */\nexport type Pools_Liquidity_Aggregate = {\n  aggregate?: Maybe<Pools_Liquidity_Aggregate_Fields>;\n  nodes: Array<Pools_Liquidity>;\n};\n\n/** aggregate fields of \"pools_liquidity\" */\nexport type Pools_Liquidity_Aggregate_Fields = {\n  avg?: Maybe<Pools_Liquidity_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pools_Liquidity_Max_Fields>;\n  min?: Maybe<Pools_Liquidity_Min_Fields>;\n  stddev?: Maybe<Pools_Liquidity_Stddev_Fields>;\n  stddev_pop?: Maybe<Pools_Liquidity_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pools_Liquidity_Stddev_Samp_Fields>;\n  sum?: Maybe<Pools_Liquidity_Sum_Fields>;\n  var_pop?: Maybe<Pools_Liquidity_Var_Pop_Fields>;\n  var_samp?: Maybe<Pools_Liquidity_Var_Samp_Fields>;\n  variance?: Maybe<Pools_Liquidity_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pools_liquidity\" */\nexport type Pools_Liquidity_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pools_Liquidity_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pools_Liquidity_Avg_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pools_liquidity\". All fields are combined with a logical 'AND'. */\nexport type Pools_Liquidity_Bool_Exp = {\n  _and?: InputMaybe<Array<Pools_Liquidity_Bool_Exp>>;\n  _not?: InputMaybe<Pools_Liquidity_Bool_Exp>;\n  _or?: InputMaybe<Array<Pools_Liquidity_Bool_Exp>>;\n  liquidity_a?: InputMaybe<Bigint_Comparison_Exp>;\n  liquidity_b?: InputMaybe<Bigint_Comparison_Exp>;\n  pool_id?: InputMaybe<Bigint_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pools_Liquidity_Max_Fields = {\n  liquidity_a?: Maybe<Scalars['bigint']['output']>;\n  liquidity_b?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pools_Liquidity_Min_Fields = {\n  liquidity_a?: Maybe<Scalars['bigint']['output']>;\n  liquidity_b?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n};\n\n/** Ordering options when selecting data from \"pools_liquidity\". */\nexport type Pools_Liquidity_Order_By = {\n  liquidity_a?: InputMaybe<Order_By>;\n  liquidity_b?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pools_liquidity\" */\nexport enum Pools_Liquidity_Select_Column {\n  /** column name */\n  LiquidityA = 'liquidity_a',\n  /** column name */\n  LiquidityB = 'liquidity_b',\n  /** column name */\n  PoolId = 'pool_id',\n  /** column name */\n  Timestamp = 'timestamp'\n}\n\n/** aggregate stddev on columns */\nexport type Pools_Liquidity_Stddev_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pools_Liquidity_Stddev_Pop_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pools_Liquidity_Stddev_Samp_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"pools_liquidity\" */\nexport type Pools_Liquidity_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Pools_Liquidity_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Pools_Liquidity_Stream_Cursor_Value_Input = {\n  liquidity_a?: InputMaybe<Scalars['bigint']['input']>;\n  liquidity_b?: InputMaybe<Scalars['bigint']['input']>;\n  pool_id?: InputMaybe<Scalars['bigint']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Pools_Liquidity_Sum_Fields = {\n  liquidity_a?: Maybe<Scalars['bigint']['output']>;\n  liquidity_b?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pools_Liquidity_Var_Pop_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pools_Liquidity_Var_Samp_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pools_Liquidity_Variance_Fields = {\n  liquidity_a?: Maybe<Scalars['Float']['output']>;\n  liquidity_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate max on columns */\nexport type Pools_Max_Fields = {\n  a_denom?: Maybe<Scalars['String']['output']>;\n  address?: Maybe<Scalars['String']['output']>;\n  b_denom?: Maybe<Scalars['String']['output']>;\n  pool_denom?: Maybe<Scalars['String']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  pool_name?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pools_Min_Fields = {\n  a_denom?: Maybe<Scalars['String']['output']>;\n  address?: Maybe<Scalars['String']['output']>;\n  b_denom?: Maybe<Scalars['String']['output']>;\n  pool_denom?: Maybe<Scalars['String']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  pool_name?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"pools\". */\nexport type Pools_Order_By = {\n  a_denom?: InputMaybe<Order_By>;\n  address?: InputMaybe<Order_By>;\n  b_denom?: InputMaybe<Order_By>;\n  pool_denom?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n  pool_name?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"pools_rates\" */\nexport type Pools_Rates = {\n  pool_id: Scalars['bigint']['output'];\n  rate: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n};\n\n/** aggregated selection of \"pools_rates\" */\nexport type Pools_Rates_Aggregate = {\n  aggregate?: Maybe<Pools_Rates_Aggregate_Fields>;\n  nodes: Array<Pools_Rates>;\n};\n\n/** aggregate fields of \"pools_rates\" */\nexport type Pools_Rates_Aggregate_Fields = {\n  avg?: Maybe<Pools_Rates_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pools_Rates_Max_Fields>;\n  min?: Maybe<Pools_Rates_Min_Fields>;\n  stddev?: Maybe<Pools_Rates_Stddev_Fields>;\n  stddev_pop?: Maybe<Pools_Rates_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pools_Rates_Stddev_Samp_Fields>;\n  sum?: Maybe<Pools_Rates_Sum_Fields>;\n  var_pop?: Maybe<Pools_Rates_Var_Pop_Fields>;\n  var_samp?: Maybe<Pools_Rates_Var_Samp_Fields>;\n  variance?: Maybe<Pools_Rates_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pools_rates\" */\nexport type Pools_Rates_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pools_Rates_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pools_Rates_Avg_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pools_rates\". All fields are combined with a logical 'AND'. */\nexport type Pools_Rates_Bool_Exp = {\n  _and?: InputMaybe<Array<Pools_Rates_Bool_Exp>>;\n  _not?: InputMaybe<Pools_Rates_Bool_Exp>;\n  _or?: InputMaybe<Array<Pools_Rates_Bool_Exp>>;\n  pool_id?: InputMaybe<Bigint_Comparison_Exp>;\n  rate?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pools_Rates_Max_Fields = {\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  rate?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pools_Rates_Min_Fields = {\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  rate?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n};\n\n/** Ordering options when selecting data from \"pools_rates\". */\nexport type Pools_Rates_Order_By = {\n  pool_id?: InputMaybe<Order_By>;\n  rate?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pools_rates\" */\nexport enum Pools_Rates_Select_Column {\n  /** column name */\n  PoolId = 'pool_id',\n  /** column name */\n  Rate = 'rate',\n  /** column name */\n  Timestamp = 'timestamp'\n}\n\n/** aggregate stddev on columns */\nexport type Pools_Rates_Stddev_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pools_Rates_Stddev_Pop_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pools_Rates_Stddev_Samp_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"pools_rates\" */\nexport type Pools_Rates_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Pools_Rates_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Pools_Rates_Stream_Cursor_Value_Input = {\n  pool_id?: InputMaybe<Scalars['bigint']['input']>;\n  rate?: InputMaybe<Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Pools_Rates_Sum_Fields = {\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pools_Rates_Var_Pop_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pools_Rates_Var_Samp_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pools_Rates_Variance_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** select columns of table \"pools\" */\nexport enum Pools_Select_Column {\n  /** column name */\n  ADenom = 'a_denom',\n  /** column name */\n  Address = 'address',\n  /** column name */\n  BDenom = 'b_denom',\n  /** column name */\n  PoolDenom = 'pool_denom',\n  /** column name */\n  PoolId = 'pool_id',\n  /** column name */\n  PoolName = 'pool_name'\n}\n\n/** aggregate stddev on columns */\nexport type Pools_Stddev_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pools_Stddev_Pop_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pools_Stddev_Samp_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"pools\" */\nexport type Pools_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Pools_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Pools_Stream_Cursor_Value_Input = {\n  a_denom?: InputMaybe<Scalars['String']['input']>;\n  address?: InputMaybe<Scalars['String']['input']>;\n  b_denom?: InputMaybe<Scalars['String']['input']>;\n  pool_denom?: InputMaybe<Scalars['String']['input']>;\n  pool_id?: InputMaybe<Scalars['bigint']['input']>;\n  pool_name?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Pools_Sum_Fields = {\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pools_Var_Pop_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pools_Var_Samp_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pools_Variance_Fields = {\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"pools_volumes\" */\nexport type Pools_Volumes = {\n  fee_a: Scalars['bigint']['output'];\n  fee_b: Scalars['bigint']['output'];\n  pool_id: Scalars['bigint']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  volume_a: Scalars['bigint']['output'];\n  volume_b: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"pools_volumes\" */\nexport type Pools_Volumes_Aggregate = {\n  aggregate?: Maybe<Pools_Volumes_Aggregate_Fields>;\n  nodes: Array<Pools_Volumes>;\n};\n\n/** aggregate fields of \"pools_volumes\" */\nexport type Pools_Volumes_Aggregate_Fields = {\n  avg?: Maybe<Pools_Volumes_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pools_Volumes_Max_Fields>;\n  min?: Maybe<Pools_Volumes_Min_Fields>;\n  stddev?: Maybe<Pools_Volumes_Stddev_Fields>;\n  stddev_pop?: Maybe<Pools_Volumes_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pools_Volumes_Stddev_Samp_Fields>;\n  sum?: Maybe<Pools_Volumes_Sum_Fields>;\n  var_pop?: Maybe<Pools_Volumes_Var_Pop_Fields>;\n  var_samp?: Maybe<Pools_Volumes_Var_Samp_Fields>;\n  variance?: Maybe<Pools_Volumes_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pools_volumes\" */\nexport type Pools_Volumes_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pools_Volumes_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pools_Volumes_Avg_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pools_volumes\". All fields are combined with a logical 'AND'. */\nexport type Pools_Volumes_Bool_Exp = {\n  _and?: InputMaybe<Array<Pools_Volumes_Bool_Exp>>;\n  _not?: InputMaybe<Pools_Volumes_Bool_Exp>;\n  _or?: InputMaybe<Array<Pools_Volumes_Bool_Exp>>;\n  fee_a?: InputMaybe<Bigint_Comparison_Exp>;\n  fee_b?: InputMaybe<Bigint_Comparison_Exp>;\n  pool_id?: InputMaybe<Bigint_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  volume_a?: InputMaybe<Bigint_Comparison_Exp>;\n  volume_b?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pools_Volumes_Max_Fields = {\n  fee_a?: Maybe<Scalars['bigint']['output']>;\n  fee_b?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  volume_a?: Maybe<Scalars['bigint']['output']>;\n  volume_b?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pools_Volumes_Min_Fields = {\n  fee_a?: Maybe<Scalars['bigint']['output']>;\n  fee_b?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  volume_a?: Maybe<Scalars['bigint']['output']>;\n  volume_b?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"pools_volumes\". */\nexport type Pools_Volumes_Order_By = {\n  fee_a?: InputMaybe<Order_By>;\n  fee_b?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  volume_a?: InputMaybe<Order_By>;\n  volume_b?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pools_volumes\" */\nexport enum Pools_Volumes_Select_Column {\n  /** column name */\n  FeeA = 'fee_a',\n  /** column name */\n  FeeB = 'fee_b',\n  /** column name */\n  PoolId = 'pool_id',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  VolumeA = 'volume_a',\n  /** column name */\n  VolumeB = 'volume_b'\n}\n\n/** aggregate stddev on columns */\nexport type Pools_Volumes_Stddev_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pools_Volumes_Stddev_Pop_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pools_Volumes_Stddev_Samp_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"pools_volumes\" */\nexport type Pools_Volumes_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Pools_Volumes_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Pools_Volumes_Stream_Cursor_Value_Input = {\n  fee_a?: InputMaybe<Scalars['bigint']['input']>;\n  fee_b?: InputMaybe<Scalars['bigint']['input']>;\n  pool_id?: InputMaybe<Scalars['bigint']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  volume_a?: InputMaybe<Scalars['bigint']['input']>;\n  volume_b?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Pools_Volumes_Sum_Fields = {\n  fee_a?: Maybe<Scalars['bigint']['output']>;\n  fee_b?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  volume_a?: Maybe<Scalars['bigint']['output']>;\n  volume_b?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pools_Volumes_Var_Pop_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pools_Volumes_Var_Samp_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pools_Volumes_Variance_Fields = {\n  fee_a?: Maybe<Scalars['Float']['output']>;\n  fee_b?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n  volume_a?: Maybe<Scalars['Float']['output']>;\n  volume_b?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"pre_commit\" */\nexport type Pre_Commit = {\n  height: Scalars['bigint']['output'];\n  proposer_priority: Scalars['bigint']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  /** An object relationship */\n  validator: Validator;\n  validator_address: Scalars['String']['output'];\n  voting_power: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"pre_commit\" */\nexport type Pre_Commit_Aggregate = {\n  aggregate?: Maybe<Pre_Commit_Aggregate_Fields>;\n  nodes: Array<Pre_Commit>;\n};\n\nexport type Pre_Commit_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Pre_Commit_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Pre_Commit_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Pre_Commit_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"pre_commit\" */\nexport type Pre_Commit_Aggregate_Fields = {\n  avg?: Maybe<Pre_Commit_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pre_Commit_Max_Fields>;\n  min?: Maybe<Pre_Commit_Min_Fields>;\n  stddev?: Maybe<Pre_Commit_Stddev_Fields>;\n  stddev_pop?: Maybe<Pre_Commit_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pre_Commit_Stddev_Samp_Fields>;\n  sum?: Maybe<Pre_Commit_Sum_Fields>;\n  var_pop?: Maybe<Pre_Commit_Var_Pop_Fields>;\n  var_samp?: Maybe<Pre_Commit_Var_Samp_Fields>;\n  variance?: Maybe<Pre_Commit_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pre_commit\" */\nexport type Pre_Commit_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"pre_commit\" */\nexport type Pre_Commit_Aggregate_Order_By = {\n  avg?: InputMaybe<Pre_Commit_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Pre_Commit_Max_Order_By>;\n  min?: InputMaybe<Pre_Commit_Min_Order_By>;\n  stddev?: InputMaybe<Pre_Commit_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Pre_Commit_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Pre_Commit_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Pre_Commit_Sum_Order_By>;\n  var_pop?: InputMaybe<Pre_Commit_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Pre_Commit_Var_Samp_Order_By>;\n  variance?: InputMaybe<Pre_Commit_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Pre_Commit_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"pre_commit\". All fields are combined with a logical 'AND'. */\nexport type Pre_Commit_Bool_Exp = {\n  _and?: InputMaybe<Array<Pre_Commit_Bool_Exp>>;\n  _not?: InputMaybe<Pre_Commit_Bool_Exp>;\n  _or?: InputMaybe<Array<Pre_Commit_Bool_Exp>>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  proposer_priority?: InputMaybe<Bigint_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  validator?: InputMaybe<Validator_Bool_Exp>;\n  validator_address?: InputMaybe<String_Comparison_Exp>;\n  voting_power?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pre_Commit_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  proposer_priority?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n  voting_power?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by max() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Max_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  validator_address?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Pre_Commit_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  proposer_priority?: Maybe<Scalars['bigint']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  validator_address?: Maybe<Scalars['String']['output']>;\n  voting_power?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by min() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Min_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  validator_address?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"pre_commit\". */\nexport type Pre_Commit_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  validator?: InputMaybe<Validator_Order_By>;\n  validator_address?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pre_commit\" */\nexport enum Pre_Commit_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  ProposerPriority = 'proposer_priority',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  ValidatorAddress = 'validator_address',\n  /** column name */\n  VotingPower = 'voting_power'\n}\n\n/** aggregate stddev on columns */\nexport type Pre_Commit_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pre_Commit_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pre_Commit_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"pre_commit\" */\nexport type Pre_Commit_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Pre_Commit_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Pre_Commit_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  proposer_priority?: InputMaybe<Scalars['bigint']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  validator_address?: InputMaybe<Scalars['String']['input']>;\n  voting_power?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Pre_Commit_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  proposer_priority?: Maybe<Scalars['bigint']['output']>;\n  voting_power?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pre_Commit_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pre_Commit_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Pre_Commit_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  proposer_priority?: Maybe<Scalars['Float']['output']>;\n  voting_power?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"pre_commit\" */\nexport type Pre_Commit_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  proposer_priority?: InputMaybe<Order_By>;\n  voting_power?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"pruning\" */\nexport type Pruning = {\n  last_pruned_height: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"pruning\" */\nexport type Pruning_Aggregate = {\n  aggregate?: Maybe<Pruning_Aggregate_Fields>;\n  nodes: Array<Pruning>;\n};\n\n/** aggregate fields of \"pruning\" */\nexport type Pruning_Aggregate_Fields = {\n  avg?: Maybe<Pruning_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Pruning_Max_Fields>;\n  min?: Maybe<Pruning_Min_Fields>;\n  stddev?: Maybe<Pruning_Stddev_Fields>;\n  stddev_pop?: Maybe<Pruning_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Pruning_Stddev_Samp_Fields>;\n  sum?: Maybe<Pruning_Sum_Fields>;\n  var_pop?: Maybe<Pruning_Var_Pop_Fields>;\n  var_samp?: Maybe<Pruning_Var_Samp_Fields>;\n  variance?: Maybe<Pruning_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"pruning\" */\nexport type Pruning_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Pruning_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Pruning_Avg_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"pruning\". All fields are combined with a logical 'AND'. */\nexport type Pruning_Bool_Exp = {\n  _and?: InputMaybe<Array<Pruning_Bool_Exp>>;\n  _not?: InputMaybe<Pruning_Bool_Exp>;\n  _or?: InputMaybe<Array<Pruning_Bool_Exp>>;\n  last_pruned_height?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Pruning_Max_Fields = {\n  last_pruned_height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Pruning_Min_Fields = {\n  last_pruned_height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"pruning\". */\nexport type Pruning_Order_By = {\n  last_pruned_height?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"pruning\" */\nexport enum Pruning_Select_Column {\n  /** column name */\n  LastPrunedHeight = 'last_pruned_height'\n}\n\n/** aggregate stddev on columns */\nexport type Pruning_Stddev_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Pruning_Stddev_Pop_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Pruning_Stddev_Samp_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"pruning\" */\nexport type Pruning_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Pruning_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Pruning_Stream_Cursor_Value_Input = {\n  last_pruned_height?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Pruning_Sum_Fields = {\n  last_pruned_height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Pruning_Var_Pop_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Pruning_Var_Samp_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Pruning_Variance_Fields = {\n  last_pruned_height?: Maybe<Scalars['Float']['output']>;\n};\n\nexport type Query_Root = {\n  /** fetch data from the table: \"_transaction\" */\n  _transaction: Array<_Transaction>;\n  /** fetch aggregated fields from the table: \"_transaction\" */\n  _transaction_aggregate: _Transaction_Aggregate;\n  /** fetch data from the table: \"_uptime_temp\" */\n  _uptime_temp: Array<_Uptime_Temp>;\n  /** fetch aggregated fields from the table: \"_uptime_temp\" */\n  _uptime_temp_aggregate: _Uptime_Temp_Aggregate;\n  /** fetch data from the table: \"account\" */\n  account: Array<Account>;\n  /** fetch aggregated fields from the table: \"account\" */\n  account_aggregate: Account_Aggregate;\n  /** fetch data from the table: \"account_balance\" */\n  account_balance: Array<Account_Balance>;\n  /** fetch aggregated fields from the table: \"account_balance\" */\n  account_balance_aggregate: Account_Balance_Aggregate;\n  /** fetch data from the table: \"account_balance\" using primary key columns */\n  account_balance_by_pk?: Maybe<Account_Balance>;\n  /** fetch data from the table: \"account\" using primary key columns */\n  account_by_pk?: Maybe<Account>;\n  /** fetch data from the table: \"average_block_time_from_genesis\" */\n  average_block_time_from_genesis: Array<Average_Block_Time_From_Genesis>;\n  /** fetch aggregated fields from the table: \"average_block_time_from_genesis\" */\n  average_block_time_from_genesis_aggregate: Average_Block_Time_From_Genesis_Aggregate;\n  /** fetch data from the table: \"average_block_time_from_genesis\" using primary key columns */\n  average_block_time_from_genesis_by_pk?: Maybe<Average_Block_Time_From_Genesis>;\n  /** fetch data from the table: \"average_block_time_per_day\" */\n  average_block_time_per_day: Array<Average_Block_Time_Per_Day>;\n  /** fetch aggregated fields from the table: \"average_block_time_per_day\" */\n  average_block_time_per_day_aggregate: Average_Block_Time_Per_Day_Aggregate;\n  /** fetch data from the table: \"average_block_time_per_day\" using primary key columns */\n  average_block_time_per_day_by_pk?: Maybe<Average_Block_Time_Per_Day>;\n  /** fetch data from the table: \"average_block_time_per_hour\" */\n  average_block_time_per_hour: Array<Average_Block_Time_Per_Hour>;\n  /** fetch aggregated fields from the table: \"average_block_time_per_hour\" */\n  average_block_time_per_hour_aggregate: Average_Block_Time_Per_Hour_Aggregate;\n  /** fetch data from the table: \"average_block_time_per_hour\" using primary key columns */\n  average_block_time_per_hour_by_pk?: Maybe<Average_Block_Time_Per_Hour>;\n  /** fetch data from the table: \"average_block_time_per_minute\" */\n  average_block_time_per_minute: Array<Average_Block_Time_Per_Minute>;\n  /** fetch aggregated fields from the table: \"average_block_time_per_minute\" */\n  average_block_time_per_minute_aggregate: Average_Block_Time_Per_Minute_Aggregate;\n  /** fetch data from the table: \"average_block_time_per_minute\" using primary key columns */\n  average_block_time_per_minute_by_pk?: Maybe<Average_Block_Time_Per_Minute>;\n  /** fetch data from the table: \"block\" */\n  block: Array<Block>;\n  /** fetch aggregated fields from the table: \"block\" */\n  block_aggregate: Block_Aggregate;\n  /** fetch data from the table: \"block\" using primary key columns */\n  block_by_pk?: Maybe<Block>;\n  /** fetch data from the table: \"contracts\" */\n  contracts: Array<Contracts>;\n  /** fetch aggregated fields from the table: \"contracts\" */\n  contracts_aggregate: Contracts_Aggregate;\n  /** fetch data from the table: \"contracts\" using primary key columns */\n  contracts_by_pk?: Maybe<Contracts>;\n  /** fetch data from the table: \"cyb_cohort\" */\n  cyb_cohort: Array<Cyb_Cohort>;\n  /** fetch aggregated fields from the table: \"cyb_cohort\" */\n  cyb_cohort_aggregate: Cyb_Cohort_Aggregate;\n  /** fetch data from the table: \"cyber_gift_proofs\" */\n  cyber_gift_proofs: Array<Cyber_Gift_Proofs>;\n  /** fetch aggregated fields from the table: \"cyber_gift_proofs\" */\n  cyber_gift_proofs_aggregate: Cyber_Gift_Proofs_Aggregate;\n  /** An array relationship */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  /** fetch data from the table: \"cyberlinks\" using primary key columns */\n  cyberlinks_by_pk?: Maybe<Cyberlinks>;\n  /** fetch data from the table: \"cyberlinks_stats\" */\n  cyberlinks_stats: Array<Cyberlinks_Stats>;\n  /** fetch aggregated fields from the table: \"cyberlinks_stats\" */\n  cyberlinks_stats_aggregate: Cyberlinks_Stats_Aggregate;\n  /** fetch data from the table: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons: Array<Daily_Amount_Of_Active_Neurons>;\n  /** fetch aggregated fields from the table: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons_aggregate: Daily_Amount_Of_Active_Neurons_Aggregate;\n  /** fetch data from the table: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas: Array<Daily_Amount_Of_Used_Gas>;\n  /** fetch aggregated fields from the table: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas_aggregate: Daily_Amount_Of_Used_Gas_Aggregate;\n  /** fetch data from the table: \"daily_number_of_transactions\" */\n  daily_number_of_transactions: Array<Daily_Number_Of_Transactions>;\n  /** fetch aggregated fields from the table: \"daily_number_of_transactions\" */\n  daily_number_of_transactions_aggregate: Daily_Number_Of_Transactions_Aggregate;\n  /** fetch data from the table: \"first_10_cyberlink\" */\n  first_10_cyberlink: Array<First_10_Cyberlink>;\n  /** fetch aggregated fields from the table: \"first_10_cyberlink\" */\n  first_10_cyberlink_aggregate: First_10_Cyberlink_Aggregate;\n  /** fetch data from the table: \"first_100_cyberlink\" */\n  first_100_cyberlink: Array<First_100_Cyberlink>;\n  /** fetch aggregated fields from the table: \"first_100_cyberlink\" */\n  first_100_cyberlink_aggregate: First_100_Cyberlink_Aggregate;\n  /** fetch data from the table: \"first_cyberlink\" */\n  first_cyberlink: Array<First_Cyberlink>;\n  /** fetch aggregated fields from the table: \"first_cyberlink\" */\n  first_cyberlink_aggregate: First_Cyberlink_Aggregate;\n  /** fetch data from the table: \"first_hero_hired\" */\n  first_hero_hired: Array<First_Hero_Hired>;\n  /** fetch aggregated fields from the table: \"first_hero_hired\" */\n  first_hero_hired_aggregate: First_Hero_Hired_Aggregate;\n  /** fetch data from the table: \"first_investmint\" */\n  first_investmint: Array<First_Investmint>;\n  /** fetch aggregated fields from the table: \"first_investmint\" */\n  first_investmint_aggregate: First_Investmint_Aggregate;\n  /** fetch data from the table: \"first_neuron_activation\" */\n  first_neuron_activation: Array<First_Neuron_Activation>;\n  /** fetch aggregated fields from the table: \"first_neuron_activation\" */\n  first_neuron_activation_aggregate: First_Neuron_Activation_Aggregate;\n  /** fetch data from the table: \"first_swap\" */\n  first_swap: Array<First_Swap>;\n  /** fetch aggregated fields from the table: \"first_swap\" */\n  first_swap_aggregate: First_Swap_Aggregate;\n  /** fetch data from the table: \"follow_stats\" */\n  follow_stats: Array<Follow_Stats>;\n  /** fetch aggregated fields from the table: \"follow_stats\" */\n  follow_stats_aggregate: Follow_Stats_Aggregate;\n  /** fetch data from the table: \"genesis\" */\n  genesis: Array<Genesis>;\n  /** fetch data from the table: \"genesis_accounts\" */\n  genesis_accounts: Array<Genesis_Accounts>;\n  /** fetch aggregated fields from the table: \"genesis_accounts\" */\n  genesis_accounts_aggregate: Genesis_Accounts_Aggregate;\n  /** fetch data from the table: \"genesis_accounts\" using primary key columns */\n  genesis_accounts_by_pk?: Maybe<Genesis_Accounts>;\n  /** fetch aggregated fields from the table: \"genesis\" */\n  genesis_aggregate: Genesis_Aggregate;\n  /** fetch data from the table: \"genesis\" using primary key columns */\n  genesis_by_pk?: Maybe<Genesis>;\n  /** fetch data from the table: \"genesis_neurons_activation\" */\n  genesis_neurons_activation: Array<Genesis_Neurons_Activation>;\n  /** fetch aggregated fields from the table: \"genesis_neurons_activation\" */\n  genesis_neurons_activation_aggregate: Genesis_Neurons_Activation_Aggregate;\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  /** fetch data from the table: \"investmints\" using primary key columns */\n  investmints_by_pk?: Maybe<Investmints>;\n  /** fetch data from the table: \"message\" */\n  message: Array<Message>;\n  /** fetch data from the table: \"message_155\" */\n  message_155: Array<Message_155>;\n  /** fetch aggregated fields from the table: \"message_155\" */\n  message_155_aggregate: Message_155_Aggregate;\n  /** fetch aggregated fields from the table: \"message\" */\n  message_aggregate: Message_Aggregate;\n  /** fetch data from the table: \"message_type\" */\n  message_type: Array<Message_Type>;\n  /** fetch aggregated fields from the table: \"message_type\" */\n  message_type_aggregate: Message_Type_Aggregate;\n  /** execute function \"messages_by_address\" which returns \"message\" */\n  messages_by_address: Array<Message>;\n  /** execute function \"messages_by_address\" and query aggregates on result of table type \"message\" */\n  messages_by_address_aggregate: Message_Aggregate;\n  /** execute function \"messages_by_type\" which returns \"message\" */\n  messages_by_type: Array<Message>;\n  /** execute function \"messages_by_type\" and query aggregates on result of table type \"message\" */\n  messages_by_type_aggregate: Message_Aggregate;\n  /** fetch data from the table: \"modules\" */\n  modules: Array<Modules>;\n  /** fetch aggregated fields from the table: \"modules\" */\n  modules_aggregate: Modules_Aggregate;\n  /** fetch data from the table: \"modules\" using primary key columns */\n  modules_by_pk?: Maybe<Modules>;\n  /** fetch data from the table: \"neuron_activation_source\" */\n  neuron_activation_source: Array<Neuron_Activation_Source>;\n  /** fetch aggregated fields from the table: \"neuron_activation_source\" */\n  neuron_activation_source_aggregate: Neuron_Activation_Source_Aggregate;\n  /** fetch data from the table: \"number_of_new_neurons\" */\n  number_of_new_neurons: Array<Number_Of_New_Neurons>;\n  /** fetch aggregated fields from the table: \"number_of_new_neurons\" */\n  number_of_new_neurons_aggregate: Number_Of_New_Neurons_Aggregate;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  /** fetch data from the table: \"particles\" using primary key columns */\n  particles_by_pk?: Maybe<Particles>;\n  /** fetch data from the table: \"pools\" */\n  pools: Array<Pools>;\n  /** fetch aggregated fields from the table: \"pools\" */\n  pools_aggregate: Pools_Aggregate;\n  /** fetch data from the table: \"pools\" using primary key columns */\n  pools_by_pk?: Maybe<Pools>;\n  /** fetch data from the table: \"pools_liquidity\" */\n  pools_liquidity: Array<Pools_Liquidity>;\n  /** fetch aggregated fields from the table: \"pools_liquidity\" */\n  pools_liquidity_aggregate: Pools_Liquidity_Aggregate;\n  /** fetch data from the table: \"pools_rates\" */\n  pools_rates: Array<Pools_Rates>;\n  /** fetch aggregated fields from the table: \"pools_rates\" */\n  pools_rates_aggregate: Pools_Rates_Aggregate;\n  /** fetch data from the table: \"pools_volumes\" */\n  pools_volumes: Array<Pools_Volumes>;\n  /** fetch aggregated fields from the table: \"pools_volumes\" */\n  pools_volumes_aggregate: Pools_Volumes_Aggregate;\n  /** fetch data from the table: \"pre_commit\" */\n  pre_commit: Array<Pre_Commit>;\n  /** fetch aggregated fields from the table: \"pre_commit\" */\n  pre_commit_aggregate: Pre_Commit_Aggregate;\n  /** fetch data from the table: \"pruning\" */\n  pruning: Array<Pruning>;\n  /** fetch aggregated fields from the table: \"pruning\" */\n  pruning_aggregate: Pruning_Aggregate;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n  /** fetch data from the table: \"routes\" using primary key columns */\n  routes_by_pk?: Maybe<Routes>;\n  /** fetch data from the table: \"supply\" */\n  supply: Array<Supply>;\n  /** fetch aggregated fields from the table: \"supply\" */\n  supply_aggregate: Supply_Aggregate;\n  /** fetch data from the table: \"supply\" using primary key columns */\n  supply_by_pk?: Maybe<Supply>;\n  /** An array relationship */\n  swaps: Array<Swaps>;\n  /** An aggregate relationship */\n  swaps_aggregate: Swaps_Aggregate;\n  /** fetch data from the table: \"swaps\" using primary key columns */\n  swaps_by_pk?: Maybe<Swaps>;\n  /** fetch data from the table: \"today_top_txs\" */\n  today_top_txs: Array<Today_Top_Txs>;\n  /** fetch aggregated fields from the table: \"today_top_txs\" */\n  today_top_txs_aggregate: Today_Top_Txs_Aggregate;\n  /** fetch data from the table: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week: Array<Top_10_Of_Active_Neurons_Week>;\n  /** fetch aggregated fields from the table: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week_aggregate: Top_10_Of_Active_Neurons_Week_Aggregate;\n  /** fetch data from the table: \"top_first_txs\" */\n  top_first_txs: Array<Top_First_Txs>;\n  /** fetch aggregated fields from the table: \"top_first_txs\" */\n  top_first_txs_aggregate: Top_First_Txs_Aggregate;\n  /** fetch data from the table: \"top_leaders\" */\n  top_leaders: Array<Top_Leaders>;\n  /** fetch aggregated fields from the table: \"top_leaders\" */\n  top_leaders_aggregate: Top_Leaders_Aggregate;\n  /** fetch data from the table: \"top_txs\" */\n  top_txs: Array<Top_Txs>;\n  /** fetch aggregated fields from the table: \"top_txs\" */\n  top_txs_aggregate: Top_Txs_Aggregate;\n  /** fetch data from the table: \"transaction\" */\n  transaction: Array<Transaction>;\n  /** fetch data from the table: \"transaction_155\" */\n  transaction_155: Array<Transaction_155>;\n  /** fetch aggregated fields from the table: \"transaction_155\" */\n  transaction_155_aggregate: Transaction_155_Aggregate;\n  /** fetch aggregated fields from the table: \"transaction\" */\n  transaction_aggregate: Transaction_Aggregate;\n  /** fetch data from the table: \"tweets_stats\" */\n  tweets_stats: Array<Tweets_Stats>;\n  /** fetch aggregated fields from the table: \"tweets_stats\" */\n  tweets_stats_aggregate: Tweets_Stats_Aggregate;\n  /** fetch data from the table: \"txs_ranked\" */\n  txs_ranked: Array<Txs_Ranked>;\n  /** fetch aggregated fields from the table: \"txs_ranked\" */\n  txs_ranked_aggregate: Txs_Ranked_Aggregate;\n  /** fetch data from the table: \"uptime\" */\n  uptime: Array<Uptime>;\n  /** fetch aggregated fields from the table: \"uptime\" */\n  uptime_aggregate: Uptime_Aggregate;\n  /** fetch data from the table: \"validator\" */\n  validator: Array<Validator>;\n  /** fetch aggregated fields from the table: \"validator\" */\n  validator_aggregate: Validator_Aggregate;\n  /** fetch data from the table: \"validator\" using primary key columns */\n  validator_by_pk?: Maybe<Validator>;\n  /** fetch data from the table: \"vesting_account\" */\n  vesting_account: Array<Vesting_Account>;\n  /** fetch aggregated fields from the table: \"vesting_account\" */\n  vesting_account_aggregate: Vesting_Account_Aggregate;\n  /** fetch data from the table: \"vesting_account\" using primary key columns */\n  vesting_account_by_pk?: Maybe<Vesting_Account>;\n  /** fetch data from the table: \"vesting_period\" */\n  vesting_period: Array<Vesting_Period>;\n  /** fetch aggregated fields from the table: \"vesting_period\" */\n  vesting_period_aggregate: Vesting_Period_Aggregate;\n  /** fetch data from the table: \"week_redelegation\" */\n  week_redelegation: Array<Week_Redelegation>;\n  /** fetch aggregated fields from the table: \"week_redelegation\" */\n  week_redelegation_aggregate: Week_Redelegation_Aggregate;\n  /** fetch data from the table: \"week_undelegation\" */\n  week_undelegation: Array<Week_Undelegation>;\n  /** fetch aggregated fields from the table: \"week_undelegation\" */\n  week_undelegation_aggregate: Week_Undelegation_Aggregate;\n};\n\n\nexport type Query_Root_TransactionArgs = {\n  distinct_on?: InputMaybe<Array<_Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Transaction_Order_By>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Query_Root_Transaction_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<_Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Transaction_Order_By>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Query_Root_Uptime_TempArgs = {\n  distinct_on?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Uptime_Temp_Order_By>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Query_Root_Uptime_Temp_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Uptime_Temp_Order_By>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Query_RootAccountArgs = {\n  distinct_on?: InputMaybe<Array<Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Order_By>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Query_RootAccount_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Order_By>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Query_RootAccount_BalanceArgs = {\n  distinct_on?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Balance_Order_By>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Query_RootAccount_Balance_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Balance_Order_By>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Query_RootAccount_Balance_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootAccount_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootAverage_Block_Time_From_GenesisArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_From_Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_From_Genesis_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_From_Genesis_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_From_Genesis_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_From_Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_From_Genesis_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_From_Genesis_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_From_Genesis_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_DayArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Day_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Day_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Day_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_Day_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Day_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Day_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Day_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_Day_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_HourArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Hour_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Hour_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Hour_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_Hour_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Hour_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Hour_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Hour_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_Hour_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_MinuteArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Minute_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Minute_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Minute_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_Minute_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Minute_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Minute_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Minute_Bool_Exp>;\n};\n\n\nexport type Query_RootAverage_Block_Time_Per_Minute_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootBlockArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Query_RootBlock_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Query_RootBlock_By_PkArgs = {\n  height: Scalars['bigint']['input'];\n};\n\n\nexport type Query_RootContractsArgs = {\n  distinct_on?: InputMaybe<Array<Contracts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Contracts_Order_By>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Query_RootContracts_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Contracts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Contracts_Order_By>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Query_RootContracts_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootCyb_CohortArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Query_RootCyb_Cohort_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Query_RootCyber_Gift_ProofsArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Query_RootCyber_Gift_Proofs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Query_RootCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Query_RootCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Query_RootCyberlinks_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootCyberlinks_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Stats_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootCyberlinks_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Stats_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Amount_Of_Active_NeuronsArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Amount_Of_Active_Neurons_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Amount_Of_Used_GasArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Amount_Of_Used_Gas_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Number_Of_TransactionsArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Number_Of_Transactions_Order_By>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Query_RootDaily_Number_Of_Transactions_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Number_Of_Transactions_Order_By>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_10_CyberlinkArgs = {\n  distinct_on?: InputMaybe<Array<First_10_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_10_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_10_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_10_Cyberlink_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_10_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_10_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_10_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_100_CyberlinkArgs = {\n  distinct_on?: InputMaybe<Array<First_100_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_100_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_100_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_100_Cyberlink_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_100_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_100_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_100_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_CyberlinkArgs = {\n  distinct_on?: InputMaybe<Array<First_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Cyberlink_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Hero_HiredArgs = {\n  distinct_on?: InputMaybe<Array<First_Hero_Hired_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Hero_Hired_Order_By>>;\n  where?: InputMaybe<First_Hero_Hired_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Hero_Hired_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Hero_Hired_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Hero_Hired_Order_By>>;\n  where?: InputMaybe<First_Hero_Hired_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_InvestmintArgs = {\n  distinct_on?: InputMaybe<Array<First_Investmint_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Investmint_Order_By>>;\n  where?: InputMaybe<First_Investmint_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Investmint_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Investmint_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Investmint_Order_By>>;\n  where?: InputMaybe<First_Investmint_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Neuron_ActivationArgs = {\n  distinct_on?: InputMaybe<Array<First_Neuron_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Neuron_Activation_Order_By>>;\n  where?: InputMaybe<First_Neuron_Activation_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Neuron_Activation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Neuron_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Neuron_Activation_Order_By>>;\n  where?: InputMaybe<First_Neuron_Activation_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_SwapArgs = {\n  distinct_on?: InputMaybe<Array<First_Swap_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Swap_Order_By>>;\n  where?: InputMaybe<First_Swap_Bool_Exp>;\n};\n\n\nexport type Query_RootFirst_Swap_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Swap_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Swap_Order_By>>;\n  where?: InputMaybe<First_Swap_Bool_Exp>;\n};\n\n\nexport type Query_RootFollow_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Follow_Stats_Order_By>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootFollow_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Follow_Stats_Order_By>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesisArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Order_By>>;\n  where?: InputMaybe<Genesis_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesis_AccountsArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Accounts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Accounts_Order_By>>;\n  where?: InputMaybe<Genesis_Accounts_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesis_Accounts_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Accounts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Accounts_Order_By>>;\n  where?: InputMaybe<Genesis_Accounts_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesis_Accounts_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootGenesis_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Order_By>>;\n  where?: InputMaybe<Genesis_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesis_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootGenesis_Neurons_ActivationArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Neurons_Activation_Order_By>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Query_RootGenesis_Neurons_Activation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Neurons_Activation_Order_By>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Query_RootInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Query_RootInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Query_RootInvestmints_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootMessageArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessage_155Args = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\nexport type Query_RootMessage_155_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\nexport type Query_RootMessage_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessage_TypeArgs = {\n  distinct_on?: InputMaybe<Array<Message_Type_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Type_Order_By>>;\n  where?: InputMaybe<Message_Type_Bool_Exp>;\n};\n\n\nexport type Query_RootMessage_Type_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Type_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Type_Order_By>>;\n  where?: InputMaybe<Message_Type_Bool_Exp>;\n};\n\n\nexport type Query_RootMessages_By_AddressArgs = {\n  args: Messages_By_Address_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessages_By_Address_AggregateArgs = {\n  args: Messages_By_Address_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessages_By_TypeArgs = {\n  args: Messages_By_Type_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootMessages_By_Type_AggregateArgs = {\n  args: Messages_By_Type_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Query_RootModulesArgs = {\n  distinct_on?: InputMaybe<Array<Modules_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Modules_Order_By>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Query_RootModules_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Modules_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Modules_Order_By>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Query_RootModules_By_PkArgs = {\n  module_name: Scalars['String']['input'];\n};\n\n\nexport type Query_RootNeuron_Activation_SourceArgs = {\n  distinct_on?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Neuron_Activation_Source_Order_By>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Query_RootNeuron_Activation_Source_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Neuron_Activation_Source_Order_By>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Query_RootNumber_Of_New_NeuronsArgs = {\n  distinct_on?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Number_Of_New_Neurons_Order_By>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Query_RootNumber_Of_New_Neurons_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Number_Of_New_Neurons_Order_By>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Query_RootParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Query_RootParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Query_RootParticles_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootPoolsArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Order_By>>;\n  where?: InputMaybe<Pools_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Order_By>>;\n  where?: InputMaybe<Pools_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_By_PkArgs = {\n  pool_id: Scalars['bigint']['input'];\n};\n\n\nexport type Query_RootPools_LiquidityArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Liquidity_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Liquidity_Order_By>>;\n  where?: InputMaybe<Pools_Liquidity_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_Liquidity_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Liquidity_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Liquidity_Order_By>>;\n  where?: InputMaybe<Pools_Liquidity_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_RatesArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Rates_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Rates_Order_By>>;\n  where?: InputMaybe<Pools_Rates_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_Rates_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Rates_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Rates_Order_By>>;\n  where?: InputMaybe<Pools_Rates_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_VolumesArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Volumes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Volumes_Order_By>>;\n  where?: InputMaybe<Pools_Volumes_Bool_Exp>;\n};\n\n\nexport type Query_RootPools_Volumes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Volumes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Volumes_Order_By>>;\n  where?: InputMaybe<Pools_Volumes_Bool_Exp>;\n};\n\n\nexport type Query_RootPre_CommitArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Query_RootPre_Commit_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Query_RootPruningArgs = {\n  distinct_on?: InputMaybe<Array<Pruning_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pruning_Order_By>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Query_RootPruning_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pruning_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pruning_Order_By>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Query_RootRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Query_RootRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Query_RootRoutes_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootSupplyArgs = {\n  distinct_on?: InputMaybe<Array<Supply_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Supply_Order_By>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Query_RootSupply_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Supply_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Supply_Order_By>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Query_RootSupply_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Query_RootSwapsArgs = {\n  distinct_on?: InputMaybe<Array<Swaps_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Swaps_Order_By>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\nexport type Query_RootSwaps_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Swaps_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Swaps_Order_By>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\nexport type Query_RootSwaps_By_PkArgs = {\n  pool_id: Scalars['bigint']['input'];\n};\n\n\nexport type Query_RootToday_Top_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Today_Top_Txs_Order_By>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootToday_Top_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Today_Top_Txs_Order_By>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_10_Of_Active_Neurons_WeekArgs = {\n  distinct_on?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Order_By>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_10_Of_Active_Neurons_Week_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Order_By>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_First_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_First_Txs_Order_By>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_First_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_First_Txs_Order_By>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_LeadersArgs = {\n  distinct_on?: InputMaybe<Array<Top_Leaders_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Leaders_Order_By>>;\n  where?: InputMaybe<Top_Leaders_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_Leaders_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_Leaders_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Leaders_Order_By>>;\n  where?: InputMaybe<Top_Leaders_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Txs_Order_By>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTop_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Txs_Order_By>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Query_RootTransactionArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Query_RootTransaction_155Args = {\n  distinct_on?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_155_Order_By>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\nexport type Query_RootTransaction_155_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_155_Order_By>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\nexport type Query_RootTransaction_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Query_RootTweets_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Tweets_Stats_Order_By>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootTweets_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Tweets_Stats_Order_By>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Query_RootTxs_RankedArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Ranked_Order_By>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Query_RootTxs_Ranked_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Ranked_Order_By>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Query_RootUptimeArgs = {\n  distinct_on?: InputMaybe<Array<Uptime_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Uptime_Order_By>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Query_RootUptime_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Uptime_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Uptime_Order_By>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Query_RootValidatorArgs = {\n  distinct_on?: InputMaybe<Array<Validator_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Validator_Order_By>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Query_RootValidator_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Validator_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Validator_Order_By>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Query_RootValidator_By_PkArgs = {\n  consensus_address: Scalars['String']['input'];\n};\n\n\nexport type Query_RootVesting_AccountArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Account_Order_By>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n\nexport type Query_RootVesting_Account_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Account_Order_By>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n\nexport type Query_RootVesting_Account_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Query_RootVesting_PeriodArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Period_Order_By>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n\nexport type Query_RootVesting_Period_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Period_Order_By>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n\nexport type Query_RootWeek_RedelegationArgs = {\n  distinct_on?: InputMaybe<Array<Week_Redelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Redelegation_Order_By>>;\n  where?: InputMaybe<Week_Redelegation_Bool_Exp>;\n};\n\n\nexport type Query_RootWeek_Redelegation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Week_Redelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Redelegation_Order_By>>;\n  where?: InputMaybe<Week_Redelegation_Bool_Exp>;\n};\n\n\nexport type Query_RootWeek_UndelegationArgs = {\n  distinct_on?: InputMaybe<Array<Week_Undelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Undelegation_Order_By>>;\n  where?: InputMaybe<Week_Undelegation_Bool_Exp>;\n};\n\n\nexport type Query_RootWeek_Undelegation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Week_Undelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Undelegation_Order_By>>;\n  where?: InputMaybe<Week_Undelegation_Bool_Exp>;\n};\n\n/** columns and relationships of \"routes\" */\nexport type Routes = {\n  /** An object relationship */\n  account: Account;\n  /** An object relationship */\n  accountBySource: Account;\n  alias: Scalars['String']['output'];\n  /** An object relationship */\n  block: Block;\n  destination: Scalars['String']['output'];\n  height: Scalars['bigint']['output'];\n  id: Scalars['Int']['output'];\n  source: Scalars['String']['output'];\n  timestamp: Scalars['timestamp']['output'];\n  transaction_hash: Scalars['String']['output'];\n  value: Array<Scalars['coin']['output']>;\n};\n\n/** aggregated selection of \"routes\" */\nexport type Routes_Aggregate = {\n  aggregate?: Maybe<Routes_Aggregate_Fields>;\n  nodes: Array<Routes>;\n};\n\nexport type Routes_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Routes_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Routes_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Routes_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Routes_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"routes\" */\nexport type Routes_Aggregate_Fields = {\n  avg?: Maybe<Routes_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Routes_Max_Fields>;\n  min?: Maybe<Routes_Min_Fields>;\n  stddev?: Maybe<Routes_Stddev_Fields>;\n  stddev_pop?: Maybe<Routes_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Routes_Stddev_Samp_Fields>;\n  sum?: Maybe<Routes_Sum_Fields>;\n  var_pop?: Maybe<Routes_Var_Pop_Fields>;\n  var_samp?: Maybe<Routes_Var_Samp_Fields>;\n  variance?: Maybe<Routes_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"routes\" */\nexport type Routes_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Routes_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"routes\" */\nexport type Routes_Aggregate_Order_By = {\n  avg?: InputMaybe<Routes_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Routes_Max_Order_By>;\n  min?: InputMaybe<Routes_Min_Order_By>;\n  stddev?: InputMaybe<Routes_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Routes_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Routes_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Routes_Sum_Order_By>;\n  var_pop?: InputMaybe<Routes_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Routes_Var_Samp_Order_By>;\n  variance?: InputMaybe<Routes_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Routes_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"routes\" */\nexport type Routes_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"routes\". All fields are combined with a logical 'AND'. */\nexport type Routes_Bool_Exp = {\n  _and?: InputMaybe<Array<Routes_Bool_Exp>>;\n  _not?: InputMaybe<Routes_Bool_Exp>;\n  _or?: InputMaybe<Array<Routes_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  accountBySource?: InputMaybe<Account_Bool_Exp>;\n  alias?: InputMaybe<String_Comparison_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  destination?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  source?: InputMaybe<String_Comparison_Exp>;\n  timestamp?: InputMaybe<Timestamp_Comparison_Exp>;\n  transaction_hash?: InputMaybe<String_Comparison_Exp>;\n  value?: InputMaybe<Coin_Array_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Routes_Max_Fields = {\n  alias?: Maybe<Scalars['String']['output']>;\n  destination?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  source?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  value?: Maybe<Array<Scalars['coin']['output']>>;\n};\n\n/** order by max() on columns of table \"routes\" */\nexport type Routes_Max_Order_By = {\n  alias?: InputMaybe<Order_By>;\n  destination?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  source?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Routes_Min_Fields = {\n  alias?: Maybe<Scalars['String']['output']>;\n  destination?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  source?: Maybe<Scalars['String']['output']>;\n  timestamp?: Maybe<Scalars['timestamp']['output']>;\n  transaction_hash?: Maybe<Scalars['String']['output']>;\n  value?: Maybe<Array<Scalars['coin']['output']>>;\n};\n\n/** order by min() on columns of table \"routes\" */\nexport type Routes_Min_Order_By = {\n  alias?: InputMaybe<Order_By>;\n  destination?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  source?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"routes\". */\nexport type Routes_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  accountBySource?: InputMaybe<Account_Order_By>;\n  alias?: InputMaybe<Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  destination?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  source?: InputMaybe<Order_By>;\n  timestamp?: InputMaybe<Order_By>;\n  transaction_hash?: InputMaybe<Order_By>;\n  value?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"routes\" */\nexport enum Routes_Select_Column {\n  /** column name */\n  Alias = 'alias',\n  /** column name */\n  Destination = 'destination',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  Source = 'source',\n  /** column name */\n  Timestamp = 'timestamp',\n  /** column name */\n  TransactionHash = 'transaction_hash',\n  /** column name */\n  Value = 'value'\n}\n\n/** aggregate stddev on columns */\nexport type Routes_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"routes\" */\nexport type Routes_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Routes_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"routes\" */\nexport type Routes_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Routes_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"routes\" */\nexport type Routes_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"routes\" */\nexport type Routes_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Routes_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Routes_Stream_Cursor_Value_Input = {\n  alias?: InputMaybe<Scalars['String']['input']>;\n  destination?: InputMaybe<Scalars['String']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  id?: InputMaybe<Scalars['Int']['input']>;\n  source?: InputMaybe<Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n  transaction_hash?: InputMaybe<Scalars['String']['input']>;\n  value?: InputMaybe<Array<Scalars['coin']['input']>>;\n};\n\n/** aggregate sum on columns */\nexport type Routes_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** order by sum() on columns of table \"routes\" */\nexport type Routes_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Routes_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"routes\" */\nexport type Routes_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Routes_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"routes\" */\nexport type Routes_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Routes_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"routes\" */\nexport type Routes_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n};\n\nexport type Subscription_Root = {\n  /** fetch data from the table: \"_transaction\" */\n  _transaction: Array<_Transaction>;\n  /** fetch aggregated fields from the table: \"_transaction\" */\n  _transaction_aggregate: _Transaction_Aggregate;\n  /** fetch data from the table in a streaming manner: \"_transaction\" */\n  _transaction_stream: Array<_Transaction>;\n  /** fetch data from the table: \"_uptime_temp\" */\n  _uptime_temp: Array<_Uptime_Temp>;\n  /** fetch aggregated fields from the table: \"_uptime_temp\" */\n  _uptime_temp_aggregate: _Uptime_Temp_Aggregate;\n  /** fetch data from the table in a streaming manner: \"_uptime_temp\" */\n  _uptime_temp_stream: Array<_Uptime_Temp>;\n  /** fetch data from the table: \"account\" */\n  account: Array<Account>;\n  /** fetch aggregated fields from the table: \"account\" */\n  account_aggregate: Account_Aggregate;\n  /** fetch data from the table: \"account_balance\" */\n  account_balance: Array<Account_Balance>;\n  /** fetch aggregated fields from the table: \"account_balance\" */\n  account_balance_aggregate: Account_Balance_Aggregate;\n  /** fetch data from the table: \"account_balance\" using primary key columns */\n  account_balance_by_pk?: Maybe<Account_Balance>;\n  /** fetch data from the table in a streaming manner: \"account_balance\" */\n  account_balance_stream: Array<Account_Balance>;\n  /** fetch data from the table: \"account\" using primary key columns */\n  account_by_pk?: Maybe<Account>;\n  /** fetch data from the table in a streaming manner: \"account\" */\n  account_stream: Array<Account>;\n  /** fetch data from the table: \"average_block_time_from_genesis\" */\n  average_block_time_from_genesis: Array<Average_Block_Time_From_Genesis>;\n  /** fetch aggregated fields from the table: \"average_block_time_from_genesis\" */\n  average_block_time_from_genesis_aggregate: Average_Block_Time_From_Genesis_Aggregate;\n  /** fetch data from the table: \"average_block_time_from_genesis\" using primary key columns */\n  average_block_time_from_genesis_by_pk?: Maybe<Average_Block_Time_From_Genesis>;\n  /** fetch data from the table in a streaming manner: \"average_block_time_from_genesis\" */\n  average_block_time_from_genesis_stream: Array<Average_Block_Time_From_Genesis>;\n  /** fetch data from the table: \"average_block_time_per_day\" */\n  average_block_time_per_day: Array<Average_Block_Time_Per_Day>;\n  /** fetch aggregated fields from the table: \"average_block_time_per_day\" */\n  average_block_time_per_day_aggregate: Average_Block_Time_Per_Day_Aggregate;\n  /** fetch data from the table: \"average_block_time_per_day\" using primary key columns */\n  average_block_time_per_day_by_pk?: Maybe<Average_Block_Time_Per_Day>;\n  /** fetch data from the table in a streaming manner: \"average_block_time_per_day\" */\n  average_block_time_per_day_stream: Array<Average_Block_Time_Per_Day>;\n  /** fetch data from the table: \"average_block_time_per_hour\" */\n  average_block_time_per_hour: Array<Average_Block_Time_Per_Hour>;\n  /** fetch aggregated fields from the table: \"average_block_time_per_hour\" */\n  average_block_time_per_hour_aggregate: Average_Block_Time_Per_Hour_Aggregate;\n  /** fetch data from the table: \"average_block_time_per_hour\" using primary key columns */\n  average_block_time_per_hour_by_pk?: Maybe<Average_Block_Time_Per_Hour>;\n  /** fetch data from the table in a streaming manner: \"average_block_time_per_hour\" */\n  average_block_time_per_hour_stream: Array<Average_Block_Time_Per_Hour>;\n  /** fetch data from the table: \"average_block_time_per_minute\" */\n  average_block_time_per_minute: Array<Average_Block_Time_Per_Minute>;\n  /** fetch aggregated fields from the table: \"average_block_time_per_minute\" */\n  average_block_time_per_minute_aggregate: Average_Block_Time_Per_Minute_Aggregate;\n  /** fetch data from the table: \"average_block_time_per_minute\" using primary key columns */\n  average_block_time_per_minute_by_pk?: Maybe<Average_Block_Time_Per_Minute>;\n  /** fetch data from the table in a streaming manner: \"average_block_time_per_minute\" */\n  average_block_time_per_minute_stream: Array<Average_Block_Time_Per_Minute>;\n  /** fetch data from the table: \"block\" */\n  block: Array<Block>;\n  /** fetch aggregated fields from the table: \"block\" */\n  block_aggregate: Block_Aggregate;\n  /** fetch data from the table: \"block\" using primary key columns */\n  block_by_pk?: Maybe<Block>;\n  /** fetch data from the table in a streaming manner: \"block\" */\n  block_stream: Array<Block>;\n  /** fetch data from the table: \"contracts\" */\n  contracts: Array<Contracts>;\n  /** fetch aggregated fields from the table: \"contracts\" */\n  contracts_aggregate: Contracts_Aggregate;\n  /** fetch data from the table: \"contracts\" using primary key columns */\n  contracts_by_pk?: Maybe<Contracts>;\n  /** fetch data from the table in a streaming manner: \"contracts\" */\n  contracts_stream: Array<Contracts>;\n  /** fetch data from the table: \"cyb_cohort\" */\n  cyb_cohort: Array<Cyb_Cohort>;\n  /** fetch aggregated fields from the table: \"cyb_cohort\" */\n  cyb_cohort_aggregate: Cyb_Cohort_Aggregate;\n  /** fetch data from the table in a streaming manner: \"cyb_cohort\" */\n  cyb_cohort_stream: Array<Cyb_Cohort>;\n  /** fetch data from the table: \"cyber_gift_proofs\" */\n  cyber_gift_proofs: Array<Cyber_Gift_Proofs>;\n  /** fetch aggregated fields from the table: \"cyber_gift_proofs\" */\n  cyber_gift_proofs_aggregate: Cyber_Gift_Proofs_Aggregate;\n  /** fetch data from the table in a streaming manner: \"cyber_gift_proofs\" */\n  cyber_gift_proofs_stream: Array<Cyber_Gift_Proofs>;\n  /** An array relationship */\n  cyberlinks: Array<Cyberlinks>;\n  /** An aggregate relationship */\n  cyberlinks_aggregate: Cyberlinks_Aggregate;\n  /** fetch data from the table: \"cyberlinks\" using primary key columns */\n  cyberlinks_by_pk?: Maybe<Cyberlinks>;\n  /** fetch data from the table: \"cyberlinks_stats\" */\n  cyberlinks_stats: Array<Cyberlinks_Stats>;\n  /** fetch aggregated fields from the table: \"cyberlinks_stats\" */\n  cyberlinks_stats_aggregate: Cyberlinks_Stats_Aggregate;\n  /** fetch data from the table in a streaming manner: \"cyberlinks_stats\" */\n  cyberlinks_stats_stream: Array<Cyberlinks_Stats>;\n  /** fetch data from the table in a streaming manner: \"cyberlinks\" */\n  cyberlinks_stream: Array<Cyberlinks>;\n  /** fetch data from the table: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons: Array<Daily_Amount_Of_Active_Neurons>;\n  /** fetch aggregated fields from the table: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons_aggregate: Daily_Amount_Of_Active_Neurons_Aggregate;\n  /** fetch data from the table in a streaming manner: \"daily_amount_of_active_neurons\" */\n  daily_amount_of_active_neurons_stream: Array<Daily_Amount_Of_Active_Neurons>;\n  /** fetch data from the table: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas: Array<Daily_Amount_Of_Used_Gas>;\n  /** fetch aggregated fields from the table: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas_aggregate: Daily_Amount_Of_Used_Gas_Aggregate;\n  /** fetch data from the table in a streaming manner: \"daily_amount_of_used_gas\" */\n  daily_amount_of_used_gas_stream: Array<Daily_Amount_Of_Used_Gas>;\n  /** fetch data from the table: \"daily_number_of_transactions\" */\n  daily_number_of_transactions: Array<Daily_Number_Of_Transactions>;\n  /** fetch aggregated fields from the table: \"daily_number_of_transactions\" */\n  daily_number_of_transactions_aggregate: Daily_Number_Of_Transactions_Aggregate;\n  /** fetch data from the table in a streaming manner: \"daily_number_of_transactions\" */\n  daily_number_of_transactions_stream: Array<Daily_Number_Of_Transactions>;\n  /** fetch data from the table: \"first_10_cyberlink\" */\n  first_10_cyberlink: Array<First_10_Cyberlink>;\n  /** fetch aggregated fields from the table: \"first_10_cyberlink\" */\n  first_10_cyberlink_aggregate: First_10_Cyberlink_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_10_cyberlink\" */\n  first_10_cyberlink_stream: Array<First_10_Cyberlink>;\n  /** fetch data from the table: \"first_100_cyberlink\" */\n  first_100_cyberlink: Array<First_100_Cyberlink>;\n  /** fetch aggregated fields from the table: \"first_100_cyberlink\" */\n  first_100_cyberlink_aggregate: First_100_Cyberlink_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_100_cyberlink\" */\n  first_100_cyberlink_stream: Array<First_100_Cyberlink>;\n  /** fetch data from the table: \"first_cyberlink\" */\n  first_cyberlink: Array<First_Cyberlink>;\n  /** fetch aggregated fields from the table: \"first_cyberlink\" */\n  first_cyberlink_aggregate: First_Cyberlink_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_cyberlink\" */\n  first_cyberlink_stream: Array<First_Cyberlink>;\n  /** fetch data from the table: \"first_hero_hired\" */\n  first_hero_hired: Array<First_Hero_Hired>;\n  /** fetch aggregated fields from the table: \"first_hero_hired\" */\n  first_hero_hired_aggregate: First_Hero_Hired_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_hero_hired\" */\n  first_hero_hired_stream: Array<First_Hero_Hired>;\n  /** fetch data from the table: \"first_investmint\" */\n  first_investmint: Array<First_Investmint>;\n  /** fetch aggregated fields from the table: \"first_investmint\" */\n  first_investmint_aggregate: First_Investmint_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_investmint\" */\n  first_investmint_stream: Array<First_Investmint>;\n  /** fetch data from the table: \"first_neuron_activation\" */\n  first_neuron_activation: Array<First_Neuron_Activation>;\n  /** fetch aggregated fields from the table: \"first_neuron_activation\" */\n  first_neuron_activation_aggregate: First_Neuron_Activation_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_neuron_activation\" */\n  first_neuron_activation_stream: Array<First_Neuron_Activation>;\n  /** fetch data from the table: \"first_swap\" */\n  first_swap: Array<First_Swap>;\n  /** fetch aggregated fields from the table: \"first_swap\" */\n  first_swap_aggregate: First_Swap_Aggregate;\n  /** fetch data from the table in a streaming manner: \"first_swap\" */\n  first_swap_stream: Array<First_Swap>;\n  /** fetch data from the table: \"follow_stats\" */\n  follow_stats: Array<Follow_Stats>;\n  /** fetch aggregated fields from the table: \"follow_stats\" */\n  follow_stats_aggregate: Follow_Stats_Aggregate;\n  /** fetch data from the table in a streaming manner: \"follow_stats\" */\n  follow_stats_stream: Array<Follow_Stats>;\n  /** fetch data from the table: \"genesis\" */\n  genesis: Array<Genesis>;\n  /** fetch data from the table: \"genesis_accounts\" */\n  genesis_accounts: Array<Genesis_Accounts>;\n  /** fetch aggregated fields from the table: \"genesis_accounts\" */\n  genesis_accounts_aggregate: Genesis_Accounts_Aggregate;\n  /** fetch data from the table: \"genesis_accounts\" using primary key columns */\n  genesis_accounts_by_pk?: Maybe<Genesis_Accounts>;\n  /** fetch data from the table in a streaming manner: \"genesis_accounts\" */\n  genesis_accounts_stream: Array<Genesis_Accounts>;\n  /** fetch aggregated fields from the table: \"genesis\" */\n  genesis_aggregate: Genesis_Aggregate;\n  /** fetch data from the table: \"genesis\" using primary key columns */\n  genesis_by_pk?: Maybe<Genesis>;\n  /** fetch data from the table: \"genesis_neurons_activation\" */\n  genesis_neurons_activation: Array<Genesis_Neurons_Activation>;\n  /** fetch aggregated fields from the table: \"genesis_neurons_activation\" */\n  genesis_neurons_activation_aggregate: Genesis_Neurons_Activation_Aggregate;\n  /** fetch data from the table in a streaming manner: \"genesis_neurons_activation\" */\n  genesis_neurons_activation_stream: Array<Genesis_Neurons_Activation>;\n  /** fetch data from the table in a streaming manner: \"genesis\" */\n  genesis_stream: Array<Genesis>;\n  /** An array relationship */\n  investmints: Array<Investmints>;\n  /** An aggregate relationship */\n  investmints_aggregate: Investmints_Aggregate;\n  /** fetch data from the table: \"investmints\" using primary key columns */\n  investmints_by_pk?: Maybe<Investmints>;\n  /** fetch data from the table in a streaming manner: \"investmints\" */\n  investmints_stream: Array<Investmints>;\n  /** fetch data from the table: \"message\" */\n  message: Array<Message>;\n  /** fetch data from the table: \"message_155\" */\n  message_155: Array<Message_155>;\n  /** fetch aggregated fields from the table: \"message_155\" */\n  message_155_aggregate: Message_155_Aggregate;\n  /** fetch data from the table in a streaming manner: \"message_155\" */\n  message_155_stream: Array<Message_155>;\n  /** fetch aggregated fields from the table: \"message\" */\n  message_aggregate: Message_Aggregate;\n  /** fetch data from the table in a streaming manner: \"message\" */\n  message_stream: Array<Message>;\n  /** fetch data from the table: \"message_type\" */\n  message_type: Array<Message_Type>;\n  /** fetch aggregated fields from the table: \"message_type\" */\n  message_type_aggregate: Message_Type_Aggregate;\n  /** fetch data from the table in a streaming manner: \"message_type\" */\n  message_type_stream: Array<Message_Type>;\n  /** execute function \"messages_by_address\" which returns \"message\" */\n  messages_by_address: Array<Message>;\n  /** execute function \"messages_by_address\" and query aggregates on result of table type \"message\" */\n  messages_by_address_aggregate: Message_Aggregate;\n  /** execute function \"messages_by_type\" which returns \"message\" */\n  messages_by_type: Array<Message>;\n  /** execute function \"messages_by_type\" and query aggregates on result of table type \"message\" */\n  messages_by_type_aggregate: Message_Aggregate;\n  /** fetch data from the table: \"modules\" */\n  modules: Array<Modules>;\n  /** fetch aggregated fields from the table: \"modules\" */\n  modules_aggregate: Modules_Aggregate;\n  /** fetch data from the table: \"modules\" using primary key columns */\n  modules_by_pk?: Maybe<Modules>;\n  /** fetch data from the table in a streaming manner: \"modules\" */\n  modules_stream: Array<Modules>;\n  /** fetch data from the table: \"neuron_activation_source\" */\n  neuron_activation_source: Array<Neuron_Activation_Source>;\n  /** fetch aggregated fields from the table: \"neuron_activation_source\" */\n  neuron_activation_source_aggregate: Neuron_Activation_Source_Aggregate;\n  /** fetch data from the table in a streaming manner: \"neuron_activation_source\" */\n  neuron_activation_source_stream: Array<Neuron_Activation_Source>;\n  /** fetch data from the table: \"number_of_new_neurons\" */\n  number_of_new_neurons: Array<Number_Of_New_Neurons>;\n  /** fetch aggregated fields from the table: \"number_of_new_neurons\" */\n  number_of_new_neurons_aggregate: Number_Of_New_Neurons_Aggregate;\n  /** fetch data from the table in a streaming manner: \"number_of_new_neurons\" */\n  number_of_new_neurons_stream: Array<Number_Of_New_Neurons>;\n  /** An array relationship */\n  particles: Array<Particles>;\n  /** An aggregate relationship */\n  particles_aggregate: Particles_Aggregate;\n  /** fetch data from the table: \"particles\" using primary key columns */\n  particles_by_pk?: Maybe<Particles>;\n  /** fetch data from the table in a streaming manner: \"particles\" */\n  particles_stream: Array<Particles>;\n  /** fetch data from the table: \"pools\" */\n  pools: Array<Pools>;\n  /** fetch aggregated fields from the table: \"pools\" */\n  pools_aggregate: Pools_Aggregate;\n  /** fetch data from the table: \"pools\" using primary key columns */\n  pools_by_pk?: Maybe<Pools>;\n  /** fetch data from the table: \"pools_liquidity\" */\n  pools_liquidity: Array<Pools_Liquidity>;\n  /** fetch aggregated fields from the table: \"pools_liquidity\" */\n  pools_liquidity_aggregate: Pools_Liquidity_Aggregate;\n  /** fetch data from the table in a streaming manner: \"pools_liquidity\" */\n  pools_liquidity_stream: Array<Pools_Liquidity>;\n  /** fetch data from the table: \"pools_rates\" */\n  pools_rates: Array<Pools_Rates>;\n  /** fetch aggregated fields from the table: \"pools_rates\" */\n  pools_rates_aggregate: Pools_Rates_Aggregate;\n  /** fetch data from the table in a streaming manner: \"pools_rates\" */\n  pools_rates_stream: Array<Pools_Rates>;\n  /** fetch data from the table in a streaming manner: \"pools\" */\n  pools_stream: Array<Pools>;\n  /** fetch data from the table: \"pools_volumes\" */\n  pools_volumes: Array<Pools_Volumes>;\n  /** fetch aggregated fields from the table: \"pools_volumes\" */\n  pools_volumes_aggregate: Pools_Volumes_Aggregate;\n  /** fetch data from the table in a streaming manner: \"pools_volumes\" */\n  pools_volumes_stream: Array<Pools_Volumes>;\n  /** fetch data from the table: \"pre_commit\" */\n  pre_commit: Array<Pre_Commit>;\n  /** fetch aggregated fields from the table: \"pre_commit\" */\n  pre_commit_aggregate: Pre_Commit_Aggregate;\n  /** fetch data from the table in a streaming manner: \"pre_commit\" */\n  pre_commit_stream: Array<Pre_Commit>;\n  /** fetch data from the table: \"pruning\" */\n  pruning: Array<Pruning>;\n  /** fetch aggregated fields from the table: \"pruning\" */\n  pruning_aggregate: Pruning_Aggregate;\n  /** fetch data from the table in a streaming manner: \"pruning\" */\n  pruning_stream: Array<Pruning>;\n  /** An array relationship */\n  routes: Array<Routes>;\n  /** An aggregate relationship */\n  routes_aggregate: Routes_Aggregate;\n  /** fetch data from the table: \"routes\" using primary key columns */\n  routes_by_pk?: Maybe<Routes>;\n  /** fetch data from the table in a streaming manner: \"routes\" */\n  routes_stream: Array<Routes>;\n  /** fetch data from the table: \"supply\" */\n  supply: Array<Supply>;\n  /** fetch aggregated fields from the table: \"supply\" */\n  supply_aggregate: Supply_Aggregate;\n  /** fetch data from the table: \"supply\" using primary key columns */\n  supply_by_pk?: Maybe<Supply>;\n  /** fetch data from the table in a streaming manner: \"supply\" */\n  supply_stream: Array<Supply>;\n  /** An array relationship */\n  swaps: Array<Swaps>;\n  /** An aggregate relationship */\n  swaps_aggregate: Swaps_Aggregate;\n  /** fetch data from the table: \"swaps\" using primary key columns */\n  swaps_by_pk?: Maybe<Swaps>;\n  /** fetch data from the table in a streaming manner: \"swaps\" */\n  swaps_stream: Array<Swaps>;\n  /** fetch data from the table: \"today_top_txs\" */\n  today_top_txs: Array<Today_Top_Txs>;\n  /** fetch aggregated fields from the table: \"today_top_txs\" */\n  today_top_txs_aggregate: Today_Top_Txs_Aggregate;\n  /** fetch data from the table in a streaming manner: \"today_top_txs\" */\n  today_top_txs_stream: Array<Today_Top_Txs>;\n  /** fetch data from the table: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week: Array<Top_10_Of_Active_Neurons_Week>;\n  /** fetch aggregated fields from the table: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week_aggregate: Top_10_Of_Active_Neurons_Week_Aggregate;\n  /** fetch data from the table in a streaming manner: \"top_10_of_active_neurons_week\" */\n  top_10_of_active_neurons_week_stream: Array<Top_10_Of_Active_Neurons_Week>;\n  /** fetch data from the table: \"top_first_txs\" */\n  top_first_txs: Array<Top_First_Txs>;\n  /** fetch aggregated fields from the table: \"top_first_txs\" */\n  top_first_txs_aggregate: Top_First_Txs_Aggregate;\n  /** fetch data from the table in a streaming manner: \"top_first_txs\" */\n  top_first_txs_stream: Array<Top_First_Txs>;\n  /** fetch data from the table: \"top_leaders\" */\n  top_leaders: Array<Top_Leaders>;\n  /** fetch aggregated fields from the table: \"top_leaders\" */\n  top_leaders_aggregate: Top_Leaders_Aggregate;\n  /** fetch data from the table in a streaming manner: \"top_leaders\" */\n  top_leaders_stream: Array<Top_Leaders>;\n  /** fetch data from the table: \"top_txs\" */\n  top_txs: Array<Top_Txs>;\n  /** fetch aggregated fields from the table: \"top_txs\" */\n  top_txs_aggregate: Top_Txs_Aggregate;\n  /** fetch data from the table in a streaming manner: \"top_txs\" */\n  top_txs_stream: Array<Top_Txs>;\n  /** fetch data from the table: \"transaction\" */\n  transaction: Array<Transaction>;\n  /** fetch data from the table: \"transaction_155\" */\n  transaction_155: Array<Transaction_155>;\n  /** fetch aggregated fields from the table: \"transaction_155\" */\n  transaction_155_aggregate: Transaction_155_Aggregate;\n  /** fetch data from the table in a streaming manner: \"transaction_155\" */\n  transaction_155_stream: Array<Transaction_155>;\n  /** fetch aggregated fields from the table: \"transaction\" */\n  transaction_aggregate: Transaction_Aggregate;\n  /** fetch data from the table in a streaming manner: \"transaction\" */\n  transaction_stream: Array<Transaction>;\n  /** fetch data from the table: \"tweets_stats\" */\n  tweets_stats: Array<Tweets_Stats>;\n  /** fetch aggregated fields from the table: \"tweets_stats\" */\n  tweets_stats_aggregate: Tweets_Stats_Aggregate;\n  /** fetch data from the table in a streaming manner: \"tweets_stats\" */\n  tweets_stats_stream: Array<Tweets_Stats>;\n  /** fetch data from the table: \"txs_ranked\" */\n  txs_ranked: Array<Txs_Ranked>;\n  /** fetch aggregated fields from the table: \"txs_ranked\" */\n  txs_ranked_aggregate: Txs_Ranked_Aggregate;\n  /** fetch data from the table in a streaming manner: \"txs_ranked\" */\n  txs_ranked_stream: Array<Txs_Ranked>;\n  /** fetch data from the table: \"uptime\" */\n  uptime: Array<Uptime>;\n  /** fetch aggregated fields from the table: \"uptime\" */\n  uptime_aggregate: Uptime_Aggregate;\n  /** fetch data from the table in a streaming manner: \"uptime\" */\n  uptime_stream: Array<Uptime>;\n  /** fetch data from the table: \"validator\" */\n  validator: Array<Validator>;\n  /** fetch aggregated fields from the table: \"validator\" */\n  validator_aggregate: Validator_Aggregate;\n  /** fetch data from the table: \"validator\" using primary key columns */\n  validator_by_pk?: Maybe<Validator>;\n  /** fetch data from the table in a streaming manner: \"validator\" */\n  validator_stream: Array<Validator>;\n  /** fetch data from the table: \"vesting_account\" */\n  vesting_account: Array<Vesting_Account>;\n  /** fetch aggregated fields from the table: \"vesting_account\" */\n  vesting_account_aggregate: Vesting_Account_Aggregate;\n  /** fetch data from the table: \"vesting_account\" using primary key columns */\n  vesting_account_by_pk?: Maybe<Vesting_Account>;\n  /** fetch data from the table in a streaming manner: \"vesting_account\" */\n  vesting_account_stream: Array<Vesting_Account>;\n  /** fetch data from the table: \"vesting_period\" */\n  vesting_period: Array<Vesting_Period>;\n  /** fetch aggregated fields from the table: \"vesting_period\" */\n  vesting_period_aggregate: Vesting_Period_Aggregate;\n  /** fetch data from the table in a streaming manner: \"vesting_period\" */\n  vesting_period_stream: Array<Vesting_Period>;\n  /** fetch data from the table: \"week_redelegation\" */\n  week_redelegation: Array<Week_Redelegation>;\n  /** fetch aggregated fields from the table: \"week_redelegation\" */\n  week_redelegation_aggregate: Week_Redelegation_Aggregate;\n  /** fetch data from the table in a streaming manner: \"week_redelegation\" */\n  week_redelegation_stream: Array<Week_Redelegation>;\n  /** fetch data from the table: \"week_undelegation\" */\n  week_undelegation: Array<Week_Undelegation>;\n  /** fetch aggregated fields from the table: \"week_undelegation\" */\n  week_undelegation_aggregate: Week_Undelegation_Aggregate;\n  /** fetch data from the table in a streaming manner: \"week_undelegation\" */\n  week_undelegation_stream: Array<Week_Undelegation>;\n};\n\n\nexport type Subscription_Root_TransactionArgs = {\n  distinct_on?: InputMaybe<Array<_Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Transaction_Order_By>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Transaction_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<_Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Transaction_Order_By>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Transaction_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<_Transaction_Stream_Cursor_Input>>;\n  where?: InputMaybe<_Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Uptime_TempArgs = {\n  distinct_on?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Uptime_Temp_Order_By>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Uptime_Temp_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<_Uptime_Temp_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<_Uptime_Temp_Order_By>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Subscription_Root_Uptime_Temp_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<_Uptime_Temp_Stream_Cursor_Input>>;\n  where?: InputMaybe<_Uptime_Temp_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccountArgs = {\n  distinct_on?: InputMaybe<Array<Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Order_By>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Order_By>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_BalanceArgs = {\n  distinct_on?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Balance_Order_By>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_Balance_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Account_Balance_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Account_Balance_Order_By>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_Balance_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootAccount_Balance_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Account_Balance_Stream_Cursor_Input>>;\n  where?: InputMaybe<Account_Balance_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAccount_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootAccount_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Account_Stream_Cursor_Input>>;\n  where?: InputMaybe<Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_From_GenesisArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_From_Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_From_Genesis_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_From_Genesis_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_From_Genesis_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_From_Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_From_Genesis_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_From_Genesis_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_From_Genesis_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootAverage_Block_Time_From_Genesis_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Average_Block_Time_From_Genesis_Stream_Cursor_Input>>;\n  where?: InputMaybe<Average_Block_Time_From_Genesis_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_DayArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Day_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Day_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Day_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Day_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Day_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Day_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Day_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Day_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Day_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Average_Block_Time_Per_Day_Stream_Cursor_Input>>;\n  where?: InputMaybe<Average_Block_Time_Per_Day_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_HourArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Hour_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Hour_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Hour_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Hour_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Hour_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Hour_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Hour_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Hour_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Hour_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Average_Block_Time_Per_Hour_Stream_Cursor_Input>>;\n  where?: InputMaybe<Average_Block_Time_Per_Hour_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_MinuteArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Minute_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Minute_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Minute_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Minute_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Average_Block_Time_Per_Minute_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Average_Block_Time_Per_Minute_Order_By>>;\n  where?: InputMaybe<Average_Block_Time_Per_Minute_Bool_Exp>;\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Minute_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootAverage_Block_Time_Per_Minute_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Average_Block_Time_Per_Minute_Stream_Cursor_Input>>;\n  where?: InputMaybe<Average_Block_Time_Per_Minute_Bool_Exp>;\n};\n\n\nexport type Subscription_RootBlockArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Subscription_RootBlock_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Subscription_RootBlock_By_PkArgs = {\n  height: Scalars['bigint']['input'];\n};\n\n\nexport type Subscription_RootBlock_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Block_Stream_Cursor_Input>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\nexport type Subscription_RootContractsArgs = {\n  distinct_on?: InputMaybe<Array<Contracts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Contracts_Order_By>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootContracts_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Contracts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Contracts_Order_By>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootContracts_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootContracts_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Contracts_Stream_Cursor_Input>>;\n  where?: InputMaybe<Contracts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyb_CohortArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyb_Cohort_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyb_Cohort_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyb_Cohort_Order_By>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyb_Cohort_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Cyb_Cohort_Stream_Cursor_Input>>;\n  where?: InputMaybe<Cyb_Cohort_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyber_Gift_ProofsArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyber_Gift_Proofs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyber_Gift_Proofs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyber_Gift_Proofs_Order_By>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyber_Gift_Proofs_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Cyber_Gift_Proofs_Stream_Cursor_Input>>;\n  where?: InputMaybe<Cyber_Gift_Proofs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinksArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootCyberlinks_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Stats_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Cyberlinks_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Cyberlinks_Stats_Order_By>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_Stats_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Cyberlinks_Stats_Stream_Cursor_Input>>;\n  where?: InputMaybe<Cyberlinks_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootCyberlinks_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Cyberlinks_Stream_Cursor_Input>>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Active_NeuronsArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Active_Neurons_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Active_Neurons_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Active_Neurons_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Daily_Amount_Of_Active_Neurons_Stream_Cursor_Input>>;\n  where?: InputMaybe<Daily_Amount_Of_Active_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Used_GasArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Used_Gas_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Amount_Of_Used_Gas_Order_By>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Amount_Of_Used_Gas_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Daily_Amount_Of_Used_Gas_Stream_Cursor_Input>>;\n  where?: InputMaybe<Daily_Amount_Of_Used_Gas_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Number_Of_TransactionsArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Number_Of_Transactions_Order_By>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Number_Of_Transactions_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Daily_Number_Of_Transactions_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Daily_Number_Of_Transactions_Order_By>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Subscription_RootDaily_Number_Of_Transactions_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Daily_Number_Of_Transactions_Stream_Cursor_Input>>;\n  where?: InputMaybe<Daily_Number_Of_Transactions_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_10_CyberlinkArgs = {\n  distinct_on?: InputMaybe<Array<First_10_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_10_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_10_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_10_Cyberlink_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_10_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_10_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_10_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_10_Cyberlink_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_10_Cyberlink_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_10_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_100_CyberlinkArgs = {\n  distinct_on?: InputMaybe<Array<First_100_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_100_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_100_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_100_Cyberlink_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_100_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_100_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_100_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_100_Cyberlink_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_100_Cyberlink_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_100_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_CyberlinkArgs = {\n  distinct_on?: InputMaybe<Array<First_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Cyberlink_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Cyberlink_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Cyberlink_Order_By>>;\n  where?: InputMaybe<First_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Cyberlink_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_Cyberlink_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_Cyberlink_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Hero_HiredArgs = {\n  distinct_on?: InputMaybe<Array<First_Hero_Hired_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Hero_Hired_Order_By>>;\n  where?: InputMaybe<First_Hero_Hired_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Hero_Hired_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Hero_Hired_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Hero_Hired_Order_By>>;\n  where?: InputMaybe<First_Hero_Hired_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Hero_Hired_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_Hero_Hired_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_Hero_Hired_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_InvestmintArgs = {\n  distinct_on?: InputMaybe<Array<First_Investmint_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Investmint_Order_By>>;\n  where?: InputMaybe<First_Investmint_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Investmint_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Investmint_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Investmint_Order_By>>;\n  where?: InputMaybe<First_Investmint_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Investmint_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_Investmint_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_Investmint_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Neuron_ActivationArgs = {\n  distinct_on?: InputMaybe<Array<First_Neuron_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Neuron_Activation_Order_By>>;\n  where?: InputMaybe<First_Neuron_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Neuron_Activation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Neuron_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Neuron_Activation_Order_By>>;\n  where?: InputMaybe<First_Neuron_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Neuron_Activation_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_Neuron_Activation_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_Neuron_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_SwapArgs = {\n  distinct_on?: InputMaybe<Array<First_Swap_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Swap_Order_By>>;\n  where?: InputMaybe<First_Swap_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Swap_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<First_Swap_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<First_Swap_Order_By>>;\n  where?: InputMaybe<First_Swap_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFirst_Swap_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<First_Swap_Stream_Cursor_Input>>;\n  where?: InputMaybe<First_Swap_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFollow_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Follow_Stats_Order_By>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFollow_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Follow_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Follow_Stats_Order_By>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootFollow_Stats_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Follow_Stats_Stream_Cursor_Input>>;\n  where?: InputMaybe<Follow_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesisArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Order_By>>;\n  where?: InputMaybe<Genesis_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_AccountsArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Accounts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Accounts_Order_By>>;\n  where?: InputMaybe<Genesis_Accounts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_Accounts_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Accounts_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Accounts_Order_By>>;\n  where?: InputMaybe<Genesis_Accounts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_Accounts_By_PkArgs = {\n  address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootGenesis_Accounts_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Genesis_Accounts_Stream_Cursor_Input>>;\n  where?: InputMaybe<Genesis_Accounts_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Order_By>>;\n  where?: InputMaybe<Genesis_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootGenesis_Neurons_ActivationArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Neurons_Activation_Order_By>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_Neurons_Activation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Genesis_Neurons_Activation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Genesis_Neurons_Activation_Order_By>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_Neurons_Activation_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Genesis_Neurons_Activation_Stream_Cursor_Input>>;\n  where?: InputMaybe<Genesis_Neurons_Activation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootGenesis_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Genesis_Stream_Cursor_Input>>;\n  where?: InputMaybe<Genesis_Bool_Exp>;\n};\n\n\nexport type Subscription_RootInvestmintsArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Subscription_RootInvestmints_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Investmints_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Investmints_Order_By>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Subscription_RootInvestmints_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootInvestmints_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Investmints_Stream_Cursor_Input>>;\n  where?: InputMaybe<Investmints_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessageArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_155Args = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_155_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_155_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Message_155_Stream_Cursor_Input>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Message_Stream_Cursor_Input>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_TypeArgs = {\n  distinct_on?: InputMaybe<Array<Message_Type_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Type_Order_By>>;\n  where?: InputMaybe<Message_Type_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_Type_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Type_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Type_Order_By>>;\n  where?: InputMaybe<Message_Type_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessage_Type_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Message_Type_Stream_Cursor_Input>>;\n  where?: InputMaybe<Message_Type_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessages_By_AddressArgs = {\n  args: Messages_By_Address_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessages_By_Address_AggregateArgs = {\n  args: Messages_By_Address_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessages_By_TypeArgs = {\n  args: Messages_By_Type_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootMessages_By_Type_AggregateArgs = {\n  args: Messages_By_Type_Args;\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\nexport type Subscription_RootModulesArgs = {\n  distinct_on?: InputMaybe<Array<Modules_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Modules_Order_By>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Subscription_RootModules_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Modules_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Modules_Order_By>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Subscription_RootModules_By_PkArgs = {\n  module_name: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootModules_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Modules_Stream_Cursor_Input>>;\n  where?: InputMaybe<Modules_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNeuron_Activation_SourceArgs = {\n  distinct_on?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Neuron_Activation_Source_Order_By>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNeuron_Activation_Source_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Neuron_Activation_Source_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Neuron_Activation_Source_Order_By>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNeuron_Activation_Source_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Neuron_Activation_Source_Stream_Cursor_Input>>;\n  where?: InputMaybe<Neuron_Activation_Source_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNumber_Of_New_NeuronsArgs = {\n  distinct_on?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Number_Of_New_Neurons_Order_By>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNumber_Of_New_Neurons_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Number_Of_New_Neurons_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Number_Of_New_Neurons_Order_By>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootNumber_Of_New_Neurons_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Number_Of_New_Neurons_Stream_Cursor_Input>>;\n  where?: InputMaybe<Number_Of_New_Neurons_Bool_Exp>;\n};\n\n\nexport type Subscription_RootParticlesArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Subscription_RootParticles_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Particles_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Particles_Order_By>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Subscription_RootParticles_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootParticles_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Particles_Stream_Cursor_Input>>;\n  where?: InputMaybe<Particles_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPoolsArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Order_By>>;\n  where?: InputMaybe<Pools_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Order_By>>;\n  where?: InputMaybe<Pools_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_By_PkArgs = {\n  pool_id: Scalars['bigint']['input'];\n};\n\n\nexport type Subscription_RootPools_LiquidityArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Liquidity_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Liquidity_Order_By>>;\n  where?: InputMaybe<Pools_Liquidity_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_Liquidity_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Liquidity_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Liquidity_Order_By>>;\n  where?: InputMaybe<Pools_Liquidity_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_Liquidity_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Pools_Liquidity_Stream_Cursor_Input>>;\n  where?: InputMaybe<Pools_Liquidity_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_RatesArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Rates_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Rates_Order_By>>;\n  where?: InputMaybe<Pools_Rates_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_Rates_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Rates_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Rates_Order_By>>;\n  where?: InputMaybe<Pools_Rates_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_Rates_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Pools_Rates_Stream_Cursor_Input>>;\n  where?: InputMaybe<Pools_Rates_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Pools_Stream_Cursor_Input>>;\n  where?: InputMaybe<Pools_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_VolumesArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Volumes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Volumes_Order_By>>;\n  where?: InputMaybe<Pools_Volumes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_Volumes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pools_Volumes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pools_Volumes_Order_By>>;\n  where?: InputMaybe<Pools_Volumes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPools_Volumes_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Pools_Volumes_Stream_Cursor_Input>>;\n  where?: InputMaybe<Pools_Volumes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_CommitArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_Commit_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPre_Commit_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Pre_Commit_Stream_Cursor_Input>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPruningArgs = {\n  distinct_on?: InputMaybe<Array<Pruning_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pruning_Order_By>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPruning_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pruning_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pruning_Order_By>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Subscription_RootPruning_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Pruning_Stream_Cursor_Input>>;\n  where?: InputMaybe<Pruning_Bool_Exp>;\n};\n\n\nexport type Subscription_RootRoutesArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootRoutes_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Routes_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Routes_Order_By>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootRoutes_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootRoutes_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Routes_Stream_Cursor_Input>>;\n  where?: InputMaybe<Routes_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSupplyArgs = {\n  distinct_on?: InputMaybe<Array<Supply_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Supply_Order_By>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSupply_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Supply_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Supply_Order_By>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSupply_By_PkArgs = {\n  one_row_id: Scalars['Boolean']['input'];\n};\n\n\nexport type Subscription_RootSupply_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Supply_Stream_Cursor_Input>>;\n  where?: InputMaybe<Supply_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSwapsArgs = {\n  distinct_on?: InputMaybe<Array<Swaps_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Swaps_Order_By>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSwaps_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Swaps_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Swaps_Order_By>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\nexport type Subscription_RootSwaps_By_PkArgs = {\n  pool_id: Scalars['bigint']['input'];\n};\n\n\nexport type Subscription_RootSwaps_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Swaps_Stream_Cursor_Input>>;\n  where?: InputMaybe<Swaps_Bool_Exp>;\n};\n\n\nexport type Subscription_RootToday_Top_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Today_Top_Txs_Order_By>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootToday_Top_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Today_Top_Txs_Order_By>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootToday_Top_Txs_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Today_Top_Txs_Stream_Cursor_Input>>;\n  where?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_10_Of_Active_Neurons_WeekArgs = {\n  distinct_on?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Order_By>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_10_Of_Active_Neurons_Week_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Order_By>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_10_Of_Active_Neurons_Week_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Top_10_Of_Active_Neurons_Week_Stream_Cursor_Input>>;\n  where?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_First_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_First_Txs_Order_By>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_First_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_First_Txs_Order_By>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_First_Txs_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Top_First_Txs_Stream_Cursor_Input>>;\n  where?: InputMaybe<Top_First_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_LeadersArgs = {\n  distinct_on?: InputMaybe<Array<Top_Leaders_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Leaders_Order_By>>;\n  where?: InputMaybe<Top_Leaders_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_Leaders_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_Leaders_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Leaders_Order_By>>;\n  where?: InputMaybe<Top_Leaders_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_Leaders_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Top_Leaders_Stream_Cursor_Input>>;\n  where?: InputMaybe<Top_Leaders_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_TxsArgs = {\n  distinct_on?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Txs_Order_By>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_Txs_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Top_Txs_Order_By>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTop_Txs_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Top_Txs_Stream_Cursor_Input>>;\n  where?: InputMaybe<Top_Txs_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransactionArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransaction_155Args = {\n  distinct_on?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_155_Order_By>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransaction_155_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_155_Order_By>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransaction_155_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Transaction_155_Stream_Cursor_Input>>;\n  where?: InputMaybe<Transaction_155_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransaction_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Transaction_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Transaction_Order_By>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTransaction_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Transaction_Stream_Cursor_Input>>;\n  where?: InputMaybe<Transaction_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTweets_StatsArgs = {\n  distinct_on?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Tweets_Stats_Order_By>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTweets_Stats_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Tweets_Stats_Order_By>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTweets_Stats_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Tweets_Stats_Stream_Cursor_Input>>;\n  where?: InputMaybe<Tweets_Stats_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTxs_RankedArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Ranked_Order_By>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTxs_Ranked_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Txs_Ranked_Order_By>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Subscription_RootTxs_Ranked_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Txs_Ranked_Stream_Cursor_Input>>;\n  where?: InputMaybe<Txs_Ranked_Bool_Exp>;\n};\n\n\nexport type Subscription_RootUptimeArgs = {\n  distinct_on?: InputMaybe<Array<Uptime_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Uptime_Order_By>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Subscription_RootUptime_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Uptime_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Uptime_Order_By>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Subscription_RootUptime_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Uptime_Stream_Cursor_Input>>;\n  where?: InputMaybe<Uptime_Bool_Exp>;\n};\n\n\nexport type Subscription_RootValidatorArgs = {\n  distinct_on?: InputMaybe<Array<Validator_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Validator_Order_By>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Subscription_RootValidator_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Validator_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Validator_Order_By>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Subscription_RootValidator_By_PkArgs = {\n  consensus_address: Scalars['String']['input'];\n};\n\n\nexport type Subscription_RootValidator_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Validator_Stream_Cursor_Input>>;\n  where?: InputMaybe<Validator_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVesting_AccountArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Account_Order_By>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVesting_Account_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Account_Order_By>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVesting_Account_By_PkArgs = {\n  id: Scalars['Int']['input'];\n};\n\n\nexport type Subscription_RootVesting_Account_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Vesting_Account_Stream_Cursor_Input>>;\n  where?: InputMaybe<Vesting_Account_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVesting_PeriodArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Period_Order_By>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVesting_Period_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Period_Order_By>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n\nexport type Subscription_RootVesting_Period_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Vesting_Period_Stream_Cursor_Input>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n\nexport type Subscription_RootWeek_RedelegationArgs = {\n  distinct_on?: InputMaybe<Array<Week_Redelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Redelegation_Order_By>>;\n  where?: InputMaybe<Week_Redelegation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootWeek_Redelegation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Week_Redelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Redelegation_Order_By>>;\n  where?: InputMaybe<Week_Redelegation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootWeek_Redelegation_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Week_Redelegation_Stream_Cursor_Input>>;\n  where?: InputMaybe<Week_Redelegation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootWeek_UndelegationArgs = {\n  distinct_on?: InputMaybe<Array<Week_Undelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Undelegation_Order_By>>;\n  where?: InputMaybe<Week_Undelegation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootWeek_Undelegation_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Week_Undelegation_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Week_Undelegation_Order_By>>;\n  where?: InputMaybe<Week_Undelegation_Bool_Exp>;\n};\n\n\nexport type Subscription_RootWeek_Undelegation_StreamArgs = {\n  batch_size: Scalars['Int']['input'];\n  cursor: Array<InputMaybe<Week_Undelegation_Stream_Cursor_Input>>;\n  where?: InputMaybe<Week_Undelegation_Bool_Exp>;\n};\n\n/** columns and relationships of \"supply\" */\nexport type Supply = {\n  coins: Array<Scalars['coin']['output']>;\n  height: Scalars['bigint']['output'];\n  one_row_id: Scalars['Boolean']['output'];\n};\n\n/** aggregated selection of \"supply\" */\nexport type Supply_Aggregate = {\n  aggregate?: Maybe<Supply_Aggregate_Fields>;\n  nodes: Array<Supply>;\n};\n\n/** aggregate fields of \"supply\" */\nexport type Supply_Aggregate_Fields = {\n  avg?: Maybe<Supply_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Supply_Max_Fields>;\n  min?: Maybe<Supply_Min_Fields>;\n  stddev?: Maybe<Supply_Stddev_Fields>;\n  stddev_pop?: Maybe<Supply_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Supply_Stddev_Samp_Fields>;\n  sum?: Maybe<Supply_Sum_Fields>;\n  var_pop?: Maybe<Supply_Var_Pop_Fields>;\n  var_samp?: Maybe<Supply_Var_Samp_Fields>;\n  variance?: Maybe<Supply_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"supply\" */\nexport type Supply_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Supply_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Supply_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"supply\". All fields are combined with a logical 'AND'. */\nexport type Supply_Bool_Exp = {\n  _and?: InputMaybe<Array<Supply_Bool_Exp>>;\n  _not?: InputMaybe<Supply_Bool_Exp>;\n  _or?: InputMaybe<Array<Supply_Bool_Exp>>;\n  coins?: InputMaybe<Coin_Array_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  one_row_id?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Supply_Max_Fields = {\n  coins?: Maybe<Array<Scalars['coin']['output']>>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Supply_Min_Fields = {\n  coins?: Maybe<Array<Scalars['coin']['output']>>;\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"supply\". */\nexport type Supply_Order_By = {\n  coins?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  one_row_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"supply\" */\nexport enum Supply_Select_Column {\n  /** column name */\n  Coins = 'coins',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  OneRowId = 'one_row_id'\n}\n\n/** aggregate stddev on columns */\nexport type Supply_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Supply_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Supply_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"supply\" */\nexport type Supply_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Supply_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Supply_Stream_Cursor_Value_Input = {\n  coins?: InputMaybe<Array<Scalars['coin']['input']>>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  one_row_id?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Supply_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Supply_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Supply_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Supply_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"swaps\" */\nexport type Swaps = {\n  address: Scalars['String']['output'];\n  /** An object relationship */\n  block: Block;\n  exchanged_demand_coin: Scalars['coin_scalar']['output'];\n  exchanged_demand_coin_fee: Scalars['coin_scalar']['output'];\n  exchanged_offer_coin: Scalars['coin_scalar']['output'];\n  exchanged_offer_coin_fee: Scalars['coin_scalar']['output'];\n  height: Scalars['bigint']['output'];\n  pool_id: Scalars['bigint']['output'];\n  swap_price: Scalars['String']['output'];\n};\n\n/** aggregated selection of \"swaps\" */\nexport type Swaps_Aggregate = {\n  aggregate?: Maybe<Swaps_Aggregate_Fields>;\n  nodes: Array<Swaps>;\n};\n\nexport type Swaps_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Swaps_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Swaps_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Swaps_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Swaps_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"swaps\" */\nexport type Swaps_Aggregate_Fields = {\n  avg?: Maybe<Swaps_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Swaps_Max_Fields>;\n  min?: Maybe<Swaps_Min_Fields>;\n  stddev?: Maybe<Swaps_Stddev_Fields>;\n  stddev_pop?: Maybe<Swaps_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Swaps_Stddev_Samp_Fields>;\n  sum?: Maybe<Swaps_Sum_Fields>;\n  var_pop?: Maybe<Swaps_Var_Pop_Fields>;\n  var_samp?: Maybe<Swaps_Var_Samp_Fields>;\n  variance?: Maybe<Swaps_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"swaps\" */\nexport type Swaps_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Swaps_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"swaps\" */\nexport type Swaps_Aggregate_Order_By = {\n  avg?: InputMaybe<Swaps_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Swaps_Max_Order_By>;\n  min?: InputMaybe<Swaps_Min_Order_By>;\n  stddev?: InputMaybe<Swaps_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Swaps_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Swaps_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Swaps_Sum_Order_By>;\n  var_pop?: InputMaybe<Swaps_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Swaps_Var_Samp_Order_By>;\n  variance?: InputMaybe<Swaps_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Swaps_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"swaps\" */\nexport type Swaps_Avg_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"swaps\". All fields are combined with a logical 'AND'. */\nexport type Swaps_Bool_Exp = {\n  _and?: InputMaybe<Array<Swaps_Bool_Exp>>;\n  _not?: InputMaybe<Swaps_Bool_Exp>;\n  _or?: InputMaybe<Array<Swaps_Bool_Exp>>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  exchanged_demand_coin?: InputMaybe<Coin_Scalar_Comparison_Exp>;\n  exchanged_demand_coin_fee?: InputMaybe<Coin_Scalar_Comparison_Exp>;\n  exchanged_offer_coin?: InputMaybe<Coin_Scalar_Comparison_Exp>;\n  exchanged_offer_coin_fee?: InputMaybe<Coin_Scalar_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  pool_id?: InputMaybe<Bigint_Comparison_Exp>;\n  swap_price?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Swaps_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  exchanged_demand_coin?: Maybe<Scalars['coin_scalar']['output']>;\n  exchanged_demand_coin_fee?: Maybe<Scalars['coin_scalar']['output']>;\n  exchanged_offer_coin?: Maybe<Scalars['coin_scalar']['output']>;\n  exchanged_offer_coin_fee?: Maybe<Scalars['coin_scalar']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  swap_price?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"swaps\" */\nexport type Swaps_Max_Order_By = {\n  address?: InputMaybe<Order_By>;\n  exchanged_demand_coin?: InputMaybe<Order_By>;\n  exchanged_demand_coin_fee?: InputMaybe<Order_By>;\n  exchanged_offer_coin?: InputMaybe<Order_By>;\n  exchanged_offer_coin_fee?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n  swap_price?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Swaps_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  exchanged_demand_coin?: Maybe<Scalars['coin_scalar']['output']>;\n  exchanged_demand_coin_fee?: Maybe<Scalars['coin_scalar']['output']>;\n  exchanged_offer_coin?: Maybe<Scalars['coin_scalar']['output']>;\n  exchanged_offer_coin_fee?: Maybe<Scalars['coin_scalar']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n  swap_price?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"swaps\" */\nexport type Swaps_Min_Order_By = {\n  address?: InputMaybe<Order_By>;\n  exchanged_demand_coin?: InputMaybe<Order_By>;\n  exchanged_demand_coin_fee?: InputMaybe<Order_By>;\n  exchanged_offer_coin?: InputMaybe<Order_By>;\n  exchanged_offer_coin_fee?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n  swap_price?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"swaps\". */\nexport type Swaps_Order_By = {\n  address?: InputMaybe<Order_By>;\n  block?: InputMaybe<Block_Order_By>;\n  exchanged_demand_coin?: InputMaybe<Order_By>;\n  exchanged_demand_coin_fee?: InputMaybe<Order_By>;\n  exchanged_offer_coin?: InputMaybe<Order_By>;\n  exchanged_offer_coin_fee?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n  swap_price?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"swaps\" */\nexport enum Swaps_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  ExchangedDemandCoin = 'exchanged_demand_coin',\n  /** column name */\n  ExchangedDemandCoinFee = 'exchanged_demand_coin_fee',\n  /** column name */\n  ExchangedOfferCoin = 'exchanged_offer_coin',\n  /** column name */\n  ExchangedOfferCoinFee = 'exchanged_offer_coin_fee',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  PoolId = 'pool_id',\n  /** column name */\n  SwapPrice = 'swap_price'\n}\n\n/** aggregate stddev on columns */\nexport type Swaps_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"swaps\" */\nexport type Swaps_Stddev_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Swaps_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"swaps\" */\nexport type Swaps_Stddev_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Swaps_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"swaps\" */\nexport type Swaps_Stddev_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"swaps\" */\nexport type Swaps_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Swaps_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Swaps_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n  exchanged_demand_coin?: InputMaybe<Scalars['coin_scalar']['input']>;\n  exchanged_demand_coin_fee?: InputMaybe<Scalars['coin_scalar']['input']>;\n  exchanged_offer_coin?: InputMaybe<Scalars['coin_scalar']['input']>;\n  exchanged_offer_coin_fee?: InputMaybe<Scalars['coin_scalar']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  pool_id?: InputMaybe<Scalars['bigint']['input']>;\n  swap_price?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Swaps_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  pool_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"swaps\" */\nexport type Swaps_Sum_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Swaps_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"swaps\" */\nexport type Swaps_Var_Pop_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Swaps_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"swaps\" */\nexport type Swaps_Var_Samp_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Swaps_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  pool_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"swaps\" */\nexport type Swaps_Variance_Order_By = {\n  height?: InputMaybe<Order_By>;\n  pool_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to compare columns of type \"timestamp\". All fields are combined with logical 'AND'. */\nexport type Timestamp_Comparison_Exp = {\n  _eq?: InputMaybe<Scalars['timestamp']['input']>;\n  _gt?: InputMaybe<Scalars['timestamp']['input']>;\n  _gte?: InputMaybe<Scalars['timestamp']['input']>;\n  _in?: InputMaybe<Array<Scalars['timestamp']['input']>>;\n  _is_null?: InputMaybe<Scalars['Boolean']['input']>;\n  _lt?: InputMaybe<Scalars['timestamp']['input']>;\n  _lte?: InputMaybe<Scalars['timestamp']['input']>;\n  _neq?: InputMaybe<Scalars['timestamp']['input']>;\n  _nin?: InputMaybe<Array<Scalars['timestamp']['input']>>;\n};\n\n/** columns and relationships of \"today_top_txs\" */\nexport type Today_Top_Txs = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"today_top_txs\" */\nexport type Today_Top_Txs_Aggregate = {\n  aggregate?: Maybe<Today_Top_Txs_Aggregate_Fields>;\n  nodes: Array<Today_Top_Txs>;\n};\n\n/** aggregate fields of \"today_top_txs\" */\nexport type Today_Top_Txs_Aggregate_Fields = {\n  avg?: Maybe<Today_Top_Txs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Today_Top_Txs_Max_Fields>;\n  min?: Maybe<Today_Top_Txs_Min_Fields>;\n  stddev?: Maybe<Today_Top_Txs_Stddev_Fields>;\n  stddev_pop?: Maybe<Today_Top_Txs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Today_Top_Txs_Stddev_Samp_Fields>;\n  sum?: Maybe<Today_Top_Txs_Sum_Fields>;\n  var_pop?: Maybe<Today_Top_Txs_Var_Pop_Fields>;\n  var_samp?: Maybe<Today_Top_Txs_Var_Samp_Fields>;\n  variance?: Maybe<Today_Top_Txs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"today_top_txs\" */\nexport type Today_Top_Txs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Today_Top_Txs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Today_Top_Txs_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"today_top_txs\". All fields are combined with a logical 'AND'. */\nexport type Today_Top_Txs_Bool_Exp = {\n  _and?: InputMaybe<Array<Today_Top_Txs_Bool_Exp>>;\n  _not?: InputMaybe<Today_Top_Txs_Bool_Exp>;\n  _or?: InputMaybe<Array<Today_Top_Txs_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Today_Top_Txs_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Today_Top_Txs_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"today_top_txs\". */\nexport type Today_Top_Txs_Order_By = {\n  count?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"today_top_txs\" */\nexport enum Today_Top_Txs_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Today_Top_Txs_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Today_Top_Txs_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Today_Top_Txs_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"today_top_txs\" */\nexport type Today_Top_Txs_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Today_Top_Txs_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Today_Top_Txs_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['bigint']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Today_Top_Txs_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Today_Top_Txs_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Today_Top_Txs_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Today_Top_Txs_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week_Aggregate = {\n  aggregate?: Maybe<Top_10_Of_Active_Neurons_Week_Aggregate_Fields>;\n  nodes: Array<Top_10_Of_Active_Neurons_Week>;\n};\n\n/** aggregate fields of \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week_Aggregate_Fields = {\n  avg?: Maybe<Top_10_Of_Active_Neurons_Week_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Top_10_Of_Active_Neurons_Week_Max_Fields>;\n  min?: Maybe<Top_10_Of_Active_Neurons_Week_Min_Fields>;\n  stddev?: Maybe<Top_10_Of_Active_Neurons_Week_Stddev_Fields>;\n  stddev_pop?: Maybe<Top_10_Of_Active_Neurons_Week_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Top_10_Of_Active_Neurons_Week_Stddev_Samp_Fields>;\n  sum?: Maybe<Top_10_Of_Active_Neurons_Week_Sum_Fields>;\n  var_pop?: Maybe<Top_10_Of_Active_Neurons_Week_Var_Pop_Fields>;\n  var_samp?: Maybe<Top_10_Of_Active_Neurons_Week_Var_Samp_Fields>;\n  variance?: Maybe<Top_10_Of_Active_Neurons_Week_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Top_10_Of_Active_Neurons_Week_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"top_10_of_active_neurons_week\". All fields are combined with a logical 'AND'. */\nexport type Top_10_Of_Active_Neurons_Week_Bool_Exp = {\n  _and?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Bool_Exp>>;\n  _not?: InputMaybe<Top_10_Of_Active_Neurons_Week_Bool_Exp>;\n  _or?: InputMaybe<Array<Top_10_Of_Active_Neurons_Week_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  pubkey?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Top_10_Of_Active_Neurons_Week_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Top_10_Of_Active_Neurons_Week_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"top_10_of_active_neurons_week\". */\nexport type Top_10_Of_Active_Neurons_Week_Order_By = {\n  count?: InputMaybe<Order_By>;\n  pubkey?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"top_10_of_active_neurons_week\" */\nexport enum Top_10_Of_Active_Neurons_Week_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Pubkey = 'pubkey'\n}\n\n/** aggregate stddev on columns */\nexport type Top_10_Of_Active_Neurons_Week_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Top_10_Of_Active_Neurons_Week_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Top_10_Of_Active_Neurons_Week_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"top_10_of_active_neurons_week\" */\nexport type Top_10_Of_Active_Neurons_Week_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Top_10_Of_Active_Neurons_Week_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Top_10_Of_Active_Neurons_Week_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['bigint']['input']>;\n  pubkey?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Top_10_Of_Active_Neurons_Week_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Top_10_Of_Active_Neurons_Week_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Top_10_Of_Active_Neurons_Week_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Top_10_Of_Active_Neurons_Week_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"top_first_txs\" */\nexport type Top_First_Txs = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"top_first_txs\" */\nexport type Top_First_Txs_Aggregate = {\n  aggregate?: Maybe<Top_First_Txs_Aggregate_Fields>;\n  nodes: Array<Top_First_Txs>;\n};\n\n/** aggregate fields of \"top_first_txs\" */\nexport type Top_First_Txs_Aggregate_Fields = {\n  avg?: Maybe<Top_First_Txs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Top_First_Txs_Max_Fields>;\n  min?: Maybe<Top_First_Txs_Min_Fields>;\n  stddev?: Maybe<Top_First_Txs_Stddev_Fields>;\n  stddev_pop?: Maybe<Top_First_Txs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Top_First_Txs_Stddev_Samp_Fields>;\n  sum?: Maybe<Top_First_Txs_Sum_Fields>;\n  var_pop?: Maybe<Top_First_Txs_Var_Pop_Fields>;\n  var_samp?: Maybe<Top_First_Txs_Var_Samp_Fields>;\n  variance?: Maybe<Top_First_Txs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"top_first_txs\" */\nexport type Top_First_Txs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Top_First_Txs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Top_First_Txs_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"top_first_txs\". All fields are combined with a logical 'AND'. */\nexport type Top_First_Txs_Bool_Exp = {\n  _and?: InputMaybe<Array<Top_First_Txs_Bool_Exp>>;\n  _not?: InputMaybe<Top_First_Txs_Bool_Exp>;\n  _or?: InputMaybe<Array<Top_First_Txs_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Top_First_Txs_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Top_First_Txs_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"top_first_txs\". */\nexport type Top_First_Txs_Order_By = {\n  count?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"top_first_txs\" */\nexport enum Top_First_Txs_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Top_First_Txs_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Top_First_Txs_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Top_First_Txs_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"top_first_txs\" */\nexport type Top_First_Txs_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Top_First_Txs_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Top_First_Txs_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['bigint']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Top_First_Txs_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Top_First_Txs_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Top_First_Txs_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Top_First_Txs_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"top_leaders\" */\nexport type Top_Leaders = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"top_leaders\" */\nexport type Top_Leaders_Aggregate = {\n  aggregate?: Maybe<Top_Leaders_Aggregate_Fields>;\n  nodes: Array<Top_Leaders>;\n};\n\n/** aggregate fields of \"top_leaders\" */\nexport type Top_Leaders_Aggregate_Fields = {\n  avg?: Maybe<Top_Leaders_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Top_Leaders_Max_Fields>;\n  min?: Maybe<Top_Leaders_Min_Fields>;\n  stddev?: Maybe<Top_Leaders_Stddev_Fields>;\n  stddev_pop?: Maybe<Top_Leaders_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Top_Leaders_Stddev_Samp_Fields>;\n  sum?: Maybe<Top_Leaders_Sum_Fields>;\n  var_pop?: Maybe<Top_Leaders_Var_Pop_Fields>;\n  var_samp?: Maybe<Top_Leaders_Var_Samp_Fields>;\n  variance?: Maybe<Top_Leaders_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"top_leaders\" */\nexport type Top_Leaders_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Top_Leaders_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Top_Leaders_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"top_leaders\". All fields are combined with a logical 'AND'. */\nexport type Top_Leaders_Bool_Exp = {\n  _and?: InputMaybe<Array<Top_Leaders_Bool_Exp>>;\n  _not?: InputMaybe<Top_Leaders_Bool_Exp>;\n  _or?: InputMaybe<Array<Top_Leaders_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Top_Leaders_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Top_Leaders_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"top_leaders\". */\nexport type Top_Leaders_Order_By = {\n  count?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"top_leaders\" */\nexport enum Top_Leaders_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Neuron = 'neuron'\n}\n\n/** aggregate stddev on columns */\nexport type Top_Leaders_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Top_Leaders_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Top_Leaders_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"top_leaders\" */\nexport type Top_Leaders_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Top_Leaders_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Top_Leaders_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['bigint']['input']>;\n  neuron?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Top_Leaders_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Top_Leaders_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Top_Leaders_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Top_Leaders_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"top_txs\" */\nexport type Top_Txs = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregated selection of \"top_txs\" */\nexport type Top_Txs_Aggregate = {\n  aggregate?: Maybe<Top_Txs_Aggregate_Fields>;\n  nodes: Array<Top_Txs>;\n};\n\n/** aggregate fields of \"top_txs\" */\nexport type Top_Txs_Aggregate_Fields = {\n  avg?: Maybe<Top_Txs_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Top_Txs_Max_Fields>;\n  min?: Maybe<Top_Txs_Min_Fields>;\n  stddev?: Maybe<Top_Txs_Stddev_Fields>;\n  stddev_pop?: Maybe<Top_Txs_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Top_Txs_Stddev_Samp_Fields>;\n  sum?: Maybe<Top_Txs_Sum_Fields>;\n  var_pop?: Maybe<Top_Txs_Var_Pop_Fields>;\n  var_samp?: Maybe<Top_Txs_Var_Samp_Fields>;\n  variance?: Maybe<Top_Txs_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"top_txs\" */\nexport type Top_Txs_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Top_Txs_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Top_Txs_Avg_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"top_txs\". All fields are combined with a logical 'AND'. */\nexport type Top_Txs_Bool_Exp = {\n  _and?: InputMaybe<Array<Top_Txs_Bool_Exp>>;\n  _not?: InputMaybe<Top_Txs_Bool_Exp>;\n  _or?: InputMaybe<Array<Top_Txs_Bool_Exp>>;\n  count?: InputMaybe<Bigint_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Top_Txs_Max_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Top_Txs_Min_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"top_txs\". */\nexport type Top_Txs_Order_By = {\n  count?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"top_txs\" */\nexport enum Top_Txs_Select_Column {\n  /** column name */\n  Count = 'count',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Top_Txs_Stddev_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Top_Txs_Stddev_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Top_Txs_Stddev_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"top_txs\" */\nexport type Top_Txs_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Top_Txs_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Top_Txs_Stream_Cursor_Value_Input = {\n  count?: InputMaybe<Scalars['bigint']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Top_Txs_Sum_Fields = {\n  count?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Top_Txs_Var_Pop_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Top_Txs_Var_Samp_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Top_Txs_Variance_Fields = {\n  count?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"transaction\" */\nexport type Transaction = {\n  /** An object relationship */\n  block: Block;\n  fee: Scalars['jsonb']['output'];\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash: Scalars['String']['output'];\n  height: Scalars['bigint']['output'];\n  logs?: Maybe<Scalars['jsonb']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  /** An array relationship */\n  message_155s: Array<Message_155>;\n  /** An aggregate relationship */\n  message_155s_aggregate: Message_155_Aggregate;\n  messages: Scalars['json']['output'];\n  /** An array relationship */\n  messagesByPartitionIdTransactionHash: Array<Message>;\n  /** An aggregate relationship */\n  messagesByPartitionIdTransactionHash_aggregate: Message_Aggregate;\n  partition_id: Scalars['bigint']['output'];\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures: Array<Scalars['String']['output']>;\n  signer_infos: Scalars['jsonb']['output'];\n  success: Scalars['Boolean']['output'];\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionFeeArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionLogsArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessage_155sArgs = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessage_155s_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_155_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_155_Order_By>>;\n  where?: InputMaybe<Message_155_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessagesArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessagesByPartitionIdTransactionHashArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionMessagesByPartitionIdTransactionHash_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction\" */\nexport type TransactionSigner_InfosArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155 = {\n  /** An object relationship */\n  block: Block;\n  fee: Scalars['jsonb']['output'];\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash: Scalars['String']['output'];\n  height: Scalars['bigint']['output'];\n  logs?: Maybe<Scalars['jsonb']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  messages: Scalars['json']['output'];\n  /** An array relationship */\n  messagesByTransactionHashPartitionId: Array<Message>;\n  /** An aggregate relationship */\n  messagesByTransactionHashPartitionId_aggregate: Message_Aggregate;\n  partition_id: Scalars['bigint']['output'];\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures: Array<Scalars['String']['output']>;\n  signer_infos: Scalars['jsonb']['output'];\n  success: Scalars['Boolean']['output'];\n};\n\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155FeeArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155LogsArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155MessagesArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155MessagesByTransactionHashPartitionIdArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155MessagesByTransactionHashPartitionId_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Message_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Message_Order_By>>;\n  where?: InputMaybe<Message_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"transaction_155\" */\nexport type Transaction_155Signer_InfosArgs = {\n  path?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregated selection of \"transaction_155\" */\nexport type Transaction_155_Aggregate = {\n  aggregate?: Maybe<Transaction_155_Aggregate_Fields>;\n  nodes: Array<Transaction_155>;\n};\n\nexport type Transaction_155_Aggregate_Bool_Exp = {\n  bool_and?: InputMaybe<Transaction_155_Aggregate_Bool_Exp_Bool_And>;\n  bool_or?: InputMaybe<Transaction_155_Aggregate_Bool_Exp_Bool_Or>;\n  count?: InputMaybe<Transaction_155_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Transaction_155_Aggregate_Bool_Exp_Bool_And = {\n  arguments: Transaction_155_Select_Column_Transaction_155_Aggregate_Bool_Exp_Bool_And_Arguments_Columns;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Transaction_155_Bool_Exp>;\n  predicate: Boolean_Comparison_Exp;\n};\n\nexport type Transaction_155_Aggregate_Bool_Exp_Bool_Or = {\n  arguments: Transaction_155_Select_Column_Transaction_155_Aggregate_Bool_Exp_Bool_Or_Arguments_Columns;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Transaction_155_Bool_Exp>;\n  predicate: Boolean_Comparison_Exp;\n};\n\nexport type Transaction_155_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Transaction_155_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"transaction_155\" */\nexport type Transaction_155_Aggregate_Fields = {\n  avg?: Maybe<Transaction_155_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Transaction_155_Max_Fields>;\n  min?: Maybe<Transaction_155_Min_Fields>;\n  stddev?: Maybe<Transaction_155_Stddev_Fields>;\n  stddev_pop?: Maybe<Transaction_155_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Transaction_155_Stddev_Samp_Fields>;\n  sum?: Maybe<Transaction_155_Sum_Fields>;\n  var_pop?: Maybe<Transaction_155_Var_Pop_Fields>;\n  var_samp?: Maybe<Transaction_155_Var_Samp_Fields>;\n  variance?: Maybe<Transaction_155_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"transaction_155\" */\nexport type Transaction_155_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Transaction_155_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"transaction_155\" */\nexport type Transaction_155_Aggregate_Order_By = {\n  avg?: InputMaybe<Transaction_155_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Transaction_155_Max_Order_By>;\n  min?: InputMaybe<Transaction_155_Min_Order_By>;\n  stddev?: InputMaybe<Transaction_155_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Transaction_155_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Transaction_155_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Transaction_155_Sum_Order_By>;\n  var_pop?: InputMaybe<Transaction_155_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Transaction_155_Var_Samp_Order_By>;\n  variance?: InputMaybe<Transaction_155_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Transaction_155_Avg_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"transaction_155\" */\nexport type Transaction_155_Avg_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"transaction_155\". All fields are combined with a logical 'AND'. */\nexport type Transaction_155_Bool_Exp = {\n  _and?: InputMaybe<Array<Transaction_155_Bool_Exp>>;\n  _not?: InputMaybe<Transaction_155_Bool_Exp>;\n  _or?: InputMaybe<Array<Transaction_155_Bool_Exp>>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  fee?: InputMaybe<Jsonb_Comparison_Exp>;\n  gas_used?: InputMaybe<Bigint_Comparison_Exp>;\n  gas_wanted?: InputMaybe<Bigint_Comparison_Exp>;\n  hash?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  logs?: InputMaybe<Jsonb_Comparison_Exp>;\n  memo?: InputMaybe<String_Comparison_Exp>;\n  messages?: InputMaybe<Json_Comparison_Exp>;\n  messagesByTransactionHashPartitionId?: InputMaybe<Message_Bool_Exp>;\n  messagesByTransactionHashPartitionId_aggregate?: InputMaybe<Message_Aggregate_Bool_Exp>;\n  partition_id?: InputMaybe<Bigint_Comparison_Exp>;\n  raw_log?: InputMaybe<String_Comparison_Exp>;\n  signatures?: InputMaybe<String_Array_Comparison_Exp>;\n  signer_infos?: InputMaybe<Jsonb_Comparison_Exp>;\n  success?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Transaction_155_Max_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n};\n\n/** order by max() on columns of table \"transaction_155\" */\nexport type Transaction_155_Max_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Transaction_155_Min_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n};\n\n/** order by min() on columns of table \"transaction_155\" */\nexport type Transaction_155_Min_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"transaction_155\". */\nexport type Transaction_155_Order_By = {\n  block?: InputMaybe<Block_Order_By>;\n  fee?: InputMaybe<Order_By>;\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  logs?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  messages?: InputMaybe<Order_By>;\n  messagesByTransactionHashPartitionId_aggregate?: InputMaybe<Message_Aggregate_Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n  signer_infos?: InputMaybe<Order_By>;\n  success?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"transaction_155\" */\nexport enum Transaction_155_Select_Column {\n  /** column name */\n  Fee = 'fee',\n  /** column name */\n  GasUsed = 'gas_used',\n  /** column name */\n  GasWanted = 'gas_wanted',\n  /** column name */\n  Hash = 'hash',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Logs = 'logs',\n  /** column name */\n  Memo = 'memo',\n  /** column name */\n  Messages = 'messages',\n  /** column name */\n  PartitionId = 'partition_id',\n  /** column name */\n  RawLog = 'raw_log',\n  /** column name */\n  Signatures = 'signatures',\n  /** column name */\n  SignerInfos = 'signer_infos',\n  /** column name */\n  Success = 'success'\n}\n\n/** select \"transaction_155_aggregate_bool_exp_bool_and_arguments_columns\" columns of table \"transaction_155\" */\nexport enum Transaction_155_Select_Column_Transaction_155_Aggregate_Bool_Exp_Bool_And_Arguments_Columns {\n  /** column name */\n  Success = 'success'\n}\n\n/** select \"transaction_155_aggregate_bool_exp_bool_or_arguments_columns\" columns of table \"transaction_155\" */\nexport enum Transaction_155_Select_Column_Transaction_155_Aggregate_Bool_Exp_Bool_Or_Arguments_Columns {\n  /** column name */\n  Success = 'success'\n}\n\n/** aggregate stddev on columns */\nexport type Transaction_155_Stddev_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"transaction_155\" */\nexport type Transaction_155_Stddev_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Transaction_155_Stddev_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"transaction_155\" */\nexport type Transaction_155_Stddev_Pop_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Transaction_155_Stddev_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"transaction_155\" */\nexport type Transaction_155_Stddev_Samp_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"transaction_155\" */\nexport type Transaction_155_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Transaction_155_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Transaction_155_Stream_Cursor_Value_Input = {\n  fee?: InputMaybe<Scalars['jsonb']['input']>;\n  gas_used?: InputMaybe<Scalars['bigint']['input']>;\n  gas_wanted?: InputMaybe<Scalars['bigint']['input']>;\n  hash?: InputMaybe<Scalars['String']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  logs?: InputMaybe<Scalars['jsonb']['input']>;\n  memo?: InputMaybe<Scalars['String']['input']>;\n  messages?: InputMaybe<Scalars['json']['input']>;\n  partition_id?: InputMaybe<Scalars['bigint']['input']>;\n  raw_log?: InputMaybe<Scalars['String']['input']>;\n  signatures?: InputMaybe<Array<Scalars['String']['input']>>;\n  signer_infos?: InputMaybe<Scalars['jsonb']['input']>;\n  success?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Transaction_155_Sum_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"transaction_155\" */\nexport type Transaction_155_Sum_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Transaction_155_Var_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"transaction_155\" */\nexport type Transaction_155_Var_Pop_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Transaction_155_Var_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"transaction_155\" */\nexport type Transaction_155_Var_Samp_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Transaction_155_Variance_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"transaction_155\" */\nexport type Transaction_155_Variance_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregated selection of \"transaction\" */\nexport type Transaction_Aggregate = {\n  aggregate?: Maybe<Transaction_Aggregate_Fields>;\n  nodes: Array<Transaction>;\n};\n\nexport type Transaction_Aggregate_Bool_Exp = {\n  bool_and?: InputMaybe<Transaction_Aggregate_Bool_Exp_Bool_And>;\n  bool_or?: InputMaybe<Transaction_Aggregate_Bool_Exp_Bool_Or>;\n  count?: InputMaybe<Transaction_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Transaction_Aggregate_Bool_Exp_Bool_And = {\n  arguments: Transaction_Select_Column_Transaction_Aggregate_Bool_Exp_Bool_And_Arguments_Columns;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Transaction_Bool_Exp>;\n  predicate: Boolean_Comparison_Exp;\n};\n\nexport type Transaction_Aggregate_Bool_Exp_Bool_Or = {\n  arguments: Transaction_Select_Column_Transaction_Aggregate_Bool_Exp_Bool_Or_Arguments_Columns;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Transaction_Bool_Exp>;\n  predicate: Boolean_Comparison_Exp;\n};\n\nexport type Transaction_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Transaction_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Transaction_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"transaction\" */\nexport type Transaction_Aggregate_Fields = {\n  avg?: Maybe<Transaction_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Transaction_Max_Fields>;\n  min?: Maybe<Transaction_Min_Fields>;\n  stddev?: Maybe<Transaction_Stddev_Fields>;\n  stddev_pop?: Maybe<Transaction_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Transaction_Stddev_Samp_Fields>;\n  sum?: Maybe<Transaction_Sum_Fields>;\n  var_pop?: Maybe<Transaction_Var_Pop_Fields>;\n  var_samp?: Maybe<Transaction_Var_Samp_Fields>;\n  variance?: Maybe<Transaction_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"transaction\" */\nexport type Transaction_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Transaction_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"transaction\" */\nexport type Transaction_Aggregate_Order_By = {\n  avg?: InputMaybe<Transaction_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Transaction_Max_Order_By>;\n  min?: InputMaybe<Transaction_Min_Order_By>;\n  stddev?: InputMaybe<Transaction_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Transaction_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Transaction_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Transaction_Sum_Order_By>;\n  var_pop?: InputMaybe<Transaction_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Transaction_Var_Samp_Order_By>;\n  variance?: InputMaybe<Transaction_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Transaction_Avg_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"transaction\" */\nexport type Transaction_Avg_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"transaction\". All fields are combined with a logical 'AND'. */\nexport type Transaction_Bool_Exp = {\n  _and?: InputMaybe<Array<Transaction_Bool_Exp>>;\n  _not?: InputMaybe<Transaction_Bool_Exp>;\n  _or?: InputMaybe<Array<Transaction_Bool_Exp>>;\n  block?: InputMaybe<Block_Bool_Exp>;\n  fee?: InputMaybe<Jsonb_Comparison_Exp>;\n  gas_used?: InputMaybe<Bigint_Comparison_Exp>;\n  gas_wanted?: InputMaybe<Bigint_Comparison_Exp>;\n  hash?: InputMaybe<String_Comparison_Exp>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  logs?: InputMaybe<Jsonb_Comparison_Exp>;\n  memo?: InputMaybe<String_Comparison_Exp>;\n  message_155s?: InputMaybe<Message_155_Bool_Exp>;\n  message_155s_aggregate?: InputMaybe<Message_155_Aggregate_Bool_Exp>;\n  messages?: InputMaybe<Json_Comparison_Exp>;\n  messagesByPartitionIdTransactionHash?: InputMaybe<Message_Bool_Exp>;\n  messagesByPartitionIdTransactionHash_aggregate?: InputMaybe<Message_Aggregate_Bool_Exp>;\n  partition_id?: InputMaybe<Bigint_Comparison_Exp>;\n  raw_log?: InputMaybe<String_Comparison_Exp>;\n  signatures?: InputMaybe<String_Array_Comparison_Exp>;\n  signer_infos?: InputMaybe<Jsonb_Comparison_Exp>;\n  success?: InputMaybe<Boolean_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Transaction_Max_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n};\n\n/** order by max() on columns of table \"transaction\" */\nexport type Transaction_Max_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Transaction_Min_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  hash?: Maybe<Scalars['String']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  memo?: Maybe<Scalars['String']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n  raw_log?: Maybe<Scalars['String']['output']>;\n  signatures?: Maybe<Array<Scalars['String']['output']>>;\n};\n\n/** order by min() on columns of table \"transaction\" */\nexport type Transaction_Min_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"transaction\". */\nexport type Transaction_Order_By = {\n  block?: InputMaybe<Block_Order_By>;\n  fee?: InputMaybe<Order_By>;\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  hash?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  logs?: InputMaybe<Order_By>;\n  memo?: InputMaybe<Order_By>;\n  message_155s_aggregate?: InputMaybe<Message_155_Aggregate_Order_By>;\n  messages?: InputMaybe<Order_By>;\n  messagesByPartitionIdTransactionHash_aggregate?: InputMaybe<Message_Aggregate_Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n  raw_log?: InputMaybe<Order_By>;\n  signatures?: InputMaybe<Order_By>;\n  signer_infos?: InputMaybe<Order_By>;\n  success?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"transaction\" */\nexport enum Transaction_Select_Column {\n  /** column name */\n  Fee = 'fee',\n  /** column name */\n  GasUsed = 'gas_used',\n  /** column name */\n  GasWanted = 'gas_wanted',\n  /** column name */\n  Hash = 'hash',\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Logs = 'logs',\n  /** column name */\n  Memo = 'memo',\n  /** column name */\n  Messages = 'messages',\n  /** column name */\n  PartitionId = 'partition_id',\n  /** column name */\n  RawLog = 'raw_log',\n  /** column name */\n  Signatures = 'signatures',\n  /** column name */\n  SignerInfos = 'signer_infos',\n  /** column name */\n  Success = 'success'\n}\n\n/** select \"transaction_aggregate_bool_exp_bool_and_arguments_columns\" columns of table \"transaction\" */\nexport enum Transaction_Select_Column_Transaction_Aggregate_Bool_Exp_Bool_And_Arguments_Columns {\n  /** column name */\n  Success = 'success'\n}\n\n/** select \"transaction_aggregate_bool_exp_bool_or_arguments_columns\" columns of table \"transaction\" */\nexport enum Transaction_Select_Column_Transaction_Aggregate_Bool_Exp_Bool_Or_Arguments_Columns {\n  /** column name */\n  Success = 'success'\n}\n\n/** aggregate stddev on columns */\nexport type Transaction_Stddev_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"transaction\" */\nexport type Transaction_Stddev_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Transaction_Stddev_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"transaction\" */\nexport type Transaction_Stddev_Pop_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Transaction_Stddev_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"transaction\" */\nexport type Transaction_Stddev_Samp_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"transaction\" */\nexport type Transaction_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Transaction_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Transaction_Stream_Cursor_Value_Input = {\n  fee?: InputMaybe<Scalars['jsonb']['input']>;\n  gas_used?: InputMaybe<Scalars['bigint']['input']>;\n  gas_wanted?: InputMaybe<Scalars['bigint']['input']>;\n  hash?: InputMaybe<Scalars['String']['input']>;\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  logs?: InputMaybe<Scalars['jsonb']['input']>;\n  memo?: InputMaybe<Scalars['String']['input']>;\n  messages?: InputMaybe<Scalars['json']['input']>;\n  partition_id?: InputMaybe<Scalars['bigint']['input']>;\n  raw_log?: InputMaybe<Scalars['String']['input']>;\n  signatures?: InputMaybe<Array<Scalars['String']['input']>>;\n  signer_infos?: InputMaybe<Scalars['jsonb']['input']>;\n  success?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Transaction_Sum_Fields = {\n  gas_used?: Maybe<Scalars['bigint']['output']>;\n  gas_wanted?: Maybe<Scalars['bigint']['output']>;\n  height?: Maybe<Scalars['bigint']['output']>;\n  partition_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"transaction\" */\nexport type Transaction_Sum_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Transaction_Var_Pop_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"transaction\" */\nexport type Transaction_Var_Pop_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Transaction_Var_Samp_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"transaction\" */\nexport type Transaction_Var_Samp_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Transaction_Variance_Fields = {\n  gas_used?: Maybe<Scalars['Float']['output']>;\n  gas_wanted?: Maybe<Scalars['Float']['output']>;\n  height?: Maybe<Scalars['Float']['output']>;\n  partition_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"transaction\" */\nexport type Transaction_Variance_Order_By = {\n  gas_used?: InputMaybe<Order_By>;\n  gas_wanted?: InputMaybe<Order_By>;\n  height?: InputMaybe<Order_By>;\n  partition_id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"tweets_stats\" */\nexport type Tweets_Stats = {\n  date?: Maybe<Scalars['date']['output']>;\n  tweets?: Maybe<Scalars['numeric']['output']>;\n  tweets_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregated selection of \"tweets_stats\" */\nexport type Tweets_Stats_Aggregate = {\n  aggregate?: Maybe<Tweets_Stats_Aggregate_Fields>;\n  nodes: Array<Tweets_Stats>;\n};\n\n/** aggregate fields of \"tweets_stats\" */\nexport type Tweets_Stats_Aggregate_Fields = {\n  avg?: Maybe<Tweets_Stats_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Tweets_Stats_Max_Fields>;\n  min?: Maybe<Tweets_Stats_Min_Fields>;\n  stddev?: Maybe<Tweets_Stats_Stddev_Fields>;\n  stddev_pop?: Maybe<Tweets_Stats_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Tweets_Stats_Stddev_Samp_Fields>;\n  sum?: Maybe<Tweets_Stats_Sum_Fields>;\n  var_pop?: Maybe<Tweets_Stats_Var_Pop_Fields>;\n  var_samp?: Maybe<Tweets_Stats_Var_Samp_Fields>;\n  variance?: Maybe<Tweets_Stats_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"tweets_stats\" */\nexport type Tweets_Stats_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Tweets_Stats_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Tweets_Stats_Avg_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"tweets_stats\". All fields are combined with a logical 'AND'. */\nexport type Tweets_Stats_Bool_Exp = {\n  _and?: InputMaybe<Array<Tweets_Stats_Bool_Exp>>;\n  _not?: InputMaybe<Tweets_Stats_Bool_Exp>;\n  _or?: InputMaybe<Array<Tweets_Stats_Bool_Exp>>;\n  date?: InputMaybe<Date_Comparison_Exp>;\n  tweets?: InputMaybe<Numeric_Comparison_Exp>;\n  tweets_per_day?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Tweets_Stats_Max_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  tweets?: Maybe<Scalars['numeric']['output']>;\n  tweets_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Tweets_Stats_Min_Fields = {\n  date?: Maybe<Scalars['date']['output']>;\n  tweets?: Maybe<Scalars['numeric']['output']>;\n  tweets_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** Ordering options when selecting data from \"tweets_stats\". */\nexport type Tweets_Stats_Order_By = {\n  date?: InputMaybe<Order_By>;\n  tweets?: InputMaybe<Order_By>;\n  tweets_per_day?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"tweets_stats\" */\nexport enum Tweets_Stats_Select_Column {\n  /** column name */\n  Date = 'date',\n  /** column name */\n  Tweets = 'tweets',\n  /** column name */\n  TweetsPerDay = 'tweets_per_day'\n}\n\n/** aggregate stddev on columns */\nexport type Tweets_Stats_Stddev_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Tweets_Stats_Stddev_Pop_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Tweets_Stats_Stddev_Samp_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"tweets_stats\" */\nexport type Tweets_Stats_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Tweets_Stats_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Tweets_Stats_Stream_Cursor_Value_Input = {\n  date?: InputMaybe<Scalars['date']['input']>;\n  tweets?: InputMaybe<Scalars['numeric']['input']>;\n  tweets_per_day?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Tweets_Stats_Sum_Fields = {\n  tweets?: Maybe<Scalars['numeric']['output']>;\n  tweets_per_day?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Tweets_Stats_Var_Pop_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Tweets_Stats_Var_Samp_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Tweets_Stats_Variance_Fields = {\n  tweets?: Maybe<Scalars['Float']['output']>;\n  tweets_per_day?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"txs_ranked\" */\nexport type Txs_Ranked = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"txs_ranked\" */\nexport type Txs_Ranked_Aggregate = {\n  aggregate?: Maybe<Txs_Ranked_Aggregate_Fields>;\n  nodes: Array<Txs_Ranked>;\n};\n\n/** aggregate fields of \"txs_ranked\" */\nexport type Txs_Ranked_Aggregate_Fields = {\n  avg?: Maybe<Txs_Ranked_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Txs_Ranked_Max_Fields>;\n  min?: Maybe<Txs_Ranked_Min_Fields>;\n  stddev?: Maybe<Txs_Ranked_Stddev_Fields>;\n  stddev_pop?: Maybe<Txs_Ranked_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Txs_Ranked_Stddev_Samp_Fields>;\n  sum?: Maybe<Txs_Ranked_Sum_Fields>;\n  var_pop?: Maybe<Txs_Ranked_Var_Pop_Fields>;\n  var_samp?: Maybe<Txs_Ranked_Var_Samp_Fields>;\n  variance?: Maybe<Txs_Ranked_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"txs_ranked\" */\nexport type Txs_Ranked_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Txs_Ranked_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Txs_Ranked_Avg_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"txs_ranked\". All fields are combined with a logical 'AND'. */\nexport type Txs_Ranked_Bool_Exp = {\n  _and?: InputMaybe<Array<Txs_Ranked_Bool_Exp>>;\n  _not?: InputMaybe<Txs_Ranked_Bool_Exp>;\n  _or?: InputMaybe<Array<Txs_Ranked_Bool_Exp>>;\n  height?: InputMaybe<Bigint_Comparison_Exp>;\n  neuron?: InputMaybe<String_Comparison_Exp>;\n  rank?: InputMaybe<Bigint_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Txs_Ranked_Max_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Txs_Ranked_Min_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  neuron?: Maybe<Scalars['String']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"txs_ranked\". */\nexport type Txs_Ranked_Order_By = {\n  height?: InputMaybe<Order_By>;\n  neuron?: InputMaybe<Order_By>;\n  rank?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"txs_ranked\" */\nexport enum Txs_Ranked_Select_Column {\n  /** column name */\n  Height = 'height',\n  /** column name */\n  Neuron = 'neuron',\n  /** column name */\n  Rank = 'rank',\n  /** column name */\n  Type = 'type',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Txs_Ranked_Stddev_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Txs_Ranked_Stddev_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Txs_Ranked_Stddev_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"txs_ranked\" */\nexport type Txs_Ranked_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Txs_Ranked_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Txs_Ranked_Stream_Cursor_Value_Input = {\n  height?: InputMaybe<Scalars['bigint']['input']>;\n  neuron?: InputMaybe<Scalars['String']['input']>;\n  rank?: InputMaybe<Scalars['bigint']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Txs_Ranked_Sum_Fields = {\n  height?: Maybe<Scalars['bigint']['output']>;\n  rank?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Txs_Ranked_Var_Pop_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Txs_Ranked_Var_Samp_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Txs_Ranked_Variance_Fields = {\n  height?: Maybe<Scalars['Float']['output']>;\n  rank?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"uptime\" */\nexport type Uptime = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  uptime?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregated selection of \"uptime\" */\nexport type Uptime_Aggregate = {\n  aggregate?: Maybe<Uptime_Aggregate_Fields>;\n  nodes: Array<Uptime>;\n};\n\n/** aggregate fields of \"uptime\" */\nexport type Uptime_Aggregate_Fields = {\n  avg?: Maybe<Uptime_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Uptime_Max_Fields>;\n  min?: Maybe<Uptime_Min_Fields>;\n  stddev?: Maybe<Uptime_Stddev_Fields>;\n  stddev_pop?: Maybe<Uptime_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Uptime_Stddev_Samp_Fields>;\n  sum?: Maybe<Uptime_Sum_Fields>;\n  var_pop?: Maybe<Uptime_Var_Pop_Fields>;\n  var_samp?: Maybe<Uptime_Var_Samp_Fields>;\n  variance?: Maybe<Uptime_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"uptime\" */\nexport type Uptime_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Uptime_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Uptime_Avg_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"uptime\". All fields are combined with a logical 'AND'. */\nexport type Uptime_Bool_Exp = {\n  _and?: InputMaybe<Array<Uptime_Bool_Exp>>;\n  _not?: InputMaybe<Uptime_Bool_Exp>;\n  _or?: InputMaybe<Array<Uptime_Bool_Exp>>;\n  consensus_address?: InputMaybe<String_Comparison_Exp>;\n  consensus_pubkey?: InputMaybe<String_Comparison_Exp>;\n  pre_commits?: InputMaybe<Bigint_Comparison_Exp>;\n  uptime?: InputMaybe<Numeric_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Uptime_Max_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  uptime?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Uptime_Min_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  uptime?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** Ordering options when selecting data from \"uptime\". */\nexport type Uptime_Order_By = {\n  consensus_address?: InputMaybe<Order_By>;\n  consensus_pubkey?: InputMaybe<Order_By>;\n  pre_commits?: InputMaybe<Order_By>;\n  uptime?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"uptime\" */\nexport enum Uptime_Select_Column {\n  /** column name */\n  ConsensusAddress = 'consensus_address',\n  /** column name */\n  ConsensusPubkey = 'consensus_pubkey',\n  /** column name */\n  PreCommits = 'pre_commits',\n  /** column name */\n  Uptime = 'uptime'\n}\n\n/** aggregate stddev on columns */\nexport type Uptime_Stddev_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Uptime_Stddev_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Uptime_Stddev_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"uptime\" */\nexport type Uptime_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Uptime_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Uptime_Stream_Cursor_Value_Input = {\n  consensus_address?: InputMaybe<Scalars['String']['input']>;\n  consensus_pubkey?: InputMaybe<Scalars['String']['input']>;\n  pre_commits?: InputMaybe<Scalars['bigint']['input']>;\n  uptime?: InputMaybe<Scalars['numeric']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Uptime_Sum_Fields = {\n  pre_commits?: Maybe<Scalars['bigint']['output']>;\n  uptime?: Maybe<Scalars['numeric']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Uptime_Var_Pop_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Uptime_Var_Samp_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Uptime_Variance_Fields = {\n  pre_commits?: Maybe<Scalars['Float']['output']>;\n  uptime?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"validator\" */\nexport type Validator = {\n  /** An array relationship */\n  blocks: Array<Block>;\n  /** An aggregate relationship */\n  blocks_aggregate: Block_Aggregate;\n  consensus_address: Scalars['String']['output'];\n  consensus_pubkey: Scalars['String']['output'];\n  /** An array relationship */\n  pre_commits: Array<Pre_Commit>;\n  /** An aggregate relationship */\n  pre_commits_aggregate: Pre_Commit_Aggregate;\n};\n\n\n/** columns and relationships of \"validator\" */\nexport type ValidatorBlocksArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"validator\" */\nexport type ValidatorBlocks_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Block_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Block_Order_By>>;\n  where?: InputMaybe<Block_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"validator\" */\nexport type ValidatorPre_CommitsArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"validator\" */\nexport type ValidatorPre_Commits_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Pre_Commit_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Pre_Commit_Order_By>>;\n  where?: InputMaybe<Pre_Commit_Bool_Exp>;\n};\n\n/** aggregated selection of \"validator\" */\nexport type Validator_Aggregate = {\n  aggregate?: Maybe<Validator_Aggregate_Fields>;\n  nodes: Array<Validator>;\n};\n\n/** aggregate fields of \"validator\" */\nexport type Validator_Aggregate_Fields = {\n  count: Scalars['Int']['output'];\n  max?: Maybe<Validator_Max_Fields>;\n  min?: Maybe<Validator_Min_Fields>;\n};\n\n\n/** aggregate fields of \"validator\" */\nexport type Validator_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Validator_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** Boolean expression to filter rows from the table \"validator\". All fields are combined with a logical 'AND'. */\nexport type Validator_Bool_Exp = {\n  _and?: InputMaybe<Array<Validator_Bool_Exp>>;\n  _not?: InputMaybe<Validator_Bool_Exp>;\n  _or?: InputMaybe<Array<Validator_Bool_Exp>>;\n  blocks?: InputMaybe<Block_Bool_Exp>;\n  blocks_aggregate?: InputMaybe<Block_Aggregate_Bool_Exp>;\n  consensus_address?: InputMaybe<String_Comparison_Exp>;\n  consensus_pubkey?: InputMaybe<String_Comparison_Exp>;\n  pre_commits?: InputMaybe<Pre_Commit_Bool_Exp>;\n  pre_commits_aggregate?: InputMaybe<Pre_Commit_Aggregate_Bool_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Validator_Max_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Validator_Min_Fields = {\n  consensus_address?: Maybe<Scalars['String']['output']>;\n  consensus_pubkey?: Maybe<Scalars['String']['output']>;\n};\n\n/** Ordering options when selecting data from \"validator\". */\nexport type Validator_Order_By = {\n  blocks_aggregate?: InputMaybe<Block_Aggregate_Order_By>;\n  consensus_address?: InputMaybe<Order_By>;\n  consensus_pubkey?: InputMaybe<Order_By>;\n  pre_commits_aggregate?: InputMaybe<Pre_Commit_Aggregate_Order_By>;\n};\n\n/** select columns of table \"validator\" */\nexport enum Validator_Select_Column {\n  /** column name */\n  ConsensusAddress = 'consensus_address',\n  /** column name */\n  ConsensusPubkey = 'consensus_pubkey'\n}\n\n/** Streaming cursor of the table \"validator\" */\nexport type Validator_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Validator_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Validator_Stream_Cursor_Value_Input = {\n  consensus_address?: InputMaybe<Scalars['String']['input']>;\n  consensus_pubkey?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** columns and relationships of \"vesting_account\" */\nexport type Vesting_Account = {\n  /** An object relationship */\n  account: Account;\n  address: Scalars['String']['output'];\n  end_time: Scalars['timestamp']['output'];\n  id: Scalars['Int']['output'];\n  original_vesting: Array<Scalars['coin']['output']>;\n  start_time?: Maybe<Scalars['timestamp']['output']>;\n  type: Scalars['String']['output'];\n  /** An array relationship */\n  vesting_periods: Array<Vesting_Period>;\n  /** An aggregate relationship */\n  vesting_periods_aggregate: Vesting_Period_Aggregate;\n};\n\n\n/** columns and relationships of \"vesting_account\" */\nexport type Vesting_AccountVesting_PeriodsArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Period_Order_By>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n\n/** columns and relationships of \"vesting_account\" */\nexport type Vesting_AccountVesting_Periods_AggregateArgs = {\n  distinct_on?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  order_by?: InputMaybe<Array<Vesting_Period_Order_By>>;\n  where?: InputMaybe<Vesting_Period_Bool_Exp>;\n};\n\n/** aggregated selection of \"vesting_account\" */\nexport type Vesting_Account_Aggregate = {\n  aggregate?: Maybe<Vesting_Account_Aggregate_Fields>;\n  nodes: Array<Vesting_Account>;\n};\n\nexport type Vesting_Account_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Vesting_Account_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Vesting_Account_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Vesting_Account_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"vesting_account\" */\nexport type Vesting_Account_Aggregate_Fields = {\n  avg?: Maybe<Vesting_Account_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Vesting_Account_Max_Fields>;\n  min?: Maybe<Vesting_Account_Min_Fields>;\n  stddev?: Maybe<Vesting_Account_Stddev_Fields>;\n  stddev_pop?: Maybe<Vesting_Account_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Vesting_Account_Stddev_Samp_Fields>;\n  sum?: Maybe<Vesting_Account_Sum_Fields>;\n  var_pop?: Maybe<Vesting_Account_Var_Pop_Fields>;\n  var_samp?: Maybe<Vesting_Account_Var_Samp_Fields>;\n  variance?: Maybe<Vesting_Account_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"vesting_account\" */\nexport type Vesting_Account_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Vesting_Account_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"vesting_account\" */\nexport type Vesting_Account_Aggregate_Order_By = {\n  avg?: InputMaybe<Vesting_Account_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Vesting_Account_Max_Order_By>;\n  min?: InputMaybe<Vesting_Account_Min_Order_By>;\n  stddev?: InputMaybe<Vesting_Account_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Vesting_Account_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Vesting_Account_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Vesting_Account_Sum_Order_By>;\n  var_pop?: InputMaybe<Vesting_Account_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Vesting_Account_Var_Samp_Order_By>;\n  variance?: InputMaybe<Vesting_Account_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Vesting_Account_Avg_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Avg_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"vesting_account\". All fields are combined with a logical 'AND'. */\nexport type Vesting_Account_Bool_Exp = {\n  _and?: InputMaybe<Array<Vesting_Account_Bool_Exp>>;\n  _not?: InputMaybe<Vesting_Account_Bool_Exp>;\n  _or?: InputMaybe<Array<Vesting_Account_Bool_Exp>>;\n  account?: InputMaybe<Account_Bool_Exp>;\n  address?: InputMaybe<String_Comparison_Exp>;\n  end_time?: InputMaybe<Timestamp_Comparison_Exp>;\n  id?: InputMaybe<Int_Comparison_Exp>;\n  original_vesting?: InputMaybe<Coin_Array_Comparison_Exp>;\n  start_time?: InputMaybe<Timestamp_Comparison_Exp>;\n  type?: InputMaybe<String_Comparison_Exp>;\n  vesting_periods?: InputMaybe<Vesting_Period_Bool_Exp>;\n  vesting_periods_aggregate?: InputMaybe<Vesting_Period_Aggregate_Bool_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Vesting_Account_Max_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  end_time?: Maybe<Scalars['timestamp']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  original_vesting?: Maybe<Array<Scalars['coin']['output']>>;\n  start_time?: Maybe<Scalars['timestamp']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by max() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Max_Order_By = {\n  address?: InputMaybe<Order_By>;\n  end_time?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  original_vesting?: InputMaybe<Order_By>;\n  start_time?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Vesting_Account_Min_Fields = {\n  address?: Maybe<Scalars['String']['output']>;\n  end_time?: Maybe<Scalars['timestamp']['output']>;\n  id?: Maybe<Scalars['Int']['output']>;\n  original_vesting?: Maybe<Array<Scalars['coin']['output']>>;\n  start_time?: Maybe<Scalars['timestamp']['output']>;\n  type?: Maybe<Scalars['String']['output']>;\n};\n\n/** order by min() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Min_Order_By = {\n  address?: InputMaybe<Order_By>;\n  end_time?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  original_vesting?: InputMaybe<Order_By>;\n  start_time?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"vesting_account\". */\nexport type Vesting_Account_Order_By = {\n  account?: InputMaybe<Account_Order_By>;\n  address?: InputMaybe<Order_By>;\n  end_time?: InputMaybe<Order_By>;\n  id?: InputMaybe<Order_By>;\n  original_vesting?: InputMaybe<Order_By>;\n  start_time?: InputMaybe<Order_By>;\n  type?: InputMaybe<Order_By>;\n  vesting_periods_aggregate?: InputMaybe<Vesting_Period_Aggregate_Order_By>;\n};\n\n/** select columns of table \"vesting_account\" */\nexport enum Vesting_Account_Select_Column {\n  /** column name */\n  Address = 'address',\n  /** column name */\n  EndTime = 'end_time',\n  /** column name */\n  Id = 'id',\n  /** column name */\n  OriginalVesting = 'original_vesting',\n  /** column name */\n  StartTime = 'start_time',\n  /** column name */\n  Type = 'type'\n}\n\n/** aggregate stddev on columns */\nexport type Vesting_Account_Stddev_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Stddev_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Vesting_Account_Stddev_Pop_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Stddev_Pop_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Vesting_Account_Stddev_Samp_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Stddev_Samp_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"vesting_account\" */\nexport type Vesting_Account_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Vesting_Account_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Vesting_Account_Stream_Cursor_Value_Input = {\n  address?: InputMaybe<Scalars['String']['input']>;\n  end_time?: InputMaybe<Scalars['timestamp']['input']>;\n  id?: InputMaybe<Scalars['Int']['input']>;\n  original_vesting?: InputMaybe<Array<Scalars['coin']['input']>>;\n  start_time?: InputMaybe<Scalars['timestamp']['input']>;\n  type?: InputMaybe<Scalars['String']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Vesting_Account_Sum_Fields = {\n  id?: Maybe<Scalars['Int']['output']>;\n};\n\n/** order by sum() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Sum_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Vesting_Account_Var_Pop_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Var_Pop_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Vesting_Account_Var_Samp_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Var_Samp_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Vesting_Account_Variance_Fields = {\n  id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"vesting_account\" */\nexport type Vesting_Account_Variance_Order_By = {\n  id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"vesting_period\" */\nexport type Vesting_Period = {\n  amount: Array<Scalars['coin']['output']>;\n  length: Scalars['bigint']['output'];\n  period_order: Scalars['bigint']['output'];\n  /** An object relationship */\n  vesting_account: Vesting_Account;\n  vesting_account_id: Scalars['bigint']['output'];\n};\n\n/** aggregated selection of \"vesting_period\" */\nexport type Vesting_Period_Aggregate = {\n  aggregate?: Maybe<Vesting_Period_Aggregate_Fields>;\n  nodes: Array<Vesting_Period>;\n};\n\nexport type Vesting_Period_Aggregate_Bool_Exp = {\n  count?: InputMaybe<Vesting_Period_Aggregate_Bool_Exp_Count>;\n};\n\nexport type Vesting_Period_Aggregate_Bool_Exp_Count = {\n  arguments?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n  filter?: InputMaybe<Vesting_Period_Bool_Exp>;\n  predicate: Int_Comparison_Exp;\n};\n\n/** aggregate fields of \"vesting_period\" */\nexport type Vesting_Period_Aggregate_Fields = {\n  avg?: Maybe<Vesting_Period_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Vesting_Period_Max_Fields>;\n  min?: Maybe<Vesting_Period_Min_Fields>;\n  stddev?: Maybe<Vesting_Period_Stddev_Fields>;\n  stddev_pop?: Maybe<Vesting_Period_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Vesting_Period_Stddev_Samp_Fields>;\n  sum?: Maybe<Vesting_Period_Sum_Fields>;\n  var_pop?: Maybe<Vesting_Period_Var_Pop_Fields>;\n  var_samp?: Maybe<Vesting_Period_Var_Samp_Fields>;\n  variance?: Maybe<Vesting_Period_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"vesting_period\" */\nexport type Vesting_Period_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Vesting_Period_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** order by aggregate values of table \"vesting_period\" */\nexport type Vesting_Period_Aggregate_Order_By = {\n  avg?: InputMaybe<Vesting_Period_Avg_Order_By>;\n  count?: InputMaybe<Order_By>;\n  max?: InputMaybe<Vesting_Period_Max_Order_By>;\n  min?: InputMaybe<Vesting_Period_Min_Order_By>;\n  stddev?: InputMaybe<Vesting_Period_Stddev_Order_By>;\n  stddev_pop?: InputMaybe<Vesting_Period_Stddev_Pop_Order_By>;\n  stddev_samp?: InputMaybe<Vesting_Period_Stddev_Samp_Order_By>;\n  sum?: InputMaybe<Vesting_Period_Sum_Order_By>;\n  var_pop?: InputMaybe<Vesting_Period_Var_Pop_Order_By>;\n  var_samp?: InputMaybe<Vesting_Period_Var_Samp_Order_By>;\n  variance?: InputMaybe<Vesting_Period_Variance_Order_By>;\n};\n\n/** aggregate avg on columns */\nexport type Vesting_Period_Avg_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by avg() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Avg_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** Boolean expression to filter rows from the table \"vesting_period\". All fields are combined with a logical 'AND'. */\nexport type Vesting_Period_Bool_Exp = {\n  _and?: InputMaybe<Array<Vesting_Period_Bool_Exp>>;\n  _not?: InputMaybe<Vesting_Period_Bool_Exp>;\n  _or?: InputMaybe<Array<Vesting_Period_Bool_Exp>>;\n  amount?: InputMaybe<Coin_Array_Comparison_Exp>;\n  length?: InputMaybe<Bigint_Comparison_Exp>;\n  period_order?: InputMaybe<Bigint_Comparison_Exp>;\n  vesting_account?: InputMaybe<Vesting_Account_Bool_Exp>;\n  vesting_account_id?: InputMaybe<Bigint_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Vesting_Period_Max_Fields = {\n  amount?: Maybe<Array<Scalars['coin']['output']>>;\n  length?: Maybe<Scalars['bigint']['output']>;\n  period_order?: Maybe<Scalars['bigint']['output']>;\n  vesting_account_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by max() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Max_Order_By = {\n  amount?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate min on columns */\nexport type Vesting_Period_Min_Fields = {\n  amount?: Maybe<Array<Scalars['coin']['output']>>;\n  length?: Maybe<Scalars['bigint']['output']>;\n  period_order?: Maybe<Scalars['bigint']['output']>;\n  vesting_account_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by min() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Min_Order_By = {\n  amount?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** Ordering options when selecting data from \"vesting_period\". */\nexport type Vesting_Period_Order_By = {\n  amount?: InputMaybe<Order_By>;\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account?: InputMaybe<Vesting_Account_Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"vesting_period\" */\nexport enum Vesting_Period_Select_Column {\n  /** column name */\n  Amount = 'amount',\n  /** column name */\n  Length = 'length',\n  /** column name */\n  PeriodOrder = 'period_order',\n  /** column name */\n  VestingAccountId = 'vesting_account_id'\n}\n\n/** aggregate stddev on columns */\nexport type Vesting_Period_Stddev_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Stddev_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Vesting_Period_Stddev_Pop_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_pop() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Stddev_Pop_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Vesting_Period_Stddev_Samp_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by stddev_samp() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Stddev_Samp_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** Streaming cursor of the table \"vesting_period\" */\nexport type Vesting_Period_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Vesting_Period_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Vesting_Period_Stream_Cursor_Value_Input = {\n  amount?: InputMaybe<Array<Scalars['coin']['input']>>;\n  length?: InputMaybe<Scalars['bigint']['input']>;\n  period_order?: InputMaybe<Scalars['bigint']['input']>;\n  vesting_account_id?: InputMaybe<Scalars['bigint']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Vesting_Period_Sum_Fields = {\n  length?: Maybe<Scalars['bigint']['output']>;\n  period_order?: Maybe<Scalars['bigint']['output']>;\n  vesting_account_id?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** order by sum() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Sum_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_pop on columns */\nexport type Vesting_Period_Var_Pop_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_pop() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Var_Pop_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate var_samp on columns */\nexport type Vesting_Period_Var_Samp_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by var_samp() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Var_Samp_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** aggregate variance on columns */\nexport type Vesting_Period_Variance_Fields = {\n  length?: Maybe<Scalars['Float']['output']>;\n  period_order?: Maybe<Scalars['Float']['output']>;\n  vesting_account_id?: Maybe<Scalars['Float']['output']>;\n};\n\n/** order by variance() on columns of table \"vesting_period\" */\nexport type Vesting_Period_Variance_Order_By = {\n  length?: InputMaybe<Order_By>;\n  period_order?: InputMaybe<Order_By>;\n  vesting_account_id?: InputMaybe<Order_By>;\n};\n\n/** columns and relationships of \"week_redelegation\" */\nexport type Week_Redelegation = {\n  redelegation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"week_redelegation\" */\nexport type Week_Redelegation_Aggregate = {\n  aggregate?: Maybe<Week_Redelegation_Aggregate_Fields>;\n  nodes: Array<Week_Redelegation>;\n};\n\n/** aggregate fields of \"week_redelegation\" */\nexport type Week_Redelegation_Aggregate_Fields = {\n  avg?: Maybe<Week_Redelegation_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Week_Redelegation_Max_Fields>;\n  min?: Maybe<Week_Redelegation_Min_Fields>;\n  stddev?: Maybe<Week_Redelegation_Stddev_Fields>;\n  stddev_pop?: Maybe<Week_Redelegation_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Week_Redelegation_Stddev_Samp_Fields>;\n  sum?: Maybe<Week_Redelegation_Sum_Fields>;\n  var_pop?: Maybe<Week_Redelegation_Var_Pop_Fields>;\n  var_samp?: Maybe<Week_Redelegation_Var_Samp_Fields>;\n  variance?: Maybe<Week_Redelegation_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"week_redelegation\" */\nexport type Week_Redelegation_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Week_Redelegation_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Week_Redelegation_Avg_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"week_redelegation\". All fields are combined with a logical 'AND'. */\nexport type Week_Redelegation_Bool_Exp = {\n  _and?: InputMaybe<Array<Week_Redelegation_Bool_Exp>>;\n  _not?: InputMaybe<Week_Redelegation_Bool_Exp>;\n  _or?: InputMaybe<Array<Week_Redelegation_Bool_Exp>>;\n  redelegation?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Week_Redelegation_Max_Fields = {\n  redelegation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Week_Redelegation_Min_Fields = {\n  redelegation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"week_redelegation\". */\nexport type Week_Redelegation_Order_By = {\n  redelegation?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"week_redelegation\" */\nexport enum Week_Redelegation_Select_Column {\n  /** column name */\n  Redelegation = 'redelegation',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Week_Redelegation_Stddev_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Week_Redelegation_Stddev_Pop_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Week_Redelegation_Stddev_Samp_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"week_redelegation\" */\nexport type Week_Redelegation_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Week_Redelegation_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Week_Redelegation_Stream_Cursor_Value_Input = {\n  redelegation?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Week_Redelegation_Sum_Fields = {\n  redelegation?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Week_Redelegation_Var_Pop_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Week_Redelegation_Var_Samp_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Week_Redelegation_Variance_Fields = {\n  redelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** columns and relationships of \"week_undelegation\" */\nexport type Week_Undelegation = {\n  undelegation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregated selection of \"week_undelegation\" */\nexport type Week_Undelegation_Aggregate = {\n  aggregate?: Maybe<Week_Undelegation_Aggregate_Fields>;\n  nodes: Array<Week_Undelegation>;\n};\n\n/** aggregate fields of \"week_undelegation\" */\nexport type Week_Undelegation_Aggregate_Fields = {\n  avg?: Maybe<Week_Undelegation_Avg_Fields>;\n  count: Scalars['Int']['output'];\n  max?: Maybe<Week_Undelegation_Max_Fields>;\n  min?: Maybe<Week_Undelegation_Min_Fields>;\n  stddev?: Maybe<Week_Undelegation_Stddev_Fields>;\n  stddev_pop?: Maybe<Week_Undelegation_Stddev_Pop_Fields>;\n  stddev_samp?: Maybe<Week_Undelegation_Stddev_Samp_Fields>;\n  sum?: Maybe<Week_Undelegation_Sum_Fields>;\n  var_pop?: Maybe<Week_Undelegation_Var_Pop_Fields>;\n  var_samp?: Maybe<Week_Undelegation_Var_Samp_Fields>;\n  variance?: Maybe<Week_Undelegation_Variance_Fields>;\n};\n\n\n/** aggregate fields of \"week_undelegation\" */\nexport type Week_Undelegation_Aggregate_FieldsCountArgs = {\n  columns?: InputMaybe<Array<Week_Undelegation_Select_Column>>;\n  distinct?: InputMaybe<Scalars['Boolean']['input']>;\n};\n\n/** aggregate avg on columns */\nexport type Week_Undelegation_Avg_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Boolean expression to filter rows from the table \"week_undelegation\". All fields are combined with a logical 'AND'. */\nexport type Week_Undelegation_Bool_Exp = {\n  _and?: InputMaybe<Array<Week_Undelegation_Bool_Exp>>;\n  _not?: InputMaybe<Week_Undelegation_Bool_Exp>;\n  _or?: InputMaybe<Array<Week_Undelegation_Bool_Exp>>;\n  undelegation?: InputMaybe<Bigint_Comparison_Exp>;\n  week?: InputMaybe<Date_Comparison_Exp>;\n};\n\n/** aggregate max on columns */\nexport type Week_Undelegation_Max_Fields = {\n  undelegation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** aggregate min on columns */\nexport type Week_Undelegation_Min_Fields = {\n  undelegation?: Maybe<Scalars['bigint']['output']>;\n  week?: Maybe<Scalars['date']['output']>;\n};\n\n/** Ordering options when selecting data from \"week_undelegation\". */\nexport type Week_Undelegation_Order_By = {\n  undelegation?: InputMaybe<Order_By>;\n  week?: InputMaybe<Order_By>;\n};\n\n/** select columns of table \"week_undelegation\" */\nexport enum Week_Undelegation_Select_Column {\n  /** column name */\n  Undelegation = 'undelegation',\n  /** column name */\n  Week = 'week'\n}\n\n/** aggregate stddev on columns */\nexport type Week_Undelegation_Stddev_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_pop on columns */\nexport type Week_Undelegation_Stddev_Pop_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate stddev_samp on columns */\nexport type Week_Undelegation_Stddev_Samp_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** Streaming cursor of the table \"week_undelegation\" */\nexport type Week_Undelegation_Stream_Cursor_Input = {\n  /** Stream column input with initial value */\n  initial_value: Week_Undelegation_Stream_Cursor_Value_Input;\n  /** cursor ordering */\n  ordering?: InputMaybe<Cursor_Ordering>;\n};\n\n/** Initial value of the column from where the streaming should start */\nexport type Week_Undelegation_Stream_Cursor_Value_Input = {\n  undelegation?: InputMaybe<Scalars['bigint']['input']>;\n  week?: InputMaybe<Scalars['date']['input']>;\n};\n\n/** aggregate sum on columns */\nexport type Week_Undelegation_Sum_Fields = {\n  undelegation?: Maybe<Scalars['bigint']['output']>;\n};\n\n/** aggregate var_pop on columns */\nexport type Week_Undelegation_Var_Pop_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate var_samp on columns */\nexport type Week_Undelegation_Var_Samp_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\n/** aggregate variance on columns */\nexport type Week_Undelegation_Variance_Fields = {\n  undelegation?: Maybe<Scalars['Float']['output']>;\n};\n\nexport type TransactionsSubscriptionVariables = Exact<{ [key: string]: never; }>;\n\n\nexport type TransactionsSubscription = { transaction: Array<{ success: boolean, messages: any, height: any, hash: string }> };\n\nexport type AccountCountQueryVariables = Exact<{ [key: string]: never; }>;\n\n\nexport type AccountCountQuery = { account_aggregate: { aggregate?: { count: number } | null } };\n\nexport type BlockByHeightQueryVariables = Exact<{\n  blockId?: InputMaybe<Scalars['bigint']['input']>;\n}>;\n\n\nexport type BlockByHeightQuery = { block: Array<{ hash: string, height: any, proposer_address?: string | null, timestamp: any, transactions: Array<{ messages: any, hash: string, height: any, success: boolean }> }> };\n\nexport type BlocksQueryVariables = Exact<{\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  where?: InputMaybe<Block_Bool_Exp>;\n}>;\n\n\nexport type BlocksQuery = { block: Array<{ hash: string, height: any, proposer_address?: string | null, timestamp: any, transactions_aggregate: { aggregate?: { count: number } | null } }> };\n\nexport type ContractsCountQueryVariables = Exact<{ [key: string]: never; }>;\n\n\nexport type ContractsCountQuery = { contracts_aggregate: { aggregate?: { count: number } | null } };\n\nexport type CyberlinksByParticleQueryVariables = Exact<{\n  limit?: InputMaybe<Scalars['Int']['input']>;\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  orderBy?: InputMaybe<Array<Cyberlinks_Order_By> | Cyberlinks_Order_By>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n}>;\n\n\nexport type CyberlinksByParticleQuery = { cyberlinks: Array<{ timestamp: any, neuron: string, transaction_hash: string, from: string, to: string }> };\n\nexport type CyberlinksCountByNeuronQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['String']['input']>;\n  particles_from?: InputMaybe<Array<Scalars['String']['input']> | Scalars['String']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n}>;\n\n\nexport type CyberlinksCountByNeuronQuery = { cyberlinks_aggregate: { aggregate?: { count: number } | null } };\n\nexport type CyberlinksCountByParticleQueryVariables = Exact<{\n  cid?: InputMaybe<Scalars['String']['input']>;\n  where?: InputMaybe<Cyberlinks_Bool_Exp>;\n}>;\n\n\nexport type CyberlinksCountByParticleQuery = { cyberlinks_aggregate: { aggregate?: { count: number } | null } };\n\nexport type MessagesByAddressCountQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['_text']['input']>;\n  timestamp?: InputMaybe<Scalars['timestamp']['input']>;\n}>;\n\n\nexport type MessagesByAddressCountQuery = { messages_by_address_aggregate: { aggregate?: { count: number } | null } };\n\nexport type MessagesByAddressSenseQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['_text']['input']>;\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  timestamp_from?: InputMaybe<Scalars['timestamp']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n  order_direction?: InputMaybe<Order_By>;\n}>;\n\n\nexport type MessagesByAddressSenseQuery = { messages_by_address: Array<{ transaction_hash: string, index: any, value: any, type: string, transaction?: { success: boolean, memo?: string | null, block: { timestamp: any, height: any } } | null }> };\n\nexport type MessagesByAddressSenseWsSubscriptionVariables = Exact<{\n  address?: InputMaybe<Scalars['_text']['input']>;\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  timestamp_from?: InputMaybe<Scalars['timestamp']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n  order_direction?: InputMaybe<Order_By>;\n}>;\n\n\nexport type MessagesByAddressSenseWsSubscription = { messages_by_address: Array<{ transaction_hash: string, index: any, value: any, type: string, transaction?: { success: boolean, memo?: string | null, block: { timestamp: any, height: any } } | null }> };\n\nexport type TransactionCountQueryVariables = Exact<{ [key: string]: never; }>;\n\n\nexport type TransactionCountQuery = { transaction_aggregate: { aggregate?: { count: number } | null } };\n\nexport type UptimeByAddressQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['String']['input']>;\n}>;\n\n\nexport type UptimeByAddressQuery = { uptime: Array<{ uptime?: any | null }> };\n\nexport type WasmDashboardPageQueryVariables = Exact<{\n  offset?: InputMaybe<Scalars['Int']['input']>;\n  limit?: InputMaybe<Scalars['Int']['input']>;\n}>;\n\n\nexport type WasmDashboardPageQuery = { contracts: Array<{ address: string, admin: string, code_id: any, creator: string, fees: any, gas: any, label: string, tx: any }>, contracts_aggregate: { aggregate?: { count: number, sum?: { gas?: any | null, fees?: any | null, tx?: any | null } | null } | null } };\n\nexport type MessagesByAddressQueryVariables = Exact<{\n  address?: InputMaybe<Scalars['_text']['input']>;\n  limit?: InputMaybe<Scalars['bigint']['input']>;\n  offset?: InputMaybe<Scalars['bigint']['input']>;\n  types?: InputMaybe<Scalars['_text']['input']>;\n}>;\n\n\nexport type MessagesByAddressQuery = { messages_by_address: Array<{ transaction_hash: string, value: any, type: string, transaction?: { success: boolean, height: any, logs?: any | null, memo?: string | null, block: { timestamp: any } } | null }> };\n\n\nexport const TransactionsDocument = gql`\n    subscription Transactions {\n  transaction(offset: 0, limit: 200, order_by: {height: desc}) {\n    success\n    messages\n    height\n    hash\n  }\n}\n    `;\n\n/**\n * __useTransactionsSubscription__\n *\n * To run a query within a React component, call `useTransactionsSubscription` and pass it any options that fit your needs.\n * When your component renders, `useTransactionsSubscription` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the subscription, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useTransactionsSubscription({\n *   variables: {\n *   },\n * });\n */\nexport function useTransactionsSubscription(baseOptions?: Apollo.SubscriptionHookOptions<TransactionsSubscription, TransactionsSubscriptionVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useSubscription<TransactionsSubscription, TransactionsSubscriptionVariables>(TransactionsDocument, options);\n      }\nexport type TransactionsSubscriptionHookResult = ReturnType<typeof useTransactionsSubscription>;\nexport type TransactionsSubscriptionResult = Apollo.SubscriptionResult<TransactionsSubscription>;\nexport const AccountCountDocument = gql`\n    query accountCount {\n  account_aggregate {\n    aggregate {\n      count(columns: address)\n    }\n  }\n}\n    `;\n\n/**\n * __useAccountCountQuery__\n *\n * To run a query within a React component, call `useAccountCountQuery` and pass it any options that fit your needs.\n * When your component renders, `useAccountCountQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useAccountCountQuery({\n *   variables: {\n *   },\n * });\n */\nexport function useAccountCountQuery(baseOptions?: Apollo.QueryHookOptions<AccountCountQuery, AccountCountQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<AccountCountQuery, AccountCountQueryVariables>(AccountCountDocument, options);\n      }\nexport function useAccountCountLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<AccountCountQuery, AccountCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<AccountCountQuery, AccountCountQueryVariables>(AccountCountDocument, options);\n        }\nexport function useAccountCountSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<AccountCountQuery, AccountCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<AccountCountQuery, AccountCountQueryVariables>(AccountCountDocument, options);\n        }\nexport type AccountCountQueryHookResult = ReturnType<typeof useAccountCountQuery>;\nexport type AccountCountLazyQueryHookResult = ReturnType<typeof useAccountCountLazyQuery>;\nexport type AccountCountSuspenseQueryHookResult = ReturnType<typeof useAccountCountSuspenseQuery>;\nexport type AccountCountQueryResult = Apollo.QueryResult<AccountCountQuery, AccountCountQueryVariables>;\nexport const BlockByHeightDocument = gql`\n    query blockByHeight($blockId: bigint) {\n  block(where: {height: {_eq: $blockId}}) {\n    hash\n    height\n    proposer_address\n    timestamp\n    transactions {\n      messages\n      hash\n      height\n      success\n    }\n  }\n}\n    `;\n\n/**\n * __useBlockByHeightQuery__\n *\n * To run a query within a React component, call `useBlockByHeightQuery` and pass it any options that fit your needs.\n * When your component renders, `useBlockByHeightQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useBlockByHeightQuery({\n *   variables: {\n *      blockId: // value for 'blockId'\n *   },\n * });\n */\nexport function useBlockByHeightQuery(baseOptions?: Apollo.QueryHookOptions<BlockByHeightQuery, BlockByHeightQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<BlockByHeightQuery, BlockByHeightQueryVariables>(BlockByHeightDocument, options);\n      }\nexport function useBlockByHeightLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<BlockByHeightQuery, BlockByHeightQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<BlockByHeightQuery, BlockByHeightQueryVariables>(BlockByHeightDocument, options);\n        }\nexport function useBlockByHeightSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<BlockByHeightQuery, BlockByHeightQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<BlockByHeightQuery, BlockByHeightQueryVariables>(BlockByHeightDocument, options);\n        }\nexport type BlockByHeightQueryHookResult = ReturnType<typeof useBlockByHeightQuery>;\nexport type BlockByHeightLazyQueryHookResult = ReturnType<typeof useBlockByHeightLazyQuery>;\nexport type BlockByHeightSuspenseQueryHookResult = ReturnType<typeof useBlockByHeightSuspenseQuery>;\nexport type BlockByHeightQueryResult = Apollo.QueryResult<BlockByHeightQuery, BlockByHeightQueryVariables>;\nexport const BlocksDocument = gql`\n    query blocks($limit: Int, $offset: Int, $where: block_bool_exp) {\n  block(where: $where, limit: $limit, offset: $offset, order_by: {height: desc}) {\n    hash\n    height\n    proposer_address\n    transactions_aggregate {\n      aggregate {\n        count\n      }\n    }\n    timestamp\n  }\n}\n    `;\n\n/**\n * __useBlocksQuery__\n *\n * To run a query within a React component, call `useBlocksQuery` and pass it any options that fit your needs.\n * When your component renders, `useBlocksQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useBlocksQuery({\n *   variables: {\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      where: // value for 'where'\n *   },\n * });\n */\nexport function useBlocksQuery(baseOptions?: Apollo.QueryHookOptions<BlocksQuery, BlocksQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<BlocksQuery, BlocksQueryVariables>(BlocksDocument, options);\n      }\nexport function useBlocksLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<BlocksQuery, BlocksQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<BlocksQuery, BlocksQueryVariables>(BlocksDocument, options);\n        }\nexport function useBlocksSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<BlocksQuery, BlocksQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<BlocksQuery, BlocksQueryVariables>(BlocksDocument, options);\n        }\nexport type BlocksQueryHookResult = ReturnType<typeof useBlocksQuery>;\nexport type BlocksLazyQueryHookResult = ReturnType<typeof useBlocksLazyQuery>;\nexport type BlocksSuspenseQueryHookResult = ReturnType<typeof useBlocksSuspenseQuery>;\nexport type BlocksQueryResult = Apollo.QueryResult<BlocksQuery, BlocksQueryVariables>;\nexport const ContractsCountDocument = gql`\n    query contractsCount {\n  contracts_aggregate {\n    aggregate {\n      count\n    }\n  }\n}\n    `;\n\n/**\n * __useContractsCountQuery__\n *\n * To run a query within a React component, call `useContractsCountQuery` and pass it any options that fit your needs.\n * When your component renders, `useContractsCountQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useContractsCountQuery({\n *   variables: {\n *   },\n * });\n */\nexport function useContractsCountQuery(baseOptions?: Apollo.QueryHookOptions<ContractsCountQuery, ContractsCountQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<ContractsCountQuery, ContractsCountQueryVariables>(ContractsCountDocument, options);\n      }\nexport function useContractsCountLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<ContractsCountQuery, ContractsCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<ContractsCountQuery, ContractsCountQueryVariables>(ContractsCountDocument, options);\n        }\nexport function useContractsCountSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<ContractsCountQuery, ContractsCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<ContractsCountQuery, ContractsCountQueryVariables>(ContractsCountDocument, options);\n        }\nexport type ContractsCountQueryHookResult = ReturnType<typeof useContractsCountQuery>;\nexport type ContractsCountLazyQueryHookResult = ReturnType<typeof useContractsCountLazyQuery>;\nexport type ContractsCountSuspenseQueryHookResult = ReturnType<typeof useContractsCountSuspenseQuery>;\nexport type ContractsCountQueryResult = Apollo.QueryResult<ContractsCountQuery, ContractsCountQueryVariables>;\nexport const CyberlinksByParticleDocument = gql`\n    query CyberlinksByParticle($limit: Int, $offset: Int, $orderBy: [cyberlinks_order_by!], $where: cyberlinks_bool_exp) {\n  cyberlinks(limit: $limit, offset: $offset, order_by: $orderBy, where: $where) {\n    from: particle_from\n    to: particle_to\n    timestamp\n    neuron\n    transaction_hash\n  }\n}\n    `;\n\n/**\n * __useCyberlinksByParticleQuery__\n *\n * To run a query within a React component, call `useCyberlinksByParticleQuery` and pass it any options that fit your needs.\n * When your component renders, `useCyberlinksByParticleQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useCyberlinksByParticleQuery({\n *   variables: {\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      orderBy: // value for 'orderBy'\n *      where: // value for 'where'\n *   },\n * });\n */\nexport function useCyberlinksByParticleQuery(baseOptions?: Apollo.QueryHookOptions<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>(CyberlinksByParticleDocument, options);\n      }\nexport function useCyberlinksByParticleLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>(CyberlinksByParticleDocument, options);\n        }\nexport function useCyberlinksByParticleSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>(CyberlinksByParticleDocument, options);\n        }\nexport type CyberlinksByParticleQueryHookResult = ReturnType<typeof useCyberlinksByParticleQuery>;\nexport type CyberlinksByParticleLazyQueryHookResult = ReturnType<typeof useCyberlinksByParticleLazyQuery>;\nexport type CyberlinksByParticleSuspenseQueryHookResult = ReturnType<typeof useCyberlinksByParticleSuspenseQuery>;\nexport type CyberlinksByParticleQueryResult = Apollo.QueryResult<CyberlinksByParticleQuery, CyberlinksByParticleQueryVariables>;\nexport const CyberlinksCountByNeuronDocument = gql`\n    query CyberlinksCountByNeuron($address: String, $particles_from: [String!], $timestamp: timestamp) {\n  cyberlinks_aggregate(\n    where: {_and: [{neuron: {_eq: $address}}, {particle_from: {_in: $particles_from}}, {timestamp: {_gt: $timestamp}}]}\n  ) {\n    aggregate {\n      count\n    }\n  }\n}\n    `;\n\n/**\n * __useCyberlinksCountByNeuronQuery__\n *\n * To run a query within a React component, call `useCyberlinksCountByNeuronQuery` and pass it any options that fit your needs.\n * When your component renders, `useCyberlinksCountByNeuronQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useCyberlinksCountByNeuronQuery({\n *   variables: {\n *      address: // value for 'address'\n *      particles_from: // value for 'particles_from'\n *      timestamp: // value for 'timestamp'\n *   },\n * });\n */\nexport function useCyberlinksCountByNeuronQuery(baseOptions?: Apollo.QueryHookOptions<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>(CyberlinksCountByNeuronDocument, options);\n      }\nexport function useCyberlinksCountByNeuronLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>(CyberlinksCountByNeuronDocument, options);\n        }\nexport function useCyberlinksCountByNeuronSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>(CyberlinksCountByNeuronDocument, options);\n        }\nexport type CyberlinksCountByNeuronQueryHookResult = ReturnType<typeof useCyberlinksCountByNeuronQuery>;\nexport type CyberlinksCountByNeuronLazyQueryHookResult = ReturnType<typeof useCyberlinksCountByNeuronLazyQuery>;\nexport type CyberlinksCountByNeuronSuspenseQueryHookResult = ReturnType<typeof useCyberlinksCountByNeuronSuspenseQuery>;\nexport type CyberlinksCountByNeuronQueryResult = Apollo.QueryResult<CyberlinksCountByNeuronQuery, CyberlinksCountByNeuronQueryVariables>;\nexport const CyberlinksCountByParticleDocument = gql`\n    query cyberlinksCountByParticle($cid: String, $where: cyberlinks_bool_exp) {\n  cyberlinks_aggregate(where: $where) {\n    aggregate {\n      count\n    }\n  }\n}\n    `;\n\n/**\n * __useCyberlinksCountByParticleQuery__\n *\n * To run a query within a React component, call `useCyberlinksCountByParticleQuery` and pass it any options that fit your needs.\n * When your component renders, `useCyberlinksCountByParticleQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useCyberlinksCountByParticleQuery({\n *   variables: {\n *      cid: // value for 'cid'\n *      where: // value for 'where'\n *   },\n * });\n */\nexport function useCyberlinksCountByParticleQuery(baseOptions?: Apollo.QueryHookOptions<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>(CyberlinksCountByParticleDocument, options);\n      }\nexport function useCyberlinksCountByParticleLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>(CyberlinksCountByParticleDocument, options);\n        }\nexport function useCyberlinksCountByParticleSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>(CyberlinksCountByParticleDocument, options);\n        }\nexport type CyberlinksCountByParticleQueryHookResult = ReturnType<typeof useCyberlinksCountByParticleQuery>;\nexport type CyberlinksCountByParticleLazyQueryHookResult = ReturnType<typeof useCyberlinksCountByParticleLazyQuery>;\nexport type CyberlinksCountByParticleSuspenseQueryHookResult = ReturnType<typeof useCyberlinksCountByParticleSuspenseQuery>;\nexport type CyberlinksCountByParticleQueryResult = Apollo.QueryResult<CyberlinksCountByParticleQuery, CyberlinksCountByParticleQueryVariables>;\nexport const MessagesByAddressCountDocument = gql`\n    query MessagesByAddressCount($address: _text, $timestamp: timestamp) {\n  messages_by_address_aggregate(\n    args: {addresses: $address, limit: \"100000000\", offset: \"0\", types: \"{}\"}\n    where: {transaction: {block: {timestamp: {_gt: $timestamp}}}}\n  ) {\n    aggregate {\n      count\n    }\n  }\n}\n    `;\n\n/**\n * __useMessagesByAddressCountQuery__\n *\n * To run a query within a React component, call `useMessagesByAddressCountQuery` and pass it any options that fit your needs.\n * When your component renders, `useMessagesByAddressCountQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useMessagesByAddressCountQuery({\n *   variables: {\n *      address: // value for 'address'\n *      timestamp: // value for 'timestamp'\n *   },\n * });\n */\nexport function useMessagesByAddressCountQuery(baseOptions?: Apollo.QueryHookOptions<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>(MessagesByAddressCountDocument, options);\n      }\nexport function useMessagesByAddressCountLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>(MessagesByAddressCountDocument, options);\n        }\nexport function useMessagesByAddressCountSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>(MessagesByAddressCountDocument, options);\n        }\nexport type MessagesByAddressCountQueryHookResult = ReturnType<typeof useMessagesByAddressCountQuery>;\nexport type MessagesByAddressCountLazyQueryHookResult = ReturnType<typeof useMessagesByAddressCountLazyQuery>;\nexport type MessagesByAddressCountSuspenseQueryHookResult = ReturnType<typeof useMessagesByAddressCountSuspenseQuery>;\nexport type MessagesByAddressCountQueryResult = Apollo.QueryResult<MessagesByAddressCountQuery, MessagesByAddressCountQueryVariables>;\nexport const MessagesByAddressSenseDocument = gql`\n    query MessagesByAddressSense($address: _text, $limit: bigint, $offset: bigint, $timestamp_from: timestamp, $types: _text, $order_direction: order_by) {\n  messages_by_address(\n    args: {addresses: $address, limit: $limit, offset: $offset, types: $types}\n    order_by: {transaction: {block: {timestamp: $order_direction}}}\n    where: {transaction: {block: {timestamp: {_gt: $timestamp_from}}}}\n  ) {\n    transaction_hash\n    index\n    value\n    transaction {\n      success\n      block {\n        timestamp\n        height\n      }\n      memo\n    }\n    type\n  }\n}\n    `;\n\n/**\n * __useMessagesByAddressSenseQuery__\n *\n * To run a query within a React component, call `useMessagesByAddressSenseQuery` and pass it any options that fit your needs.\n * When your component renders, `useMessagesByAddressSenseQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useMessagesByAddressSenseQuery({\n *   variables: {\n *      address: // value for 'address'\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      timestamp_from: // value for 'timestamp_from'\n *      types: // value for 'types'\n *      order_direction: // value for 'order_direction'\n *   },\n * });\n */\nexport function useMessagesByAddressSenseQuery(baseOptions?: Apollo.QueryHookOptions<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>(MessagesByAddressSenseDocument, options);\n      }\nexport function useMessagesByAddressSenseLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>(MessagesByAddressSenseDocument, options);\n        }\nexport function useMessagesByAddressSenseSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>(MessagesByAddressSenseDocument, options);\n        }\nexport type MessagesByAddressSenseQueryHookResult = ReturnType<typeof useMessagesByAddressSenseQuery>;\nexport type MessagesByAddressSenseLazyQueryHookResult = ReturnType<typeof useMessagesByAddressSenseLazyQuery>;\nexport type MessagesByAddressSenseSuspenseQueryHookResult = ReturnType<typeof useMessagesByAddressSenseSuspenseQuery>;\nexport type MessagesByAddressSenseQueryResult = Apollo.QueryResult<MessagesByAddressSenseQuery, MessagesByAddressSenseQueryVariables>;\nexport const MessagesByAddressSenseWsDocument = gql`\n    subscription MessagesByAddressSenseWs($address: _text, $limit: bigint, $offset: bigint, $timestamp_from: timestamp, $types: _text, $order_direction: order_by) {\n  messages_by_address(\n    args: {addresses: $address, limit: $limit, offset: $offset, types: $types}\n    order_by: {transaction: {block: {timestamp: $order_direction}}}\n    where: {transaction: {block: {timestamp: {_gt: $timestamp_from}}}}\n  ) {\n    transaction_hash\n    index\n    value\n    transaction {\n      success\n      block {\n        timestamp\n        height\n      }\n      memo\n    }\n    type\n  }\n}\n    `;\n\n/**\n * __useMessagesByAddressSenseWsSubscription__\n *\n * To run a query within a React component, call `useMessagesByAddressSenseWsSubscription` and pass it any options that fit your needs.\n * When your component renders, `useMessagesByAddressSenseWsSubscription` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the subscription, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useMessagesByAddressSenseWsSubscription({\n *   variables: {\n *      address: // value for 'address'\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      timestamp_from: // value for 'timestamp_from'\n *      types: // value for 'types'\n *      order_direction: // value for 'order_direction'\n *   },\n * });\n */\nexport function useMessagesByAddressSenseWsSubscription(baseOptions?: Apollo.SubscriptionHookOptions<MessagesByAddressSenseWsSubscription, MessagesByAddressSenseWsSubscriptionVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useSubscription<MessagesByAddressSenseWsSubscription, MessagesByAddressSenseWsSubscriptionVariables>(MessagesByAddressSenseWsDocument, options);\n      }\nexport type MessagesByAddressSenseWsSubscriptionHookResult = ReturnType<typeof useMessagesByAddressSenseWsSubscription>;\nexport type MessagesByAddressSenseWsSubscriptionResult = Apollo.SubscriptionResult<MessagesByAddressSenseWsSubscription>;\nexport const TransactionCountDocument = gql`\n    query transactionCount {\n  transaction_aggregate {\n    aggregate {\n      count(columns: hash)\n    }\n  }\n}\n    `;\n\n/**\n * __useTransactionCountQuery__\n *\n * To run a query within a React component, call `useTransactionCountQuery` and pass it any options that fit your needs.\n * When your component renders, `useTransactionCountQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useTransactionCountQuery({\n *   variables: {\n *   },\n * });\n */\nexport function useTransactionCountQuery(baseOptions?: Apollo.QueryHookOptions<TransactionCountQuery, TransactionCountQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<TransactionCountQuery, TransactionCountQueryVariables>(TransactionCountDocument, options);\n      }\nexport function useTransactionCountLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<TransactionCountQuery, TransactionCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<TransactionCountQuery, TransactionCountQueryVariables>(TransactionCountDocument, options);\n        }\nexport function useTransactionCountSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<TransactionCountQuery, TransactionCountQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<TransactionCountQuery, TransactionCountQueryVariables>(TransactionCountDocument, options);\n        }\nexport type TransactionCountQueryHookResult = ReturnType<typeof useTransactionCountQuery>;\nexport type TransactionCountLazyQueryHookResult = ReturnType<typeof useTransactionCountLazyQuery>;\nexport type TransactionCountSuspenseQueryHookResult = ReturnType<typeof useTransactionCountSuspenseQuery>;\nexport type TransactionCountQueryResult = Apollo.QueryResult<TransactionCountQuery, TransactionCountQueryVariables>;\nexport const UptimeByAddressDocument = gql`\n    query uptimeByAddress($address: String) {\n  uptime(where: {consensus_address: {_eq: $address}}) {\n    uptime\n  }\n}\n    `;\n\n/**\n * __useUptimeByAddressQuery__\n *\n * To run a query within a React component, call `useUptimeByAddressQuery` and pass it any options that fit your needs.\n * When your component renders, `useUptimeByAddressQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useUptimeByAddressQuery({\n *   variables: {\n *      address: // value for 'address'\n *   },\n * });\n */\nexport function useUptimeByAddressQuery(baseOptions?: Apollo.QueryHookOptions<UptimeByAddressQuery, UptimeByAddressQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<UptimeByAddressQuery, UptimeByAddressQueryVariables>(UptimeByAddressDocument, options);\n      }\nexport function useUptimeByAddressLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<UptimeByAddressQuery, UptimeByAddressQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<UptimeByAddressQuery, UptimeByAddressQueryVariables>(UptimeByAddressDocument, options);\n        }\nexport function useUptimeByAddressSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<UptimeByAddressQuery, UptimeByAddressQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<UptimeByAddressQuery, UptimeByAddressQueryVariables>(UptimeByAddressDocument, options);\n        }\nexport type UptimeByAddressQueryHookResult = ReturnType<typeof useUptimeByAddressQuery>;\nexport type UptimeByAddressLazyQueryHookResult = ReturnType<typeof useUptimeByAddressLazyQuery>;\nexport type UptimeByAddressSuspenseQueryHookResult = ReturnType<typeof useUptimeByAddressSuspenseQuery>;\nexport type UptimeByAddressQueryResult = Apollo.QueryResult<UptimeByAddressQuery, UptimeByAddressQueryVariables>;\nexport const WasmDashboardPageDocument = gql`\n    query wasmDashboardPage($offset: Int, $limit: Int) {\n  contracts(limit: $limit, offset: $offset, order_by: {tx: desc}) {\n    address\n    admin\n    code_id\n    creator\n    fees\n    gas\n    label\n    tx\n  }\n  contracts_aggregate {\n    aggregate {\n      sum {\n        gas\n        fees\n        tx\n      }\n      count(columns: address)\n    }\n  }\n}\n    `;\n\n/**\n * __useWasmDashboardPageQuery__\n *\n * To run a query within a React component, call `useWasmDashboardPageQuery` and pass it any options that fit your needs.\n * When your component renders, `useWasmDashboardPageQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useWasmDashboardPageQuery({\n *   variables: {\n *      offset: // value for 'offset'\n *      limit: // value for 'limit'\n *   },\n * });\n */\nexport function useWasmDashboardPageQuery(baseOptions?: Apollo.QueryHookOptions<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>(WasmDashboardPageDocument, options);\n      }\nexport function useWasmDashboardPageLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>(WasmDashboardPageDocument, options);\n        }\nexport function useWasmDashboardPageSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>(WasmDashboardPageDocument, options);\n        }\nexport type WasmDashboardPageQueryHookResult = ReturnType<typeof useWasmDashboardPageQuery>;\nexport type WasmDashboardPageLazyQueryHookResult = ReturnType<typeof useWasmDashboardPageLazyQuery>;\nexport type WasmDashboardPageSuspenseQueryHookResult = ReturnType<typeof useWasmDashboardPageSuspenseQuery>;\nexport type WasmDashboardPageQueryResult = Apollo.QueryResult<WasmDashboardPageQuery, WasmDashboardPageQueryVariables>;\nexport const MessagesByAddressDocument = gql`\n    query MessagesByAddress($address: _text, $limit: bigint, $offset: bigint, $types: _text) {\n  messages_by_address(\n    args: {addresses: $address, limit: $limit, offset: $offset, types: $types}\n    order_by: {transaction: {block: {height: desc}}}\n  ) {\n    transaction_hash\n    value\n    transaction {\n      success\n      height\n      logs\n      memo\n      block {\n        timestamp\n      }\n    }\n    type\n  }\n}\n    `;\n\n/**\n * __useMessagesByAddressQuery__\n *\n * To run a query within a React component, call `useMessagesByAddressQuery` and pass it any options that fit your needs.\n * When your component renders, `useMessagesByAddressQuery` returns an object from Apollo Client that contains loading, error, and data properties\n * you can use to render your UI.\n *\n * @param baseOptions options that will be passed into the query, supported options are listed on: https://www.apollographql.com/docs/react/api/react-hooks/#options;\n *\n * @example\n * const { data, loading, error } = useMessagesByAddressQuery({\n *   variables: {\n *      address: // value for 'address'\n *      limit: // value for 'limit'\n *      offset: // value for 'offset'\n *      types: // value for 'types'\n *   },\n * });\n */\nexport function useMessagesByAddressQuery(baseOptions?: Apollo.QueryHookOptions<MessagesByAddressQuery, MessagesByAddressQueryVariables>) {\n        const options = {...defaultOptions, ...baseOptions}\n        return Apollo.useQuery<MessagesByAddressQuery, MessagesByAddressQueryVariables>(MessagesByAddressDocument, options);\n      }\nexport function useMessagesByAddressLazyQuery(baseOptions?: Apollo.LazyQueryHookOptions<MessagesByAddressQuery, MessagesByAddressQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useLazyQuery<MessagesByAddressQuery, MessagesByAddressQueryVariables>(MessagesByAddressDocument, options);\n        }\nexport function useMessagesByAddressSuspenseQuery(baseOptions?: Apollo.SuspenseQueryHookOptions<MessagesByAddressQuery, MessagesByAddressQueryVariables>) {\n          const options = {...defaultOptions, ...baseOptions}\n          return Apollo.useSuspenseQuery<MessagesByAddressQuery, MessagesByAddressQueryVariables>(MessagesByAddressDocument, options);\n        }\nexport type MessagesByAddressQueryHookResult = ReturnType<typeof useMessagesByAddressQuery>;\nexport type MessagesByAddressLazyQueryHookResult = ReturnType<typeof useMessagesByAddressLazyQuery>;\nexport type MessagesByAddressSuspenseQueryHookResult = ReturnType<typeof useMessagesByAddressSuspenseQuery>;\nexport type MessagesByAddressQueryResult = Apollo.QueryResult<MessagesByAddressQuery, MessagesByAddressQueryVariables>;","import { Coin } from 'cosmjs-types/cosmos/base/v1beta1/coin';\nimport { CyberLinkSimple, NeuronAddress } from 'src/types/base';\n\ninterface GenericIndexerTransaction<T> {\n  value: T;\n  type: string;\n  transaction_hash: string;\n  index: number;\n  transaction: {\n    memo?: string;\n    success: boolean;\n    block: {\n      timestamp: string;\n    };\n  };\n}\nexport const MSG_SEND_TRANSACTION_TYPE = 'cosmos.bank.v1beta1.MsgSend';\n\nexport const MSG_MULTI_SEND_TRANSACTION_TYPE =\n  'cosmos.bank.v1beta1.MsgMultiSend';\n\nexport const CYBER_LINK_TRANSACTION_TYPE = 'cyber.graph.v1beta1.MsgCyberlink';\n\ninterface Input {\n  address: NeuronAddress;\n  coins: Coin[];\n}\n\ninterface Output {\n  address: NeuronAddress;\n  coins: Coin[];\n}\n\nexport interface MsgMultiSendValue {\n  inputs: Input[];\n  outputs: Output[];\n}\n\nexport interface MsgSendValue {\n  amount: Coin[];\n  from_address: NeuronAddress;\n  to_address: NeuronAddress;\n}\n\ninterface MsgDelegateValue {\n  amount: Coin;\n  delegator_address: NeuronAddress;\n  validator_address: NeuronAddress;\n}\n\nexport interface CyberLinkValue {\n  neuron: NeuronAddress;\n  links: CyberLinkSimple[];\n}\n\nexport interface CyberLinkTransaction\n  extends GenericIndexerTransaction<CyberLinkValue> {\n  type: typeof CYBER_LINK_TRANSACTION_TYPE;\n}\n\nexport interface MsgMultiSendTransaction\n  extends GenericIndexerTransaction<MsgMultiSendValue> {\n  type: typeof MSG_MULTI_SEND_TRANSACTION_TYPE;\n}\n\nexport interface MsgSendTransaction\n  extends GenericIndexerTransaction<MsgSendValue> {\n  type: typeof MSG_SEND_TRANSACTION_TYPE;\n}\n\nexport type Transaction =\n  // | DelegateTransaction\n  CyberLinkTransaction | MsgMultiSendTransaction | MsgSendTransaction;\n","import { Tx } from 'cosmjs-types/cosmos/tx/v1beta1/tx';\nimport { MsgSend, MsgMultiSend } from 'cosmjs-types/cosmos/bank/v1beta1/tx';\n\nimport { fromBase64 } from '@cosmjs/encoding';\nimport {\n  MSG_MULTI_SEND_TRANSACTION_TYPE,\n  MSG_SEND_TRANSACTION_TYPE,\n} from 'src/services/backend/services/indexer/types';\nimport { NeuronAddress } from 'src/types/base';\nimport { TransactionDto } from 'src/services/CozoDb/types/dto';\nimport { getNowUtcNumber } from 'src/utils/date';\n\n// eslint-disable-next-line import/no-unused-modules\nexport const extractTxData = (data: string) => {\n  const result = Tx.decode(fromBase64(data));\n  const memo = result.body?.memo;\n  const messages = result.body?.messages\n    .map((message) => {\n      const msgType = message.typeUrl.slice(1);\n      if (msgType === MSG_SEND_TRANSACTION_TYPE) {\n        return MsgSend.decode(message.value);\n      }\n\n      if (msgType === MSG_MULTI_SEND_TRANSACTION_TYPE) {\n        return MsgMultiSend.decode(message.value);\n      }\n      return undefined;\n    })\n    .filter((message) => message !== undefined);\n\n  return { memo, messages };\n};\n\n// eslint-disable-next-line import/no-unused-modules\nexport const mapWebsocketTxToTransactions = (\n  neuron: NeuronAddress,\n  result: any\n) => {\n  const { data, events } = result;\n\n  const hash = events['tx.hash'][0];\n  const transactionType = events['message.action'][0].slice(1);\n  const timestamp = getNowUtcNumber();\n  const blockHeight = events['tx.height'][0];\n\n  const { memo = '', messages } = extractTxData(data.value.TxResult.tx);\n\n  const transactions: TransactionDto[] = [];\n  messages!.forEach((message, index) => {\n    transactions.push({\n      hash,\n      index,\n      type: transactionType,\n      timestamp,\n      success: true,\n      value: message!,\n      memo,\n      neuron,\n      blockHeight,\n    });\n  });\n\n  return transactions;\n};\n","import { ApolloClient, DocumentNode, InMemoryCache } from '@apollo/client';\n\nimport { GraphQLWsLink } from '@apollo/client/link/subscriptions';\nimport { GraphQLClient } from 'graphql-request';\nimport { createClient } from 'graphql-ws';\nimport { Observable } from 'rxjs';\nimport { INDEX_WEBSOCKET, INDEX_HTTPS } from 'src/constants/config';\n\nconst cyberGraphQLWsLink = new GraphQLWsLink(\n  createClient({\n    url: INDEX_WEBSOCKET,\n    shouldRetry: (errOrCloseEvent: unknown) => true,\n    retryAttempts: 10,\n    retryWait: async (retries: number): Promise<void> => {\n      setTimeout(() => Promise.resolve(), Math.min(1000 * 2 ** retries, 10000));\n    },\n    // on: {\n    //   error: (err) => {\n    //     console.log('---ws errr', err);\n    //   },\n    //   message: (msg) => {\n    //     console.log('---ws message', msg);\n    //   },\n    //   // Handle connection opened event\n    //   opened: () => {\n    //     console.log('---ws opened');\n    //   },\n    //   // Handle connection closed event\n    //   closed: () => {\n    //     console.log('---ws closed');\n    //   },\n    // },\n  })\n);\n\nexport const createIndexerClient = (abortSignal: AbortSignal) =>\n  new GraphQLClient(INDEX_HTTPS, {\n    signal: abortSignal,\n  });\n\n// eslint-disable-next-line import/no-unused-modules\nexport function createIndexerWebsocket<T>(\n  query: DocumentNode,\n  variables: object\n): Observable<T> {\n  const client = new ApolloClient({\n    link: cyberGraphQLWsLink,\n    cache: new InMemoryCache(),\n  });\n\n  const apolloObservable = client.subscribe({ query, variables });\n  return new Observable((subscriber) => {\n    const subscription = apolloObservable.subscribe({\n      next(result) {\n        subscriber.next(result.data as T);\n      },\n      error(err) {\n        subscriber.error(err);\n      },\n      complete() {\n        subscriber.complete();\n      },\n    });\n\n    // Cleanup subscription on unsubscribe\n    return () => subscription.unsubscribe();\n  });\n}\n","/* eslint-disable import/no-unused-modules */\n\nimport { ParticleCid, NeuronAddress } from 'src/types/base';\nimport { numberToUtcDate } from 'src/utils/date';\n\nimport { CYBERLINKS_BATCH_LIMIT } from './consts';\nimport { createIndexerClient } from './utils/graphqlClient';\nimport { fetchIterableByOffset } from 'src/utils/async/iterable';\nimport {\n  CyberlinksByParticleDocument,\n  CyberlinksByParticleQuery,\n  CyberlinksByParticleQueryVariables,\n  CyberlinksCountByNeuronDocument,\n  CyberlinksCountByNeuronQuery,\n  CyberlinksCountByNeuronQueryVariables,\n  Order_By,\n} from 'src/generated/graphql';\n\nconst fetchCyberlinks = async ({\n  particleCid,\n  timestampFrom,\n  offset = 0,\n  abortSignal,\n}: {\n  particleCid: ParticleCid;\n  timestampFrom: number;\n  offset?: number;\n  abortSignal: AbortSignal;\n}) => {\n  const res = await createIndexerClient(abortSignal).request<\n    CyberlinksByParticleQuery,\n    CyberlinksByParticleQueryVariables\n  >(CyberlinksByParticleDocument, {\n    limit: CYBERLINKS_BATCH_LIMIT,\n    offset,\n    orderBy: [{ timestamp: Order_By.Asc }],\n    where: {\n      _or: [\n        { particle_to: { _eq: particleCid } },\n        { particle_from: { _eq: particleCid } },\n      ],\n      timestamp: { _gt: numberToUtcDate(timestampFrom) },\n    },\n  });\n\n  return res.cyberlinks;\n};\n\nconst fetchCyberlinksCount = async (\n  address: NeuronAddress,\n  particlesFrom: ParticleCid[],\n  timestampFrom: number,\n  abortSignal: AbortSignal\n) => {\n  const res = await createIndexerClient(abortSignal).request<\n    CyberlinksCountByNeuronQuery,\n    CyberlinksCountByNeuronQueryVariables\n  >(CyberlinksCountByNeuronDocument, {\n    address,\n    particles_from: particlesFrom,\n    timestamp: numberToUtcDate(timestampFrom),\n  });\n\n  return res.cyberlinks_aggregate.aggregate?.count;\n};\n\nconst fetchCyberlinksByNeroun = async ({\n  neuron,\n  particlesFrom,\n  timestampFrom,\n  batchSize,\n  offset = 0,\n  abortSignal,\n}: {\n  neuron: NeuronAddress;\n  particlesFrom: ParticleCid[];\n  timestampFrom: number;\n  batchSize: number;\n  offset: number;\n  abortSignal: AbortSignal;\n}) => {\n  const where = {\n    _and: [\n      {\n        timestamp: {\n          _gt: numberToUtcDate(timestampFrom),\n        },\n      },\n      {\n        neuron: {\n          _eq: neuron,\n        },\n      },\n      { particle_from: { _in: particlesFrom } },\n    ],\n  };\n\n  const res = await createIndexerClient(abortSignal).request<\n    CyberlinksByParticleQuery,\n    CyberlinksByParticleQueryVariables\n  >(CyberlinksByParticleDocument, {\n    limit: batchSize,\n    offset,\n    orderBy: [\n      {\n        timestamp: Order_By.Asc,\n      },\n    ],\n    where,\n  });\n\n  return res.cyberlinks;\n};\n\nexport const fetchCyberlinksByNerounIterable = async (\n  neuron: NeuronAddress,\n  particlesFrom: ParticleCid[],\n  timestampFrom: number,\n  batchSize: number,\n  abortSignal: AbortSignal\n) =>\n  fetchIterableByOffset(fetchCyberlinksByNeroun, {\n    neuron,\n    particlesFrom,\n    timestampFrom,\n    batchSize,\n    abortSignal,\n  });\n\nconst fetchCyberlinksIterable = (\n  particleCid: ParticleCid,\n  timestampFrom: number,\n  abortSignal: AbortSignal\n) =>\n  fetchIterableByOffset(fetchCyberlinks, {\n    particleCid,\n    timestampFrom,\n    abortSignal,\n  });\n\nexport { fetchCyberlinksIterable, fetchCyberlinksCount };\n","const TRANSACTIONS_BATCH_LIMIT = 500;\nconst CYBERLINKS_BATCH_LIMIT = 200;\n\nexport { TRANSACTIONS_BATCH_LIMIT, CYBERLINKS_BATCH_LIMIT };\n","import { CyberLinkSimple, CyberlinkTxHash, ParticleCid } from 'src/types/base';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { asyncIterableBatchProcessor } from 'src/utils/async/iterable';\nimport { CID_TWEET } from 'src/constants/app';\nimport { LinkDto, TransactionDto } from 'src/services/CozoDb/types/dto';\n\nimport { fetchCyberlinksIterable } from '../../../indexer/cyberlinks';\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { MAX_LINKS_RESOLVE_BATCH } from '../consts';\nimport {\n  CYBER_LINK_TRANSACTION_TYPE,\n  CyberLinkValue,\n} from '../../../indexer/types';\nimport { SyncQueueJobType } from 'src/services/CozoDb/types/entities';\n\nconst getUniqueParticlesFromLinks = (links: CyberLinkSimple[]) =>\n  [\n    ...new Set([\n      ...links.map((link) => link.to),\n      ...links.map((link) => link.from),\n    ]),\n  ] as ParticleCid[];\n\n// eslint-disable-next-line import/no-unused-modules\nexport const fetchCyberlinksAndResolveParticles = async (\n  cid: ParticleCid,\n  timestampUpdate: number,\n  particlesResolver: ParticlesResolverQueue,\n  queuePriority: QueuePriority,\n  abortSignal: AbortSignal\n) => {\n  const cyberlinksIterable = fetchCyberlinksIterable(\n    cid,\n    timestampUpdate,\n    abortSignal\n  );\n  const links = [];\n  // eslint-disable-next-line no-restricted-syntax\n  for await (const batch of cyberlinksIterable) {\n    links.push(...batch);\n    const particles = getUniqueParticlesFromLinks(batch);\n    if (particles.length > 0) {\n      await asyncIterableBatchProcessor(\n        particles,\n        (cids: ParticleCid[]) =>\n          particlesResolver!.enqueueBatch(\n            cids,\n            SyncQueueJobType.particle,\n            queuePriority\n          ),\n        MAX_LINKS_RESOLVE_BATCH\n      );\n    }\n  }\n\n  return links;\n};\n\nexport function extractCybelinksFromTransaction(batch: TransactionDto[]) {\n  const cyberlinks = batch.filter(\n    (l) => l.type === CYBER_LINK_TRANSACTION_TYPE\n  );\n  const particlesFound = new Set<string>();\n  const links: LinkDto[] = [];\n  // Get links: only from TWEETS\n  const tweets: Record<ParticleCid, LinkDto> = cyberlinks.reduce<\n    Record<ParticleCid, LinkDto>\n  >((acc, { value, hash, timestamp }: TransactionDto) => {\n    (value as CyberLinkValue).links.forEach((link) => {\n      particlesFound.add(link.to);\n      particlesFound.add(link.from);\n      const txLink = {\n        ...link,\n        timestamp,\n        neuron: (value as CyberLinkValue).neuron,\n        transactionHash: hash,\n      };\n      links.push(txLink);\n\n      if (link.from === CID_TWEET) {\n        acc[txLink.to] = txLink;\n      }\n    });\n    return acc;\n  }, {});\n\n  return {\n    tweets,\n    particlesFound: [...particlesFound],\n    links,\n  };\n}\n","import { NeuronAddress } from 'src/types/base';\nimport { numberToUtcDate } from 'src/utils/date';\nimport { fetchIterableByOffset } from 'src/utils/async/iterable';\nimport {\n  MessagesByAddressCountDocument,\n  MessagesByAddressCountQuery,\n  MessagesByAddressCountQueryVariables,\n  MessagesByAddressSenseDocument,\n  MessagesByAddressSenseQuery,\n  MessagesByAddressSenseQueryVariables,\n} from 'src/generated/graphql';\n\nimport { createIndexerClient } from './utils/graphqlClient';\nimport { Transaction } from './types';\n\ntype OrderDirection = 'desc' | 'asc';\ntype Abortable = { abortSignal: AbortSignal };\n\nexport type MessagesByAddressVariables = {\n  neuron: NeuronAddress;\n  timestampFrom: number;\n  offset?: number;\n  types: Transaction['type'][];\n  orderDirection: OrderDirection;\n  limit: number;\n} & Abortable;\n\nexport const mapMessagesByAddressVariables = ({\n  neuron,\n  timestampFrom,\n  offset = 0,\n  types = [],\n  orderDirection = 'desc',\n  limit,\n}: MessagesByAddressVariables) => ({\n  address: `{${neuron}}`,\n  limit,\n  timestamp_from: numberToUtcDate(timestampFrom),\n  offset,\n  types: `{${types.map((t) => `\"${t}\"`).join(' ,')}}`,\n  order_direction: orderDirection,\n});\n\nconst fetchTransactions = async ({\n  neuron,\n  timestampFrom,\n  offset = 0,\n  types = [],\n  orderDirection = 'desc',\n  limit,\n  abortSignal,\n}: MessagesByAddressVariables) => {\n  const res = await createIndexerClient(abortSignal).request<\n    MessagesByAddressSenseQuery,\n    MessagesByAddressSenseQueryVariables\n  >(\n    MessagesByAddressSenseDocument,\n    mapMessagesByAddressVariables({\n      neuron,\n      timestampFrom,\n      offset,\n      types,\n      orderDirection,\n      limit,\n      abortSignal,\n    }) as MessagesByAddressSenseQueryVariables\n  );\n\n  return res?.messages_by_address as Transaction[];\n};\n\nexport const fetchTransactionMessagesCount = async (\n  address: NeuronAddress,\n  timestampFrom: number,\n  abortSignal: AbortSignal\n) => {\n  const res = await createIndexerClient(abortSignal).request<\n    MessagesByAddressCountQuery,\n    MessagesByAddressCountQueryVariables\n  >(MessagesByAddressCountDocument, {\n    address: `{${address}}`,\n    timestamp: numberToUtcDate(timestampFrom),\n  });\n\n  return res?.messages_by_address_aggregate.aggregate?.count;\n};\n\nexport const fetchTransactionsIterable = ({\n  neuron,\n  timestampFrom,\n  types,\n  orderDirection,\n  limit,\n  abortSignal,\n}: MessagesByAddressVariables) =>\n  fetchIterableByOffset(fetchTransactions, {\n    neuron,\n    timestampFrom,\n    types,\n    orderDirection,\n    limit,\n    abortSignal,\n  });\n","import { TransactionDto } from 'src/services/CozoDb/types/dto';\nimport { SenseChat } from 'src/services/backend/types/sense';\nimport { NeuronAddress } from 'src/types/base';\nimport { Coin } from 'cosmjs-types/cosmos/base/v1beta1/coin';\n\nimport {\n  MSG_SEND_TRANSACTION_TYPE,\n  MSG_MULTI_SEND_TRANSACTION_TYPE,\n  MsgSendTransaction,\n} from '../../../indexer/types';\n\nexport const extractSenseChats = (\n  myAddress: NeuronAddress,\n  transactions: TransactionDto[]\n) => {\n  const sendTransactions =\n    transactions!.filter(\n      (t) =>\n        t.type === MSG_SEND_TRANSACTION_TYPE ||\n        t.type === MSG_MULTI_SEND_TRANSACTION_TYPE\n    ) || [];\n\n  if (sendTransactions.length === 0) {\n    return [];\n  }\n  const chats = new Map<NeuronAddress, SenseChat>();\n  transactions.forEach((t) => {\n    let userAddress = '';\n    if (t.type === MSG_MULTI_SEND_TRANSACTION_TYPE) {\n      const { inputs, outputs } = t.value;\n      const isSender = inputs.find((i) => i.address === myAddress);\n      const userMessages = isSender ? outputs : inputs;\n      userMessages.forEach((msg) =>\n        updateSenseChat(chats, msg.address, t, msg.coins, isSender)\n      );\n    } else if (t.type === MSG_SEND_TRANSACTION_TYPE) {\n      const { fromAddress, toAddress, amount } =\n        t.value as MsgSendTransaction['value'];\n      const isSender = fromAddress === myAddress;\n      userAddress = isSender ? toAddress : fromAddress;\n      updateSenseChat(chats, userAddress, t, amount, isSender);\n    }\n  });\n\n  return chats;\n};\n\nconst updateSenseChat = (\n  chats: Map<NeuronAddress, SenseChat>,\n  addr: string,\n  t: TransactionDto,\n  amount: Coin[],\n  isSender: boolean\n): Map<string, SenseChat> => {\n  const chat = chats.get(addr);\n  const transactions = chat?.transactions || [];\n\n  transactions.push(t);\n  chats.set(addr, {\n    userAddress: addr,\n    lastSendTimestamp: isSender ? t.timestamp : chat?.lastSendTimestamp || 0,\n    last: { amount, memo: t.memo, direction: isSender ? 'to' : 'from' },\n    transactions,\n  });\n  return chats;\n};\n","import { EntryType } from 'src/services/CozoDb/types/entities';\nimport DbApiWrapper from 'src/services/backend/services/DbApi/DbApi';\nimport { NeuronAddress } from 'src/types/base';\nimport {\n  SenseListItem,\n  SenseTransactionMeta,\n} from 'src/services/backend/types/sense';\nimport { throwIfAborted } from 'src/utils/async/promise';\nimport { extractSenseChats } from '../../utils/sense';\n\n// eslint-disable-next-line import/prefer-default-export\nexport const syncMyChats = async (\n  db: DbApiWrapper,\n  myAddress: NeuronAddress,\n  timestampFrom: number,\n  signal: AbortSignal,\n  shouldUpdateTimestamp = true\n) => {\n  const syncItems = await db.findSyncStatus({\n    ownerId: myAddress,\n    entryType: EntryType.chat,\n  });\n\n  const syncItemsMap = new Map(syncItems?.map((i) => [i.id, i]));\n\n  const myTransactions = await db.getTransactions(myAddress, {\n    order: 'asc',\n    timestampFrom,\n  });\n\n  const myChats = extractSenseChats(myAddress, myTransactions!);\n\n  const results: SenseListItem[] = [];\n\n  // eslint-disable-next-line no-restricted-syntax\n  for (const chat of myChats.values()) {\n    const syncItem = syncItemsMap.get(chat.userAddress);\n    const lastTransaction = chat.transactions.at(-1)!;\n\n    const { timestamp: transactionTimestamp, hash, index } = lastTransaction;\n    const syncItemHeader = {\n      entryType: EntryType.chat,\n      ownerId: myAddress,\n      meta: {\n        transactionHash: hash,\n        index,\n      } as SenseTransactionMeta,\n    };\n\n    // if no sync item(first message/initial)\n    if (!syncItem) {\n      const unreadCount = chat.transactions.filter(\n        (t) => t.timestamp > chat.lastSendTimestamp\n      ).length; // uread count on top of my last send message\n\n      const newItem = {\n        ...syncItemHeader,\n        id: chat.userAddress,\n        unreadCount,\n        // if 'fast' then no shift update poiter till 'slow' reupdate\n        timestampUpdate: shouldUpdateTimestamp ? transactionTimestamp : 0,\n        timestampRead: chat.lastSendTimestamp,\n        disabled: false,\n      };\n\n      // eslint-disable-next-line no-await-in-loop\n      await throwIfAborted(db.putSyncStatus.bind(db), signal)(newItem);\n\n      results.push({ ...newItem, meta: lastTransaction });\n    } else {\n      const {\n        id,\n        timestampRead,\n        timestampUpdate,\n        meta,\n        unreadCount: prevUnreadCount,\n      } = syncItem;\n\n      const lastTimestampRead = Math.max(\n        timestampRead!,\n        chat.lastSendTimestamp\n      );\n      const { timestampUpdateContent = 0, timestampUpdateChat = 0 } = meta;\n      const timestampUnreadFrom = Math.max(\n        chat.lastSendTimestamp,\n        timestampUpdateChat\n      );\n      const unreadCount =\n        prevUnreadCount +\n        chat.transactions.filter((t) => t.timestamp > timestampUnreadFrom) // + new messages count\n          .length;\n\n      if (timestampUpdate < transactionTimestamp) {\n        // if message source is 'fast' then no update till 'slow' reupdate\n        const newTimestampUpdateChat = shouldUpdateTimestamp\n          ? transactionTimestamp\n          : timestampUpdateChat;\n\n        const syncStatusChanges = {\n          ...syncItemHeader,\n          id: id!,\n          unreadCount,\n          timestampRead: lastTimestampRead,\n          // show max timestamp to use in sorting, in sense list\n          // real timestamp shold be resynced with 'slow' data source by timestampUpdateChat\n          timestampUpdate: Math.max(\n            transactionTimestamp,\n            timestampUpdateContent,\n            newTimestampUpdateChat\n          ),\n\n          meta: {\n            ...syncItemHeader.meta,\n            timestampUpdateChat: newTimestampUpdateChat,\n            timestampUpdateContent,\n          },\n        };\n\n        // eslint-disable-next-line no-await-in-loop\n        await throwIfAborted(\n          db.updateSyncStatus.bind(db),\n          signal\n        )(syncStatusChanges);\n\n        results.push({\n          ...syncItem,\n          ...syncStatusChanges,\n          meta: lastTransaction,\n        } as SenseListItem);\n      }\n    }\n  }\n  return results;\n};\n","import { ProgressTracking } from 'src/services/backend/types/services';\n\nconst ROLLING_WINDOW = 10;\n\ntype onProgressUpdateFunc = (progress: ProgressTracking) => void;\n\ntype RequestRecord = {\n  timestamp: number;\n  itemCount: number;\n};\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport class ProgressTracker {\n  private requestRecords: RequestRecord[] = [];\n\n  private totalRequests = 0;\n\n  private completedRequests = 0;\n\n  private estimatedTime = -1;\n\n  private batchSize = 1;\n\n  private onProgressUpdate?: onProgressUpdateFunc;\n\n  public get progress(): ProgressTracking {\n    return {\n      totalCount: this.totalRequests,\n      completeCount: this.completedRequests,\n      estimatedTime: this.estimatedTime,\n    };\n  }\n\n  constructor(onProgressUpdate?: onProgressUpdateFunc) {\n    this.onProgressUpdate = onProgressUpdate;\n  }\n\n  public start(totalRequests: number, batchSize = 1) {\n    this.totalRequests = totalRequests;\n    this.requestRecords = [];\n    this.completedRequests = 0;\n    this.estimatedTime = -1;\n    this.batchSize = batchSize;\n\n    return this.progress;\n  }\n\n  public add(extraRequests: number) {\n    this.totalRequests += extraRequests;\n\n    return this.progress;\n  }\n\n  public trackProgress(processedCount: number) {\n    this.addRequestRecord(processedCount);\n\n    if (this.requestRecords.length > ROLLING_WINDOW) {\n      this.requestRecords.shift();\n    }\n\n    if (this.requestRecords.length > 1) {\n      const averageTimePerItem = this.calculateAverageTimePerItem();\n      const remainingRequests = this.totalRequests - this.completedRequests;\n      const estimatedRemainingItems = remainingRequests * processedCount; // Assuming remaining requests will process the same number of items\n      const estimatedRemainingTime =\n        averageTimePerItem * estimatedRemainingItems;\n\n      this.completedRequests += processedCount;\n      this.estimatedTime = Math.round(estimatedRemainingTime); // Convert to seconds;\n      this.onProgressUpdate && this.onProgressUpdate(this.progress);\n    }\n\n    return this.progress;\n  }\n\n  private addRequestRecord(itemCount: number) {\n    this.requestRecords.push({ timestamp: Date.now(), itemCount });\n  }\n\n  private calculateAverageTimePerItem(): number {\n    let totalDiff = 0;\n    let totalItems = 0;\n\n    for (let i = 1; i < this.requestRecords.length; i++) {\n      const timeDiff =\n        this.requestRecords[i].timestamp - this.requestRecords[i - 1].timestamp;\n      const { itemCount } = this.requestRecords[i];\n\n      totalDiff += timeDiff * itemCount;\n      totalItems += itemCount;\n    }\n\n    return totalItems === 0 ? 0 : totalDiff / totalItems;\n  }\n}\n","import {\n  Observable,\n  filter,\n  distinctUntilChanged,\n  map,\n  switchMap,\n  take,\n  tap,\n} from 'rxjs';\n\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport { broadcastStatus } from 'src/services/backend/channels/broadcastStatus';\nimport { SyncEntryName } from 'src/services/backend/types/services';\nimport { CyblogChannel, createCyblogChannel } from 'src/utils/logging/cyblog';\n\nimport DbApiWrapper from '../../../DbApi/DbApi';\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { ProgressTracker } from '../ProgressTracker/ProgressTracker';\nimport { ServiceDeps } from '../types';\nimport { SyncServiceParams } from '../../types';\n\nabstract class BaseSync {\n  protected name: string;\n\n  protected abortController: AbortController;\n\n  protected db: DbApiWrapper | undefined;\n\n  protected progressTracker = new ProgressTracker();\n\n  protected channelApi = new BroadcastChannelSender();\n\n  protected particlesResolver: ParticlesResolverQueue | undefined;\n\n  protected statusApi: ReturnType<typeof broadcastStatus>;\n\n  protected params: SyncServiceParams = {\n    myAddress: null,\n  };\n\n  protected readonly isInitialized$: Observable<boolean>;\n\n  protected cyblogCh: CyblogChannel;\n\n  constructor(\n    name: SyncEntryName,\n    deps: ServiceDeps,\n    particlesResolver: ParticlesResolverQueue\n  ) {\n    this.name = name;\n\n    this.abortController = new AbortController();\n\n    this.statusApi = broadcastStatus(name, this.channelApi);\n    this.particlesResolver = particlesResolver;\n    this.cyblogCh = createCyblogChannel({ thread: 'bckd', module: name });\n    if (!deps.params$) {\n      throw new Error('params$ is not defined');\n    }\n\n    deps.dbInstance$.subscribe((db) => {\n      this.db = db;\n    });\n\n    this.particlesResolver = particlesResolver;\n\n    this.isInitialized$ = this.createIsInitializedObserver(deps);\n\n    this.isInitialized$.subscribe((isInitialized) => {\n      this.cyblogCh.info(\n        `>>> ${this.name} - ${isInitialized ? 'initialized' : 'inactive'}`\n      );\n      this.statusApi.sendStatus(isInitialized ? 'initialized' : 'inactive');\n    });\n\n    this.isInitialized$\n      .pipe(switchMap(() => deps.params$!))\n      .subscribe((params) => {\n        this.params = params;\n        this.cyblogCh.info(`>>> ${this.name} - params updated`, {\n          data: params,\n        });\n      });\n\n    // Restart observer\n    this.isInitialized$\n      .pipe(\n        filter((isInitialized) => !!isInitialized),\n        switchMap(() => this.createRestartObserver(deps.params$!))\n      )\n      .subscribe(() => {\n        this.restart();\n      });\n  }\n\n  protected initAbortController() {\n    this.abortController = new AbortController();\n  }\n\n  protected abstract createIsInitializedObserver(\n    deps: ServiceDeps\n  ): Observable<boolean>;\n\n  // eslint-disable-next-line class-methods-use-this\n  protected createRestartObserver(params$: Observable<SyncServiceParams>) {\n    return params$.pipe(\n      map((params) => params.myAddress),\n      distinctUntilChanged((addrBefore, addrAfter) => addrBefore === addrAfter),\n      map((v) => !!v),\n      filter((v) => !!v)\n    );\n  }\n\n  public abstract restart(): void;\n\n  public abstract start(): void;\n}\n\nexport default BaseSync;\n","/* eslint-disable import/prefer-default-export */\nimport {\n  distinctUntilChanged,\n  filter,\n  Observable,\n  share,\n  switchMap,\n  tap,\n} from 'rxjs';\n\nexport const switchWhenInitialized = (\n  isInitialized$: Observable<boolean>,\n  actionObservable$: Observable<any>,\n  onChange?: (isInitialized: boolean) => void\n) =>\n  isInitialized$.pipe(\n    distinctUntilChanged(),\n    tap((isInitialized) => onChange?.(isInitialized)),\n    filter((initialized) => initialized),\n    switchMap(() => actionObservable$),\n    share()\n  );\n","import { Observable, Subject, from, startWith, switchMap, tap } from 'rxjs';\n\nimport { SyncEntryName } from 'src/services/backend/types/services';\n\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { ServiceDeps } from '../types';\nimport BaseSync from './BaseSync';\nimport { switchWhenInitialized } from '../utils/rxjs/withInitializer';\nimport { SyncServiceParams } from '../../types';\n\nabstract class BaseSyncClient extends BaseSync {\n  protected readonly source$: Observable<any>;\n\n  protected readonly reloadTrigger$ = new Subject<void>();\n\n  constructor(\n    name: SyncEntryName,\n    deps: ServiceDeps,\n    particlesResolver: ParticlesResolverQueue\n  ) {\n    super(name, deps, particlesResolver);\n\n    const source$ = switchWhenInitialized(\n      this.isInitialized$!,\n      this.reloadTrigger$.pipe(\n        startWith(null),\n        tap(() => {\n          // initialize abort conteoller for restart strategy\n          this.initAbortController();\n        }),\n        switchMap(() =>\n          this.createInitObservable().pipe(\n            switchMap((timestampFrom: number) =>\n              this.createClientObservable(timestampFrom).pipe(\n                tap(() => this.statusApi.sendStatus('listen')),\n                switchMap((data) => from(this.onUpdate(data, this.params)))\n              )\n            )\n          )\n        )\n      ),\n      (isInitialized) => {\n        console.log(`>>> ${name} isInitialized`, isInitialized);\n        this.statusApi.sendStatus(isInitialized ? 'initialized' : 'inactive');\n      }\n    );\n\n    source$.subscribe({\n      next: () => {\n        this.statusApi.sendStatus('listen');\n      },\n      error: (err) => {\n        this.statusApi.sendStatus('error', err);\n      },\n    });\n    this.source$ = source$;\n  }\n\n  protected abstract createClientObservable(\n    timestampFrom: number\n  ): Observable<any>;\n\n  protected abstract createInitObservable(): Observable<number>;\n\n  public restart() {\n    this.abortController?.abort();\n    this.reloadTrigger$.next();\n    console.log(`>>> ${this.name} client restart`);\n  }\n\n  protected abstract onUpdate(\n    data: any,\n    params: SyncServiceParams\n  ): Promise<void>;\n\n  public start() {\n    this.source$.subscribe(() => {\n      // dummy subscriber to keep pipeline running - don't remove\n    });\n    return this;\n  }\n}\n\nexport default BaseSyncClient;\n","/* eslint-disable camelcase */\nimport {\n  map,\n  combineLatest,\n  Observable,\n  from,\n  defer,\n  distinctUntilChanged,\n  merge,\n  filter,\n} from 'rxjs';\nimport { isEmpty } from 'lodash';\n\nimport {\n  EntryType,\n  SyncQueueJobType,\n} from 'src/services/CozoDb/types/entities';\nimport { mapIndexerTransactionToEntity } from 'src/services/CozoDb/mapping';\nimport { numberToUtcDate } from 'src/utils/date';\nimport { NeuronAddress } from 'src/types/base';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { SyncStatusDto, TransactionDto } from 'src/services/CozoDb/types/dto';\nimport { asyncIterableBatchProcessor } from 'src/utils/async/iterable';\nimport { throwIfAborted } from 'src/utils/async/promise';\nimport {\n  createNodeWebsocketObservable,\n  getIncomingTransfersQuery,\n} from 'src/services/lcd/websocket';\nimport {\n  MessagesByAddressSenseQueryVariables,\n  MessagesByAddressSenseWsDocument,\n  MessagesByAddressSenseWsSubscription,\n} from 'src/generated/graphql';\n\nimport { mapWebsocketTxToTransactions } from 'src/services/lcd/utils/mapping';\n\nimport { ServiceDeps } from '../types';\nimport { extractCybelinksFromTransaction } from '../utils/links';\n\nimport {\n  fetchTransactionsIterable,\n  mapMessagesByAddressVariables,\n  fetchTransactionMessagesCount,\n} from '../../../indexer/transactions';\nimport { syncMyChats } from './services/chat';\nimport { TRANSACTIONS_BATCH_LIMIT } from '../../../indexer/consts';\nimport BaseSyncClient from '../BaseSyncLoop/BaseSyncClient';\nimport { createIndexerWebsocket } from '../../../indexer/utils/graphqlClient';\nimport { SyncServiceParams } from '../../types';\nimport { MAX_DATABASE_PUT_SIZE } from '../consts';\n\ntype DataStreamResult = {\n  source: 'indexer' | 'node';\n  transactions: TransactionDto[];\n};\n\nclass SyncTransactionsLoop extends BaseSyncClient {\n  protected createIsInitializedObserver(deps: ServiceDeps) {\n    const isInitialized$ = combineLatest([\n      deps.dbInstance$,\n      deps.params$!.pipe(\n        map((params) => params.myAddress),\n        distinctUntilChanged()\n      ),\n      this.particlesResolver!.isInitialized$,\n    ]).pipe(\n      map(\n        ([dbInstance, myAddress, syncQueueInitialized]) =>\n          !!dbInstance && !!syncQueueInitialized && !!myAddress\n      )\n    );\n\n    return isInitialized$;\n  }\n\n  // eslint-disable-next-line class-methods-use-this\n  protected createClientObservable(\n    timestampFrom: number\n  ): Observable<DataStreamResult> {\n    const { myAddress } = this.params;\n    this.cyblogCh.info(\n      `>>> ${this.name} subscribe ${myAddress} from ${numberToUtcDate(\n        timestampFrom\n      )}`\n    );\n\n    const variables = mapMessagesByAddressVariables({\n      neuron: myAddress!,\n      timestampFrom,\n      types: [],\n      orderDirection: 'desc',\n      limit: 100,\n    }) as MessagesByAddressSenseQueryVariables;\n\n    const indexerObservable$ =\n      createIndexerWebsocket<MessagesByAddressSenseWsSubscription>(\n        MessagesByAddressSenseWsDocument,\n        variables\n      ).pipe(\n        map((response: MessagesByAddressSenseWsSubscription) => {\n          return {\n            source: 'indexer',\n            transactions: response.messages_by_address.map((i) =>\n              mapIndexerTransactionToEntity(myAddress!, i)\n            ),\n          };\n        })\n      );\n\n    const nodeObservample$ = createNodeWebsocketObservable(\n      myAddress!,\n      getIncomingTransfersQuery(myAddress!),\n      (message, ctx) => this.cyblogCh.info(message, { unit: 'node-ws', ...ctx })\n    ).pipe(\n      filter((data) => !isEmpty(data)),\n      map((data) => {\n        return {\n          source: 'node',\n          transactions: mapWebsocketTxToTransactions(myAddress!, data),\n        };\n      })\n    );\n\n    return merge(\n      indexerObservable$,\n      nodeObservample$\n    ) as Observable<DataStreamResult>;\n  }\n\n  protected createInitObservable() {\n    return defer(() => from(this.initSync()));\n    // return from(this.initSync());\n  }\n\n  public async initSync() {\n    const { myAddress } = this.params;\n    const { signal } = this.abortController;\n    const syncItem = await this.db!.getSyncStatus(myAddress!, myAddress!);\n\n    const lastTransactionTimestamp = await this.syncTransactions(\n      myAddress!,\n      myAddress!,\n      syncItem\n    );\n\n    this.statusApi.sendStatus('in-progress', `sync my chats`);\n    const syncStatusItems = await syncMyChats(\n      this.db!,\n      myAddress!,\n      syncItem.timestampUpdate,\n      signal\n    );\n\n    this.channelApi.postSenseUpdate(syncStatusItems);\n    this.statusApi.sendStatus('active');\n\n    return lastTransactionTimestamp;\n  }\n\n  protected async onUpdate(\n    { source, transactions }: DataStreamResult,\n    params: SyncServiceParams\n  ) {\n    const { myAddress } = params;\n    const { signal } = this.abortController;\n    if (transactions.length === 0) {\n      this.cyblogCh.info(`>>> ${this.name} ${myAddress} recived 0 updates `);\n      return;\n    }\n    const syncItem = await this.db!.getSyncStatus(myAddress!, myAddress!);\n\n    await this.processBatchTransactions(\n      myAddress!,\n      myAddress!,\n      transactions,\n      syncItem,\n      source\n    );\n\n    this.statusApi.sendStatus('in-progress', `sync my chats`);\n    const syncStatusItems = await syncMyChats(\n      this.db!,\n      myAddress!,\n      syncItem.timestampUpdate,\n      signal,\n      source !== 'node'\n    );\n\n    this.channelApi.postSenseUpdate(syncStatusItems);\n    this.statusApi.sendStatus('listen');\n  }\n\n  public async processBatchTransactions(\n    myAddress: NeuronAddress,\n    address: NeuronAddress,\n    transactions: TransactionDto[],\n    { timestampRead, unreadCount, timestampUpdate }: SyncStatusDto,\n    source: DataStreamResult['source']\n  ) {\n    const { signal } = this.abortController;\n\n    // node transaction is limited by incoming messages,\n    // to prevent missing of other msg types let's avoid to change ts\n    const shouldUpdateTimestamp = source !== 'node';\n\n    this.cyblogCh.info(\n      `   syncTransactions - process ${address}[${source}],  count: ${\n        transactions.length\n      }, from: ${transactions.at(0)?.timestamp}, to: ${\n        transactions.at(-1)?.timestamp\n      }`\n    );\n\n    // save transaction\n    await throwIfAborted(this.db!.putTransactions, signal)(transactions);\n\n    // save links\n    this.syncLinks(transactions, signal);\n\n    const {\n      hash,\n      index,\n\n      timestamp,\n    } = transactions.at(-1)!;\n\n    const lastTimestampFrom = timestamp;\n\n    // Update transaction sync items\n    const newSyncItem = {\n      ownerId: myAddress,\n      entryType: EntryType.transactions,\n      id: address,\n      timestampUpdate: shouldUpdateTimestamp\n        ? lastTimestampFrom\n        : timestampUpdate!,\n      unreadCount: unreadCount! + transactions.length,\n      timestampRead: timestampRead || 0,\n      disabled: false,\n      meta: {\n        transactionHash: hash,\n        index,\n      },\n    };\n\n    await throwIfAborted(this.db!.putSyncStatus, signal)(newSyncItem);\n\n    return lastTimestampFrom;\n  }\n\n  public async syncTransactions(\n    myAddress: NeuronAddress,\n    address: NeuronAddress,\n    syncItem: SyncStatusDto\n  ) {\n    const { unreadCount, timestampUpdate } = syncItem;\n    const timestampFrom = timestampUpdate + 1; // ofsset + 1 to fix milliseconds precision bug\n\n    this.statusApi.sendStatus('estimating');\n\n    const totalMessageCount = await fetchTransactionMessagesCount(\n      address,\n      timestampFrom,\n      this.abortController!.signal\n    );\n\n    this.cyblogCh.info(\n      `>>> syncTransactions - start ${address},  count: ${totalMessageCount}, from: ${timestampFrom}`\n    );\n\n    if (totalMessageCount === 0) {\n      return timestampFrom;\n    }\n\n    this.statusApi.sendStatus(\n      'in-progress',\n      `sync ${address}...`,\n      this.progressTracker.start(\n        Math.ceil(totalMessageCount / TRANSACTIONS_BATCH_LIMIT)\n      )\n    );\n\n    const transactionsAsyncIterable = fetchTransactionsIterable({\n      neuron: address,\n      timestampFrom,\n      types: [], // SENSE_TRANSACTIONS,\n      orderDirection: 'asc',\n      limit: TRANSACTIONS_BATCH_LIMIT,\n      abortSignal: this.abortController?.signal,\n    });\n\n    let transactionCount = 0;\n    let lastTimestampFrom = timestampFrom;\n\n    // eslint-disable-next-line no-restricted-syntax\n    for await (const batch of transactionsAsyncIterable) {\n      this.statusApi.sendStatus(\n        'in-progress',\n        `sync ${address}...`,\n        this.progressTracker.trackProgress(1)\n      );\n\n      transactionCount += batch.length;\n\n      const transactions = batch.map((i) =>\n        mapIndexerTransactionToEntity(address, i)\n      );\n\n      lastTimestampFrom = await this.processBatchTransactions(\n        myAddress,\n        address,\n        transactions,\n        {\n          ...syncItem,\n          unreadCount: unreadCount + transactionCount,\n        },\n        'indexer'\n      );\n    }\n\n    return lastTimestampFrom;\n  }\n\n  private async syncLinks(batch: TransactionDto[], signal: AbortSignal) {\n    const { tweets, particlesFound, links } =\n      extractCybelinksFromTransaction(batch);\n    if (links.length > 0) {\n      await asyncIterableBatchProcessor(\n        links,\n        (links) => throwIfAborted(this.db!.putCyberlinks, signal)(links),\n        MAX_DATABASE_PUT_SIZE\n      );\n    }\n\n    const tweetParticles = Object.keys(tweets);\n\n    const nonTweetParticles = particlesFound.filter(\n      (cid) => !tweetParticles.includes(cid)\n    );\n\n    // pre-resolve 'tweets' particles\n    await this.particlesResolver!.enqueueBatch(\n      tweetParticles,\n      SyncQueueJobType.particle,\n      QueuePriority.HIGH\n    );\n\n    // pre-resolve all the rest particles\n    if (nonTweetParticles.length > 0) {\n      await this.particlesResolver!.enqueueBatch(\n        nonTweetParticles,\n        SyncQueueJobType.particle,\n        QueuePriority.LOW\n      );\n    }\n  }\n}\n\nexport default SyncTransactionsLoop;\n","import { Observable } from 'rxjs';\nimport { WEBSOCKET_URL } from 'src/constants/config';\nimport { NeuronAddress } from 'src/types/base';\nimport { LogFunc } from 'src/utils/logging/cyblog';\n\nexport const getIncomingTransfersQuery = (address: NeuronAddress) =>\n  `tm.event='Tx' AND transfer.recipient='${address}'`;\n\n// eslint-disable-next-line import/no-unused-modules\nexport function createNodeWebsocketObservable(\n  address: NeuronAddress,\n  query: string,\n  log: LogFunc\n) {\n  return new Observable((subscriber) => {\n    const ws = new WebSocket(WEBSOCKET_URL);\n\n    ws.onopen = () => {\n      log(`node ws connected to ${WEBSOCKET_URL} with ${query}`);\n      ws.send(\n        JSON.stringify({\n          jsonrpc: '2.0',\n          method: 'subscribe',\n          id: '0',\n          params: { query },\n        })\n      );\n    };\n\n    ws.onmessage = (event) => {\n      const message = JSON.parse(event.data);\n      log(`node ws ${address} onmessage`, message);\n      subscriber.next(message.result);\n    };\n\n    ws.onerror = (event) => {\n      log(`node ws ${address} error`, { error: event });\n      subscriber.error(event);\n    };\n\n    ws.onclose = () => {\n      log(`node ws ${address} closed`);\n      subscriber.complete();\n    };\n\n    return () => {\n      ws.close();\n    };\n  });\n}\n","import { EntityToDto, DtoToEntity } from 'src/types/dto';\nimport { deserializeString } from './string';\n\nexport const snakeToCamel = (str: string) =>\n  str.replace(/([-_][a-z])/g, (group) =>\n    group.toUpperCase().replace('-', '').replace('_', '')\n  );\n\nexport const camelToSnake = (str: string) =>\n  str.replace(/[A-Z]/g, (letter) => `_${letter.toLowerCase()}`);\n// Function to transform a DB entity to a DTO\n\n// eslint-disable-next-line import/no-unused-modules\nexport function entityToDto<T extends Record<string, any>>(\n  dbEntity: T\n): EntityToDto<T> {\n  if (!dbEntity || typeof dbEntity !== 'object') {\n    return dbEntity;\n  }\n  const dto: Record<string, any> = {}; // Specify the type for dto\n  Object.keys(dbEntity).forEach((key) => {\n    if (Object.prototype.hasOwnProperty.call(dbEntity, key)) {\n      const camelCaseKey = snakeToCamel(key);\n      let value = dbEntity[key];\n      if (Array.isArray(dbEntity[key])) {\n        value = dbEntity[key].map((item) => entityToDto(item));\n      } else if (typeof dbEntity[key] === 'object') {\n        value = entityToDto(dbEntity[key]);\n      } else if (typeof dbEntity[key] === 'string') {\n        value = deserializeString(value);\n      }\n      dto[camelCaseKey] = value;\n    }\n  });\n  return dto as EntityToDto<T>;\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport function dtoToEntity<T extends Record<string, any>>(\n  dto: T\n): DtoToEntity<T> {\n  // in case of recursive calls\n  if (!dto || typeof dto !== 'object') {\n    return dto;\n  }\n  const dbEntity: any = {};\n\n  Object.keys(dto).forEach((key) => {\n    if (Object.prototype.hasOwnProperty.call(dto, key)) {\n      const snakeCaseKey = camelToSnake(key);\n      let value = dto[key];\n      if (Array.isArray(value)) {\n        value = value.map((item) => dtoToEntity(item));\n      } else if (typeof value === 'object') {\n        value = dtoToEntity(value);\n      }\n      //  else if (typeof value === 'string') {\n      //   value = replaceQuotes(value);\n      // }\n      dbEntity[snakeCaseKey] = value;\n    }\n  });\n  return dbEntity as DtoToEntity<T>; // Replace T with the appropriate DB Entity type if known\n}\n\nexport function dtoListToEntity<T extends Record<string, any>>(\n  array: T[]\n): DtoToEntity<T>[] {\n  return array.map((dto) => dtoToEntity(dto));\n}\n\nexport function entityListToDto<T extends Record<string, any>>(\n  array: T[]\n): EntityToDto<T>[] {\n  return array.map((dto) => entityToDto(dto));\n}\n\nexport function removeUndefinedFields(entity: Record<string, any>) {\n  Object.keys(entity).forEach((key) => {\n    if (entity[key] === undefined) {\n      delete entity[key];\n    }\n  });\n  return entity;\n}\n","import { NeuronAddress } from 'src/types/base';\nimport { LinkDto, SyncStatusDto } from 'src/services/CozoDb/types/dto';\nimport { EntryType } from 'src/services/CozoDb/types/entities';\n\nimport { findLastIndex } from 'lodash';\nimport { entityToDto } from 'src/utils/dto';\n\nimport { SenseItemLinkMeta } from '../../types/sense';\nimport { SyncEntryName } from '../../types/services';\n\nexport function getLastReadInfo(\n  links: LinkDto[],\n  ownerId: NeuronAddress,\n  prevTimestampRead = 0,\n  prevUnreadCount = 0\n) {\n  const lastUnreadLinks = links.filter(\n    (link) => link.timestamp > prevTimestampRead\n  );\n  const lastMyLinkIndex = findLastIndex(\n    lastUnreadLinks,\n    (link) => link.neuron === ownerId\n  );\n\n  const unreadCount =\n    lastMyLinkIndex < 0\n      ? prevUnreadCount + lastUnreadLinks.length\n      : lastUnreadLinks.length - lastMyLinkIndex - 1;\n\n  const timestampRead =\n    lastMyLinkIndex < 0 ? prevTimestampRead : links[lastMyLinkIndex].timestamp;\n\n  return {\n    timestampRead,\n    unreadCount,\n  };\n}\n\nexport function changeParticleSyncStatus(\n  syncStatus: Partial<SyncStatusDto>,\n  links: LinkDto[],\n  ownerId: NeuronAddress,\n  shouldUpdateTimestamp = true\n) {\n  const { timestampRead, unreadCount } = getLastReadInfo(\n    links,\n    ownerId,\n    syncStatus.timestampRead,\n    syncStatus.unreadCount\n  );\n\n  const lastLink = entityToDto(links[links.length - 1]);\n  const timestampUpdate = lastLink.timestamp;\n  return {\n    ...syncStatus,\n    ownerId,\n    entryType: EntryType.particle,\n    disabled: false,\n    unreadCount,\n    meta: {\n      ...lastLink,\n      timestamp: timestampUpdate,\n    } as SenseItemLinkMeta,\n    timestampRead,\n    timestampUpdate: shouldUpdateTimestamp\n      ? timestampUpdate\n      : syncStatus.timestampUpdate,\n  } as SyncStatusDto;\n}\n\nconst mapSyncEntryReadable: Record<SyncEntryName, string> = {\n  'my-friends': \"friend's logs\",\n  particles: 'log cyberlinks',\n  resolver: 'particles',\n  transactions: 'transactions',\n  pin: 'ipfs pins',\n};\n\nexport const syncEntryNameToReadable = (name: SyncEntryName) =>\n  mapSyncEntryReadable[name] || name;\n","export const isAbortException = (e: Error) =>\n  e instanceof DOMException && e.name === 'AbortError';\n","import { Observable, defer, filter, from, tap } from 'rxjs';\n\nimport { SyncEntryName } from 'src/services/backend/types/services';\nimport { isAbortException } from 'src/utils/exceptions/helpers';\nimport { clone } from 'ramda';\n\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { ServiceDeps } from '../types';\nimport { createLoopObservable } from '../utils/rxjs/loop';\nimport BaseSync from './BaseSync';\nimport { SyncServiceParams } from '../../types';\n\nabstract class BaseSyncLoop extends BaseSync {\n  private restartLoop: (() => void) | undefined;\n\n  public readonly loop$: Observable<boolean>;\n\n  constructor(\n    name: SyncEntryName,\n    intervalMs: number,\n    deps: ServiceDeps,\n    particlesResolver: ParticlesResolverQueue,\n    {\n      warmupMs,\n    }: {\n      warmupMs: number;\n    } = { warmupMs: 0 }\n  ) {\n    super(name, deps, particlesResolver);\n\n    const { loop$, restartLoop } = createLoopObservable(\n      this.isInitialized$,\n      // defer(() => from(this.sync())),\n      defer(() => from(this.doSync())),\n      {\n        intervalMs,\n        warmupMs,\n        // onStartInterval: () => this.initAbortController(),\n        onError: (error) => {\n          this.cyblogCh.info(`>>> ${name} error`, error.toString());\n          this.statusApi.sendStatus('error', error.toString());\n        },\n        onChange: (isInitialized) => {\n          this.cyblogCh.info(`>>> ${name} initialized: ${isInitialized}`);\n          this.statusApi.sendStatus(isInitialized ? 'initialized' : 'inactive');\n        },\n      }\n    );\n\n    this.loop$ = loop$;\n    this.restartLoop = restartLoop;\n  }\n\n  public restart() {\n    this.abortController?.abort();\n    this.restartLoop?.();\n    this.cyblogCh.info(`>>> ${this.name} loop restart`);\n  }\n\n  public start() {\n    this.loop$.subscribe(() => this.statusApi.sendStatus('active'));\n    return this;\n  }\n\n  private async doSync() {\n    const params = clone(this.params);\n    this.initAbortController();\n    try {\n      await this.sync(params);\n    } catch (e) {\n      const isAborted = isAbortException(e);\n      this.cyblogCh.info(\n        `>>> ${this.name} ${params.myAddress} sync error [abrt:${isAborted}]:`,\n        {\n          error: e,\n        }\n      );\n\n      if (!isAborted) {\n        throw e;\n      }\n    }\n  }\n\n  protected abstract sync(params: SyncServiceParams): Promise<void>;\n}\n\nexport default BaseSyncLoop;\n","/* eslint-disable import/prefer-default-export */\nimport {\n  Observable,\n  switchMap,\n  interval,\n  startWith,\n  tap,\n  retry,\n  delay,\n  exhaustMap,\n  Subject,\n} from 'rxjs';\nimport { switchWhenInitialized } from './withInitializer';\n\ntype LoopObservableOptions = {\n  warmupMs?: number;\n  retryDelayMs?: number;\n  onStartInterval?: () => void;\n  onError?: (error: any) => void;\n  onChange?: (isInitialized: boolean) => void;\n  intervalMs?: number;\n};\n\nexport const createLoopObservable = (\n  isInitialized$: Observable<boolean>,\n  actionObservable$: Observable<any>,\n  options: LoopObservableOptions = {}\n) => {\n  const {\n    intervalMs,\n    warmupMs = 0,\n    onStartInterval,\n    onError,\n    retryDelayMs = 0,\n    onChange,\n  } = options;\n\n  const restartTrigger$ = new Subject<void>();\n\n  const intervalOrRestart$ = restartTrigger$.pipe(\n    startWith(null),\n    switchMap(() => interval(intervalMs).pipe(startWith(0), delay(warmupMs)))\n  );\n\n  const source$ = switchWhenInitialized(\n    isInitialized$,\n    intervalOrRestart$.pipe(\n      tap(() => onStartInterval && onStartInterval()),\n      exhaustMap(() =>\n        actionObservable$.pipe(\n          retry({\n            delay: (error) => {\n              console.log('retry', error);\n              onError && onError(error);\n              return interval(retryDelayMs);\n            },\n          })\n        )\n      )\n    ),\n    (isInitialized) => onChange?.(isInitialized)\n  );\n\n  return {\n    loop$: source$,\n    restartLoop: () => {\n      // console.log('>>> createLoopObservable restart');\n      // Trigger a restart by emitting a new value\n      restartTrigger$.next();\n    },\n  };\n};\n","import { map, combineLatest, distinctUntilChanged } from 'rxjs';\nimport { EntryType } from 'src/services/CozoDb/types/entities';\nimport { SyncStatusDto } from 'src/services/CozoDb/types/dto';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { NeuronAddress } from 'src/types/base';\n\nimport { mapLinkFromIndexerToDto } from 'src/services/CozoDb/mapping';\nimport { CID_TWEET } from 'src/constants/app';\nimport { dateToUtcNumber } from 'src/utils/date';\nimport { SenseListItem } from 'src/services/backend/types/sense';\nimport { asyncIterableBatchProcessor } from 'src/utils/async/iterable';\nimport { throwIfAborted } from 'src/utils/async/promise';\nimport { entityToDto } from 'src/utils/dto';\n\nimport { ServiceDeps } from '../types';\nimport { fetchCyberlinksAndResolveParticles } from '../utils/links';\n\nimport { changeParticleSyncStatus } from '../../utils';\nimport {\n  fetchCyberlinksByNerounIterable,\n  fetchCyberlinksCount,\n} from '../../../indexer/cyberlinks';\nimport { CYBERLINKS_BATCH_LIMIT } from '../../../indexer/consts';\nimport BaseSyncLoop from '../BaseSyncLoop/BaseSyncLoop';\nimport { MAX_DATABASE_PUT_SIZE } from '../consts';\nimport { SyncServiceParams } from '../../types';\n\nclass SyncParticlesLoop extends BaseSyncLoop {\n  protected createIsInitializedObserver(deps: ServiceDeps) {\n    const isInitialized$ = combineLatest([\n      deps.dbInstance$,\n      deps.ipfsInstance$,\n      deps.params$!.pipe(\n        map((params) => params.myAddress),\n        distinctUntilChanged()\n      ),\n      this.particlesResolver!.isInitialized$,\n    ]).pipe(\n      map(\n        ([dbInstance, ipfsInstance, myAddress, particleResolverInitialized]) =>\n          !!ipfsInstance &&\n          !!dbInstance &&\n          !!particleResolverInitialized &&\n          !!myAddress\n      )\n    );\n\n    return isInitialized$;\n  }\n\n  protected async sync(params: SyncServiceParams): Promise<void> {\n    const { myAddress } = params;\n    const { signal } = this.abortController;\n    this.statusApi.sendStatus('estimating');\n\n    const syncItemParticles = await this.db!.findSyncStatus({\n      ownerId: myAddress!,\n      entryType: EntryType.particle,\n    });\n\n    const timestampUpdate = syncItemParticles.at(0)?.timestampUpdate || 0;\n\n    // Get count of new links after last update\n    const newLinkCount = await fetchCyberlinksCount(\n      myAddress!,\n      [CID_TWEET],\n      timestampUpdate,\n      signal\n    );\n\n    this.cyblogCh.info(\n      `>>> syncMyParticles ${myAddress} count ${newLinkCount}`\n    );\n    this.progressTracker.start(newLinkCount + syncItemParticles.length);\n    this.statusApi.sendStatus(\n      'in-progress',\n      'preparing...',\n      this.progressTracker.progress\n    );\n\n    if (newLinkCount > 0) {\n      // fetch and save new particles\n      const newSyncItemParticles = await this.fetchNewTweets(\n        myAddress!,\n        timestampUpdate,\n        signal\n      );\n\n      // add to fetch-sync linked particles\n      syncItemParticles.push(...newSyncItemParticles);\n    }\n    await this.syncParticles(myAddress!, syncItemParticles, signal);\n  }\n\n  private async fetchNewTweets(\n    myAddress: NeuronAddress,\n    timestampUpdate: number,\n    signal: AbortSignal\n  ) {\n    const tweetsAsyncIterable = await fetchCyberlinksByNerounIterable(\n      myAddress,\n      [CID_TWEET],\n      timestampUpdate,\n      CYBERLINKS_BATCH_LIMIT,\n      this.abortController?.signal\n    );\n\n    const newTweets: SyncStatusDto[] = [];\n    const existingParticles = await this.db!.findSyncStatus({\n      ownerId: myAddress,\n      entryType: EntryType.particle,\n    });\n    const existingParticlesMap = new Map(\n      existingParticles.map((i) => [i.id, i])\n    );\n    // eslint-disable-next-line no-await-in-loop, no-restricted-syntax\n    for await (const tweetsBatch of tweetsAsyncIterable) {\n      this.statusApi.sendStatus(\n        'in-progress',\n        `fetching new tweets...`,\n        this.progressTracker.trackProgress(1)\n      );\n      const syncStatusEntities = tweetsBatch.map(entityToDto).map((item) => {\n        const { timestamp, to } = item;\n        const timestampUpdate = dateToUtcNumber(timestamp);\n\n        // In case my tweet already linked from other neuron, resync from beginning\n        const timestampSyncFrom = existingParticlesMap.get(to)\n          ? dateToUtcNumber(timestamp)\n          : 0;\n\n        // Initial state\n        return {\n          ownerId: myAddress,\n          id: to,\n          entryType: EntryType.particle,\n          timestampUpdate: timestampSyncFrom,\n          timestampRead: timestampUpdate,\n          unreadCount: 0,\n          disabled: false,\n          meta: { ...item, timestamp: timestampUpdate },\n        } as SyncStatusDto;\n      });\n\n      if (syncStatusEntities.length > 0) {\n        await throwIfAborted(\n          this.db!.putSyncStatus,\n          signal\n        )(syncStatusEntities);\n        newTweets.push(...syncStatusEntities);\n      }\n    }\n\n    return newTweets;\n  }\n\n  private async syncParticles(\n    myAddress: NeuronAddress,\n    syncItems: SyncStatusDto[],\n    signal: AbortSignal\n  ) {\n    const updatedSyncItems: SyncStatusDto[] = [];\n\n    // eslint-disable-next-line no-restricted-syntax\n    for (const syncItem of syncItems) {\n      const { id, timestampUpdate } = syncItem;\n\n      this.statusApi.sendStatus(\n        'in-progress',\n        `fetching tweet updates...`,\n        this.progressTracker.trackProgress(1)\n      );\n      // eslint-disable-next-line no-await-in-loop\n      const linksIndexer = await fetchCyberlinksAndResolveParticles(\n        id,\n        timestampUpdate,\n        this.particlesResolver!,\n        QueuePriority.MEDIUM,\n        this.abortController?.signal\n      );\n\n      if (linksIndexer.length > 0) {\n        const links = linksIndexer.map(mapLinkFromIndexerToDto);\n\n        // save links\n        // eslint-disable-next-line no-await-in-loop\n        await asyncIterableBatchProcessor(\n          links,\n          (links) => throwIfAborted(this.db!.putCyberlinks, signal)(links),\n          MAX_DATABASE_PUT_SIZE\n        );\n\n        const newItem = changeParticleSyncStatus(syncItem, links, myAddress);\n\n        updatedSyncItems.push(newItem);\n      }\n    }\n\n    if (updatedSyncItems.length > 0) {\n      await throwIfAborted(this.db!.putSyncStatus, signal)(updatedSyncItems);\n    }\n    this.channelApi.postSenseUpdate(updatedSyncItems as SenseListItem[]);\n  }\n}\n\nexport default SyncParticlesLoop;\n","/* eslint-disable camelcase */\nimport {\n  map,\n  combineLatest,\n  distinctUntilChanged,\n  BehaviorSubject,\n} from 'rxjs';\n\nimport {\n  EntryType,\n  SyncQueueJobType,\n} from 'src/services/CozoDb/types/entities';\n\nimport { NeuronAddress } from 'src/types/base';\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { isAbortException } from 'src/utils/exceptions/helpers';\n\nimport { mapLinkFromIndexerToDto } from 'src/services/CozoDb/mapping';\nimport { throwIfAborted } from 'src/utils/async/promise';\n\nimport { SyncEntryName } from 'src/services/backend/types/services';\nimport { SenseItemLinkMeta } from 'src/services/backend/types/sense';\nimport { entityToDto } from 'src/utils/dto';\nimport { ServiceDeps } from '../types';\n\nimport { fetchCyberlinksByNerounIterable } from '../../../indexer/cyberlinks';\nimport { CYBERLINKS_BATCH_LIMIT } from '../../../indexer/consts';\nimport BaseSyncLoop from '../BaseSyncLoop/BaseSyncLoop';\nimport { SyncServiceParams } from '../../types';\nimport { getLastReadInfo } from '../../utils';\n\nimport ParticlesResolverQueue from '../ParticlesResolverQueue/ParticlesResolverQueue';\nimport { SENSE_FRIEND_PARTICLES } from '../consts';\n\nclass SyncMyFriendsLoop extends BaseSyncLoop {\n  protected followings: NeuronAddress[] = [];\n\n  constructor(\n    name: SyncEntryName,\n    intervalMs: number,\n    deps: ServiceDeps,\n    particlesResolver: ParticlesResolverQueue,\n    { warmupMs }: { warmupMs: number } = { warmupMs: 0 }\n  ) {\n    if (!deps.followings$) {\n      throw new Error('followings$ is required');\n    }\n\n    super(name, intervalMs, deps, particlesResolver, {\n      warmupMs,\n    });\n  }\n\n  protected createIsInitializedObserver(deps: ServiceDeps) {\n    const followingsInitialized$ = new BehaviorSubject<boolean>(false);\n    deps.params$\n      ?.pipe(\n        map((params) => params.myAddress),\n        distinctUntilChanged()\n      )\n      .subscribe(() => {\n        followingsInitialized$.next(false);\n      });\n\n    deps.followings$!.subscribe((followings) => {\n      this.followings = followings;\n      followingsInitialized$.next(true);\n\n      this.restart();\n    });\n\n    const isInitialized$ = combineLatest([\n      deps.dbInstance$,\n      deps.params$!,\n      this.particlesResolver!.isInitialized$,\n      followingsInitialized$!,\n    ]).pipe(\n      map(\n        ([dbInstance, params, syncQueueInitialized, followingsInitialized]) =>\n          !!dbInstance &&\n          !!params.myAddress &&\n          !!syncQueueInitialized &&\n          followingsInitialized\n      )\n    );\n\n    return isInitialized$;\n  }\n\n  protected async sync(params: SyncServiceParams) {\n    const { signal } = this.abortController;\n\n    this.statusApi.sendStatus('in-progress', 'preparing...');\n    const { myAddress } = params;\n\n    const { followings } = this;\n\n    this.statusApi.sendStatus('estimating');\n\n    this.cyblogCh.info(\n      `>>> syncMyFriends ${myAddress} count ${followings.length}`,\n      {\n        unit: 'friends-sync',\n        data: followings,\n      }\n    );\n\n    this.progressTracker.start(followings.length);\n    this.statusApi.sendStatus(\n      'in-progress',\n      `sync...`,\n      this.progressTracker.progress\n    );\n\n    // eslint-disable-next-line no-restricted-syntax\n    for (const addr of followings) {\n      // eslint-disable-next-line no-await-in-loop\n      await this.syncLinks(myAddress!, addr, signal);\n    }\n  }\n\n  public async syncLinks(\n    myAddress: NeuronAddress,\n    address: NeuronAddress,\n    signal: AbortSignal\n  ) {\n    let syncUpdates = [];\n    try {\n      this.statusApi.sendStatus(\n        'in-progress',\n        `starting sync ${address}...`,\n        this.progressTracker.progress\n      );\n      const { timestampRead, unreadCount, meta } = await this.db!.getSyncStatus(\n        myAddress,\n        address\n      );\n\n      const { timestampUpdateChat = 0, timestampUpdateContent = 0 } =\n        meta || {};\n\n      const timestampFrom = timestampUpdateContent + 1; // ofsset + 1 to fix milliseconds precision bug\n\n      const linksAsyncIterable = await fetchCyberlinksByNerounIterable(\n        address,\n        SENSE_FRIEND_PARTICLES,\n        timestampFrom,\n        CYBERLINKS_BATCH_LIMIT,\n        signal\n      );\n\n      // eslint-disable-next-line no-restricted-syntax\n      for await (const linksBatch of linksAsyncIterable) {\n        this.statusApi.sendStatus(\n          'in-progress',\n          `sync ${address}...`,\n          this.progressTracker.trackProgress(1)\n        );\n\n        const links = linksBatch.map(mapLinkFromIndexerToDto);\n\n        const { timestampRead: newTimestampRead, unreadCount: newUnreadCount } =\n          getLastReadInfo(links, myAddress, timestampRead, unreadCount);\n\n        // const unreadItemsCount = unreadCount + links.length;\n\n        if (links.length > 0) {\n          const lastLink = entityToDto(links.at(-1)!);\n          const newTimestampUpdateContent = lastLink!.timestamp;\n\n          await throwIfAborted(this.db!.putCyberlinks, signal)(links);\n\n          const particles = links.map((t) => t.to);\n          await this.particlesResolver!.enqueueBatch(\n            particles,\n            SyncQueueJobType.particle,\n            QueuePriority.HIGH\n          );\n\n          const newSyncItem = {\n            ownerId: myAddress,\n            entryType: EntryType.chat,\n            id: address,\n            timestampUpdate: Math.max(\n              newTimestampUpdateContent,\n              timestampUpdateChat\n            ),\n            unreadCount: newUnreadCount,\n            timestampRead: newTimestampRead,\n            disabled: false,\n            meta: {\n              ...lastLink!,\n              timestampUpdateContent: newTimestampUpdateContent,\n              timestampUpdateChat,\n            } as SenseItemLinkMeta,\n          };\n          // Update transaction\n          await throwIfAborted(this.db!.putSyncStatus, signal)(newSyncItem);\n\n          syncUpdates.push(newSyncItem);\n        }\n      }\n    } catch (err) {\n      this.cyblogCh.error(`>>> SyncMyFriends ${address} error`, {\n        error: err,\n      });\n      if (!isAbortException(err)) {\n        this.statusApi.sendStatus('error', err.toString());\n      } else {\n        syncUpdates = [];\n        throw err;\n      }\n    } finally {\n      // console.log('-----syncUpdates with redux', syncUpdates);\n      this.channelApi.postSenseUpdate(syncUpdates);\n    }\n  }\n\n  // eslint-disable-next-line class-methods-use-this\n  // protected createRestartObserver(\n  //   params$: Observable<SyncServiceParams>\n  // ): Observable<boolean> {\n  //   return super\n  //     .createRestartObserver(params$)\n  //     .pipe(switchMap((addressChanged) => this.isInitialized$));\n  // }\n}\n\nexport default SyncMyFriendsLoop;\n","import { NeuronAddress, ParticleCid } from 'src/types/base';\nimport { getIpfsHash } from 'src/utils/ipfs/helpers';\nimport { PATTERN_CYBER } from 'src/constants/patterns';\nimport { Subject, Observable } from 'rxjs';\n\nimport DbApiWrapper from '../backend/services/DbApi/DbApi';\nimport { getFollowsAsCid, getFollowers } from './lcd';\nimport { FetchParticleAsync, QueuePriority } from '../QueueManager/types';\nimport { CommunityDto } from '../CozoDb/types/dto';\nimport { FetchIpfsFunc } from '../backend/services/sync/types';\nimport { createCyblogChannel } from 'src/utils/logging/cyblog';\n\nexport type SyncCommunityResult = {\n  action: 'reset' | 'add' | 'complete';\n  items: CommunityDto[];\n};\n\nconst cyblogCh = createCyblogChannel({\n  thread: 'bckd',\n  unit: 'fetchStoredSyncCommunity',\n});\n\n// eslint-disable-next-line import/prefer-default-export, import/no-unused-modules\nexport const fetchStoredSyncCommunity$ = (\n  dbApi: DbApiWrapper,\n  address: NeuronAddress,\n  fetchParticleAsync?: FetchIpfsFunc,\n  signal?: AbortSignal\n): Observable<SyncCommunityResult> => {\n  return new Observable<SyncCommunityResult>((subscriber) => {\n    subscriber.next({ action: 'reset', items: [] });\n\n    (async () => {\n      const storedCommunity = await dbApi.getCommunity(address);\n\n      subscriber.next({ action: 'add', items: storedCommunity });\n\n      const communityUpdatesMap = new Map<ParticleCid, CommunityDto>(\n        storedCommunity.map((c) => [c.particle, c])\n      );\n\n      const getExistingOrDefault = (cid: ParticleCid): Partial<CommunityDto> =>\n        communityUpdatesMap.get(cid) || {\n          ownerId: address,\n          name: '',\n          following: false,\n          follower: false,\n        };\n\n      const followsCids = await getFollowsAsCid(address, signal);\n      const followers = await getFollowers(address, signal);\n\n      const newFollowerCids = followsCids.filter(\n        (cid) => !storedCommunity.some((i) => i.particle === cid && i.following)\n      );\n\n      const newFollowingNeurons = followers.filter(\n        (addr) => !storedCommunity.some((i) => i.neuron === addr && i.follower)\n      );\n\n      cyblogCh.info(\n        `>>>$ sync community ${address} processing, stored ${storedCommunity.length} new followers: ${newFollowerCids.length} new following: ${newFollowingNeurons.length}`\n      );\n\n      const followersCommunity = await Promise.all(\n        newFollowingNeurons.map(async (neuron) => {\n          const cid = await getIpfsHash(neuron);\n\n          const communityItem = {\n            ...getExistingOrDefault(cid),\n            particle: cid,\n            neuron,\n            follower: true,\n          } as CommunityDto;\n\n          await dbApi.putCommunity(communityItem);\n          communityUpdatesMap.set(cid, communityItem);\n          return communityItem;\n        })\n      );\n\n      subscriber.next({ action: 'add', items: followersCommunity });\n\n      await Promise.all(\n        newFollowerCids.map(async (cid: ParticleCid) => {\n          const neuron = (await fetchParticleAsync!(cid, QueuePriority.URGENT))\n            ?.result?.textPreview;\n          if (neuron && neuron.match(PATTERN_CYBER)) {\n            const communityItem = {\n              ...getExistingOrDefault(cid),\n              neuron,\n              particle: cid,\n              following: true,\n            } as CommunityDto;\n\n            await dbApi.putCommunity(communityItem);\n            communityUpdatesMap.set(cid, communityItem);\n            subscriber.next({ action: 'add', items: [communityItem] });\n          }\n        })\n      );\n\n      cyblogCh.info(`>>>$ sync community ${address}, done`);\n      // const communityUpdates = [...communityUpdatesMap.values()];\n\n      // if (communityUpdates.length > 0) {\n      //   subscriber.next(communityUpdates);\n      // }\n      subscriber.next({ action: 'complete', items: [] });\n\n      subscriber.complete();\n    })().catch((err) => {\n      cyblogCh.error(`>>>$ sync community ${address}, error`, { error: err });\n      subscriber.error(err);\n    });\n  });\n};\n\n// eslint-disable-next-line import/no-unused-modules\nexport const fetchCommunity = async (\n  address: NeuronAddress,\n  fetchParticleAsync?: FetchParticleAsync,\n  onResolve?: (community: CommunityDto[]) => void,\n  signal?: AbortSignal\n) => {\n  const communityUpdatesMap = new Map<ParticleCid, CommunityDto>();\n\n  const getExistingOrDefault = (cid: ParticleCid): Partial<CommunityDto> =>\n    communityUpdatesMap.get(cid) || {\n      ownerId: address,\n      name: '',\n      following: false,\n      follower: false,\n    };\n\n  const followsCids = await getFollowsAsCid(address, signal);\n  const followers = await getFollowers(address, signal);\n\n  console.log(`>>> sync community ${address} processing without store`);\n\n  const followsPromise = Promise.all(\n    followsCids.map(async (cid) => {\n      const neuron = (await fetchParticleAsync!(cid))?.result?.textPreview;\n      if (neuron && neuron.match(PATTERN_CYBER)) {\n        const communityItem = {\n          ...getExistingOrDefault(cid),\n          neuron,\n          particle: cid,\n          following: true,\n        } as CommunityDto;\n        communityUpdatesMap.set(cid, communityItem);\n        onResolve && !signal?.aborted && onResolve([communityItem]);\n      }\n    })\n  );\n\n  const followersPromise = Promise.all(\n    followers.map(async (neuron) => {\n      const cid = await getIpfsHash(neuron);\n\n      const communityItem = {\n        ...getExistingOrDefault(cid),\n        particle: cid,\n        neuron,\n        follower: true,\n      } as CommunityDto;\n\n      communityUpdatesMap.set(cid, communityItem);\n      onResolve && !signal?.aborted && onResolve([communityItem]);\n    })\n  );\n\n  await Promise.all([followersPromise, followsPromise]);\n};\n","import { NeuronAddress, ParticleCid } from 'src/types/base';\nimport { CID_FOLLOW } from 'src/constants/app';\nimport { getIpfsHash } from 'src/utils/ipfs/helpers';\nimport { getTransactions } from 'src/services/transactions/lcd';\n\nexport const getFollowsAsCid = async (\n  address: NeuronAddress,\n  signal?: AbortSignal\n): Promise<ParticleCid[]> => {\n  const response = await getTransactions({\n    events: [\n      {\n        key: 'cyberlink.neuron',\n        value: address,\n      },\n      {\n        key: 'cyberlink.particleFrom',\n        value: CID_FOLLOW,\n      },\n    ],\n    pagination: {\n      limit: 1000000000,\n    },\n    config: {\n      signal,\n    },\n  });\n\n  if (!response.txResponses.length) {\n    return [];\n  }\n\n  return response.txResponses.map(\n    (item) => item?.tx?.body.messages[0].links[0].to\n  );\n};\n\n// use src/services/transactions/lcd.tsx\nexport const getFollowers = async (\n  address: NeuronAddress,\n  signal?: AbortSignal\n): Promise<NeuronAddress[]> => {\n  const addressHash = await getIpfsHash(address);\n\n  const response = await getTransactions({\n    events: [\n      {\n        key: 'cyberlink.particleFrom',\n        value: CID_FOLLOW,\n      },\n      {\n        key: 'cyberlink.particleTo',\n        value: addressHash,\n      },\n    ],\n    pagination: {\n      limit: 1000000000,\n    },\n    config: {\n      signal,\n    },\n  });\n\n  if (!response.txResponses.length) {\n    return [];\n  }\n\n  return response.txResponses.map((item) => item?.tx?.body.messages[0].neuron);\n};\n","import { BehaviorSubject, Observable, first } from 'rxjs';\nimport { LinkDto } from 'src/services/CozoDb/types/dto';\nimport { IPFSContent } from 'src/services/ipfs/types';\nimport { mapParticleToEntity } from 'src/services/CozoDb/mapping';\nimport { QueueChannelMessage } from './types';\nimport { CYB_QUEUE_CHANNEL } from '../consts';\n\nimport { enqueueParticleEmbeddingMaybe } from './backendQueueSenders';\nimport ParticlesResolverQueue from '../../services/sync/services/ParticlesResolverQueue/ParticlesResolverQueue';\nimport DbApi from '../../services/DbApi/DbApi';\n\nimport { SyncQueueItem } from '../../services/sync/services/ParticlesResolverQueue/types';\nimport { Option } from 'src/types';\n\nclass BackendQueueChannelListener {\n  private channel = new BroadcastChannel(CYB_QUEUE_CHANNEL);\n\n  private particlesResolver: ParticlesResolverQueue;\n\n  private dbInstance$: BehaviorSubject<Option<DbApi>>;\n\n  constructor(\n    particlesResolver: ParticlesResolverQueue,\n    dbInstance$: Observable<DbApi | undefined>\n  ) {\n    this.particlesResolver = particlesResolver;\n    dbInstance$.subscribe((v) => {\n      this.dbInstance$.next(v);\n    });\n    this.dbInstance$ = new BehaviorSubject<Option<DbApi>>(undefined);\n\n    this.channel.onmessage = (event) => this.onMessage(event);\n\n    this.channel.onmessageerror = (event) =>\n      console.error(`${CYB_QUEUE_CHANNEL} error`, event);\n  }\n\n  private async getDeffredDbApi(): Promise<DbApi> {\n    return new Promise((resolve) => {\n      const dbApi = this.dbInstance$.getValue();\n      if (dbApi) {\n        resolve(dbApi);\n      }\n\n      this.dbInstance$\n        .pipe(\n          first((value) => value !== undefined) // Automatically unsubscribes after the first valid value\n        )\n        .subscribe((value) => {\n          resolve(value as DbApi);\n        });\n    });\n  }\n\n  private async saveLinks(links: LinkDto[]) {\n    const dbApi = await this.getDeffredDbApi();\n    const res = await dbApi.putCyberlinks(links);\n    // console.log('---saveLinks done', links, res);\n  }\n\n  private async saveParticles(content: IPFSContent) {\n    try {\n      const dbApi = await this.getDeffredDbApi();\n      const entity = mapParticleToEntity(content);\n      const result = await dbApi.putParticles(entity);\n      if (result.ok) {\n        await enqueueParticleEmbeddingMaybe(content);\n      }\n    } catch (e) {\n      console.log(\n        '---saveParticle e',\n        content,\n        content.textPreview,\n        e.toString()\n      );\n      throw e;\n    }\n  }\n\n  private async enquueSync(data: SyncQueueItem | SyncQueueItem[]) {\n    // TODO: TMP ASYNC WAIT TO INIT DB\n    await this.getDeffredDbApi();\n\n    this.particlesResolver.enqueue(Array.isArray(data) ? data : [data]);\n  }\n\n  private onMessage(msg: MessageEvent<QueueChannelMessage>) {\n    const { type, data } = msg.data;\n    if (type === 'link') {\n      this.saveLinks(data);\n    } else if (type === 'particle') {\n      this.saveParticles(data);\n    } else if (type === 'sync') {\n      this.enquueSync(data);\n    }\n  }\n}\n\nexport default BackendQueueChannelListener;\n","/* eslint-disable no-restricted-syntax */\nimport { Observable, combineLatest } from 'rxjs';\nimport { map } from 'rxjs/operators';\n\nimport BroadcastChannelSender from '../../channels/BroadcastChannelSender';\n\nimport ParticlesResolverQueue from './services/ParticlesResolverQueue/ParticlesResolverQueue';\n\n// import SyncIpfsLoop from './services/SyncIpfsLoop/SyncIpfsLoop';\nimport SyncTransactionsLoop from './services/SyncTransactionsLoop/SyncTransactionsLoop';\nimport SyncParticlesLoop from './services/SyncParticlesLoop/SyncParticlesLoop';\n\nimport { ServiceDeps } from './services/types';\nimport {\n  MY_FRIENDS_SYNC_INTERVAL,\n  MY_PARTICLES_SYNC_INTERVAL,\n} from './services/consts';\nimport SyncMyFriendsLoop from './services/SyncMyFriendsLoop/SyncMyFriendsLoop';\nimport { SyncEntryName } from '../../types/services';\nimport BaseSyncLoop from './services/BaseSyncLoop/BaseSyncLoop';\nimport createCommunitySync$ from './services/CommunitySync/CommunitySync';\nimport { createCyblogChannel } from 'src/utils/logging/cyblog';\nimport BackendQueueChannelListener from '../../channels/BackendQueueChannel/BackendQueueChannel';\n\nconst cyblogCh = createCyblogChannel({ thread: 'bckd' });\n\n// eslint-disable-next-line import/prefer-default-export\nexport class SyncService {\n  private isInitialized$: Observable<boolean>;\n\n  private channelApi = new BroadcastChannelSender();\n\n  private loops: Partial<Record<SyncEntryName, BaseSyncLoop>> = {};\n\n  constructor(deps: ServiceDeps) {\n    const { dbInstance$, ipfsInstance$ } = deps;\n\n    const particlesResolver = new ParticlesResolverQueue(deps).start();\n\n    const queueListener = new BackendQueueChannelListener(\n      particlesResolver,\n      dbInstance$\n    );\n\n    this.isInitialized$ = combineLatest([dbInstance$, ipfsInstance$]).pipe(\n      map(([dbInstance, ipfsInstance]) => !!dbInstance && !!ipfsInstance)\n    );\n    // subscribe when started\n    this.isInitialized$.subscribe({\n      next: (result) => {\n        return result && this.channelApi.postServiceStatus('sync', 'started');\n      },\n      error: (err) => this.channelApi.postServiceStatus('sync', 'error', err),\n    });\n\n    const communitySync$ = createCommunitySync$(deps);\n    communitySync$.subscribe((community) => {\n      cyblogCh.info('--> community fetched', {\n        unit: 'community',\n        data: community,\n      });\n    });\n\n    const followings$ = communitySync$.pipe(\n      map((c) => c.filter((i) => i.following)),\n      map((c) => c.map((i) => i.neuron))\n    );\n\n    // new SyncIpfsLoop(deps, particlesResolver).start();\n\n    new SyncTransactionsLoop('transactions', deps, particlesResolver).start();\n\n    new SyncParticlesLoop(\n      'particles',\n      MY_PARTICLES_SYNC_INTERVAL,\n      deps,\n      particlesResolver\n    ).start();\n\n    new SyncMyFriendsLoop(\n      'my-friends',\n      MY_FRIENDS_SYNC_INTERVAL,\n      { ...deps, followings$ },\n      particlesResolver\n      // { warmupMs: 1000 }\n    ).start();\n  }\n\n  public restart(name: SyncEntryName) {\n    this.loops[name]?.restart();\n  }\n}\n","import {\n  Observable,\n  combineLatest,\n  defer,\n  distinctUntilChanged,\n  filter,\n  map,\n  switchMap,\n} from 'rxjs';\n\nimport {\n  SyncCommunityResult,\n  fetchStoredSyncCommunity$,\n} from 'src/services/community/community';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport { CommunityDto } from 'src/services/CozoDb/types/dto';\nimport { ServiceDeps } from '../types';\n\n// eslint-disable-next-line import/no-unused-modules\nexport default function createCommunitySync$(\n  deps: ServiceDeps\n): Observable<CommunityDto[]> {\n  const { dbInstance$, ipfsInstance$, params$ } = deps;\n  const channel = new BroadcastChannelSender();\n\n  return combineLatest([\n    dbInstance$,\n    params$!.pipe(\n      map((params) => params.myAddress),\n      distinctUntilChanged()\n    ),\n    ipfsInstance$,\n  ]).pipe(\n    filter(\n      ([dbInstance, myAddress, ipfsInstance]) =>\n        !!dbInstance && !!ipfsInstance && !!myAddress\n    ),\n    switchMap(([dbApi, myAddress, ipfsInstance]) => {\n      const { waitForParticleResolve } = deps;\n      let community: CommunityDto[] = []; // Fix: Add type declaration for community array\n      return new Observable<CommunityDto[]>((observer) => {\n        observer.next([]);\n\n        fetchStoredSyncCommunity$(\n          dbApi!,\n          myAddress!,\n          waitForParticleResolve!\n        ).subscribe(({ action, items }: SyncCommunityResult) => {\n          channel.post({ type: 'load_community', value: { action, items } });\n\n          if (action === 'reset') {\n            community = [];\n          } else if (['add', 'complete'].some((s) => s === action)) {\n            community.push(...items);\n          }\n\n          if (action === 'complete') {\n            observer.next(community);\n            observer.complete();\n          }\n        });\n      });\n    })\n  );\n}\n","/* eslint-disable valid-jsdoc */\n/* eslint-disable import/no-unused-modules */\nimport { fileTypeFromBuffer } from 'file-type';\nimport { concat as uint8ArrayConcat } from 'uint8arrays/concat';\nimport { Uint8ArrayLike } from '../types';\n\ntype ResultWithMime = {\n  result: Uint8ArrayLike;\n  mime: string | undefined;\n  firstChunk: Uint8Array | undefined;\n};\n\ntype StreamDoneCallback = (\n  chunks: Array<Uint8Array>,\n  mime: string | undefined\n) => Promise<void> | void;\n\n// interface AsyncIterableWithReturn<T> extends AsyncIterable<T> {\n//   return?: (value?: unknown) => Promise<IteratorResult<T>>;\n// }\n\nexport const getMimeFromUint8Array = async (\n  raw: Uint8Array | undefined\n): Promise<string | undefined> => {\n  if (!raw) {\n    return 'unknown';\n  }\n  // TODO: try to pass only first N-bytes\n  const fileType = await fileTypeFromBuffer(raw);\n\n  return fileType?.mime || 'text/plain';\n};\n\nexport async function toAsyncIterableWithMime(\n  stream: ReadableStream<Uint8Array>,\n  flush?: StreamDoneCallback\n): Promise<ResultWithMime> {\n  const [firstChunkStream, fullStream] = stream.tee();\n  const chunks: Array<Uint8Array> = []; // accumulate all the data to pim/save\n\n  // Read the first chunk from the stream\n  const firstReader = firstChunkStream.getReader();\n  const { value } = await firstReader.read();\n  const mime = value ? await getMimeFromUint8Array(value) : undefined;\n\n  const restReader = fullStream.getReader();\n\n  const asyncIterable: AsyncIterable<Uint8Array> = {\n    async *[Symbol.asyncIterator]() {\n      while (true) {\n        const { done, value } = await restReader.read();\n        if (done) {\n          flush && flush(chunks, mime);\n          return; // Exit the loop when done\n        }\n        flush && chunks.push(value);\n        yield value; // Yield the value to the consumer\n      }\n    },\n  };\n\n  return { mime, result: asyncIterable, firstChunk: value };\n}\n\nexport async function toReadableStreamWithMime(\n  stream: ReadableStream<Uint8Array>,\n  flush?: StreamDoneCallback\n): Promise<ResultWithMime> {\n  const [firstChunkStream, fullStream] = stream.tee();\n  const chunks: Array<Uint8Array> = []; // accumulate all the data to pim/save\n\n  // Read the first chunk from the stream\n  const firstReader = firstChunkStream.getReader();\n  const { value } = await firstReader.read();\n  const mime = value ? await getMimeFromUint8Array(value) : undefined;\n\n  const modifiedStream = new ReadableStream<Uint8Array>({\n    async pull(controller) {\n      const restReader = fullStream.getReader();\n      const { done, value } = await restReader.read();\n      if (done) {\n        controller.close();\n        flush && flush(chunks, mime);\n      } else {\n        controller.enqueue(value);\n        flush && chunks.push(value);\n      }\n      restReader.releaseLock();\n    },\n    cancel() {\n      firstChunkStream.cancel();\n      fullStream.cancel();\n    },\n  });\n\n  return { mime, result: modifiedStream, firstChunk: value };\n}\n\nexport type onProgressCallback = (progress: number) => void;\n\nexport const getResponseResult = async (\n  response: Uint8ArrayLike,\n  onProgress?: onProgressCallback\n) => {\n  let bytesDownloaded = 0;\n  try {\n    if (response instanceof Uint8Array) {\n      onProgress && onProgress(response.byteLength);\n      return response;\n    }\n    const chunks: Array<Uint8Array> = [];\n\n    if (response instanceof ReadableStream) {\n      const reader = response.getReader();\n\n      const readStream = async ({\n        done,\n        value,\n      }: ReadableStreamReadResult<Uint8Array>): Promise<Uint8Array> => {\n        if (done) {\n          return uint8ArrayConcat(chunks);\n        }\n\n        chunks.push(value!);\n        bytesDownloaded += value!.byteLength;\n        onProgress && onProgress(bytesDownloaded);\n        return reader.read().then(readStream);\n      };\n\n      const readArray: Uint8Array = await reader.read().then(readStream);\n\n      return readArray;\n    }\n\n    if (Symbol.asyncIterator in response) {\n      const reader = response[Symbol.asyncIterator]();\n\n      // if (cid === 'QmRqms6Utkk6L4mtyLQXY2spcQ8Pk7fBBTNjvxa9jTNrXp') {\n      //   debugger;\n      // }\n      // eslint-disable-next-line no-restricted-syntax\n      for await (const chunk of reader) {\n        if (chunk instanceof Uint8Array) {\n          chunks.push(chunk);\n          bytesDownloaded += chunk.byteLength;\n          onProgress && onProgress(bytesDownloaded);\n        }\n      }\n      const result = uint8ArrayConcat(chunks);\n      return result;\n    }\n    return undefined;\n  } catch (error) {\n    console.error(\n      `Error reading stream/iterable.\\r\\n Probably Hot reload error!`,\n      error\n    );\n\n    // throw error;\n\n    return undefined;\n  }\n};\n","import Dexie from 'dexie';\n\nconst db = new Dexie('cyber-page-cash');\ndb.version(3).stores({\n  cid: 'cid',\n  following: 'cid',\n});\n\nexport default db;\n","import db from 'src/db';\n\nconst ipfsCacheDb = () => {\n  const add = async (cid: string, raw: Uint8Array): Promise<void> => {\n    const dbValue = await db.table('cid').get({ cid });\n\n    if (!dbValue) {\n      const ipfsContentAddtToInddexdDB = {\n        cid,\n        data: raw,\n      };\n      db.table('cid').add(ipfsContentAddtToInddexdDB);\n    }\n  };\n\n  const get = async (cid: string): Promise<Uint8Array | undefined> => {\n    // TODO: use cursor\n    const dbValue = await db.table('cid').get({ cid });\n\n    // backward compatibility\n    return dbValue?.data || dbValue?.content;\n  };\n\n  return { add, get };\n};\n\nexport default ipfsCacheDb();\n","import { IPFSNodes, IpfsOptsType } from './types';\n\nexport const CYBER_NODE_SWARM_PEER_ID =\n  'QmUgmRxoLtGERot7Y6G7UyF6fwvnusQZfGR15PuE6pY3aB';\n\nexport const CYBERNODE_SWARM_ADDR_WSS = `/dns4/swarm.io.cybernode.ai/tcp/443/wss/p2p/${CYBER_NODE_SWARM_PEER_ID}`;\nexport const CYBERNODE_SWARM_ADDR_TCP = `/ip4/88.99.105.146/tcp/4001/p2p/${CYBER_NODE_SWARM_PEER_ID}`;\n\nexport const IPFS_CLUSTER_URL = 'https://io.cybernode.ai';\n\nexport const CYBER_GATEWAY_URL = 'https://gateway.ipfs.cybernode.ai';\n\nexport const FILE_SIZE_DOWNLOAD = 20 * 10 ** 6;\n\nexport const getIpfsOpts = () => {\n  let ipfsOpts: IpfsOptsType = {\n    ipfsNodeType: IPFSNodes.HELIA,\n    urlOpts: '/ip4/127.0.0.1/tcp/5001', // default url\n    userGateway: 'http://127.0.0.1:8080',\n  };\n\n  // get type ipfs\n  const lsTypeIpfs = localStorage.getItem('ipfsState');\n  if (lsTypeIpfs !== null) {\n    const lsTypeIpfsData = JSON.parse(lsTypeIpfs);\n    ipfsOpts = { ...ipfsOpts, ...lsTypeIpfsData };\n  }\n\n  localStorage.setItem('ipfsState', JSON.stringify(ipfsOpts));\n\n  return ipfsOpts as IpfsOptsType;\n};\n","import {\n  AddResponse,\n  PinResponse,\n} from '@nftstorage/ipfs-cluster/dist/src/interface';\n\nimport { Cluster } from '@nftstorage/ipfs-cluster';\nimport { IPFS_CLUSTER_URL } from '../config';\n\nconst cyberCluster = () => {\n  const cluster = new Cluster(IPFS_CLUSTER_URL);\n\n  const add = async (\n    file: File | string\n  ): Promise<AddResponse | PinResponse | undefined> => {\n    const dataFile =\n      typeof file === 'string' ? new File([file], 'file.txt') : file;\n    return cluster.add(dataFile, { cidVersion: 0, rawLeaves: false });\n  };\n\n  const status = async (cid: string) => cluster.status(cid);\n  return { add, status };\n};\n\nexport default cyberCluster();\n","import { toString as uint8ArrayToAsciiString } from 'uint8arrays/to-string';\nimport isSvg from 'is-svg';\nimport { PATTERN_HTTP, PATTERN_IPFS_HASH } from 'src/constants/patterns';\nimport { Option } from 'src/types';\n\nimport { shortenString } from 'src/utils/string';\nimport {\n  IPFSContentDetails,\n  IPFSContent,\n  IpfsContentType,\n  IpfsGatewayContentType,\n} from '../types';\nimport { getResponseResult, onProgressCallback } from './stream';\n\nfunction createObjectURL(rawData: Uint8Array, type: string) {\n  const blob = new Blob([rawData], { type });\n  return URL.createObjectURL(blob);\n}\n\nfunction createImgData(rawData: Uint8Array, type: string) {\n  const imgBase64 = uint8ArrayToAsciiString(rawData, 'base64');\n  const file = `data:${type};base64,${imgBase64}`;\n  return file;\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport const detectGatewayContentType = (\n  mime: string | undefined\n): Option<IpfsGatewayContentType> => {\n  if (mime) {\n    if (mime.includes('video')) {\n      return 'video';\n    }\n\n    if (mime.includes('audio')) {\n      return 'audio';\n    }\n\n    if (mime.includes('epub')) {\n      return 'epub';\n    }\n  }\n  return undefined;\n};\n\nconst basic = /\\s?<!doctype html>|(<html\\b[^>]*>|<body\\b[^>]*>|<x-[^>]+>)+/i;\n\nfunction isHtml(string: string) {\n  const newString = string.trim().slice(0, 1000);\n  return basic.test(newString);\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport const chunksToBlob = (\n  chunks: Array<Uint8Array>,\n  mime: string | undefined\n) => new Blob(chunks, mime ? { type: mime } : {});\n\n// eslint-disable-next-line import/no-unused-modules\nexport const mimeToBaseContentType = (\n  mime: string | undefined\n): IpfsContentType => {\n  if (!mime) {\n    return 'other';\n  }\n\n  const initialType = detectGatewayContentType(mime);\n  if (initialType) {\n    return initialType;\n  }\n\n  if (\n    mime.indexOf('text/plain') !== -1 ||\n    mime.indexOf('application/xml') !== -1\n  ) {\n    return 'text';\n  }\n  if (mime.indexOf('image') !== -1) {\n    return 'image';\n  }\n  if (mime.indexOf('application/pdf') !== -1) {\n    return 'pdf';\n  }\n  return 'other';\n};\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport const parseArrayLikeToDetails = async (\n  content: Option<IPFSContent>,\n  cid: string,\n  onProgress?: onProgressCallback\n): Promise<IPFSContentDetails> => {\n  // try {\n  if (!content || !content?.result) {\n    return {\n      gateway: true,\n      text: cid.toString(),\n      cid,\n    };\n  }\n\n  const { result, meta } = content;\n\n  const { mime, contentType } = meta;\n\n  if (!mime) {\n    return {\n      cid,\n      gateway: true,\n      text: `Can't detect MIME for ${cid.toString()}`,\n    };\n  }\n  const contentCid = content.cid;\n\n  const response: IPFSContentDetails = {\n    link: `/ipfs/${cid}`,\n    gateway: false,\n    cid: contentCid,\n    type: contentType,\n  };\n\n  if (detectGatewayContentType(mime)) {\n    return { ...response, gateway: true };\n  }\n\n  const rawData =\n    typeof result !== 'string'\n      ? await getResponseResult(result, onProgress)\n      : result;\n\n  const isStringData = typeof rawData === 'string';\n\n  // console.log(rawData);\n  if (!rawData) {\n    return {\n      ...response,\n      gateway: true,\n      text: `Can't parse content for ${cid.toString()}`,\n    };\n  }\n\n  // clarify text-content subtypes\n  if (response.type === 'text') {\n    // render svg as image\n    if (!isStringData && isSvg(Buffer.from(rawData))) {\n      return {\n        ...response,\n        type: 'image',\n        content: createImgData(rawData, 'image/svg+xml'),\n      };\n    }\n\n    const str = isStringData ? rawData : uint8ArrayToAsciiString(rawData);\n\n    if (str.match(PATTERN_IPFS_HASH)) {\n      return {\n        ...response,\n        type: 'cid',\n        content: str,\n      };\n    }\n    if (str.match(PATTERN_HTTP)) {\n      return {\n        ...response,\n        type: 'link',\n        content: str,\n      };\n    }\n    if (isHtml(str)) {\n      return {\n        ...response,\n        type: 'html',\n        gateway: true,\n        content: cid.toString(),\n      };\n    }\n\n    // TODO: search can bel longer for 42???!\n    // also cover ipns links\n    return {\n      ...response,\n      link: str.length > 42 ? `/ipfs/${cid}` : `/search/${str}`,\n      type: 'text',\n      text: shortenString(str),\n      content: str,\n    };\n  }\n\n  if (!isStringData) {\n    if (response.type === 'image') {\n      return { ...response, content: createImgData(rawData, mime) }; // file\n    }\n    if (response.type === 'pdf') {\n      return {\n        ...response,\n        content: createObjectURL(rawData, mime),\n        gateway: true,\n      }; // file\n    }\n  }\n\n  return response;\n  // } catch (e) {\n  //   console.log('----parseRawIpfsData', e, cid);\n  //   return undefined;\n  // }\n};\n\nexport const contentToUint8Array = async (\n  content: File | string\n): Promise<Uint8Array> => {\n  return new Uint8Array(\n    typeof content === 'string'\n      ? Buffer.from(content)\n      : await content.arrayBuffer()\n  );\n};\n\nexport const createTextPreview = (\n  array: Uint8Array | undefined | string,\n  contentType: IpfsContentType,\n  previewLength = 150\n) => {\n  if (!array) {\n    return undefined;\n  }\n  if (typeof array === 'string') {\n    return array.slice(0, previewLength);\n  }\n  return contentType && contentType === 'text'\n    ? uint8ArrayToAsciiString(array).slice(0, previewLength)\n    : undefined;\n};\n","/* eslint-disable import/no-unused-modules */\nimport { concat as uint8ArrayConcat } from 'uint8arrays/concat';\n\nimport { Option } from 'src/types';\nimport {\n  // getIpfsUserGatewanAndNodeType,\n  IPFSContentMeta,\n  CallBackFuncStatus,\n  IpfsContentSource,\n  IpfsNode,\n  IpfsFileStats,\n  IPFSContent,\n} from '../types';\n\nimport { getMimeFromUint8Array, toAsyncIterableWithMime } from './stream';\n\nimport ipfsCacheDb from './ipfsCacheDb';\nimport cyberCluster from './cluster';\n\nimport {\n  contentToUint8Array,\n  createTextPreview,\n  mimeToBaseContentType,\n} from './content';\n\nimport { CYBER_GATEWAY_URL, FILE_SIZE_DOWNLOAD } from '../config';\n\n// Get data by CID from local storage\nconst loadIPFSContentFromDb = async (\n  cid: string\n): Promise<Option<IPFSContent>> => {\n  // TODO: enable, disabled for tests\n\n  // TODO: use cursor\n  const data = await ipfsCacheDb.get(cid);\n  if (data && data.length) {\n    // TODO: use cursor\n    const mime = await getMimeFromUint8Array(data);\n    const contentType = mimeToBaseContentType(mime);\n\n    const textPreview = createTextPreview(data, contentType);\n\n    const meta: IPFSContentMeta = {\n      type: 'file', // `TODO: ipfs refactor dir support ?\n      size: data.length,\n      sizeLocal: data.length,\n      mime,\n      contentType,\n    };\n    return { result: data, cid, meta, source: 'db', textPreview };\n  }\n\n  return undefined;\n};\n\nconst emptyStats: IpfsFileStats = {\n  type: 'file',\n  size: undefined,\n  sizeLocal: undefined,\n  blocks: undefined,\n};\n\nconst fetchIPFSContentStat = async (\n  cid: string,\n  node?: IpfsNode,\n  signal?: AbortSignal\n): Promise<IpfsFileStats> => {\n  if (node) {\n    const stats = await node.stat(cid, { signal });\n    return stats;\n  }\n  return emptyStats;\n};\n\nconst fetchIPFSContentFromNode = async (\n  cid: string,\n  node?: IpfsNode,\n  controller?: AbortController\n): Promise<Option<IPFSContent>> => {\n  const controllerLegacy = controller || new AbortController();\n  const { signal } = controllerLegacy;\n  let timer: NodeJS.Timeout | undefined;\n\n  if (!node) {\n    console.log('--------fetchIPFSContentFromNode NO NODE INTIALIZED--------');\n    return undefined;\n  }\n\n  if (!controller) {\n    timer = setTimeout(() => {\n      controllerLegacy.abort();\n    }, 1000 * 60 * 1);\n  } // 1 min\n\n  // TODO: cover ipns case\n  try {\n    // const stat = await node.files.stat(path, { signal });\n    const startTime = Date.now();\n    const stats = await fetchIPFSContentStat(cid, node, signal);\n    const meta = stats as IPFSContentMeta;\n    const statsDoneTime = Date.now();\n    meta.statsTime = statsDoneTime - startTime;\n    const allowedSize = stats.size ? stats.size < FILE_SIZE_DOWNLOAD : false;\n    timer && clearTimeout(timer);\n\n    switch (stats.type) {\n      case 'directory': {\n        // TODO: return directory structure\n        return { cid, availableDownload: true, source: 'node', meta: stats };\n      }\n      default: {\n        // Get sample of content\n        const { value: firstChunk } = await node\n          .cat(cid, { signal, length: 2048, offset: 0 })\n          [Symbol.asyncIterator]()\n          .next();\n\n        meta.mime = await getMimeFromUint8Array(firstChunk);\n        meta.contentType = mimeToBaseContentType(meta.mime);\n        const fullyDownloaded =\n          stats.size && stats.size > -1 && firstChunk.length >= stats.size;\n\n        const textPreview = createTextPreview(firstChunk, meta.contentType);\n\n        if (fullyDownloaded) {\n          await ipfsCacheDb.add(cid, uint8ArrayConcat([firstChunk]));\n        }\n\n        // If all content fits in first chunk return byte-array instead iterable\n        const stream = fullyDownloaded\n          ? firstChunk\n          : allowedSize\n          ? node.cat(cid, { signal })\n          : undefined;\n\n        meta.catTime = Date.now() - statsDoneTime;\n\n        // TODO: add to db flag that content is pinned TO local node\n        // if already pinned skip pin\n        if (!meta.local && allowedSize) {\n          node.pin(cid);\n\n          meta.pinTime = Date.now() - meta.catTime;\n        } else {\n          meta.pinTime = -1;\n        }\n\n        return {\n          result: stream,\n          textPreview,\n          cid,\n          meta,\n          source: 'node',\n        };\n        // }\n      }\n    }\n  } catch (error) {\n    console.debug('error fetchIPFSContentFromNode', error);\n    return {\n      cid,\n      availableDownload: true,\n      source: 'node',\n      meta: { ...emptyStats } as IPFSContentMeta,\n    };\n  }\n};\n\nconst fetchIPFSContentFromGateway = async (\n  cid: string,\n  node?: IpfsNode,\n  controller?: AbortController,\n  headers?: Record<string, string>\n): Promise<Option<IPFSContent>> => {\n  // fetch META only from external node(toooo slow), TODO: fetch meta from cybernode\n  const isExternalNode = node?.nodeType === 'external';\n\n  const stats = isExternalNode\n    ? await fetchIPFSContentStat(cid, node, controller?.signal)\n    : emptyStats;\n\n  const contentUrl = `${CYBER_GATEWAY_URL}/ipfs/${cid}`;\n  const response = await fetch(contentUrl, {\n    method: 'GET',\n    signal: controller?.signal,\n    headers,\n  });\n  if (response && response.body) {\n    // fetch doesn't provide any headers in our case :(\n\n    // const contentLength = parseInt(\n    //   response.headers['content-length'] || '-1',\n    //   10\n    // );\n    // const contentType = response.headers['content-type'];\n\n    // Extract meta if ipfs prob/node not started yet\n    // if (!meta.mime) {\n    //   meta = { ...meta, mime: contentType };\n    // }\n\n    // TODO: fix\n    const flushResults = (chunks: Uint8Array[]) =>\n      !isExternalNode\n        ? ipfsCacheDb.add(cid, uint8ArrayConcat(chunks))\n        : Promise.resolve();\n\n    const { mime, result, firstChunk } = await toAsyncIterableWithMime(\n      response.body,\n      flushResults\n    );\n\n    const contentType = mimeToBaseContentType(mime);\n\n    const textPreview = createTextPreview(firstChunk, contentType);\n    return {\n      cid,\n      textPreview,\n      meta: { ...stats, mime, contentType },\n      result,\n      source: 'gateway',\n      contentUrl,\n    };\n  }\n\n  return undefined;\n};\n\ntype fetchContentOptions = {\n  controller?: AbortController;\n  node?: IpfsNode;\n  headers?: Record<string, string>;\n};\n\nasync function fetchIpfsContent(\n  cid: string,\n  source: IpfsContentSource,\n  options: fetchContentOptions\n): Promise<Option<IPFSContent>> {\n  const { node, controller, headers } = options;\n\n  try {\n    switch (source) {\n      case 'db':\n        return loadIPFSContentFromDb(cid);\n      case 'node':\n        return fetchIPFSContentFromNode(cid, node, controller);\n      case 'gateway':\n        return fetchIPFSContentFromGateway(cid, node, controller, headers);\n      default:\n        return undefined;\n    }\n  } catch (e) {\n    console.log('----fetchIpfsContent error', e);\n    return undefined;\n  }\n}\n\nconst getIPFSContent = async (\n  cid: string,\n  node?: IpfsNode,\n  controller?: AbortController,\n  callBackFuncStatus?: CallBackFuncStatus\n): Promise<Option<IPFSContent>> => {\n  const dataRsponseDb = await loadIPFSContentFromDb(cid);\n  if (dataRsponseDb !== undefined) {\n    return dataRsponseDb;\n  }\n\n  if (node) {\n    callBackFuncStatus && callBackFuncStatus('trying to get with a node');\n    // console.log('----Fetch from node', cid);\n    const ipfsContent = await fetchIPFSContentFromNode(cid, node, controller);\n\n    return ipfsContent;\n  }\n\n  callBackFuncStatus && callBackFuncStatus('trying to get with a gatway');\n  // console.log('----Fetch from gateway', cid);\n  const respnseGateway = await fetchIPFSContentFromGateway(\n    cid,\n    node,\n    controller\n  );\n\n  return respnseGateway;\n};\n\nconst catIPFSContentFromNode = (\n  cid: string,\n  node?: IpfsNode,\n  offset?: number,\n  controller?: AbortController\n): AsyncIterable<Uint8Array> | undefined => {\n  if (!node) {\n    console.log(\n      '--------fetchIPFSContentFromNode NO NODE INTIALIZED TODO: cover case--------'\n    );\n    return undefined;\n  }\n\n  // TODO: cover ipns case\n\n  return node.cat(cid, { offset, signal: controller?.signal });\n};\n\n// const nodeContentFindProvs = async (\n//   node: AppIPFS,\n//   cid: string,\n//   offset: number,\n//   controller?: AbortController\n// ): AsyncIterable<number> | undefined => {\n//   if (!node) {\n//     console.log(\n//       '--------fetchIPFSContentFromNode NO NODE INTIALIZED TODO: cover case--------'\n//     );\n//     return undefined;\n//   }\n\n//   // TODO: cover ipns case\n//   const path = `/ipfs/${cid}`;\n\n//   const providers = node.dht.findProvs(path, {\n//     signal: controller?.signal,\n//   });\n\n//   let count = 0;\n//   for await (const provider of providers) {\n//     //  console.log(provider.id.toString())\n//     //  id: PeerId\n//     // multiaddrs: Multiaddr[]\n//     // protocols: string[]\n//     count++;\n//   }\n\n//   return count;\n// };\n\nconst addContenToIpfs = async (\n  node: IpfsNode,\n  content: File | string\n): Promise<Option<string>> => {\n  let cid;\n  if (node) {\n    cid = await node.add(content);\n  }\n  // TODO: WARN - TMP solution make cluster call non-awaitable\n  cyberCluster.add(content);\n  // Save to local cache\n  cid && (await ipfsCacheDb.add(cid, await contentToUint8Array(content)));\n  return cid;\n};\n\nexport {\n  getIPFSContent,\n  catIPFSContentFromNode,\n  fetchIpfsContent,\n  addContenToIpfs,\n};\n","import { IQueueStrategy, QueueSettings, QueueSource } from './types';\n\nexport class QueueStrategy implements IQueueStrategy {\n  settings: QueueSettings;\n\n  order: QueueSource[];\n\n  constructor(settings: QueueSettings, order: QueueSource[]) {\n    this.settings = settings;\n    this.order = order;\n  }\n\n  getNextSource(source: QueueSource): QueueSource | undefined {\n    const index = this.order.indexOf(source);\n    return index < this.order.length ? this.order[index + 1] : undefined;\n  }\n}\n","export class QueueItemTimeoutError extends Error {\n  constructor(timeoutMs: number) {\n    super(`Timeout after ${timeoutMs}`);\n    Object.setPrototypeOf(this, QueueItemTimeoutError.prototype);\n  }\n}\n","/* eslint-disable import/prefer-default-export */\nexport const CustomHeaders = {\n  XCybSource: 'X-Cyb-Source',\n};\n\nexport enum XCybSourceValues {\n  sharedWorker = 'shared-worker',\n}\n","import {\n  BehaviorSubject,\n  EMPTY,\n  Observable,\n  catchError,\n  combineLatest,\n  debounceTime,\n  distinctUntilChanged,\n  filter,\n  interval,\n  map,\n  merge,\n  mergeMap,\n  of,\n  share,\n  tap,\n  throwError,\n  timeout,\n  withLatestFrom,\n} from 'rxjs';\n\nimport * as R from 'ramda';\n\nimport { CybIpfsNode, IpfsContentSource } from 'src/services/ipfs/types';\nimport { fetchIpfsContent } from 'src/services/ipfs/utils/utils-ipfs';\nimport { ParticleCid } from 'src/types/base';\n\nimport { promiseToObservable } from '../../utils/rxjs/helpers';\n\nimport type {\n  QueueItem,\n  QueueItemAsyncResult,\n  QueueItemCallback,\n  QueueItemOptions,\n  QueueItemResult,\n  QueueSource,\n  QueueStats,\n} from './types';\n\nimport { QueueStrategy } from './QueueStrategy';\n\nimport { enqueueParticleSave } from '../backend/channels/BackendQueueChannel/backendQueueSenders';\nimport BroadcastChannelSender from '../backend/channels/BroadcastChannelSender';\nimport { RuneEngine } from '../scripting/engine';\n\nimport { QueueItemTimeoutError } from './QueueItemTimeoutError';\nimport { CustomHeaders, XCybSourceValues } from './constants';\n\nconst QUEUE_DEBOUNCE_MS = 33;\nconst CONNECTION_KEEPER_RETRY_MS = 15000;\n\nfunction getQueueItemTotalPriority(item: QueueItem): number {\n  return (item.priority || 0) + (item.viewPortPriority || 0);\n}\n\nconst debugCid = (cid: ParticleCid, prefix: string, ...args) => {\n  console.log(`>>> ${prefix}: ${cid}`, ...args);\n};\n\nconst strategies = {\n  external: new QueueStrategy(\n    {\n      db: { timeout: 5000, maxConcurrentExecutions: 999 },\n      node: { timeout: 60 * 1000, maxConcurrentExecutions: 30 },\n      gateway: { timeout: 10000, maxConcurrentExecutions: 11 },\n    },\n    ['db', 'node', 'gateway']\n  ),\n  embedded: new QueueStrategy(\n    {\n      db: { timeout: 5000, maxConcurrentExecutions: 999 },\n      node: { timeout: 60 * 1000, maxConcurrentExecutions: 30 },\n      gateway: { timeout: 21000, maxConcurrentExecutions: 11 },\n    },\n    ['db', 'gateway', 'node']\n  ),\n  helia: new QueueStrategy(\n    {\n      db: { timeout: 5000, maxConcurrentExecutions: 999 },\n      node: { timeout: 60 * 1000, maxConcurrentExecutions: 50 },\n      gateway: { timeout: 10000, maxConcurrentExecutions: 11 },\n    },\n    ['db', 'node', 'gateway']\n  ),\n};\n\ntype QueueMap = Map<ParticleCid, QueueItem>;\n\nclass QueueManager {\n  private queue$ = new BehaviorSubject<QueueMap>(new Map());\n\n  private node: CybIpfsNode | undefined = undefined;\n\n  private strategy: QueueStrategy;\n\n  private queueDebounceMs: number;\n\n  private lastNodeCallTime: number = Date.now();\n\n  private channel = new BroadcastChannelSender();\n\n  private executing: Record<QueueSource, Set<ParticleCid>> = {\n    db: new Set(),\n    node: new Set(),\n    gateway: new Set(),\n  };\n\n  private switchStrategy(strategy: QueueStrategy): void {\n    this.strategy = strategy;\n  }\n\n  public async setNode(node: CybIpfsNode, customStrategy?: QueueStrategy) {\n    console.log(\n      `* switch node from ${this.node?.nodeType || '<none>'} to ${\n        node.nodeType\n      }`\n    );\n    this.node = node;\n    this.switchStrategy(customStrategy || strategies[node.nodeType]);\n  }\n\n  private getItemBySourceAndPriority(queue: QueueMap) {\n    const pendingItems = [...queue.values()].filter(\n      (i) => i.status === 'pending'\n    );\n\n    const pendingBySource = R.groupBy((i) => i.source, pendingItems);\n\n    const itemsToExecute: QueueItem[] = [];\n    // eslint-disable-next-line no-loop-func, no-restricted-syntax\n    for (const [queueSource, items] of Object.entries(pendingBySource)) {\n      const settings = this.strategy.settings[queueSource as IpfsContentSource];\n\n      const executeCount =\n        settings.maxConcurrentExecutions -\n        this.executing[queueSource as IpfsContentSource].size;\n      const itemsByPriority = items\n        .sort(\n          (a, b) => getQueueItemTotalPriority(b) - getQueueItemTotalPriority(a)\n        )\n        .slice(0, executeCount);\n\n      itemsToExecute.push(...itemsByPriority);\n    }\n\n    return itemsToExecute;\n  }\n\n  private postSummary() {\n    const summary = `(total: ${this.queue$.value.size} |  db - ${this.executing.db.size} node - ${this.executing.node.size} gateway - ${this.executing.gateway.size})`;\n    this.channel.postServiceStatus('ipfs', 'started', summary);\n  }\n\n  private fetchData$(item: QueueItem) {\n    const { cid, source, callbacks, controller } = item;\n    // const abortController = controller || new AbortController();\n    const settings = this.strategy.settings[source];\n    this.executing[source].add(cid);\n    this.postSummary();\n    const queueItem = this.queue$.value.get(cid);\n    // Mutate item without next\n    this.queue$.value.set(cid, {\n      ...queueItem,\n      status: 'executing',\n      executionTime: Date.now(),\n      controller: new AbortController(),\n    } as QueueItem);\n    // debugCid(cid, 'fetchData', cid, source);\n    callbacks.map((callback) => callback(cid, 'executing', source));\n\n    return promiseToObservable(async () => {\n      return fetchIpfsContent(cid, source, {\n        controller,\n        node: this.node,\n        headers: {\n          [CustomHeaders.XCybSource]: XCybSourceValues.sharedWorker,\n        },\n      }).then((content) => {\n        // put saveto db msg into bus\n        if (content && source !== 'db') {\n          enqueueParticleSave(content);\n        }\n\n        return content;\n      });\n    }).pipe(\n      timeout({\n        each: settings.timeout,\n        with: () =>\n          throwError(() => {\n            controller?.abort('timeout');\n\n            return new QueueItemTimeoutError(settings.timeout);\n          }),\n      }),\n      map((result): QueueItemResult => {\n        return {\n          item,\n          status: result ? 'completed' : 'error',\n          source,\n          result,\n        };\n      }),\n      catchError((error): Observable<QueueItemResult> => {\n        // debugCid(cid, 'fetchData - fetchIpfsContent catchErr', error);\n        if (error instanceof QueueItemTimeoutError) {\n          return of({\n            item,\n            status: 'timeout',\n            source,\n          });\n        }\n\n        if (error?.name === 'AbortError') {\n          return of({ item, status: 'cancelled', source });\n        }\n        return of({ item, status: 'error', source });\n      })\n    );\n  }\n\n  /**\n   * Mutate queue item, and return new queue\n   * @param cid\n   * @param changes\n   * @returns\n   */\n  private mutateQueueItem(cid: string, changes: Partial<QueueItem>) {\n    const queue = this.queue$.value;\n    const item = queue.get(cid);\n    if (item) {\n      queue.set(cid, { ...item, ...changes });\n    }\n\n    return this.queue$.next(queue);\n  }\n\n  private removeAndNext(cid: string): void {\n    const queue = this.queue$.value;\n    queue.delete(cid);\n    this.queue$.next(queue);\n  }\n\n  // reset status and switch to next source\n  private switchSourceAndNext(item: QueueItem, nextSource: QueueSource): void {\n    item.callbacks.map((callback) => callback(item.cid, 'pending', nextSource));\n\n    this.mutateQueueItem(item.cid, { status: 'pending', source: nextSource });\n  }\n\n  private cancelDeprioritizedItems(queue: QueueMap): QueueMap {\n    (['node', 'gateway'] as IpfsContentSource[]).forEach((source) => {\n      Array.from(this.executing[source]).forEach((cid) => {\n        const item = queue.get(cid);\n        if (item && getQueueItemTotalPriority(item) < 0 && item.controller) {\n          // abort request and move to pending\n          item.controller.abort('cancelled');\n          item.callbacks.map((callback) =>\n            callback(item.cid, 'pending', item.source)\n          );\n\n          queue.set(cid, { ...item, status: 'pending' });\n          // console.log('-----cancel item', item, queue);\n\n          this.executing[source].delete(cid);\n        }\n      });\n    });\n\n    return queue;\n  }\n\n  private releaseExecution(cid: string) {\n    // eslint-disable-next-line no-restricted-syntax\n    Object.keys(this.executing).forEach((key) =>\n      this.executing[key as IpfsContentSource].delete(cid)\n    );\n  }\n\n  constructor(\n    ipfsInstance$: Observable<CybIpfsNode | undefined>,\n    {\n      strategy,\n      queueDebounceMs,\n    }: {\n      strategy?: QueueStrategy;\n      queueDebounceMs?: number;\n    }\n  ) {\n    ipfsInstance$.subscribe((node) => {\n      if (node) {\n        this.setNode(node);\n      }\n    });\n\n    this.strategy = strategy || strategies.embedded;\n    this.queueDebounceMs = queueDebounceMs || QUEUE_DEBOUNCE_MS;\n\n    // Little hack to handle keep-alive connection to swarm cyber node\n    // Fix some lag with node peers(when it shown swarm node in peers but not  connected anymore)\n    interval(CONNECTION_KEEPER_RETRY_MS)\n      .pipe(\n        filter(\n          () =>\n            !!this.node &&\n            !![...this.queue$.value.values()].find((i) => i.source === 'node')\n        )\n      )\n      .subscribe(() => {\n        // console.log(\n        //   '-----reconnect cnt',\n        //   this.queue$.value.size,\n        //   this.queue$.value\n        // );\n        this.node!.reconnectToSwarm(true);\n      });\n\n    const isInitialized$ = combineLatest([ipfsInstance$]).pipe(\n      map(([ipfsInstance]) => !!ipfsInstance && ipfsInstance?.isStarted),\n      distinctUntilChanged(),\n      share()\n    );\n\n    isInitialized$.subscribe((isInitialized) => {\n      isInitialized && console.log(' ipfs queue initialized');\n      this.node?.reconnectToSwarm(true);\n    });\n\n    this.queue$\n      .pipe(\n        withLatestFrom(isInitialized$),\n        filter(([, isInitialized]) => isInitialized),\n        debounceTime(this.queueDebounceMs),\n        map(([queue]) => this.cancelDeprioritizedItems(queue)),\n        mergeMap((queue) => {\n          const workItems = this.getItemBySourceAndPriority(queue);\n          // console.log('---workItems', workItems);\n          if (workItems.length > 0) {\n            // wake up connnection to swarm cyber node\n            this.node?.reconnectToSwarm(false);\n\n            return merge(...workItems.map((item) => this.fetchData$(item)));\n          }\n          return EMPTY;\n        })\n      )\n      .subscribe(({ item, status, source, result }) => {\n        const { cid } = item;\n        const callbacks = this.queue$.value.get(cid)?.callbacks || [];\n        // fix to process dublicated items\n        // debugCid(cid, 'subscribe', cid, source, status, result, callbacks);\n\n        callbacks.map((callback) => callback(cid, status, source, result));\n\n        // HACK to use with GracePeriod for reconnection\n        if (source === 'node') {\n          this.lastNodeCallTime = Date.now();\n        }\n\n        this.executing[source].delete(cid);\n\n        // success execution -> next\n        if (status === 'completed' || status === 'cancelled') {\n          // debugCid(cid, '------done', item, status, source, result);\n          this.removeAndNext(cid);\n        } else {\n          // debugCid(cid, '------error', item, status, source, result);\n\n          // Retry -> (next sources) or -> next\n          const nextSource = this.strategy.getNextSource(source);\n\n          if (nextSource) {\n            this.switchSourceAndNext(item, nextSource);\n          } else {\n            this.removeAndNext(cid);\n            // notify thatn nothing found from all sources\n            callbacks.map((callback) =>\n              callback(cid, 'not_found', source, result)\n            );\n          }\n        }\n\n        this.postSummary();\n      });\n  }\n\n  public enqueue(\n    cid: string,\n    callback: QueueItemCallback,\n    options: QueueItemOptions = {}\n  ): void {\n    const queue = this.queue$.value;\n    const existingItem = queue.get(cid);\n    // debugCid(cid, '----/--enqueue ', cid, existingItem);\n\n    // In case if item already in queue,\n    // just attach one more callback to quieued item\n    if (existingItem) {\n      this.mutateQueueItem(cid, {\n        callbacks: [...existingItem.callbacks, callback],\n      });\n    } else {\n      const source = options.initialSource || this.strategy.order[0];\n      const item: QueueItem = {\n        cid,\n        callbacks: [callback],\n        source, // initial method to fetch\n        status: 'pending',\n        postProcessing: true, // by default rune-post-processing enabled\n        ...options,\n      };\n\n      callback(cid, 'pending', source);\n\n      queue.set(cid, item);\n      this.queue$.next(queue);\n    }\n  }\n\n  public enqueueAndWait(\n    cid: string,\n    options: QueueItemOptions = {}\n  ): Promise<QueueItemAsyncResult> {\n    return new Promise((resolve) => {\n      const callback = ((cid, status, source, result) => {\n        if (status === 'completed' || status === 'not_found') {\n          resolve({ status, source, result });\n        }\n      }) as QueueItemCallback;\n\n      this.enqueue(cid, callback, options);\n    });\n  }\n\n  public updateViewPortPriority(cid: string, viewPortPriority: number) {\n    this.mutateQueueItem(cid, { viewPortPriority });\n  }\n\n  public cancel(cid: string): void {\n    const queue = this.queue$.value;\n    const item = queue.get(cid);\n    // console.log('-----cancel item', item, item?.controller);\n    if (item) {\n      // If item has no abortController we can just remove it,\n      // otherwise abort&keep-to-finalize\n      if (!item.controller) {\n        this.removeAndNext(cid);\n      } else {\n        item.controller.abort('cancelled');\n      }\n    }\n  }\n\n  public cancelByParent(parent: string): void {\n    const queue = this.queue$.value;\n\n    queue.forEach((item, cid) => {\n      if (item.parent === parent) {\n        this.releaseExecution(cid);\n        item.controller?.abort('cancelled');\n        queue.delete(cid);\n      }\n    });\n\n    this.queue$.next(queue);\n  }\n\n  public clear(): void {\n    const queue = this.queue$.value;\n\n    queue.forEach((item, cid) => {\n      this.releaseExecution(cid);\n      item.controller?.abort('cancelled');\n      queue.delete(cid);\n    });\n\n    this.queue$.next(new Map());\n  }\n\n  public getQueueMap(): QueueMap {\n    return this.queue$.value;\n  }\n\n  public getQueueList(): QueueItem[] {\n    return Array.from(this.queue$.value.values());\n  }\n\n  public getStats(): QueueStats[] {\n    const fn = R.pipe(\n      R.countBy<QueueItem>(R.prop('status')),\n      R.toPairs,\n      R.map(R.zipObj(['status', 'count']))\n    );\n\n    return fn(this.getQueueList()) as QueueStats[];\n  }\n}\n\nexport default QueueManager;\n","import { Observable } from 'rxjs';\n\n/**\n * Convert promise to observable\n * @param promiseFactory\n * @returns\n */\nexport function promiseToObservable<T>(promiseFactory: () => Promise<T>) {\n  return new Observable<T>((observer) => {\n    promiseFactory()\n      .then((response) => {\n        observer.next(response);\n        observer.complete();\n      })\n      .catch((error) => {\n        console.debug('----promiseToObservable error', error); //, error\n        observer.error(error);\n      });\n  });\n}\n","import { CID } from 'multiformats/cid';\n\nexport const stringToCid = (s: string) => CID.parse(s);\nexport const stringToIpfsPath = (s: string) => `/ipfs/${s}`;\n","import { IPFSHTTPClient, create as createKuboClient } from 'kubo-rpc-client';\nimport { multiaddr } from '@multiformats/multiaddr';\n\nimport { stringToCid, stringToIpfsPath } from '../../utils/cid';\nimport {\n  AbortOptions,\n  CatOptions,\n  IpfsNodeType,\n  InitOptions,\n  IpfsFileStats,\n  IpfsNode,\n  IpfsNodePrperties,\n} from '../../types';\nimport { CYBER_GATEWAY_URL } from '../../config';\n\nclass KuboNode implements IpfsNode {\n  readonly nodeType: IpfsNodeType = 'external';\n\n  private node?: IPFSHTTPClient;\n\n  private _config: IpfsNodePrperties = {};\n\n  get config() {\n    return this._config;\n  }\n\n  private _isStarted: boolean = false;\n\n  get isStarted() {\n    return this._isStarted;\n  }\n\n  private async initConfig() {\n    const response = await this.node!.config.get('Addresses.Gateway');\n    if (!response) {\n      return { gatewayUrl: CYBER_GATEWAY_URL };\n    }\n    const address = multiaddr(response as string).nodeAddress();\n\n    return { gatewayUrl: `http://${address.address}:${address.port}` };\n  }\n\n  async init(options?: InitOptions) {\n    this.node = createKuboClient(options);\n    this._config = await this.initConfig();\n\n    if (typeof window !== 'undefined') {\n      window.node = this.node;\n      window.toCid = stringToCid;\n    }\n    console.log(\n      'IPFS - Kubo addrs',\n      (await this.node.swarm.localAddrs()).map((a) => a.toString())\n    );\n    this._isStarted = true;\n  }\n\n  async stat(cid: string, options: AbortOptions = {}): Promise<IpfsFileStats> {\n    return this.node!.files.stat(stringToIpfsPath(cid), {\n      ...options,\n      withLocal: true,\n      size: true,\n    }).then((result) => {\n      const { type, size, sizeLocal, local, blocks } = result;\n      return {\n        type,\n        size: size || -1,\n        sizeLocal: sizeLocal || -1,\n        blocks,\n      };\n    });\n  }\n\n  cat(cid: string, options: CatOptions = {}) {\n    return this.node!.cat(stringToCid(cid), options);\n  }\n\n  async add(content: File | string, options: AbortOptions = {}) {\n    return (await this.node!.add(content, options)).cid.toString();\n  }\n\n  async pin(cid: string, options: AbortOptions = {}) {\n    return (await this.node!.pin.add(stringToCid(cid), options)).toString();\n  }\n\n  async getPeers() {\n    return (await this.node!.swarm.peers()).map((c) => c.peer.toString());\n  }\n\n  async stop() {}\n  async start() {}\n\n  async connectPeer(address: string) {\n    const addr = multiaddr(address);\n    await this.node!.bootstrap.add(addr);\n\n    await this.node!.swarm.connect(addr);\n    return true;\n  }\n\n  ls() {\n    return this.node!.pin.ls();\n  }\n\n  async info() {\n    const { repoSize } = await this.node!.stats.repo();\n\n    const responseId = await this.node!.id();\n    const { agentVersion, id } = responseId;\n    return { id: id.toString(), agentVersion, repoSize };\n  }\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport default KuboNode;\n","import { Helia, Pin, createHelia } from 'helia';\nimport { IDBBlockstore } from 'blockstore-idb';\nimport { IDBDatastore } from 'datastore-idb';\nimport { Libp2p, createLibp2p } from 'libp2p';\nimport { noise } from '@chainsafe/libp2p-noise';\nimport { yamux } from '@chainsafe/libp2p-yamux';\n// import { mplex } from '@libp2p/mplex';\n\nimport { circuitRelayTransport } from 'libp2p/circuit-relay';\nimport { UnixFS, unixfs, AddOptions } from '@helia/unixfs';\nimport { bootstrap } from '@libp2p/bootstrap';\nimport { webRTC, webRTCDirect } from '@libp2p/webrtc';\nimport { webSockets } from '@libp2p/websockets';\nimport { webTransport } from '@libp2p/webtransport';\nimport { identifyService } from 'libp2p/identify';\nimport { multiaddr, protocols } from '@multiformats/multiaddr';\nimport { LsResult } from 'ipfs-core-types/src/pin';\n\nimport {\n  AbortOptions,\n  CatOptions,\n  IpfsNodeType,\n  IpfsFileStats,\n  IpfsNode,\n} from '../../types';\n// import { all } from '@libp2p/websockets/filters';\nimport { stringToCid } from '../../utils/cid';\nimport { CYBER_GATEWAY_URL } from '../../config';\n\nasync function* mapToLsResult(\n  iterable: AsyncIterable<Pin>\n): AsyncIterable<LsResult> {\n  // eslint-disable-next-line no-restricted-syntax\n  for await (const item of iterable) {\n    const { cid, metadata } = item;\n    yield { cid: cid.toV0(), metadata, type: 'recursive' };\n  }\n}\n\nconst libp2pFactory = async (\n  datastore: IDBDatastore,\n  bootstrapList: string[] = []\n) => {\n  const libp2p = await createLibp2p({\n    datastore,\n    // addresses: {\n    //   listen: [\n    //     '/ip4/127.0.0.1/tcp/0',\n    //     '/dns4/swarm.io.cybernode.ai/tcp/443/wss/p2p/QmUgmRxoLtGERot7Y6G7UyF6fwvnusQZfGR15PuE6pY3aB',\n    //   ],\n    // },\n    transports: [\n      webSockets(),\n      webTransport(),\n      webRTC({\n        rtcConfiguration: {\n          iceServers: [\n            {\n              urls: [\n                'stun:stun.l.google.com:19302',\n                'stun:global.stun.twilio.com:3478',\n                'STUN:freestun.net:3479',\n                'STUN:stun.bernardoprovenzano.net:3478',\n                'STUN:stun.aa.net.uk:3478',\n              ],\n            },\n            {\n              credential: 'free',\n              username: 'free',\n              urls: ['TURN:freestun.net:3479', 'TURNS:freestun.net:5350'],\n            },\n          ],\n        },\n      }),\n      webRTCDirect(),\n      circuitRelayTransport({\n        discoverRelays: 1,\n      }),\n    ],\n    connectionEncryption: [noise()],\n    streamMuxers: [yamux()],\n    connectionGater: {\n      denyDialMultiaddr: () => {\n        return false;\n        // by default we refuse to dial local addresses from the browser since they\n        // are usually sent by remote peers broadcasting undialable multiaddrs but\n        // here we are explicitly connecting to a local node so do not deny dialing\n        // any discovered address\n      },\n    },\n    peerDiscovery: [\n      bootstrap({\n        list: bootstrapList,\n      }),\n    ],\n    services: {\n      identify: identifyService(),\n    },\n  });\n  return libp2p;\n};\n\nconst addOptionsV0: Partial<AddOptions> = {\n  cidVersion: 0,\n  rawLeaves: false,\n};\n\nclass HeliaNode implements IpfsNode {\n  readonly nodeType: IpfsNodeType = 'helia';\n\n  get config() {\n    return { gatewayUrl: CYBER_GATEWAY_URL };\n  }\n\n  private _isStarted = false;\n\n  get isStarted() {\n    return this._isStarted;\n  }\n\n  private node?: Helia;\n\n  private fs?: UnixFS;\n\n  async init() {\n    const blockstore = new IDBBlockstore('helia-bs');\n    await blockstore.open();\n\n    const datastore = new IDBDatastore('helia-ds');\n    await datastore.open();\n\n    const bootstrapList = [\n      '/dnsaddr/bootstrap.libp2p.io/p2p/QmNnooDu7bfjPFoTZYxMNLWUQJyrVwtbZg5gBMjTezGAJN',\n      '/dnsaddr/bootstrap.libp2p.io/p2p/QmQCU2EcMqAqQPR2i9bChDtGNJchTbq5TbXJJ16u19uLTa',\n      '/dnsaddr/bootstrap.libp2p.io/p2p/QmbLHAnMoJPWSCR5Zhtx6BHJX9KiKNN6tpvbUcqanj75Nb',\n      '/dnsaddr/bootstrap.libp2p.io/p2p/QmcZf59bWwK5XFi76CZX8cbJ4BhTzzA3gU1ZjYZcYW3dwt',\n      '/dns4/swarm.io.cybernode.ai/tcp/443/wss/p2p/QmUgmRxoLtGERot7Y6G7UyF6fwvnusQZfGR15PuE6pY3aB',\n    ];\n    const libp2p = await libp2pFactory(datastore, bootstrapList);\n\n    this.node = await createHelia({ blockstore, datastore, libp2p });\n\n    this.fs = unixfs(this.node);\n\n    if (typeof window !== 'undefined') {\n      window.libp2p = libp2p;\n      window.node = this.node;\n      window.fs = this.fs;\n      window.toCid = stringToCid;\n    }\n\n    // DEBUG\n    libp2p.addEventListener('peer:connect', (evt) => {\n      const peerId = evt.detail.toString();\n      const conn = libp2p.getConnections(peerId) || [];\n      const transportsByAddr = Object.fromEntries(\n        conn.map((c) => [\n          c.remoteAddr.toString(),\n          c.remoteAddr.protoCodes().map((v) => protocols(v)?.name),\n        ])\n      );\n      console.debug(`Connected to ${peerId}`, transportsByAddr);\n\n      // console.log(\n      //   '---------ppppp',\n      //   peerId,\n      //   conn,\n      //   conn?.remoteAddr.protoCodes().map((v) => protocols(v)?.name)\n      // ); //.includes(WEBRTC_CODE)\n      // if (conn && conn.stat) {\n      //   const transport = conn.stat.transport; // This might vary based on libp2p version\n      //   console.log(`Connected to ${peerId} using transport ${transport}`);\n      // } else {\n      //   console.log(`Connected to ${peerId}`);\n      // }\n    });\n    libp2p.addEventListener('peer:disconnect', (evt) => {\n      console.debug(`Disconnected from ${evt.detail.toString()}`);\n    });\n    console.log(\n      'IPFS - Helia addrs',\n      libp2p.getMultiaddrs().map((a) => a.toString())\n    );\n    // const webrtcConn = await libp2p.dial(\n    //   multiaddr(\n    //     '/ip4/127.0.0.1/udp/4001/quic-v1/webtransport/certhash/uEiDHumbyZRFV1Av7qH9-2l5HGgU2a2UqM6eloqO0vYz5pQ/certhash/uEiDD_TuVgih5_ua31Z4MVbNq7WSw095UAQmZqdUFMDTVRA/p2p/12D3KooWEYGfgK4dEY3spfuDKVq6Jpiyj4KxP1r6HS5RFp5WHebz'\n    //   )\n    // );\n    // console.log('----webrtcConn', webrtcConn);\n\n    this._isStarted = true;\n  }\n\n  async stat(cid: string, options: AbortOptions = {}): Promise<IpfsFileStats> {\n    return this.fs!.stat(stringToCid(cid), options).then((result) => {\n      const { type, fileSize, localFileSize, blocks, dagSize, mtime } = result;\n      return {\n        type,\n        size: fileSize || -1,\n        sizeLocal: localFileSize || -1,\n        blocks,\n      };\n    });\n  }\n\n  cat(cid: string, options: CatOptions = {}) {\n    return this.fs!.cat(stringToCid(cid), options);\n  }\n\n  async add(content: File | string, options: AbortOptions = {}) {\n    // Options to keep CID in V0 format 'Qm....';\n    const optionsV0 = {\n      ...options,\n      ...addOptionsV0,\n    } as Partial<AddOptions>;\n\n    let cid;\n\n    if (content instanceof File) {\n      const fileName = content.name;\n      const arrayBuffer = await content.arrayBuffer();\n      const data = new Uint8Array(arrayBuffer);\n      cid = await this.fs!.addFile(\n        { path: fileName, content: data },\n        optionsV0\n      );\n    } else {\n      const data = new TextEncoder().encode(content);\n      cid = await this.fs!.addBytes(data, optionsV0);\n    }\n    // console.log('----added to helia', cid.toString());\n    this.pin(cid.toString(), options);\n    return cid.toString();\n  }\n\n  async pin(cid: string, options: AbortOptions = {}) {\n    const cid_ = stringToCid(cid);\n    const isPinned = await this.node?.pins.isPinned(cid_, options);\n    if (!isPinned) {\n      const pinResult = (\n        await this.node?.pins.add(cid_, options)\n      )?.cid.toString();\n      // console.log('------pin', pinResult);\n    }\n    // console.log('------pinned', cid, isPinned);\n    return undefined;\n  }\n\n  async getPeers() {\n    return this.node!.libp2p!.getConnections().map((c) =>\n      c.remotePeer.toString()\n    );\n  }\n\n  async stop() {\n    await this.node?.stop();\n  }\n\n  async start() {\n    await this.node?.start();\n  }\n\n  async connectPeer(address: string) {\n    const conn = await this.node!.libp2p!.dial(multiaddr(address));\n    return true;\n  }\n\n  ls() {\n    const result = mapToLsResult(this.node!.pins.ls());\n    return result;\n  }\n\n  async info() {\n    const id = this.node!.libp2p.peerId.toString();\n    const agentVersion = this.node!.libp2p!.services!.identify!.host!\n      .agentVersion as string;\n    return { id, agentVersion, repoSize: -1 };\n  }\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport default HeliaNode;\n","// eslint-disable-next-line import/no-unresolved\nimport { webSockets } from '@libp2p/websockets';\nimport * as filters from '@libp2p/websockets/filters';\nimport { Options } from 'ipfs-core/dist/src/types';\n\nconst configIpfs = (): Options => ({\n  start: true,\n  repo: 'ipfs-repo-cyber-v2',\n  relay: {\n    enabled: false,\n    hop: {\n      enabled: false,\n    },\n  },\n  preload: {\n    enabled: false,\n  },\n  config: {\n    API: {\n      HTTPHeaders: {\n        'Access-Control-Allow-Methods': ['PUT', 'POST'],\n        'Access-Control-Allow-Origin': [\n          'http://localhost:3000',\n          'http://127.0.0.1:5001',\n          'http://127.0.0.1:8888',\n          'http://localhost:8888',\n        ],\n      },\n    },\n    Addresses: {\n      Gateway: '/ip4/127.0.0.1/tcp/8080',\n      Swarm: [\n        // '/dns4/ws-star.discovery.cybernode.ai/tcp/443/wss/p2p-webrtc-star',\n        // '/dns4/wrtc-star1.par.dwebops.pub/tcp/443/wss/p2p-webrtc-star',\n        // '/dns4/wrtc-star2.sjc.dwebops.pub/tcp/443/wss/p2p-webrtc-star',\n      ],\n      Delegates: [\n        // '/dns4/node0.delegate.ipfs.io/tcp/443/https',\n        // '/dns4/node1.delegate.ipfs.io/tcp/443/https',\n        // '/dns4/node2.delegate.ipfs.io/tcp/443/https',\n      ],\n    },\n    Discovery: {\n      MDNS: {\n        Enabled: true,\n        Interval: 10,\n      },\n      webRTCStar: {\n        Enabled: false,\n      },\n    },\n    Bootstrap: [\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmNnooDu7bfjPFoTZYxMNLWUQJyrVwtbZg5gBMjTezGAJN',\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmbLHAnMoJPWSCR5Zhtx6BHJX9KiKNN6tpvbUcqanj75Nb',\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmZa1sAxajnQjVM8WjWXoMbmPd7NsWhfKsPkErzpm9wGkp',\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmQCU2EcMqAqQPR2i9bChDtGNJchTbq5TbXJJ16u19uLTa',\n      // '/dnsaddr/bootstrap.libp2p.io/p2p/QmcZf59bWwK5XFi76CZX8cbJ4BhTzzA3gU1ZjYZcYW3dwt',\n      // '/dns4/ws-star.discovery.cybernode.ai/tcp/4430/wss/p2p/QmUgmRxoLtGERot7Y6G7UyF6fwvnusQZfGR15PuE6pY3aB',\n    ],\n    Pubsub: {\n      Enabled: false,\n    },\n    Swarm: {\n      ConnMgr: {\n        HighWater: 300,\n        LowWater: 50,\n      },\n      DisableNatPortMap: false,\n    },\n    Routing: {\n      Type: 'dhtclient',\n    },\n  },\n  libp2p: {\n    transports: [\n      // This is added for local demo!\n      // In a production environment the default filter should be used\n      // where only DNS + WSS addresses will be dialed by websockets in the browser.\n      webSockets({\n        filter: filters.dnsWss,\n      }),\n    ],\n    nat: {\n      enabled: false,\n    },\n  },\n  EXPERIMENTAL: {\n    ipnsPubsub: false,\n  },\n});\n\nexport default configIpfs;\n","import {\n  AbortOptions,\n  CatOptions,\n  IpfsNodeType,\n  IpfsFileStats,\n  IpfsNode,\n  IpfsNodePrperties,\n} from '../../types';\nimport { create as createJsIpfsClient, IPFS } from 'ipfs-core';\nimport { stringToCid, stringToIpfsPath } from '../../utils/cid';\nimport { multiaddr } from '@multiformats/multiaddr';\n\nimport configIpfs from './configs/jsIpfsConfig';\nimport { CYBER_GATEWAY_URL } from '../../config';\n\nclass JsIpfsNode implements IpfsNode {\n  readonly nodeType: IpfsNodeType = 'embedded';\n\n  get config() {\n    return { gatewayUrl: CYBER_GATEWAY_URL };\n  }\n\n  private _isStarted = false;\n\n  get isStarted() {\n    return this._isStarted;\n  }\n\n  private node?: IPFS;\n\n  async init() {\n    this.node = await createJsIpfsClient(configIpfs());\n    if (typeof window !== 'undefined') {\n      window.node = this.node;\n      window.toCid = stringToCid;\n    }\n\n    this._isStarted = true;\n  }\n\n  async stat(cid: string, options: AbortOptions = {}): Promise<IpfsFileStats> {\n    return this.node!.files.stat(stringToIpfsPath(cid), {\n      ...options,\n      withLocal: true,\n      size: true,\n    }).then((result) => {\n      const { type, size, sizeLocal, local, blocks } = result;\n      return {\n        type,\n        size: size || -1,\n        sizeLocal: sizeLocal || -1,\n        blocks,\n      };\n    });\n  }\n\n  cat(cid: string, options: CatOptions = {}) {\n    return this.node!.cat(stringToCid(cid), options);\n  }\n\n  async add(content: File | string, options: AbortOptions = {}) {\n    return (await this.node!.add(content, options)).cid.toString();\n  }\n\n  async pin(cid: string, options: AbortOptions = {}) {\n    return (await this.node!.pin.add(stringToCid(cid), options)).toString();\n  }\n\n  async getPeers() {\n    return (await this.node!.swarm.peers()).map((c) => c.peer.toString());\n  }\n\n  async stop() {}\n  async start() {}\n\n  async connectPeer(address: string) {\n    const addr = multiaddr(address);\n    await this.node!.bootstrap.add(addr);\n\n    await this.node!.swarm.connect(addr);\n    return true;\n  }\n\n  ls() {\n    return this.node!.pin.ls();\n  }\n\n  async info() {\n    const response = await this.node!.stats.repo();\n    const repoSize = Number(response.repoSize);\n\n    const responseId = await this.node!.id();\n    const { agentVersion, id } = responseId;\n    return { id: id.toString(), agentVersion, repoSize };\n  }\n}\n\n// eslint-disable-next-line import/no-unused-modules\nexport default JsIpfsNode;\n","// import { getNodeAutoDialInterval } from './utils-ipfs';\nimport { IpfsNodeType, IpfsNode, CybIpfsNode, IpfsOptsType } from '../types';\nimport KuboNode from './impl/kubo';\nimport HeliaNode from './impl/helia';\nimport JsIpfsNode from './impl/js-ipfs';\n// import EnhancedIpfsNode from './node/enhancedNode';\nimport {\n  CYBERNODE_SWARM_ADDR_TCP,\n  CYBERNODE_SWARM_ADDR_WSS,\n  CYBER_NODE_SWARM_PEER_ID,\n} from '../config';\nimport { withCybFeatures } from './mixins/withCybFeatures';\n\nconst nodeClassMap: Record<IpfsNodeType, new () => IpfsNode> = {\n  helia: HeliaNode,\n  embedded: JsIpfsNode,\n  external: KuboNode,\n};\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport async function initIpfsNode(\n  options: IpfsOptsType\n): Promise<CybIpfsNode> {\n  const { ipfsNodeType, ...restOptions } = options;\n\n  const swarmPeerId = CYBER_NODE_SWARM_PEER_ID;\n\n  const swarmPeerAddress =\n    ipfsNodeType === 'external'\n      ? CYBERNODE_SWARM_ADDR_TCP\n      : CYBERNODE_SWARM_ADDR_WSS;\n\n  const EnhancedClass = withCybFeatures(nodeClassMap[ipfsNodeType], {\n    swarmPeerId,\n    swarmPeerAddress,\n  });\n\n  const instance = new EnhancedClass();\n\n  await instance.init({ url: restOptions.urlOpts });\n  // TODO: REFACT\n  //   instance.connMgrGracePeriod = await getNodeAutoDialInterval(instance);\n  // window.ipfs = instance;\n\n  await instance.reconnectToSwarm();\n  return instance;\n}\n","import { IpfsNode, CybIpfsNode, IpfsContentType } from '../../types';\nimport { parseArrayLikeToDetails } from '../../utils/content';\nimport { addContenToIpfs, getIPFSContent } from '../../utils/utils-ipfs';\n\ntype WithCybFeaturesOptions = {\n  swarmPeerId: string;\n  swarmPeerAddress: string;\n};\n\nfunction withCybFeatures<TBase extends new (...args: any[]) => IpfsNode>(\n  Base: TBase,\n  options: WithCybFeaturesOptions\n) {\n  return class CybIpfsNodeMixin extends Base implements CybIpfsNode {\n    async fetchWithDetails(\n      cid: string,\n      parseAs?: IpfsContentType,\n      abortController?: AbortController\n    ) {\n      const content = await getIPFSContent(cid, this, abortController);\n\n      const details = await parseArrayLikeToDetails(content, cid);\n      return !parseAs\n        ? details\n        : details?.type === parseAs\n        ? details\n        : undefined;\n    }\n\n    async addContent(content: File | string) {\n      return addContenToIpfs(this, content);\n    }\n\n    async isConnectedToSwarm() {\n      const peers = await super.getPeers();\n      return !!peers.find((peerId) => peerId === options.swarmPeerId);\n    }\n\n    async reconnectToSwarm(forced = false) {\n      const isConnectedToSwarm = await this.isConnectedToSwarm();\n      if (!isConnectedToSwarm || forced) {\n        // TODO: refactor using timeout for node config\n\n        //   const needToReconnect =\n        //     Date.now() - lastConnectedTimestamp <\n        //     DEFAULT_CONNECTION_LIFETIME_SECONDS;\n        super\n          .connectPeer(options.swarmPeerAddress)\n          .then(() => {\n            console.log(` connected to swarm - ${options.swarmPeerAddress}`);\n            return true;\n          })\n          .catch((err) => {\n            console.log(\n              `Can't connect to swarm ${options.swarmPeerAddress}: ${err.message}`\n            );\n            return false;\n          });\n      }\n    }\n  };\n}\n\nexport { withCybFeatures };\n","import {\n  PipelineType,\n  pipeline,\n  env,\n  FeatureExtractionPipeline,\n} from '@xenova/transformers';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport DbApiWrapper from 'src/services/backend/services/DbApi/DbApi';\nimport {\n  Subject,\n  combineLatest,\n  Observable,\n  shareReplay,\n  ReplaySubject,\n  filter,\n} from 'rxjs';\nimport { proxy } from 'comlink';\n\nenv.allowLocalModels = false;\n\ntype MlModelParams = {\n  name: PipelineType;\n  model: string;\n};\nconst mlModelMap: Record<string, MlModelParams> = {\n  featureExtractor: {\n    name: 'feature-extraction',\n    model: 'Xenova/all-MiniLM-L6-v2',\n  },\n  // summarization: {\n  //   name: 'summarization',\n  //   model: 'ahmedaeb/distilbart-cnn-6-6-optimised',\n  // },\n  // qa: {\n  //   name: 'question-answering',\n  //   model: 'Xenova/distilbert-base-uncased-distilled-squad',\n  // },\n};\n\nconst loadPipeline = (\n  name: PipelineType,\n  model: string,\n  onProgress: (data: any) => void\n) => {\n  return pipeline(name, model, {\n    progress_callback: (progressData: any) => {\n      try {\n        const {\n          status,\n          progress,\n          // name: modelName,\n          loaded,\n          total,\n        } = progressData;\n\n        const message = loaded ? `${model} - ${loaded}/${total} bytes` : model;\n\n        const done = ['ready', 'error'].some((s) => s === status);\n\n        const progrssStateItem = {\n          status,\n          message,\n          done,\n          progress: progress ? Math.round(progress) : 0,\n        };\n        // console.log('progress_callback', name, progressData);\n\n        onProgress(progrssStateItem);\n      } catch (e) {\n        console.log('-------progresss error', model, e.toString());\n      }\n    },\n  });\n};\n\nexport type EmbeddingApi = {\n  createEmbedding: (text: string) => Promise<number[]>;\n  searchByEmbedding: (\n    text: string,\n    count?: number\n  ) => ReturnType<DbApiWrapper['searchByEmbedding']>;\n};\n\nconst createEmbeddingApi$ = (\n  dbInstance$: Subject<DbApiWrapper>,\n  featureExtractor$: Subject<FeatureExtractionPipeline>\n) => {\n  const replaySubject = new ReplaySubject(1);\n\n  combineLatest([dbInstance$, featureExtractor$]).subscribe(\n    ([dbInstance, featureExtractor]) => {\n      if (dbInstance && featureExtractor) {\n        const createEmbedding = async (text: string) => {\n          const output = await featureExtractor(text, {\n            pooling: 'mean',\n            normalize: true,\n          });\n\n          return output.data as number[];\n        };\n\n        const searchByEmbedding = async (text: string, count?: number) => {\n          const vec = await createEmbedding(text);\n          // console.log('----searchByEmbedding', vec);\n\n          const rows = await dbInstance.searchByEmbedding(vec, count);\n          //   console.log('----searcByEmbedding rows', rows);\n\n          return rows;\n        };\n\n        const api = {\n          createEmbedding,\n          searchByEmbedding,\n        };\n        replaySubject.next(proxy(api));\n      }\n    }\n  );\n  // .pipe(filter((v) => !!v))\n  return replaySubject as Observable<EmbeddingApi>;\n};\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport const createMlApi = (\n  dbInstance$: Subject<DbApiWrapper>,\n  broadcastApi: BroadcastChannelSender\n) => {\n  const featureExtractor$ = new Subject<FeatureExtractionPipeline>();\n  const embeddingApi$ = createEmbeddingApi$(dbInstance$, featureExtractor$);\n\n  const initPipelineInstance = async (alias: keyof typeof mlModelMap) => {\n    const { name, model } = mlModelMap[alias];\n\n    const pipeline = await loadPipeline(name, model, (data) =>\n      broadcastApi.postMlSyncEntryProgress(alias, data)\n    );\n    if (name === 'feature-extraction') {\n      featureExtractor$.next(pipeline as FeatureExtractionPipeline);\n    }\n    console.log(`${alias} - loaded`);\n  };\n\n  const init = async () => {\n    broadcastApi.postServiceStatus('ml', 'starting');\n    console.time(' ml initialized');\n\n    return Promise.all([\n      initPipelineInstance('featureExtractor'),\n      // initMlInstance('summarization'),\n      // initMlInstance('qa'),\n    ])\n      .then((result) => {\n        setTimeout(() => broadcastApi.postServiceStatus('ml', 'started'), 0);\n        console.timeEnd(' ml initialized');\n\n        return result;\n      })\n      .catch((e) =>\n        broadcastApi.postServiceStatus('ml', 'error', e.toString())\n      );\n  };\n\n  init();\n\n  return { embeddingApi$, init };\n};\n","import initAsync, { compile } from 'cyb-rune-wasm';\n\nimport { v4 as uuidv4 } from 'uuid';\n\nimport { TabularKeyValues } from 'src/types/data';\nimport { keyValuesToObject } from 'src/utils/localStorage';\nimport { entityToDto } from 'src/utils/dto';\n\nimport { mapObjIndexed } from 'ramda';\nimport { removeBrokenUnicode } from 'src/utils/string';\n\nimport {\n  BehaviorSubject,\n  ReplaySubject,\n  combineLatest,\n  distinctUntilChanged,\n  map,\n} from 'rxjs';\n\nimport defaultParticleScript from 'src/services/scripting/rune/default/particle.rn';\nimport runtimeScript from 'src/services/scripting/rune/runtime.rn';\n\nimport {\n  ScriptCallback,\n  ScriptParticleParams,\n  ScriptContext,\n  ScriptParticleResult,\n  // ScriptMyParticleParams,\n  ScriptEntrypoints,\n  EntrypointParams,\n  EngineContext,\n  ScriptMyCampanion,\n} from './types';\n\nimport { extractRuneScript } from './helpers';\n\ntype RuneEntrypoint = {\n  readOnly: boolean;\n  execute: boolean;\n  funcName: string;\n  funcParams: any[];\n  params: Object; // context data\n  input: string; // main code\n  script: string; // runtime code\n};\n\ntype RuneRunResult = {\n  output: string;\n  diagnosticsOutput: string;\n  diagnostics: any[];\n  error: string;\n  result: any;\n};\n\nconst compileConfig = {\n  budget: 1_000_000,\n  experimental: false,\n  instructions: false,\n  options: [],\n};\n\nconst defaultRuneEntrypoint: RuneEntrypoint = {\n  readOnly: false,\n  execute: true,\n  funcName: 'main',\n  funcParams: {},\n  params: {},\n  input: defaultParticleScript,\n  script: runtimeScript,\n};\n\nconst toRecord = (item: TabularKeyValues) =>\n  keyValuesToObject(Object.values(item));\n\nexport type LoadParams = {\n  entrypoints: ScriptEntrypoints;\n  secrets: TabularKeyValues;\n};\n\n// eslint-disable-next-line import/prefer-default-export\nfunction enigine() {\n  let entrypoints: Partial<ScriptEntrypoints> = {};\n  let context: EngineContext = { params: {}, user: {}, secrets: {} };\n  const isInitialized$ = new BehaviorSubject<boolean>(false);\n  const entrypoints$ = new BehaviorSubject<Partial<ScriptEntrypoints>>({});\n\n  const scriptCallbacks = new Map<string, ScriptCallback>();\n\n  const isSoulInitialized$ = new ReplaySubject(1);\n  combineLatest([isInitialized$, entrypoints$])\n    .pipe(\n      map(\n        ([isInitialized, entrypoints]) =>\n          !!(isInitialized && entrypoints.particle)\n      ),\n      distinctUntilChanged()\n    )\n    .subscribe((v) => {\n      isSoulInitialized$.next(v);\n    });\n\n  entrypoints$.subscribe((v) => {\n    entrypoints = v;\n  });\n\n  const init = async () => {\n    console.time(' rune initialized');\n    await initAsync();\n    // window.rune = rune; // debug\n    console.timeEnd(' rune initialized');\n    isInitialized$.next(true);\n  };\n\n  const pushContext = <K extends keyof ScriptContext>(\n    name: K,\n    value: ScriptContext[K] //| TabularKeyValues\n  ) => {\n    // context[name] =  toRecord(value as TabularKeyValues);\n    context[name] = value;\n  };\n\n  const popContext = (names: (keyof ScriptContext)[]) => {\n    const newContext = context;\n    names.forEach((name) => {\n      newContext[name] = {};\n    });\n    context = newContext;\n  };\n\n  const setEntrypoints = (scriptEntrypoints: ScriptEntrypoints) => {\n    entrypoints = mapObjIndexed(\n      (v) => ({ ...v, script: extractRuneScript(v.script) }),\n      scriptEntrypoints\n    );\n    entrypoints$.next(entrypoints);\n  };\n\n  const run = async (\n    script: string,\n    compileParams: Partial<RuneEntrypoint>,\n    callback?: ScriptCallback,\n    scripts: Record<string, string> = {}\n  ) => {\n    const refId = uuidv4().toString();\n\n    callback && scriptCallbacks.set(refId, callback);\n    const scriptParams = {\n      app: context,\n      refId,\n    };\n    const compilerParams = {\n      ...defaultRuneEntrypoint,\n      ...compileParams,\n      input: script,\n      scripts: { ...scripts, runtime: runtimeScript },\n      params: scriptParams,\n    };\n    // console.log('-----run', scriptParams);\n    const outputData = await compile(compilerParams, compileConfig);\n\n    // Parse the JSON string\n    const { result, error } = outputData;\n\n    try {\n      scriptCallbacks.delete(refId);\n\n      return {\n        ...entityToDto(outputData),\n        error,\n        result: result\n          ? result === '()'\n            ? ''\n            : JSON.parse(removeBrokenUnicode(result))\n          : { action: 'error', message: 'No result' },\n      } as RuneRunResult;\n    } catch (e) {\n      scriptCallbacks.delete(refId);\n\n      console.log(\n        `engine.run err ${compilerParams.funcName}`,\n        e,\n        outputData,\n        compilerParams\n      );\n      return {\n        diagnosticsOutput: `scripting engine error ${e}`,\n        ...outputData,\n        result: { action: 'error', message: e?.toString() || 'Unknown error' },\n      } as RuneRunResult;\n    }\n  };\n\n  const getParticleScriptOrAction = ():\n    | ['error' | 'pass' | 'script', string] => {\n    if (!entrypoints.particle) {\n      return ['error', ''];\n    }\n\n    const { script, enabled } = entrypoints.particle;\n\n    if (!enabled) {\n      return ['pass', ''];\n    }\n\n    return ['script', script];\n  };\n\n  const personalProcessor = async (\n    params: ScriptParticleParams\n  ): Promise<ScriptParticleResult> => {\n    const [resultType, script] = getParticleScriptOrAction();\n\n    if (resultType === 'error') {\n      return { action: 'error', message: 'No particle entrypoint' };\n    }\n\n    if (resultType !== 'script') {\n      return { action: 'pass' };\n    }\n\n    const { cid, contentType, content } = params;\n    const output = await run(script, {\n      funcName: 'personal_processor',\n      funcParams: [cid, contentType, content], //params as EntrypointParams,\n    });\n    const { action, content: outputContent } = output.result;\n\n    if (action === 'error') {\n      console.error(\n        `RUNE: personalProcessor error: ${params.cid}`,\n        params,\n        output\n      );\n    }\n\n    if (outputContent) {\n      return { ...output.result, content: outputContent };\n    }\n\n    return output.result;\n  };\n\n  const executeFunction = async (\n    script: string,\n    funcName: string,\n    funcParams: EntrypointParams\n  ): Promise<ScriptParticleResult> => {\n    console.log('-----executeFunction rune', funcName, funcParams);\n    const output = await run(script, {\n      funcName,\n      funcParams,\n      readOnly: true, // block to sign tx and add to ipfs\n    });\n\n    return output.result;\n  };\n\n  // const particleInference = async (\n  //   userScript: string,\n  //   funcParams: EntrypointParams\n  // ): Promise<ScriptMyParticleResult> => {\n  //   const output = await run(userScript, {\n  //     funcName: 'particle_inference',\n  //     funcParams,\n  //   });\n\n  //   return output.result;\n  // };\n\n  const askCompanion = async (\n    cid: string,\n    contentType: string,\n    content: string,\n    callback?: ScriptCallback\n  ): Promise<ScriptMyCampanion> => {\n    const [resultType, script] = getParticleScriptOrAction();\n    if (resultType === 'error') {\n      return {\n        action: 'error',\n        metaItems: [[{ type: 'text', text: 'No particle entrypoint' }]],\n      };\n    }\n\n    if (resultType === 'pass') {\n      return { action: 'pass', metaItems: [] };\n    }\n\n    const output = await run(\n      script,\n      {\n        funcName: 'ask_companion',\n        funcParams: [cid, contentType, content],\n      },\n      callback\n    );\n\n    if (output.result.action === 'error') {\n      console.error('---askCompanion error', output);\n      return {\n        action: 'error',\n        metaItems: [[{ type: 'text', text: output.error }]],\n      };\n    }\n\n    return { action: 'answer', metaItems: output.result.content };\n  };\n\n  const executeCallback = async (refId: string, data: any) => {\n    const callback = scriptCallbacks.get(refId);\n\n    if (callback) {\n      await callback(data);\n    }\n  };\n\n  return {\n    init,\n    isSoulInitialized$,\n    run,\n    // particleInference,\n    askCompanion,\n    personalProcessor,\n    setEntrypoints,\n    pushContext,\n    popContext,\n    executeFunction,\n    executeCallback,\n    getDebug: () => ({\n      context,\n      entrypoints,\n    }),\n  };\n}\n\nconst scriptEngine = enigine();\n\nexport type RuneEngine = typeof scriptEngine;\n\nexport default scriptEngine;\n","import { Observable } from 'rxjs';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport rune from 'src/services/scripting/engine';\nimport runeDeps, { RuneInnerDeps } from 'src/services/scripting/runeDeps';\nimport DbApiWrapper from 'src/services/backend/services/DbApi/DbApi';\nimport { EmbeddingApi } from './mlApi';\n\n// eslint-disable-next-line import/no-unused-modules, import/prefer-default-export\nexport const createRuneApi = (\n  embeddingApi$: Observable<EmbeddingApi>,\n  dbInstance$: Observable<DbApiWrapper>,\n  broadcastApi: BroadcastChannelSender\n) => {\n  const setInnerDeps = (deps: Partial<RuneInnerDeps>) =>\n    runeDeps.setInnerDeps(deps);\n\n  embeddingApi$.subscribe((embeddingApi) => {\n    setInnerDeps({ embeddingApi });\n  });\n\n  dbInstance$.subscribe((dbApi) => {\n    setInnerDeps({ dbApi });\n  });\n\n  rune.isSoulInitialized$.subscribe((value) => {\n    value\n      ? setTimeout(() => broadcastApi.postServiceStatus('rune', 'started'), 0)\n      : broadcastApi.postServiceStatus('rune', 'inactive');\n  });\n\n  const init = async () => {\n    broadcastApi.postServiceStatus('rune', 'starting');\n\n    await rune.init();\n    setInnerDeps({ rune });\n  };\n\n  init();\n\n  return { rune, setInnerDeps, abort: runeDeps.abort };\n};\n","import { proxy } from 'comlink';\n\nimport { QueuePriority } from 'src/services/QueueManager/types';\nimport { ParticleCid } from 'src/types/base';\nimport { BehaviorSubject, Subject } from 'rxjs';\nimport { RuneInnerDeps } from 'src/services/scripting/runeDeps';\n\nimport { exposeWorkerApi } from '../factoryMethods';\n\nimport { SyncService } from '../../services/sync/sync';\nimport { SyncServiceParams } from '../../services/sync/types';\n\nimport DbApi from '../../services/DbApi/DbApi';\n\nimport BroadcastChannelSender from '../../channels/BroadcastChannelSender';\nimport { createIpfsApi } from './api/ipfsApi';\nimport { createMlApi } from './api/mlApi';\nimport { createRuneApi } from './api/runeApi';\n\n// import { initRuneDeps } from 'src/services/scripting/wasmBindings';\n\nconst createBackgroundWorkerApi = () => {\n  const broadcastApi = new BroadcastChannelSender();\n\n  const dbInstance$ = new Subject<DbApi>();\n\n  const injectDb = (db: DbApi) => dbInstance$.next(db);\n\n  const params$ = new BehaviorSubject<SyncServiceParams>({\n    myAddress: null,\n  });\n\n  const { embeddingApi$ } = createMlApi(dbInstance$, broadcastApi);\n\n  const { setInnerDeps, rune } = createRuneApi(\n    embeddingApi$,\n    dbInstance$,\n    broadcastApi\n  );\n\n  const {\n    ipfsQueue,\n    ipfsInstance$,\n    api: ipfsApi,\n  } = createIpfsApi(rune, broadcastApi);\n\n  const waitForParticleResolve = (\n    cid: ParticleCid,\n    priority: QueuePriority = QueuePriority.MEDIUM\n  ) => ipfsQueue.enqueueAndWait(cid, { postProcessing: false, priority });\n\n  const serviceDeps = {\n    waitForParticleResolve,\n    dbInstance$,\n    ipfsInstance$,\n    embeddingApi$,\n    params$,\n  };\n\n  // service to sync updates about cyberlinks, transactions, swarm etc.\n  const syncService = new SyncService(serviceDeps);\n\n  // INITIALIZATION\n  setInnerDeps({ ipfsApi });\n\n  return {\n    injectDb,\n    isIpfsInitialized: () => !!ipfsInstance$.getValue(),\n    // syncDrive,\n    ipfsApi: proxy(ipfsApi),\n    rune: proxy(rune),\n    embeddingApi$,\n    // ipfsInstance$,\n    ipfsQueue: proxy(ipfsQueue),\n    setRuneDeps: (\n      deps: Partial<Omit<RuneInnerDeps, 'embeddingApi' | 'dbApi'>>\n    ) => setInnerDeps(deps),\n    // restartSync: (name: SyncEntryName) => syncService.restart(name),\n    setParams: (params: Partial<SyncServiceParams>) =>\n      params$.next({ ...params$.value, ...params }),\n  };\n};\n\nconst backgroundWorker = createBackgroundWorkerApi();\n\nexport type BackgroundWorker = typeof backgroundWorker;\n\n// Expose the API to the main thread as shared/regular worker\nexposeWorkerApi(self, backgroundWorker);\n","import { proxy } from 'comlink';\nimport { BehaviorSubject, Subject } from 'rxjs';\nimport QueueManager from 'src/services/QueueManager/QueueManager';\nimport {\n  QueueItemCallback,\n  QueueItemOptions,\n} from 'src/services/QueueManager/types';\nimport BroadcastChannelSender from 'src/services/backend/channels/BroadcastChannelSender';\nimport { initIpfsNode } from 'src/services/ipfs/node/factory';\n\nimport {\n  CybIpfsNode,\n  IpfsContentType,\n  IpfsOptsType,\n} from 'src/services/ipfs/types';\nimport { RuneEngine } from 'src/services/scripting/engine';\n\n// eslint-disable-next-line import/prefer-default-export\nexport const createIpfsApi = (\n  rune: RuneEngine,\n  broadcastApi: BroadcastChannelSender\n) => {\n  const ipfsInstance$ = new BehaviorSubject<CybIpfsNode | undefined>(undefined);\n  const ipfsQueue = new QueueManager(ipfsInstance$, {\n    rune,\n  });\n  const stopIpfs = async () => {\n    const ipfsNode = ipfsInstance$.getValue();\n\n    if (ipfsNode) {\n      await ipfsNode.stop();\n    }\n    ipfsInstance$.next(undefined);\n    broadcastApi.postServiceStatus('ipfs', 'inactive');\n  };\n\n  const startIpfs = async (ipfsOpts: IpfsOptsType) => {\n    try {\n      const ipfsNode = ipfsInstance$.getValue();\n      if (ipfsNode) {\n        // console.log('Ipfs node already started!');\n        setTimeout(() => broadcastApi.postServiceStatus('ipfs', 'started'), 0);\n        return Promise.resolve();\n        // await ipfsNode.stop();\n      }\n      broadcastApi.postServiceStatus('ipfs', 'starting');\n      console.time(' ipfs initialized');\n\n      const newIpfsNode = await initIpfsNode(ipfsOpts);\n      console.timeEnd(' ipfs initialized');\n\n      ipfsInstance$.next(newIpfsNode);\n      setTimeout(() => broadcastApi.postServiceStatus('ipfs', 'started'), 0);\n      return true;\n    } catch (err) {\n      console.log('----ipfs node init error ', err);\n      const msg = err instanceof Error ? err.message : (err as string);\n      broadcastApi.postServiceStatus('ipfs', 'error', msg);\n      throw Error(msg);\n    }\n  };\n\n  const api = {\n    start: startIpfs,\n    stop: stopIpfs,\n    config: async () => ipfsInstance$.getValue()?.config,\n    info: async () => ipfsInstance$.getValue()?.info(),\n    fetchWithDetails: async (\n      cid: string,\n      parseAs?: IpfsContentType,\n      controller?: AbortController\n    ) => {\n      const ipfsNode = ipfsInstance$.getValue();\n      if (!ipfsNode) {\n        throw new Error('ipfs node not initialized');\n      }\n      return ipfsNode.fetchWithDetails(cid, parseAs, controller);\n    },\n    enqueue: async (\n      cid: string,\n      callback: QueueItemCallback,\n      options: QueueItemOptions\n    ) => ipfsQueue.enqueue(cid, callback, options),\n    enqueueAndWait: async (cid: string, options?: QueueItemOptions) =>\n      ipfsQueue!.enqueueAndWait(cid, options),\n    dequeue: async (cid: string) => ipfsQueue.cancel(cid),\n    dequeueByParent: async (parent: string) => ipfsQueue.cancelByParent(parent),\n    clearQueue: async () => ipfsQueue.clear(),\n    addContent: async (content: string | File) =>\n      ipfsInstance$.getValue()?.addContent(content),\n  };\n\n  return { ipfsInstance$, ipfsQueue, api };\n};\n\nexport type IpfsApi = ReturnType<typeof createIpfsApi>['api'];\n","import { Nullable } from 'src/types';\nimport { v4 as uuidv4 } from 'uuid';\n\nexport async function getScriptFromParticle(cid?: Nullable<string>) {\n  throw new Error('Not implemented');\n  // if (!cid || !isCID(cid)) {\n  //   // throw new Error('cid is not valid');\n  //   return undefined;\n  // }\n\n  // const queueResult = await queueManager.enqueueAndWait(cid, {\n  //   postProcessing: false,\n  // });\n  // const result = queueResult?.result;\n  // if (!result?.result || result?.contentType !== 'text') {\n  //   // throw new Error('content is not valid');\n  //   return undefined;\n  // }\n\n  // return getTextFromIpfsContent(result.result);\n}\n\nexport function extractRuneContent(markdown: string) {\n  // Regular expression to match the content between ```rune``` tags\n  const runeRegex = /```rune\\s*([\\s\\S]*?)```/g;\n\n  let match;\n  let runeScript = '';\n  let modifiedMarkdown = markdown;\n  let hasRune = false;\n  // Iterate through all matches of the regular expression\n  while ((match = runeRegex.exec(markdown)) !== null) {\n    hasRune = true;\n    // Append the matched content between ```rune``` tags to runeContent variable\n    runeScript += match[1] + '\\n';\n\n    // Replace the entire matched block, including the tags, with an empty string\n    modifiedMarkdown = modifiedMarkdown.replace(match[0], '');\n  }\n\n  // Returning both the extracted content and the modified markdown without the tags\n  return {\n    script: runeScript.trim(),\n    markdown: modifiedMarkdown,\n    hasRune,\n  };\n}\n\nexport function extractRuneScript(markdown: string) {\n  const { script, markdown: md, hasRune } = extractRuneContent(markdown);\n  // if no rune tag, consider this like pure script\n  return hasRune ? script : md;\n}\n\nexport const generateRefId = () => uuidv4().toString();\n","import React, { useContext } from 'react';\nimport { CyberClient } from '@cybercongress/cyber-js';\nimport { Option } from 'src/types';\nimport { useQuery } from '@tanstack/react-query';\nimport { RPC_URL } from 'src/constants/config';\n\nconst QueryClientContext = React.createContext<Option<CyberClient>>(undefined);\n\n/**\n * @deprecated use queryCyberClient\n */\nexport function useQueryClient() {\n  return useContext(QueryClientContext);\n}\n\nfunction QueryClientProvider({ children }: { children: React.ReactNode }) {\n  const {\n    data: client,\n    error,\n    isFetching,\n  } = useQuery({\n    queryKey: ['cyberClient', 'connect'],\n    queryFn: async () => {\n      return CyberClient.connect(RPC_URL);\n    },\n  });\n\n  if (isFetching) {\n    return null;\n  }\n\n  if (error) {\n    console.error('Error queryClient connect: ', error.message);\n\n    return 'api connection error';\n    // return <APIError />;\n  }\n\n  return (\n    <QueryClientContext.Provider value={client}>\n      {children}\n    </QueryClientContext.Provider>\n  );\n}\n\nexport default QueryClientProvider;\n","import { useEffect, useState, useMemo } from 'react';\nimport axios from 'axios';\nimport { useQueryClient } from 'src/contexts/queryClient';\nimport { AccountValue } from 'src/types/defaultAccount';\nimport { useQuery } from '@tanstack/react-query';\nimport { Nullable } from 'src/types';\nimport { Citizenship } from 'src/types/citizenship';\nimport { CyberClient } from '@cybercongress/cyber-js';\nimport { getPassport } from 'src/services/passports/lcd';\n\nconst AMOUNT_ALL_STAGE = 90;\nconst NEW_RELEASE = 1000; // release 1% every 1k claims\nconst CONSTITUTION_HASH = 'QmcHB9GKHAKCLQhmSj71qNJhENJJg8Gymd1PvvsCQBhG7M';\n\n// test root\n// const CONTRACT_ADDRESS_GIFT =\n//   'bostrom1dwzfa74hzpt6393czajlldnxjup8zk3xh3skegnm67yzqx33k2cssyduk8';\n// const CONTRACT_ADDRESS_PASSPORT =\n//   'bostrom1fzm6gzyccl8jvdv3qq6hp9vs6ylaruervs4m06c7k0ntzn2f8faq7ha2z2';\n\n// prod root\nconst CONTRACT_ADDRESS_GIFT =\n  'bostrom16t6tucgcqdmegye6c9ltlkr237z8yfndmasrhvh7ucrfuqaev6xq7cpvek';\nconst CONTRACT_ADDRESS_PASSPORT =\n  'bostrom1xut80d09q0tgtch8p0z4k5f88d3uvt8cvtzm5h3tu3tsy4jk9xlsfzhxel';\n\nconst DICTIONARY = {\n  Astronauts: 'Astronaut',\n  'Average Citizens. ETH Analysis': 'Average Citizens',\n  'Cyberpunks. ERC20 and ERC721 Analysis': 'Cyberpunk',\n  'Extraordinary Hackers. Gas Analysis': 'Extraordinary Hacker',\n  'Key Opinion Leaders. ERC20 Analysis': 'Key Opinion Leader',\n  'Masters of the Great Web. Gas and ERC721 Analysis':\n    'Master of the Great Web',\n  'Passionate Investors. ERC20 Analysis': 'Passionate Investor',\n  'Heroes of the Great Web. Genesis and ETH2 Stakers':\n    'True Hero of the Great Web',\n  Leeches: 'Devil',\n};\n\nconst GIFT_ICON = '';\nconst BOOT_ICON = '';\n\nconst useGetActivePassport = (\n  addressActive: Nullable<AccountValue>,\n  updateFunc?: number\n) => {\n  const queryClient = useQueryClient();\n  const data = useQuery(\n    ['active_passport', addressActive?.bech32],\n    () => activePassport(queryClient, addressActive?.bech32),\n    {\n      enabled: Boolean(addressActive) && Boolean(queryClient),\n    }\n  );\n\n  useEffect(() => {\n    if (updateFunc) {\n      data.refetch();\n    }\n  }, [updateFunc]);\n\n  return {\n    citizenship: data.data,\n    loading: data.isFetching,\n  };\n};\n\n// TODO: replace with hook\nconst activePassport = async (\n  client: CyberClient,\n  address: string\n): Promise<Nullable<Citizenship>> => {\n  try {\n    const query = {\n      active_passport: {\n        address,\n      },\n    };\n    const response = await client.queryContractSmart(\n      CONTRACT_ADDRESS_PASSPORT,\n      query\n    );\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst parseValue = (data) => {\n  if (data.length > 0) {\n    const newData = data.replace(/'/g, '\"');\n    return JSON.parse(newData);\n  }\n  return null;\n};\n\nconst parseValueDetails = (data) => {\n  const value = parseValue(data);\n  if (value !== null) {\n    const details = {};\n    value.forEach((item) => {\n      details[DICTIONARY[item.audience]] = { gift: item.gift };\n    });\n    return details;\n  }\n  return null;\n};\n\nconst parseResponse = (obj) => {\n  return {\n    ...obj,\n    details: parseValueDetails(obj.details),\n    proof: parseValue(obj.proof),\n  };\n};\n\nconst checkGift = async (address) => {\n  try {\n    const response = await axios({\n      method: 'GET',\n      url: `https://titan.cybernode.ai/graphql/api/rest/get-cybergift/${address}`, // prod root\n      // url: `https://titan.cybernode.ai/graphql/api/rest/get-test-gift/${address}`, // test root\n    });\n\n    if (response && response.data) {\n      const { data } = response;\n      if (\n        Object.prototype.hasOwnProperty.call(data, 'cyber_gift_proofs') &&\n        Object.keys(data.cyber_gift_proofs).length > 0\n      ) {\n        const { cyber_gift_proofs: cyberGiftData } = data;\n        return parseResponse(cyberGiftData[0]);\n      }\n      if (\n        Object.prototype.hasOwnProperty.call(data, 'test_gift') &&\n        Object.keys(data.test_gift).length > 0\n      ) {\n        const { test_gift: cyberGiftData } = data;\n        return parseResponse(cyberGiftData[0]);\n      }\n    }\n    return null;\n  } catch (error) {\n    return null;\n  }\n};\n\nconst queryContractSmartPassport = async (client, query) => {\n  try {\n    const response = await getPassport(query);\n    // const response = await client.queryContractSmart(\n    //   CONTRACT_ADDRESS_PASSPORT,\n    //   query\n    // );\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst queryContractSmartGift = async (client, query) => {\n  try {\n    const response = await client.queryContractSmart(\n      CONTRACT_ADDRESS_GIFT,\n      query\n    );\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getStateGift = async (client) => {\n  try {\n    const query = {\n      state: {},\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getConfigGift = async (client) => {\n  try {\n    const query = {\n      config: {},\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getReleaseState = async (client, address) => {\n  try {\n    const query = {\n      release_state: {\n        address,\n      },\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getClaimedAmount = async (client, address) => {\n  try {\n    const query = {\n      claim: {\n        address,\n      },\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getIsClaimed = async (client, address) => {\n  try {\n    const query = {\n      is_claimed: {\n        address,\n      },\n    };\n    const response = await queryContractSmartGift(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getPassportByNickname = async (client, nickname) => {\n  try {\n    const query = {\n      passport_by_nickname: {\n        nickname,\n      },\n    };\n    const response = await queryContractSmartPassport(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst getNumTokens = async (client) => {\n  try {\n    const query = {\n      num_tokens: {},\n    };\n    const response = await queryContractSmartPassport(client, query);\n    return response;\n  } catch (error) {\n    console.log('error', error);\n    return null;\n  }\n};\n\nconst tooMuthAddressError =\n  'failed to execute message; message index: 0: Address is not eligible to claim airdrop, Too many addresses: execute wasm contract failed';\n\nconst canProve8AddressNewError =\n  'You can prove only 8 addresses for one passport';\n\nconst parseRowLog = (rawlog) => {\n  if (rawlog === tooMuthAddressError) {\n    return canProve8AddressNewError;\n  }\n\n  return rawlog;\n};\n\nexport {\n  activePassport,\n  CONTRACT_ADDRESS_PASSPORT,\n  useGetActivePassport,\n  CONSTITUTION_HASH,\n  CONTRACT_ADDRESS_GIFT,\n  GIFT_ICON,\n  BOOT_ICON,\n  AMOUNT_ALL_STAGE,\n  NEW_RELEASE,\n  checkGift,\n  getConfigGift,\n  getStateGift,\n  getReleaseState,\n  getClaimedAmount,\n  getIsClaimed,\n  getPassportByNickname,\n  parseRowLog,\n  getNumTokens,\n};\n","import { CONTRACT_ADDRESS_PASSPORT } from 'src/containers/portal/utils';\nimport { toAscii, toBase64 } from '@cosmjs/encoding';\nimport { PassportContractQuery } from 'src/services/soft.js/api/passport';\n\nimport axios from 'axios';\nimport defaultNetworks from 'src/constants/defaultNetworks';\n\n// need this request to query passports with any queryClient chain\n// eslint-disable-next-line import/prefer-default-export\nexport async function getPassport(query: PassportContractQuery) {\n  const response = await axios.get(\n    `${\n      defaultNetworks.bostrom.LCD_URL\n    }/cosmwasm/wasm/v1/contract/${CONTRACT_ADDRESS_PASSPORT}/smart/${toBase64(\n      toAscii(JSON.stringify(query))\n    )}`\n  );\n  return response.data.data;\n}\n","import { DeliverTxResponse } from '@cosmjs/stargate';\n\nexport class SigningCyberClientError extends Error {\n  public code: number;\n\n  constructor(response: string[] | DeliverTxResponse) {\n    let message = '';\n    let code = -1;\n    if (response instanceof Array) {\n      message = response.join('\\r\\n');\n    } else if (response.rawLog) {\n      message = response.rawLog.toString();\n      code = response.code;\n    } else {\n      message = message?.error;\n    }\n\n    super(message);\n    cyblog.error(message, { error: response });\n\n    this.code = code;\n  }\n}\n\nexport const throwErrorOrResponse = (\n  response: string[] | DeliverTxResponse\n) => {\n  const isResponseError = response instanceof Array || response.code !== 0;\n  if (isResponseError) {\n    throw new SigningCyberClientError(response);\n  }\n  return response as DeliverTxResponse;\n};\n","/* eslint-disable import/no-unused-modules */\nimport { Coin, OfflineSigner, StdFee } from '@cosmjs/launchpad';\nimport { SigningCyberClient } from '@cybercongress/cyber-js';\nimport { SenseApi } from 'src/contexts/backend/services/senseApi';\nimport { CyberLinkSimple, NeuronAddress, ParticleCid } from 'src/types/base';\nimport { getNowUtcNumber } from 'src/utils/date';\n\nimport { DEFAULT_GAS_LIMITS } from 'src/constants/config';\nimport { CONTRACT_ADDRESS_PASSPORT } from 'src/containers/portal/utils';\nimport BigNumber from 'bignumber.js';\nimport { asyncForEach } from 'src/utils/utils';\nimport { LinkDto } from '../CozoDb/types/dto';\nimport { throwErrorOrResponse } from './errors';\n\nimport Soft3MessageFactory from '../soft.js/api/msgs';\n\nconst defaultFee = {\n  amount: [],\n  gas: DEFAULT_GAS_LIMITS.toString(),\n} as StdFee;\n\nexport const sendCyberlink = async (\n  neuron: NeuronAddress,\n  from: ParticleCid,\n  to: ParticleCid,\n  {\n    senseApi,\n    signingClient,\n  }: {\n    senseApi: SenseApi;\n    signingClient: SigningCyberClient;\n  },\n  fee: StdFee = defaultFee\n) => {\n  const response = await signingClient!.cyberlink(neuron, from, to, fee);\n  const result = throwErrorOrResponse(response);\n\n  const { transactionHash } = result;\n  const link = {\n    from,\n    to,\n    transactionHash,\n    timestamp: getNowUtcNumber(),\n    neuron,\n  } as LinkDto;\n\n  // TODO: add from/toparticle to DB ??\n  await senseApi?.putCyberlink(link);\n  await senseApi?.addCyberlinkLocal(link);\n\n  return transactionHash;\n};\n\nexport const sendCyberlinkArray = async (\n  neuron: NeuronAddress,\n  arrLinks: CyberLinkSimple[],\n  {\n    signingClient,\n    senseApi,\n  }: {\n    senseApi: SenseApi;\n    signingClient: SigningCyberClient;\n  }\n) => {\n  const multiplier = new BigNumber(2).multipliedBy(arrLinks.length);\n\n  const cyberlinkMsg = {\n    typeUrl: '/cyber.graph.v1beta1.MsgCyberlink',\n    value: {\n      neuron,\n      links: arrLinks,\n    },\n  };\n\n  const response = await signingClient.signAndBroadcast(\n    neuron,\n    [cyberlinkMsg],\n    Soft3MessageFactory.fee(multiplier.toNumber())\n  );\n\n  const result = throwErrorOrResponse(response);\n\n  const { transactionHash } = result;\n\n  const links = arrLinks.map((item) => {\n    return {\n      from: item.from,\n      to: item.to,\n      transactionHash,\n      timestamp: getNowUtcNumber(),\n      neuron,\n    } as LinkDto;\n  });\n\n  await senseApi?.putCyberlink(links);\n\n  await asyncForEach(links, async (item: LinkDto) => {\n    await senseApi?.addCyberlinkLocal(item);\n  });\n\n  return transactionHash;\n};\n\nexport const sendTokensWithMessage = async (\n  address: NeuronAddress,\n  recipient: string,\n  offerCoin: Coin[],\n  memo: string | ParticleCid,\n  {\n    senseApi,\n    signingClient,\n  }: { signingClient: SigningCyberClient; senseApi: SenseApi }\n) => {\n  const response = await signingClient.sendTokens(\n    address,\n    recipient,\n    offerCoin,\n    'auto',\n    memo\n  );\n  const result = throwErrorOrResponse(response);\n  const { transactionHash } = result;\n\n  await senseApi?.addMsgSendAsLocal({\n    transactionHash,\n    fromAddress: address,\n    toAddress: recipient,\n    amount: offerCoin,\n    memo,\n  });\n\n  return transactionHash;\n};\n\nexport const investmint = async (\n  address: NeuronAddress,\n  amount: Coin,\n  resource: string,\n  length: number,\n  signingClient: SigningCyberClient\n) => {\n  const response = await signingClient.investmint(\n    address,\n    amount,\n    resource,\n    length,\n    'auto'\n  );\n\n  const { transactionHash } = throwErrorOrResponse(response);\n  return transactionHash;\n};\n\nexport const updatePassportParticle = async (\n  nickname: string,\n  particle: ParticleCid,\n  {\n    signer,\n    signingClient,\n  }: {\n    signer: OfflineSigner;\n    signingClient: SigningCyberClient;\n  }\n) => {\n  const [{ address }] = await signer.getAccounts();\n\n  const msgObject = {\n    update_particle: {\n      nickname,\n      particle,\n    },\n  };\n  return signingClient.execute(\n    address,\n    CONTRACT_ADDRESS_PASSPORT,\n    msgObject,\n    'auto'\n  );\n};\n","import { ProxyMarked, Remote } from 'comlink';\n\nimport { BehaviorSubject, Subject, first, tap } from 'rxjs';\nimport { CyberClient, SigningCyberClient } from '@cybercongress/cyber-js';\nimport { RPC_URL } from 'src/constants/config';\nimport { SenseApi } from 'src/contexts/backend/services/senseApi';\nimport { Option } from 'src/types';\nimport { getSearchQuery, searchByHash } from 'src/utils/search/utils';\nimport { NeuronAddress, ParticleCid } from 'src/types/base';\nimport { getPassportByNickname } from 'src/containers/portal/utils';\nimport { sendCyberlink } from '../neuron/neuronApi';\n\nimport { extractRuneScript } from './helpers';\nimport { RuneEngine } from './engine';\nimport DbApiWrapper from '../backend/services/DbApi/DbApi';\nimport { IpfsApi } from '../backend/workers/background/api/ipfsApi';\nimport { EmbeddingApi } from '../backend/workers/background/api/mlApi';\n\nexport type RuneInnerDeps = {\n  ipfsApi: Option<IpfsApi>;\n  rune: Option<RuneEngine>;\n  queryClient: Option<CyberClient>;\n  embeddingApi: Option<EmbeddingApi>;\n  dbApi: Option<DbApiWrapper>;\n  signingClient: Option<SigningCyberClient & ProxyMarked>;\n  // signer?: Option<OfflineSigner>;\n  senseApi: Option<SenseApi & ProxyMarked>;\n  address: Option<NeuronAddress>;\n};\n\ntype SubjectDeps<T> = {\n  [K in keyof T]: BehaviorSubject<T[K]> | Subject<T[K]>;\n};\n\nconst createRuneDeps = () => {\n  const subjectDeps: SubjectDeps<RuneInnerDeps> = {\n    // Initialize subjects for each dependency\n    ipfsApi: new BehaviorSubject<RuneInnerDeps['ipfsApi']>(undefined),\n    rune: new BehaviorSubject<RuneInnerDeps['rune']>(undefined),\n    queryClient: new BehaviorSubject<RuneInnerDeps['queryClient']>(undefined),\n    embeddingApi: new BehaviorSubject<Option<EmbeddingApi>>(undefined),\n    signingClient: new BehaviorSubject<RuneInnerDeps['signingClient']>(\n      undefined\n    ),\n    senseApi: new BehaviorSubject<RuneInnerDeps['senseApi']>(undefined),\n    address: new BehaviorSubject<RuneInnerDeps['address']>(undefined),\n    dbApi: new BehaviorSubject<RuneInnerDeps['dbApi']>(undefined),\n  };\n\n  let abortController: Option<AbortController>;\n\n  const defferedDependency = (\n    name: keyof RuneInnerDeps\n  ): Promise<RuneInnerDeps[typeof name]> => {\n    return new Promise((resolve) => {\n      const item$ = subjectDeps[name] as BehaviorSubject<\n        RuneInnerDeps[typeof name]\n      >;\n      if (item$.getValue()) {\n        resolve(item$.getValue());\n      }\n\n      item$\n        .pipe(\n          first((value) => !!value) // Automatically unsubscribes after the first valid value\n          // tap((v) => console.log('------defferedDependency', name, v))\n        )\n        .subscribe((value) => {\n          resolve(value);\n        });\n    });\n  };\n\n  CyberClient.connect(RPC_URL).then((client) => {\n    subjectDeps.queryClient?.next(client);\n  });\n\n  const setInnerDeps = (externalDeps: Partial<RuneInnerDeps>) => {\n    Object.keys(externalDeps)\n      .filter((name) => externalDeps[name as keyof RuneInnerDeps] !== undefined)\n      .forEach((name) => {\n        const item = externalDeps[name as keyof RuneInnerDeps];\n        subjectDeps[name as keyof RuneInnerDeps].next(item);\n      });\n  };\n\n  const graphSearch = async (query: string, page = 0) => {\n    const queryClient = (await defferedDependency(\n      'queryClient'\n    )) as CyberClient;\n\n    const keywordHash = await getSearchQuery(query);\n\n    return searchByHash(queryClient, keywordHash, page);\n  };\n\n  const getIpfsTextConent = async (cid: string) => {\n    const ipfsApi = (await defferedDependency('ipfsApi')) as IpfsApi;\n    return ipfsApi.fetchWithDetails(cid, 'text');\n  };\n\n  const evalScriptFromIpfs = async (\n    cid: ParticleCid,\n    funcName: string,\n    params = {}\n  ) => {\n    try {\n      const result = await getIpfsTextConent(cid);\n      if (result?.content === undefined) {\n        return { action: 'error', message: 'Particle not found' };\n      }\n      // in case of soul script is mixed with markdown\n      // need to extract pure script\n      const pureScript = extractRuneScript(result.content);\n\n      const rune = (await defferedDependency('rune')) as RuneEngine;\n\n      return rune.executeFunction(pureScript, funcName, params);\n    } catch (e) {\n      return { action: 'error', message: e.toString() };\n    }\n  };\n\n  const executeScriptCallback = async (refId: string, data = {}) => {\n    try {\n      const rune = (await defferedDependency('rune')) as RuneEngine;\n      return rune.executeCallback(refId, data);\n    } catch (e) {\n      return { action: 'error', message: e.toString() };\n    }\n  };\n\n  const createAbortController = () => {\n    abortController = new AbortController();\n    return abortController;\n  };\n\n  const abort = () => {\n    abortController?.abort();\n  };\n\n  const cybApi = {\n    createAbortController,\n    graphSearch,\n    cyberlink: async (from: string, to: string) => {\n      const address = subjectDeps.address.getValue();\n      if (!address) {\n        throw new Error('Connect your wallet first');\n      }\n      const senseApi = (await defferedDependency('senseApi')) as SenseApi;\n      const signingClient = (await defferedDependency(\n        'signingClient'\n      )) as SigningCyberClient;\n\n      return sendCyberlink(address, from, to, {\n        senseApi,\n        signingClient,\n      });\n    },\n    getPassportByNickname: async (nickname: string) => {\n      const queryClient = await defferedDependency('queryClient');\n      const passport = await getPassportByNickname(queryClient, nickname);\n\n      return passport;\n    },\n    searcByEmbedding: async (text: string, count = 10) => {\n      const embeddingApi = (await defferedDependency(\n        'embeddingApi'\n      )) as EmbeddingApi;\n      await defferedDependency('dbApi');\n      // console.log('----searcByEmbedding', text);\n      return embeddingApi.searchByEmbedding(text, count);\n    },\n    evalScriptFromIpfs,\n    getIpfsTextConent,\n    addContenToIpfs: async (content: string) => {\n      const ipfsApi = (await defferedDependency('ipfsApi')) as IpfsApi;\n\n      return ipfsApi.addContent(content);\n    },\n    executeScriptCallback,\n  };\n\n  return { setInnerDeps, cybApi, abort };\n};\n\nconst runeDeps = createRuneDeps();\n\nexport type RuneDeps = typeof runeDeps;\n\n// export type EngineDeps = ReturnType<typeof createRuneDeps>;\n\nexport default runeDeps;\n","import axios from 'axios';\n\nimport { CyberClient } from '@cybercongress/cyber-js';\nimport { LCD_URL } from 'src/constants/config';\nimport { ParticleCid } from 'src/types/base';\nimport { PATTERN_IPFS_HASH } from 'src/constants/patterns';\n\nimport { getIpfsHash } from '../ipfs/helpers';\nimport { encodeSlash } from '../utils';\n\n// TODO: move rank to features/rank\nexport const formatNumber = (number, toFixed) => {\n  let formatted = +number;\n\n  if (toFixed) {\n    formatted = +formatted.toFixed(toFixed);\n  }\n\n  return formatted.toLocaleString('en').replace(/,/g, ' ');\n};\n\nexport const getRankGrade = (rank) => {\n  let from;\n  let to;\n  let value;\n\n  if (rank > 0.00000276) {\n    from = 0.00000276;\n    to = 0.01;\n    value = 1;\n  } else if (rank > 0.00000254879356777504 && rank <= 0.00000276) {\n    from = 0.00000254879356777504;\n    to = 0.00000276;\n    value = 2;\n  } else if (rank > 0.00000233758713555007 && rank <= 0.00000254879356777504) {\n    from = 0.00000233758713555007;\n    to = 0.00000254879356777504;\n    value = 3;\n  } else if (rank > 0.00000191517427110014 && rank <= 0.00000233758713555007) {\n    from = 0.00000191517427110014;\n    to = 0.00000233758713555007;\n    value = 4;\n  } else if (rank > 0.00000128155497442525 && rank <= 0.00000191517427110014) {\n    from = 0.00000128155497442525;\n    to = 0.00000191517427110014;\n    value = 5;\n  } else if (rank > 0.00000022552281330043 && rank <= 0.00000128155497442525) {\n    from = 0.00000022552281330043;\n    to = 0.00000128155497442525;\n    value = 6;\n  } else if (rank > 0 && rank <= 0.00000022552281330043) {\n    from = 0;\n    to = 0.00000022552281330043;\n    value = 7;\n  } else {\n    from = 'n/a';\n    to = 'n/a';\n    value = 'n/a';\n  }\n\n  return {\n    from,\n    to,\n    value,\n  };\n};\n\nexport const getRelevance = async (page = 0, limit = 50) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cyber/rank/v1beta1/rank/top`,\n      params: {\n        'pagination.page': page,\n        'pagination.perPage': limit,\n      },\n    });\n    return response.data;\n  } catch (error) {\n    return {};\n  }\n};\n\nexport const keybaseCheck = async (identity) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `https://keybase.io/_/api/1.0/user/lookup.json?key_suffix=${identity}&fields=basics`,\n    });\n    return response.data;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const keybaseAvatar = async (identity) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `https://keybase.io/_/api/1.0/user/lookup.json?key_suffix=${identity}&fields=pictures`,\n    });\n    return response.data;\n  } catch (e) {\n    return undefined;\n  }\n};\n\nexport const authAccounts = async (address) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cosmos/auth/v1beta1/accounts/${address}`,\n    });\n    return response.data;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\n// Access-Control-Allow-Origin\nexport const getCredit = async (address) => {\n  try {\n    const headers = {\n      'Content-Type': 'application/json',\n    };\n    const fromData = {\n      denom: 'boot',\n      address,\n    };\n    const response = await axios({\n      method: 'post',\n      // url: 'http://localhost:8000/credit',\n      url: 'https://titan.cybernode.ai/credit',\n      headers,\n      data: JSON.stringify(fromData),\n    });\n\n    return response;\n  } catch (error) {\n    return null;\n  }\n};\n\nexport const getSearchQuery = async (query: ParticleCid | string) =>\n  query.match(PATTERN_IPFS_HASH) ? query : getIpfsHash(encodeSlash(query));\n\nexport const searchByHash = async (\n  client: CyberClient,\n  hash: string,\n  page: number\n) => {\n  try {\n    const results = await client.search(hash, page);\n\n    return results;\n  } catch (error) {\n    // TODO: handle\n    console.error(error);\n    return undefined;\n  }\n};\n\n// don't add funcs more\n","/* eslint-disable no-await-in-loop */\nimport bech32 from 'bech32';\nimport { fromBase64, fromUtf8, toBech32 } from '@cosmjs/encoding';\nimport { Sha256 } from '@cosmjs/crypto';\nimport BigNumber from 'bignumber.js';\nimport { ObjKeyValue } from 'src/types/data';\nimport { Pool } from '@cybercongress/cyber-js/build/codec/tendermint/liquidity/v1beta1/liquidity';\nimport { Option } from 'src/types';\nimport { Key } from '@keplr-wallet/types';\nimport { AccountValue } from 'src/types/defaultAccount';\nimport { BECH32_PREFIX, BECH32_PREFIX_VAL_CONS } from 'src/constants/config';\nimport { LEDGER } from './config';\n\nimport cyberSpace from '../image/large-purple-circle.png';\nimport customNetwork from '../image/large-orange-circle.png';\nimport cyberBostrom from '../image/large-green.png';\n\nconst DEFAULT_DECIMAL_DIGITS = 3;\nconst DEFAULT_CURRENCY = 'GoL';\n\nconst roundNumber = (num, scale) => {\n  if (!`${num}`.includes('e')) {\n    return +`${Math.floor(`${num}e+${scale}`)}e-${scale}`;\n  }\n  const arr = `${num}`.split('e');\n  let sig = '';\n  if (+arr[1] + scale > 0) {\n    sig = '+';\n  }\n  const i = `${+arr[0]}e${sig}${+arr[1] + scale}`;\n  const j = Math.floor(i);\n  const k = +`${j}e-${scale}`;\n  return k;\n};\n\nfunction numberWithCommas(x) {\n  const parts = x.split('.');\n  parts[0] = parts[0].replace(/\\B(?=(\\d{3})+(?!\\d))/g, ' ');\n  return parts.join('.');\n}\n\nconst formatNumber = (number: number | string, toFixed?: number): string => {\n  let formatted = number;\n\n  if (toFixed) {\n    formatted = roundNumber(formatted, toFixed);\n    formatted = formatted.toFixed(toFixed + 1);\n  }\n\n  if (typeof number === 'string') {\n    return numberWithCommas(formatted);\n  }\n\n  return formatted\n    .toLocaleString('en')\n    .replace(/(\\.\\d{0,})0+$/, '$1')\n    .replace(/,/g, ' ');\n};\n\nconst PREFIXES = [\n  {\n    prefix: 'T',\n    power: 10 ** 12,\n  },\n  {\n    prefix: 'G',\n    power: 10 ** 9,\n  },\n  {\n    prefix: 'M',\n    power: 10 ** 6,\n  },\n  {\n    prefix: 'K',\n    power: 10 ** 3,\n  },\n];\n\nexport function formatCurrency(\n  value,\n  currency = DEFAULT_CURRENCY,\n  decimalDigits = DEFAULT_DECIMAL_DIGITS,\n  prefixCustom = PREFIXES\n) {\n  const { prefix = '', power = 1 } =\n    prefixCustom.find((obj) => value >= obj.power) || {};\n\n  return `${roundNumber(\n    Number(value) / power,\n    decimalDigits\n  )} ${prefix}${currency.toLocaleUpperCase()}`;\n}\n\nconst getDecimal = (number, toFixed) => {\n  const nstring = number.toString();\n  const narray = nstring.split('.');\n  const result = narray.length > 1 ? narray[1] : '000';\n  return result;\n};\n\nconst asyncForEach = async (array, callback) => {\n  for (let index = 0; index < array.length; index++) {\n    await callback(array[index], index, array);\n  }\n};\n\nconst fromBech32 = (operatorAddr, prefix = BECH32_PREFIX) => {\n  const address = bech32.decode(operatorAddr);\n  return bech32.encode(prefix, address.words);\n};\n\nexport const consensusPubkey = (pubKey: string) => {\n  const ed25519PubkeyRaw = fromBase64(pubKey);\n  const addressData = sha256(ed25519PubkeyRaw).slice(0, 20);\n  return toBech32(BECH32_PREFIX_VAL_CONS, addressData);\n};\n\nconst trimString = (address: string, first = 3, second = 8) => {\n  if (address && address.length > 11) {\n    return `${address.substring(0, first)}...${address.substring(\n      address.length - second\n    )}`;\n  }\n  if (address && address.length < 11) {\n    return address;\n  }\n  return '';\n};\n\nconst exponentialToDecimal = (exponential) => {\n  let decimal = exponential.toString().toLowerCase();\n  if (decimal.includes('e+')) {\n    const exponentialSplitted = decimal.split('e+');\n    let postfix = '';\n    for (\n      let i = 0;\n      i <\n      +exponentialSplitted[1] -\n        (exponentialSplitted[0].includes('.')\n          ? exponentialSplitted[0].split('.')[1].length\n          : 0);\n      i++\n    ) {\n      postfix += '0';\n    }\n    decimal = exponentialSplitted[0].replace('.', '') + postfix;\n  }\n  if (decimal.toLowerCase().includes('e-')) {\n    const exponentialSplitted = decimal.split('e-');\n    let prefix = '0.';\n    for (let i = 0; i < +exponentialSplitted[1] - 1; i++) {\n      prefix += '0';\n    }\n    decimal = prefix + exponentialSplitted[0].replace('.', '');\n  }\n  return decimal;\n};\n\nfunction dhm(t) {\n  const cd = 24 * 60 * 60 * 1000;\n  const ch = 60 * 60 * 1000;\n  let d = Math.floor(t / cd);\n  let h = Math.floor((t - d * cd) / ch);\n  let m = Math.round((t - d * cd - h * ch) / 60000);\n  const pad = (n, unit) => {\n    return n < 10 ? `0${n}${unit}` : `${n}${unit}`;\n  };\n  if (m === 60) {\n    h += 1;\n    m = 0;\n  }\n  if (h === 24) {\n    d += 1;\n    h = 0;\n  }\n  return [`${d}d`, pad(h, 'h'), pad(m, 'm')].join(':');\n}\n\nconst downloadObjectAsJson = (exportObj, exportName) => {\n  const dataStr = `data:text/json;charset=utf-8,${encodeURIComponent(\n    JSON.stringify(exportObj)\n  )}`;\n  const downloadAnchorNode = document.createElement('a');\n\n  downloadAnchorNode.setAttribute('href', dataStr);\n  downloadAnchorNode.setAttribute('download', `${exportName}.json`);\n  document.body.appendChild(downloadAnchorNode);\n  downloadAnchorNode.click();\n  downloadAnchorNode.remove();\n};\n\nconst isMobileTablet = () => {\n  let hasTouchScreen = false;\n  if ('maxTouchPoints' in navigator) {\n    hasTouchScreen = navigator.maxTouchPoints > 0;\n  } else if ('msMaxTouchPoints' in navigator) {\n    hasTouchScreen = navigator.msMaxTouchPoints > 0;\n  } else {\n    const mQ = window.matchMedia && matchMedia('(pointer:coarse)');\n    if (mQ && mQ.media === '(pointer:coarse)') {\n      hasTouchScreen = !!mQ.matches;\n    } else if ('orientation' in window) {\n      hasTouchScreen = true; // deprecated, but good fallback\n    } else {\n      // Only as a last resort, fall back to user agent sniffing\n      const UA = navigator.userAgent;\n      hasTouchScreen =\n        /\\b(BlackBerry|webOS|iPhone|IEMobile)\\b/i.test(UA) ||\n        /\\b(Android|Windows Phone|iPad|iPod)\\b/i.test(UA);\n    }\n  }\n  return hasTouchScreen;\n};\n\nconst coinDecimals = (number) => {\n  return number * 10 ** -18;\n};\n\nconst convertResources = (number) => {\n  return Math.floor(number * 10 ** -3);\n};\n\nfunction timeSince(timeMS: number) {\n  const seconds = Math.floor(timeMS / 1000);\n\n  if (seconds === 0) {\n    return 'now';\n  }\n\n  let interval = Math.floor(seconds / 31536000);\n\n  if (interval > 1) {\n    return `${interval} years`;\n  }\n  interval = Math.floor(seconds / 2592000);\n  if (interval > 1) {\n    return `${interval} months`;\n  }\n  interval = Math.floor(seconds / 86400);\n  if (interval > 1) {\n    return `${interval} days`;\n  }\n  interval = Math.floor(seconds / 3600);\n  if (interval > 1) {\n    return `${interval} hours`;\n  }\n  interval = Math.floor(seconds / 60);\n  if (interval > 1) {\n    return `${interval} min`;\n  }\n  return `${Math.floor(seconds)} sec`;\n}\n\nconst reduceBalances = (data): ObjKeyValue => {\n  try {\n    let balances = {};\n    if (Object.keys(data).length > 0) {\n      balances = data.reduce(\n        (obj, item) => ({\n          ...obj,\n          [item.denom]: parseFloat(item.amount),\n        }),\n        {}\n      );\n    }\n    return balances;\n  } catch (error) {\n    console.log(`error reduceBalances`, error);\n    return {};\n  }\n};\n\n// example: oneLiner -> message.module=wasm&message.action=/cosmwasm.wasm.v1.MsgStoreCode&store_code.code_id=${codeId}\nfunction makeTags(oneLiner) {\n  return oneLiner.split('&').map((pair) => {\n    if (pair.indexOf('=') === -1) {\n      throw new Error('Parsing error: Equal sign missing');\n    }\n    const parts = pair.split('=');\n    if (parts.length > 2) {\n      throw new Error(\n        'Parsing error: Multiple equal signs found. If you need escaping support, please create a PR.'\n      );\n    }\n    const [key, value] = parts;\n    if (!key) {\n      throw new Error('Parsing error: Key must not be empty');\n    }\n    return { key, value };\n  });\n}\n\nfunction parseMsgContract(msg) {\n  const json = fromUtf8(msg);\n\n  return JSON.parse(json);\n}\nconst replaceSlash = (text) => text.replace(/\\//g, '%2F');\n\nconst encodeSlash = (text) => text.replace(/%2F/g, '/');\n\nconst groupMsg = (ArrMsg, size = 2) => {\n  const link = [];\n  for (let i = 0; i < Math.ceil(ArrMsg.length / size); i += 1) {\n    link[i] = ArrMsg.slice(i * size, i * size + size);\n  }\n  return link;\n};\n\nconst selectNetworkImg = (network) => {\n  switch (network) {\n    case 'bostrom':\n      return cyberBostrom;\n    case 'space-pussy':\n      return cyberSpace;\n\n    default:\n      return customNetwork;\n  }\n};\n\nconst sha256 = (data) => {\n  return new Uint8Array(new Sha256().update(data).digest());\n};\n\nfunction getDenomHash(path, baseDenom) {\n  const parts = path.split('/');\n  parts.push(baseDenom);\n  const newPath = parts.slice().join('/');\n  return `ibc/${Buffer.from(sha256(Buffer.from(newPath)))\n    .toString('hex')\n    .toUpperCase()}`;\n}\n\nfunction convertAmount(rawAmount, precision) {\n  return new BigNumber(rawAmount)\n    .shiftedBy(-precision)\n    .dp(precision, BigNumber.ROUND_FLOOR)\n    .toNumber();\n}\n\nfunction convertAmountReverce(rawAmount, precision) {\n  return new BigNumber(rawAmount)\n    .shiftedBy(precision)\n    .dp(precision, BigNumber.ROUND_FLOOR)\n    .toNumber();\n}\n\nfunction getDisplayAmount(\n  rawAmount: number | string,\n  precision: number\n): number {\n  return parseFloat(\n    new BigNumber(rawAmount)\n      .shiftedBy(-precision)\n      .dp(precision, BigNumber.ROUND_FLOOR)\n      .toFixed(precision > 0 ? 3 : 0, BigNumber.ROUND_FLOOR)\n  );\n}\n\nfunction getDisplayAmountReverce(rawAmount, precision) {\n  return new BigNumber(rawAmount)\n    .shiftedBy(precision)\n    .dp(precision, BigNumber.ROUND_FLOOR)\n    .toFixed(precision > 0 ? 3 : 0, BigNumber.ROUND_FLOOR);\n}\n\nfunction isNative(denom) {\n  if (denom && denom.includes('ibc')) {\n    return false;\n  }\n  return true;\n}\n\nconst findPoolDenomInArr = (\n  baseDenom: string,\n  dataPools: Pool[]\n): Option<Pool> => {\n  const findObj = dataPools.find((item) => item.poolCoinDenom === baseDenom);\n  return findObj;\n};\n\n// REFACTOR: Probably wrong timestamp\nconst getNowUtcTime = (): number => {\n  const now = new Date();\n  const utcTime = new Date(\n    now.getUTCFullYear(),\n    now.getUTCMonth(),\n    now.getUTCDate(),\n    now.getUTCHours(),\n    now.getUTCMinutes(),\n    now.getUTCSeconds()\n  );\n\n  return utcTime.getTime();\n};\n\nconst accountsKeplr = (accounts: Key): AccountValue => {\n  const { pubKey, bech32Address, name } = accounts;\n  const pk = Buffer.from(pubKey).toString('hex');\n\n  return {\n    bech32: bech32Address,\n    keys: 'keplr',\n    pk,\n    path: LEDGER.HDPATH,\n    name,\n  };\n};\n\nexport function covertUint8ArrayToString(data: Uint8Array): string {\n  return new TextDecoder().decode(data);\n}\n\nexport {\n  formatNumber,\n  asyncForEach,\n  getDecimal,\n  fromBech32,\n  trimString,\n  exponentialToDecimal,\n  dhm,\n  downloadObjectAsJson,\n  isMobileTablet,\n  coinDecimals,\n  convertResources,\n  timeSince,\n  reduceBalances,\n  makeTags,\n  parseMsgContract,\n  replaceSlash,\n  encodeSlash,\n  groupMsg,\n  selectNetworkImg,\n  getDenomHash,\n  getDisplayAmount,\n  getDisplayAmountReverce,\n  convertAmount,\n  convertAmountReverce,\n  isNative,\n  findPoolDenomInArr,\n  getNowUtcTime,\n  accountsKeplr,\n};\n","/* eslint-disable import/prefer-default-export */\n/* eslint-disable import/no-unused-modules */\n\n// https://platform.openai.com/docs/models/overview\n// gpt-3.5-turbo\n\ntype OpenAiMessage = {\n  role: 'system' | 'user' | 'assistant';\n  content: string;\n};\n\ninterface OpenAIParams {\n  model: string;\n  messages: OpenAiMessage[];\n  stream?: boolean;\n  [key: string]: any;\n}\n\nconst defaultOpenAIParams: Partial<OpenAIParams> = {\n  model: 'gpt-3.5-turbo',\n};\n\nexport const openAICompletion = async (\n  messages: OpenAiMessage[],\n  apiKey: string,\n  params: Partial<OpenAIParams> = {},\n  cb: (s: string) => Promise<void>,\n  abortController?: AbortController\n): Promise<string> => {\n  const body = JSON.stringify({\n    messages,\n    ...defaultOpenAIParams,\n    ...params,\n  });\n\n  const headers = {\n    'Content-Type': 'application/json',\n    Authorization: `Bearer ${apiKey}`,\n  };\n\n  const response = await fetch('https://api.openai.com/v1/chat/completions', {\n    method: 'POST',\n    signal: abortController?.signal,\n    headers,\n    body,\n  });\n\n  if (!params.stream) {\n    // Non-streaming request\n    const data = await response.json();\n    return data.choices[0].message.content;\n  }\n  // Streaming request\n  const reader = response.body?.getReader();\n  const decoder = new TextDecoder();\n  let result = '';\n  let buffer = '';\n\n  if (reader) {\n    // eslint-disable-next-line no-constant-condition\n    while (true) {\n      // eslint-disable-next-line no-await-in-loop\n      const { done, value } = await reader.read();\n      if (done || abortController?.signal.aborted) {\n        break;\n      }\n\n      buffer += decoder.decode(value, { stream: true });\n      const lines = buffer.split('\\n');\n\n      // Keep the last partial line in the buffer\n      buffer = lines.pop() || '';\n\n      // eslint-disable-next-line no-restricted-syntax\n      for (const line of lines) {\n        const message = line.replace(/^data: /, '');\n        if (message === '[DONE]') {\n          return result;\n        }\n        try {\n          const parsed = JSON.parse(message);\n          if (parsed.choices && parsed.choices.length > 0) {\n            const { content } = parsed.choices[0].delta;\n            result += content;\n            if (content) {\n              // eslint-disable-next-line no-await-in-loop\n              await cb(content);\n            }\n          }\n        } catch (error) {\n          console.error('Error parsing stream message:', message, error);\n        }\n      }\n    }\n  }\n\n  return result;\n};\n","/* eslint-disable import/no-unused-modules */\n\nimport { getFromLink, getToLink } from '../transactions/lcd';\nimport runeDeps from './runeDeps';\nimport { openAICompletion } from './services/llmRequests/openai';\n\n// let runeDeps;\n\n// export const initRuneDeps = (deps) => {\n//   console.log('---initRuneDeps', deps);\n\n//   runeDeps = deps;\n// };\nexport async function jsCyberSearch(query) {\n  return runeDeps.cybApi.graphSearch(query);\n}\n\nexport async function jsCyberLink(fromCid, toCid) {\n  return runeDeps.cybApi.cyberlink(fromCid, toCid);\n}\n\nexport async function jsGetPassportByNickname(nickname) {\n  return runeDeps.cybApi.getPassportByNickname(nickname);\n}\n\nexport async function jsEvalScriptFromIpfs(cid, funcName, params = {}) {\n  return runeDeps.cybApi.evalScriptFromIpfs(cid, funcName, params);\n}\n\nexport async function jsGetIpfsTextContent(cid) {\n  return runeDeps.getIpfsTextConent(cid);\n}\n\nexport async function jsAddContenToIpfs(content) {\n  return runeDeps.addContenToIpfs(content);\n}\n\nexport async function jsExecuteScriptCallback(refId, data) {\n  console.log('exec deps callback', refId);\n  return runeDeps.cybApi.executeScriptCallback(refId, data);\n}\n\nexport async function jsOpenAICompletions(messages, apiKey, params, refId) {\n  const callback = async (data) => jsExecuteScriptCallback(refId, data);\n  const result = await openAICompletion(\n    messages,\n    apiKey,\n    params,\n    callback,\n    runeDeps.cybApi.createAbortController()\n  );\n  return result;\n}\n\nexport async function jsSearchByEmbedding(text, count) {\n  return runeDeps.cybApi.searcByEmbedding(text, count);\n}\n\nexport async function jsCyberLinksFrom(cid) {\n  const result = await getFromLink(cid);\n  return result;\n}\n\nexport async function jsCyberLinksTo(cid) {\n  const result = await getToLink(cid);\n  return result;\n}\n","import {\n  GetTxsEventRequest,\n  GetTxsEventResponse,\n  GetTxsEventResponseAmino,\n  OrderBy,\n} from '@cybercongress/cyber-ts/cosmos/tx/v1beta1/service';\nimport axios from 'axios';\nimport { CID_FOLLOW, CID_TWEET } from 'src/constants/app';\nimport { LCD_URL } from 'src/constants/config';\nimport { LinksType, LinksTypeFilter } from 'src/containers/Search/types';\n\ntype PropsTx = {\n  events: ReadonlyArray<{ key: string; value: string }>;\n  pagination?: GetTxsEventRequest['pagination'];\n  orderBy?: GetTxsEventRequest['orderBy'];\n};\n\nexport const getTxs = async (txsHash) => {\n  try {\n    const response = await axios({\n      method: 'get',\n      url: `${LCD_URL}/cosmos/tx/v1beta1/txs/${txsHash}`,\n    });\n    return response.data;\n  } catch (e) {\n    console.error(e);\n    return null;\n  }\n};\n\n/**\n * @deprecated don't use lcd, use cyber-ts instead\n */\nexport async function getTransactions({\n  events,\n  pagination = { limit: 20, offset: 0 },\n  orderBy,\n  config,\n}: PropsTx) {\n  const { offset, limit } = pagination;\n  const response = await axios.get<GetTxsEventResponseAmino>(\n    `${LCD_URL}/cosmos/tx/v1beta1/txs`,\n    {\n      params: {\n        'pagination.offset': offset,\n        'pagination.limit': limit,\n        orderBy,\n        events: events.map((evn) => `${evn.key}='${evn.value}'`),\n      },\n      paramsSerializer: {\n        indexes: null,\n      },\n      signal: config?.signal,\n    }\n  );\n\n  const { txs } = response.data;\n\n  // bullshit formatting FIXME:\n  // const formatted = GetTxsEventResponse.fromAmino(response.data);\n  // from amino to protobuf\n  const formatted = {\n    txs,\n    pagination: response.data.pagination || {\n      total: response.data.total,\n    },\n    txResponses: response.data.tx_responses,\n  } as GetTxsEventResponse;\n\n  if (!formatted.pagination?.total) {\n    formatted.pagination.total = formatted.txResponses.length;\n  }\n\n  return formatted;\n}\n\nconst getLink = async (\n  cid: string,\n  type: LinksType = LinksTypeFilter.from,\n  { offset, limit, order = OrderBy.ORDER_BY_DESC }\n) => {\n  try {\n    const response = await getTransactions({\n      events: [\n        {\n          key: `cyberlink.particle${\n            type === LinksTypeFilter.to ? 'To' : 'From'\n          }`,\n          value: cid,\n        },\n      ],\n      pagination: {\n        limit,\n        offset,\n      },\n      orderBy: order,\n    });\n\n    return response;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const getFromLink = async (cid, offset, limit) => {\n  return getLink(cid, LinksTypeFilter.from, { offset, limit });\n};\n\nexport const getToLink = async (cid, offset, limit) => {\n  return getLink(cid, LinksTypeFilter.to, { offset, limit });\n};\n\nexport const getFollows = async (address) => {\n  try {\n    const response = await getTransactions({\n      events: [\n        { key: 'cyberlink.particleFrom', value: CID_FOLLOW },\n        { key: 'cyberlink.neuron', value: address },\n      ],\n      pagination: { limit: 1000000000 },\n    });\n\n    return response;\n  } catch (e) {\n    console.log(e);\n    return null;\n  }\n};\n\nexport const getTweet = async (address) => {\n  try {\n    const response = await getTransactions({\n      events: [\n        { key: 'cyberlink.particleFrom', value: CID_TWEET },\n        { key: 'cyberlink.neuron', value: address },\n      ],\n      pagination: { limit: 1000000000 },\n    });\n    return response;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\nexport const checkFollow = async (address, addressFollowHash) => {\n  try {\n    const response = await getTransactions({\n      events: [\n        { key: 'cyberlink.particleFrom', value: CID_FOLLOW },\n        { key: 'cyberlink.neuron', value: address },\n        { key: 'cyberlink.particleTo', value: addressFollowHash },\n      ],\n      pagination: { limit: 1000000000 },\n    });\n    return response;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n\nexport const getFollowers = async (addressHash) => {\n  try {\n    const response = await getTransactions({\n      events: [\n        {\n          key: 'cyberlink.particleFrom',\n          value: CID_FOLLOW,\n        },\n        {\n          key: 'cyberlink.particleTo',\n          value: addressHash,\n        },\n      ],\n      pagination: {\n        limit: 1000000000,\n      },\n    });\n\n    return response;\n  } catch (error) {\n    console.log(error);\n    return null;\n  }\n};\n","export const enum Networks {\n  BOSTROM = 'bostrom',\n  LOCAL_BOSTROM = 'localbostrom',\n  SPACE_PUSSY = 'space-pussy',\n  ETH = 'eth',\n  OSMO = 'osmo',\n  TERRA = 'terra',\n  COSMOS = 'cosmoshub-4',\n}\n\nexport type NetworkConfig = {\n  CHAIN_ID: Networks;\n  BASE_DENOM: string;\n  DENOM_LIQUID: string;\n  RPC_URL: string;\n  LCD_URL: string;\n  WEBSOCKET_URL: string;\n  INDEX_HTTPS: string;\n  INDEX_WEBSOCKET: string;\n  BECH32_PREFIX: string;\n  MEMO_KEPLR: string;\n};\n\nexport type NetworksList = {\n  [key in Networks]: NetworkConfig;\n};\n","const LEDGER = {\n  STAGE_INIT: 0,\n  STAGE_SELECTION: 1,\n  STAGE_LEDGER_INIT: 2,\n  STAGE_READY: 3,\n  STAGE_WAIT: 4,\n  STAGE_GENERATED: 5,\n  STAGE_SUBMITTED: 6,\n  STAGE_CONFIRMING: 7,\n  STAGE_CONFIRMED: 8,\n  STAGE_ERROR: 15,\n  HDPATH: [44, 118, 0, 0, 0],\n};\n\nconst GENESIS_SUPPLY = 1000000000000000;\nconst TOTAL_GOL_GENESIS_SUPPLY = 50000000000000;\n\nconst POCKET = {\n  STAGE_TWEET_ACTION_BAR: {\n    ADD_AVATAR: 'addAvatar',\n    FOLLOW: 'follow',\n    TWEET: 'tweet',\n  },\n};\n\nexport { LEDGER, GENESIS_SUPPLY, TOTAL_GOL_GENESIS_SUPPLY, POCKET };\n","import dateFormat from 'dateformat';\n\nexport const numberToUtcDate = (timestamp: number) =>\n  dateFormat(new Date(timestamp), 'yyyy-mm-dd\"T\"HH:MM:ss.l', true);\n\nexport const dateToUtcNumber = (isoString: string) =>\n  Date.parse(isoString.endsWith('Z') ? isoString : `${isoString}Z`);\n\nexport const getNowUtcNumber = () => Date.now();\n\nfunction roundMilliseconds(dateTimeString: string) {\n  const date = new Date(dateTimeString);\n  const roundedMilliseconds = Math.round(date.getMilliseconds() / 1000) * 1000;\n  date.setMilliseconds(roundedMilliseconds);\n  return dateFormat(date, 'yyyy-mm-dd\"T\"HH:MM:ss.l');\n}\nfunction getCurrentTimezoneOffset() {\n  const now = new Date();\n  return -now.getTimezoneOffset() / 60;\n}\n\nfunction pluralizeUnit(quantity: number, unit: string): string {\n  return quantity === 1 ? unit : `${unit}s`;\n}\n\nconst minuteInMs = 60000; // 60 seconds * 1000 milliseconds\nconst hourInMs = 3600000; // 60 minutes * 60 seconds * 1000 milliseconds\nconst dayInMs = 86400000; // 24 hours * 60 minutes * 60 seconds * 1000 milliseconds\n\nfunction convertTimestampToString(timestamp: number): string {\n  if (timestamp < minuteInMs) {\n    const seconds = Math.floor(timestamp / 1000);\n    return `${seconds} ${pluralizeUnit(seconds, 'second')}`;\n  }\n  if (timestamp < hourInMs) {\n    const minutes = Math.floor(timestamp / minuteInMs);\n    return `${minutes} ${pluralizeUnit(minutes, 'minute')}`;\n  }\n  if (timestamp < dayInMs) {\n    const hours = Math.floor(timestamp / hourInMs);\n    return `${hours} ${pluralizeUnit(hours, 'hour')}`;\n  }\n\n  const days = Math.floor(timestamp / dayInMs);\n  return `${days} ${pluralizeUnit(days, 'day')}`;\n}\n\nexport { roundMilliseconds, convertTimestampToString };\n","import Unixfs from 'ipfs-unixfs';\nimport { DAGNode, util as DAGUtil } from 'ipld-dag-pb';\nimport { isString } from 'lodash';\nimport { RemoteIpfsApi } from 'src/services/backend/workers/background/worker';\nimport { ParticleCid } from 'src/types/base';\nimport { PATTERN_IPFS_HASH } from 'src/constants/patterns';\nimport { Remote } from 'comlink';\nimport { IpfsApi } from 'src/services/backend/workers/background/api/ipfsApi';\n\nexport const isCID = (cid: string): boolean => {\n  return cid.match(PATTERN_IPFS_HASH) !== null;\n};\n\n// eslint-disable-next-line import/prefer-default-export\nexport const getIpfsHash = (string: string): Promise<ParticleCid> =>\n  new Promise((resolve, reject) => {\n    const unixFsFile = new Unixfs('file', Buffer.from(string));\n\n    const buffer = unixFsFile.marshal();\n    DAGNode.create(buffer, (err, dagNode) => {\n      if (err) {\n        reject(new Error('Cannot create ipfs DAGNode'));\n      }\n\n      DAGUtil.cid(dagNode, (error, cid) => {\n        resolve(cid.toBaseEncodedString());\n      });\n    });\n  });\nexport const addIfpsMessageOrCid = async (\n  message: string | ParticleCid | File,\n  { ipfsApi }: { ipfsApi: Remote<IpfsApi> | null }\n) => {\n  if (!ipfsApi) {\n    throw Error('IpfsApi is not initialized');\n  }\n\n  return (\n    isString(message) && message.match(PATTERN_IPFS_HASH)\n      ? message\n      : ((await ipfsApi!.addContent(message)) as string)\n  ) as ParticleCid;\n};\n","export const CYBLOG_LOG_SHOW = 'cyblog_show';\n\nexport const CYBLOG_BROADCAST_CHANNEL_NAME = 'CYBLOG_BROADCST_CHANNEL';\n\nexport const CYBLOG_CONSOLE_PARAMS_DEFAULT = {\n  thread: 'all',\n  unit: 'all',\n  module: 'all',\n};\n","import _, { isEmpty } from 'lodash';\nimport { ConsoleLogParams, LogContext, LogItem, LogLevel } from './types';\nimport { CYBLOG_BROADCAST_CHANNEL_NAME } from './constants';\n\nconst logList: LogItem[] = [];\n\nfunction createCybLog<T>(defaultContext: Partial<LogContext<T>> = {}) {\n  function appendLog(logItem: LogItem, truncate = true) {\n    logList.push(logItem);\n\n    while (truncate && logList.length > 1000) {\n      logList.shift(); // Remove the first element to keep the list size <= 1000\n    }\n  }\n  let consoleLogParams = {} as ConsoleLogParams;\n\n  const channel = new BroadcastChannel(CYBLOG_BROADCAST_CHANNEL_NAME);\n\n  channel.onmessage = (event) => {\n    if (event.data.type === 'params') {\n      consoleLogParams = { ...consoleLogParams, ...event.data.value };\n    }\n  };\n\n  const getConsoleLogParams = () => consoleLogParams;\n\n  function consoleLog<T>(\n    level: LogLevel,\n    message: T,\n    context: Partial<LogContext<T>>\n  ) {\n    const ctx = _.omit(context, [\n      'formatter',\n      'thread',\n      'module',\n      'unit',\n      'data',\n    ]);\n    const { thread = '', module = '', unit = '', data = '' } = context;\n    const ctxItem = isEmpty(ctx) ? '' : ctx;\n\n    if (Array.isArray(message)) {\n      console[level](...message, ctxItem);\n      return;\n    }\n\n    if (context?.formatter) {\n      console[level](context?.formatter(message), ctxItem);\n      return;\n    }\n\n    console[level](`[${thread}:${module}:${unit}] ${message}`, data, ctxItem);\n  }\n\n  // eslint-disable-next-line import/no-unused-modules\n  function log<T>(\n    level: LogLevel,\n    message: string | T,\n    context: LogContext<any> = defaultContext\n  ) {\n    try {\n      const formattedMessage = context?.formatter\n        ? context?.formatter(message)\n        : message;\n\n      const logEntry = {\n        timestamp: new Date(),\n        level,\n        message: formattedMessage,\n        stacktrace: context?.stacktrace,\n        context: _.omit(context, ['formatter', 'stacktrace']),\n      };\n\n      appendLog(logEntry);\n      // !!localStorage.getItem(LOCAL_STORAGE_USE_CONSOLE_LOG_KEY) &&\n      const showConsoleLog = Object.keys(consoleLogParams).reduce(\n        (acc: boolean, key: string) => {\n          const params = consoleLogParams[key];\n          const contextItem = context[key];\n          if (params && contextItem) {\n            return (\n              acc ||\n              params === 'all' ||\n              params.length === 0 ||\n              params.some((p) => p === contextItem)\n            );\n          }\n          return acc;\n        },\n        false\n      );\n\n      if (showConsoleLog) {\n        consoleLog(level, message, context);\n      }\n    } catch (error) {\n      console.log('cyblog error', error);\n    }\n  }\n\n  function info<T>(message: T, context?: LogContext<string | T>) {\n    return log('info', message, context);\n  }\n\n  function error<T>(message: T, context?: LogContext<string | T>) {\n    return log('error', message, context);\n  }\n\n  function warn<T>(message: T, context?: LogContext<string | T>) {\n    return log('warn', message, context);\n  }\n\n  function trace<T>(message: T, context?: LogContext<string | T>) {\n    return log('warn', message, context);\n  }\n\n  function normalizeLog() {\n    return logList.map((logItem) => {\n      const { context, ...rest } = logItem;\n      const {\n        unit = '',\n        module = '',\n        thread = '',\n        data = '',\n        error = '',\n        stacktrace = '',\n      } = context || {};\n      return {\n        ...rest,\n        unit,\n        module,\n        thread,\n        data, //: JSON.stringify(data),\n        error,\n        stacktrace,\n      };\n    });\n  }\n\n  return {\n    log,\n    info,\n    error,\n    warn,\n    trace,\n    logList,\n    getLogs: () => normalizeLog(),\n    clear: () => logList.splice(0, logList.length),\n    getConsoleLogParams,\n  };\n}\n\nexport const createCyblogChannel = (\n  defaultContext: Partial<LogContext<T>> = {}\n) => {\n  const channel = new BroadcastChannel(CYBLOG_BROADCAST_CHANNEL_NAME);\n\n  function postLogToChannel<T>(\n    level: LogLevel,\n    message: T,\n    context?: LogContext<string | T>\n  ) {\n    const ctx = { ...defaultContext, ...context };\n    if (context?.error) {\n      ctx.error = JSON.stringify(context.error);\n    }\n    channel.postMessage({\n      type: 'log',\n      value: { level, message, context: ctx },\n    });\n  }\n\n  function info<T>(message: T, context?: LogContext<string | T>) {\n    return postLogToChannel('info', message, context);\n  }\n\n  function error<T>(message: T, context?: LogContext<string | T>) {\n    return postLogToChannel('error', message, context);\n  }\n\n  function warn<T>(message: T, context?: LogContext<string | T>) {\n    return postLogToChannel('warn', message, context);\n  }\n\n  function trace<T>(message: T, context?: LogContext<string | T>) {\n    return postLogToChannel('warn', message, context);\n  }\n\n  return { info, error, warn, trace };\n};\n\nconst cyblog = createCybLog({ thread: 'main' });\n\nexport type LogFunc = (message: T, context?: LogContext<string | T>) => void;\n\nexport type CyblogChannel = ReturnType<typeof createCyblogChannel>;\n\nexport default cyblog;\n","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\tid: moduleId,\n\t\tloaded: false,\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n\t// Flag the module as loaded\n\tmodule.loaded = true;\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n// expose the modules object (__webpack_modules__)\n__webpack_require__.m = __webpack_modules__;\n\n// the startup function\n__webpack_require__.x = function() {\n\t// Load entry module and return exports\n\t// This entry module depends on other loaded chunks and execution need to be delayed\n\tvar __webpack_exports__ = __webpack_require__.O(undefined, [667,742,45,168,112,235,187], function() { return __webpack_require__(55723); })\n\t__webpack_exports__ = __webpack_require__.O(__webpack_exports__);\n\treturn __webpack_exports__;\n};\n\n","__webpack_require__.amdO = {};","// getDefaultExport function for compatibility with non-harmony modules\n__webpack_require__.n = function(module) {\n\tvar getter = module && module.__esModule ?\n\t\tfunction() { return module['default']; } :\n\t\tfunction() { return module; };\n\t__webpack_require__.d(getter, { a: getter });\n\treturn getter;\n};","// define getter functions for harmony exports\n__webpack_require__.d = function(exports, definition) {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.f = {};\n// This file contains only the entry chunk.\n// The chunk loading function for additional chunks\n__webpack_require__.e = function(chunkId) {\n\treturn Promise.all(Object.keys(__webpack_require__.f).reduce(function(promises, key) {\n\t\t__webpack_require__.f[key](chunkId, promises);\n\t\treturn promises;\n\t}, []));\n};","// This function allow to reference async chunks and sibling chunks for the entrypoint\n__webpack_require__.u = function(chunkId) {\n\t// return url for filenames not based on template\n\tif (chunkId === 667) return \"667.5e07a487.js\";\n\tif (chunkId === 742) return \"742.1cf6e28d.js\";\n\tif (chunkId === 45) return \"45.6a43f7cf.js\";\n\t// return url for filenames based on template\n\treturn \"\" + chunkId + \".\" + {\"112\":\"4614d4ee\",\"168\":\"0ce26f6d\",\"187\":\"1146adee\",\"198\":\"44cc2170\",\"235\":\"c8ba16cc\"}[chunkId] + \".chunk.js\";\n};","// This function allow to reference async chunks and sibling chunks for the entrypoint\n__webpack_require__.miniCssF = function(chunkId) {\n\t// return url for filenames based on template\n\treturn undefined;\n};","__webpack_require__.g = (function() {\n\tif (typeof globalThis === 'object') return globalThis;\n\ttry {\n\t\treturn this || new Function('return this')();\n\t} catch (e) {\n\t\tif (typeof window === 'object') return window;\n\t}\n})();","__webpack_require__.hmd = function(module) {\n\tmodule = Object.create(module);\n\tif (!module.children) module.children = [];\n\tObject.defineProperty(module, 'exports', {\n\t\tenumerable: true,\n\t\tset: function() {\n\t\t\tthrow new Error('ES Modules may not assign module.exports or exports.*, Use ESM export syntax, instead: ' + module.id);\n\t\t}\n\t});\n\treturn module;\n};","__webpack_require__.o = function(obj, prop) { return Object.prototype.hasOwnProperty.call(obj, prop); }","// define __esModule on exports\n__webpack_require__.r = function(exports) {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","__webpack_require__.nmd = function(module) {\n\tmodule.paths = [];\n\tif (!module.children) module.children = [];\n\treturn module;\n};","__webpack_require__.p = \"/\";","__webpack_require__.b = self.location + \"\";\n\n// object to store loaded chunks\n// \"1\" means \"already loaded\"\nvar installedChunks = {\n\t585: 1\n};\n\n// importScripts chunk loading\nvar installChunk = function(data) {\n\tvar chunkIds = data[0];\n\tvar moreModules = data[1];\n\tvar runtime = data[2];\n\tfor(var moduleId in moreModules) {\n\t\tif(__webpack_require__.o(moreModules, moduleId)) {\n\t\t\t__webpack_require__.m[moduleId] = moreModules[moduleId];\n\t\t}\n\t}\n\tif(runtime) runtime(__webpack_require__);\n\twhile(chunkIds.length)\n\t\tinstalledChunks[chunkIds.pop()] = 1;\n\tparentChunkLoadingFunction(data);\n};\n__webpack_require__.f.i = function(chunkId, promises) {\n\t// \"1\" is the signal for \"already loaded\"\n\tif(!installedChunks[chunkId]) {\n\t\tif(true) { // all chunks have JS\n\t\t\timportScripts(__webpack_require__.p + __webpack_require__.u(chunkId));\n\t\t}\n\t}\n};\n\nvar chunkLoadingGlobal = self[\"webpackChunkcyb\"] = self[\"webpackChunkcyb\"] || [];\nvar parentChunkLoadingFunction = chunkLoadingGlobal.push.bind(chunkLoadingGlobal);\nchunkLoadingGlobal.push = installChunk;\n\n// no HMR\n\n// no HMR manifest","// run startup\nvar __webpack_exports__ = __webpack_require__.x();\n"],"names":["deferred","leafPrototypes","getProto","next","isWorker","WorkerGlobalScope","self","DEFAULT_CHAIN_ID","localStorage","getItem","LCD_URL","RPC_URL","WEBSOCKET_URL","INDEX_HTTPS","INDEX_WEBSOCKET","BECH32_PREFIX","BECH32_PREFIX_VAL","BECH32_PREFIX_VALOPER","DEFAULT_GAS_LIMITS","BASE_DENOM","DENOM_LIQUID","MEMO_KEPLR","defaultNetworks","bostrom","CHAIN_ID","BOSTROM","localbostrom","LOCAL_BOSTROM","SPACE_PUSSY","PATTERN_CYBER","RegExp","PATTERN_IPFS_HASH","PATTERN_COSMOS","PATTERN_HTTP","LinksTypeFilter","QueuePriority","createAsyncIterable","port","async","Symbol","asyncIterator","done","promise","Promise","resolve","onmessage","event","data","value","IPFSContentTransferHandler","canHandle","obj","result","serialize","rest","port1","port2","MessageChannel","postMessage","close","deserialize","serializedObj","SharedWorker","process","env","IS_DEV","installTransferHandlers","set","Observable","observer","remote","get","subscribe","error","complete","then","subscription","add","unsubscribe","Subscription","overrideLogging","worker","consoleLogMap","log","original","console","warn","replaceConsoleLog","method","args","apply","serializableArgs","map","arg","JSON","stringify","String","safeStringify","type","Object","keys","forEach","EntryType","SyncQueueStatus","SyncQueueJobType","isParticle","Boolean","match","initialState","list","isLoading","chats","summary","unreadCount","total","particles","neurons","llm","threads","parse","currentThreadId","formatApiData","item","entryType","chat","meta","to","particle","formatted","timestamp","Date","toISOString","transactionHash","hash","transaction_hash","memo","senseChatId","id","transactions","from","ownerId","fromAddress","inputs","address","assign","neuron","fromLog","getSenseList","senseApi","getList","getSenseChat","getLinks","filter","getFriendItems","markAsRead","newChatStructure","checkIfMessageExists","newMessage","slice","some","msg","name","reducers","updateSenseList","reducer","state","action","payload","message","concat","caseReducers","orderSenseList","prepare","addSenseItem","push","status","newList","unshift","updateSenseItem","chatId","txHash","isSuccess","find","sorted","reduce","acc","length","lastMsg","sort","a","b","i","reset","createLLMThread","newThread","messages","dateUpdated","now","title","setItem","selectLLMThread","addLLMMessageToThread","thread","t","threadId","replaceLastLLMMessageInThread","deleteLLMThread","newT","clearLLMThreads","removeItem","extraReducers","builder","addCase","pending","fulfilled","rejected","sense","unreadCountParticle","unreadCountNeuron","values","actions","POCKET","POCKET_ACCOUNT","actionBar","tweet","STAGE_TWEET_ACTION_BAR","TWEET","isInitialized","defaultAccount","account","accounts","saveToLocalStorage","setDefaultAccount","setAccounts","setInitialized","setStageTweetActionBar","deleteAddress","accountKey","networkKey","bech32","cyber","entryCyber","entries","CYB_QUEUE_CHANNEL","constructor","this","channel","BroadcastChannel","postServiceStatus","postSyncEntryProgress","entry","postMlSyncEntryProgress","postSenseUpdate","senseList","postSetDefaultAccount","post","broadcastStatus","channelApi","sendStatus","progress","s","asyncIterableBatchProcessor","items","batchProcess","batchSize","batch","fetchIterableByOffset","fetchFunction","params","offset","busSender","enqueue","createBackendQueueSender","enqueueParticleEmbeddingMaybe","content","contentToEmbed","getTextContentIfShouldEmbed","cid","jobType","embedding","priority","MEDIUM","CID_TWEET","CID_FOLLOW","SENSE_FRIEND_PARTICLES","contentType","textPreview","getContentToEmbed","shouldEmbed","deps","statusApi","_syncQueue$","BehaviorSubject","Map","waitForParticleResolve","Error","embeddingApi$","embeddingApi","queue","size","dbInstance$","pipe","first","db","loadSyncQueue","isInitialized$","combineLatest","ipfsInstance$","dbInstance","ipfsInstance","canEmbed","getValue","loop$","_loop$","catch","text","existEmbedding","vec","createEmbedding","putEmbedding","err","toString","pendingItems","all","jobPromise","saveEmbedding","resolveIpfsParticle","removeSyncQueue","updateSyncQueue","delete","start","source$","tap","q","mergeMap","executing","jobTypeFilter","processSyncQueue","share","cids","putSyncQueue","getSyncQueue","statuses","shortenString","string","specialCharsRegexe","mapIndexerTransactionToEntity","tx","index","transaction","block","height","success","date","blockHeight","mapLinkFromIndexerToDto","throwIfAborted","func","signal","aborted","DOMException","Order_By","CyberlinksByParticleDocument","CyberlinksCountByNeuronDocument","MessagesByAddressCountDocument","MessagesByAddressSenseDocument","MessagesByAddressSenseWsDocument","MSG_SEND_TRANSACTION_TYPE","MSG_MULTI_SEND_TRANSACTION_TYPE","mapWebsocketTxToTransactions","events","transactionType","Tx","decode","fromBase64","body","msgType","typeUrl","MsgSend","MsgMultiSend","extractTxData","TxResult","cyberGraphQLWsLink","url","shouldRetry","errOrCloseEvent","retryAttempts","retryWait","retries","setTimeout","Math","min","createIndexerClient","abortSignal","fetchCyberlinks","particleCid","timestampFrom","request","limit","orderBy","Asc","where","_or","particle_to","_eq","particle_from","_gt","cyberlinks","fetchCyberlinksByNeroun","particlesFrom","_and","_in","fetchCyberlinksByNerounIterable","getUniqueParticlesFromLinks","links","Set","link","fetchCyberlinksAndResolveParticles","timestampUpdate","particlesResolver","queuePriority","cyberlinksIterable","fetchCyberlinksIterable","enqueueBatch","mapMessagesByAddressVariables","types","orderDirection","timestamp_from","join","order_direction","fetchTransactions","res","messages_by_address","updateSenseChat","addr","amount","isSender","userAddress","lastSendTimestamp","last","direction","syncMyChats","myAddress","shouldUpdateTimestamp","syncItems","findSyncStatus","syncItemsMap","myChats","outputs","coins","toAddress","extractSenseChats","getTransactions","order","results","syncItem","lastTransaction","at","transactionTimestamp","syncItemHeader","timestampRead","prevUnreadCount","lastTimestampRead","max","timestampUpdateContent","timestampUpdateChat","timestampUnreadFrom","newTimestampUpdateChat","syncStatusChanges","updateSyncStatus","bind","newItem","disabled","putSyncStatus","ProgressTracker","onProgressUpdate","requestRecords","totalRequests","completedRequests","estimatedTime","totalCount","completeCount","extraRequests","trackProgress","processedCount","addRequestRecord","shift","estimatedRemainingTime","calculateAverageTimePerItem","round","itemCount","totalDiff","totalItems","timeDiff","progressTracker","abortController","AbortController","cyblogCh","module","params$","createIsInitializedObserver","info","switchMap","createRestartObserver","restart","initAbortController","distinctUntilChanged","addrBefore","addrAfter","v","switchWhenInitialized","actionObservable$","onChange","initialized","super","reloadTrigger$","Subject","startWith","createInitObservable","createClientObservable","onUpdate","abort","syncQueueInitialized","variables","indexerObservable$","query","apolloObservable","ApolloClient","cache","subscriber","createIndexerWebsocket","response","source","nodeObservample$","ws","WebSocket","onopen","send","jsonrpc","onerror","onclose","createNodeWebsocketObservable","ctx","unit","isEmpty","merge","defer","initSync","getSyncStatus","lastTransactionTimestamp","syncTransactions","syncStatusItems","processBatchTransactions","putTransactions","syncLinks","lastTimestampFrom","newSyncItem","totalMessageCount","messages_by_address_aggregate","aggregate","count","fetchTransactionMessagesCount","ceil","transactionsAsyncIterable","fetchTransactionsIterable","transactionCount","tweets","particlesFound","l","txLink","extractCybelinksFromTransaction","putCyberlinks","tweetParticles","nonTweetParticles","includes","HIGH","LOW","snakeToCamel","str","replace","group","toUpperCase","entityToDto","dbEntity","dto","key","prototype","hasOwnProperty","call","camelCaseKey","Array","isArray","getLastReadInfo","prevTimestampRead","lastUnreadLinks","lastMyLinkIndex","findLastIndex","changeParticleSyncStatus","syncStatus","lastLink","isAbortException","e","intervalMs","warmupMs","restartLoop","options","onStartInterval","onError","retryDelayMs","restartTrigger$","intervalOrRestart$","interval","delay","exhaustMap","retry","createLoopObservable","doSync","sync","isAborted","particleResolverInitialized","syncItemParticles","newLinkCount","particles_from","cyberlinks_aggregate","fetchCyberlinksCount","newSyncItemParticles","fetchNewTweets","syncParticles","tweetsAsyncIterable","newTweets","existingParticles","existingParticlesMap","tweetsBatch","syncStatusEntities","timestampSyncFrom","updatedSyncItems","linksIndexer","followings$","followings","followingsInitialized$","followingsInitialized","syncUpdates","linksAsyncIterable","linksBatch","newTimestampRead","newUnreadCount","newTimestampUpdateContent","fetchStoredSyncCommunity$","dbApi","fetchParticleAsync","storedCommunity","getCommunity","communityUpdatesMap","c","getExistingOrDefault","following","follower","followsCids","pagination","config","txResponses","followers","addressHash","newFollowerCids","newFollowingNeurons","followersCommunity","communityItem","putCommunity","URGENT","onMessage","onmessageerror","getDeffredDbApi","entity","mime","blocks","sizeLocal","markdown","removeMarkdownFormatting","size_local","mapParticleToEntity","putParticles","ok","saveLinks","saveParticles","enquueSync","SyncService","loops","BackendQueueChannel","communitySync$","community","createCommunitySync$","getMimeFromUint8Array","raw","fileType","version","stores","table","ipfsContentAddtToInddexdDB","dbValue","CYBER_NODE_SWARM_PEER_ID","CYBERNODE_SWARM_ADDR_WSS","CYBERNODE_SWARM_ADDR_TCP","CYBER_GATEWAY_URL","cluster","file","dataFile","File","cidVersion","rawLeaves","createObjectURL","rawData","blob","Blob","URL","createImgData","detectGatewayContentType","basic","mimeToBaseContentType","initialType","indexOf","parseArrayLikeToDetails","onProgress","gateway","bytesDownloaded","Uint8Array","byteLength","chunks","ReadableStream","reader","getReader","readStream","read","chunk","getResponseResult","isStringData","Buffer","newString","trim","test","isHtml","createTextPreview","array","previewLength","loadIPFSContentFromDb","emptyStats","fetchIPFSContentStat","node","stat","fetchIPFSContentFromNode","controller","controllerLegacy","timer","startTime","stats","statsDoneTime","statsTime","allowedSize","clearTimeout","availableDownload","firstChunk","cat","fullyDownloaded","stream","catTime","local","pin","pinTime","debug","fetchIPFSContentFromGateway","headers","isExternalNode","nodeType","contentUrl","fetch","flushResults","flush","firstChunkStream","fullStream","tee","firstReader","restReader","asyncIterable","toAsyncIterableWithMime","getIPFSContent","callBackFuncStatus","dataRsponseDb","addContenToIpfs","arrayBuffer","contentToUint8Array","QueueStrategy","settings","getNextSource","QueueItemTimeoutError","timeoutMs","setPrototypeOf","CustomHeaders","XCybSourceValues","getQueueItemTotalPriority","viewPortPriority","strategies","external","timeout","maxConcurrentExecutions","embedded","helia","strategy","queueDebounceMs","queue$","lastNodeCallTime","setNode","reconnectToSwarm","isStarted","withLatestFrom","debounceTime","cancelDeprioritizedItems","workItems","getItemBySourceAndPriority","fetchData$","callbacks","callback","removeAndNext","nextSource","switchSourceAndNext","postSummary","switchStrategy","customStrategy","pendingBySource","itemsToExecute","queueSource","executeCount","itemsByPriority","queueItem","executionTime","promiseFactory","fetchIpfsContent","sharedWorker","enqueueParticleSave","each","with","throwError","catchError","of","mutateQueueItem","changes","releaseExecution","existingItem","initialSource","postProcessing","enqueueAndWait","updateViewPortPriority","cancel","cancelByParent","parent","clear","getQueueMap","getQueueList","getStats","fn","stringToCid","stringToIpfsPath","_config","_isStarted","gatewayUrl","nodeAddress","initConfig","window","toCid","swarm","localAddrs","files","withLocal","peers","peer","bootstrap","connect","ls","repoSize","repo","responseId","agentVersion","addOptionsV0","blockstore","open","datastore","libp2p","bootstrapList","transports","rtcConfiguration","iceServers","urls","credential","username","discoverRelays","connectionEncryption","streamMuxers","connectionGater","denyDialMultiaddr","peerDiscovery","services","identify","libp2pFactory","fs","addEventListener","evt","peerId","detail","conn","getConnections","transportsByAddr","fromEntries","remoteAddr","protoCodes","getMultiaddrs","fileSize","localFileSize","dagSize","mtime","optionsV0","fileName","addFile","path","TextEncoder","encode","addBytes","cid_","pins","isPinned","remotePeer","stop","dial","iterable","metadata","toV0","mapToLsResult","host","relay","enabled","hop","preload","API","HTTPHeaders","Addresses","Gateway","Swarm","Delegates","Discovery","MDNS","Enabled","Interval","webRTCStar","Bootstrap","Pubsub","ConnMgr","HighWater","LowWater","DisableNatPortMap","Routing","Type","filters","nat","EXPERIMENTAL","ipnsPubsub","Number","nodeClassMap","initIpfsNode","ipfsNodeType","restOptions","EnhancedClass","Base","parseAs","details","getPeers","swarmPeerId","forced","isConnectedToSwarm","connectPeer","swarmPeerAddress","withCybFeatures","instance","init","urlOpts","allowLocalModels","mlModelMap","featureExtractor","model","createMlApi","broadcastApi","featureExtractor$","replaySubject","ReplaySubject","pooling","normalize","searchByEmbedding","api","createEmbeddingApi$","initPipelineInstance","alias","pipeline","progress_callback","progressData","loaded","progrssStateItem","loadPipeline","time","timeEnd","compileConfig","budget","experimental","instructions","defaultRuneEntrypoint","readOnly","execute","funcName","funcParams","input","script","scriptEngine","entrypoints","context","user","secrets","entrypoints$","scriptCallbacks","isSoulInitialized$","run","compileParams","scripts","refId","compilerParams","runtime","app","outputData","diagnosticsOutput","getParticleScriptOrAction","askCompanion","resultType","metaItems","output","personalProcessor","outputContent","setEntrypoints","scriptEntrypoints","pushContext","popContext","names","newContext","executeFunction","executeCallback","getDebug","enigine","backgroundWorker","setInnerDeps","rune","runeDeps","createRuneApi","ipfsQueue","ipfsApi","ipfsOpts","newIpfsNode","ipfsNode","fetchWithDetails","dequeue","dequeueByParent","clearQueue","addContent","createIpfsApi","serviceDeps","injectDb","isIpfsInitialized","setRuneDeps","setParams","createBackgroundWorkerApi","onconnect","ports","extractRuneScript","md","hasRune","runeRegex","runeScript","modifiedMarkdown","exec","extractRuneContent","queryContractSmartPassport","client","toBase64","toAscii","getPassport","SigningCyberClientError","code","rawLog","cyblog","defaultFee","gas","sendCyberlink","signingClient","fee","cyberlink","putCyberlink","addCyberlinkLocal","subjectDeps","queryClient","defferedDependency","item$","CyberClient","getIpfsTextConent","cybApi","createAbortController","graphSearch","page","keywordHash","getSearchQuery","search","searchByHash","getPassportByNickname","nickname","passport","passport_by_nickname","searcByEmbedding","evalScriptFromIpfs","pureScript","executeScriptCallback","externalDeps","createRuneDeps","defaultOpenAIParams","openAICompletion","apiKey","cb","Authorization","json","choices","decoder","TextDecoder","buffer","lines","split","pop","line","parsed","delta","jsCyberSearch","jsCyberLink","fromCid","jsGetPassportByNickname","jsEvalScriptFromIpfs","jsGetIpfsTextContent","jsAddContenToIpfs","jsExecuteScriptCallback","jsOpenAICompletions","jsSearchByEmbedding","jsCyberLinksFrom","jsCyberLinksTo","evn","paramsSerializer","indexes","txs","tx_responses","getLink","ORDER_BY_DESC","getFromLink","getToLink","Networks","ADD_AVATAR","FOLLOW","numberToUtcDate","dateToUtcNumber","isoString","endsWith","getNowUtcNumber","getIpfsHash","reject","marshal","DAGNode","dagNode","toBaseEncodedString","CYBLOG_BROADCAST_CHANNEL_NAME","logList","createCyblogChannel","defaultContext","postLogToChannel","level","trace","consoleLogParams","formattedMessage","formatter","logItem","truncate","appendLog","stacktrace","contextItem","p","ctxItem","consoleLog","getLogs","splice","getConsoleLogParams","createCybLog","__webpack_module_cache__","__webpack_require__","moduleId","cachedModule","undefined","exports","__webpack_modules__","m","x","__webpack_exports__","O","amdO","chunkIds","notFulfilled","Infinity","j","every","r","n","getter","__esModule","d","getPrototypeOf","__proto__","mode","ns","create","def","current","getOwnPropertyNames","definition","o","defineProperty","enumerable","f","chunkId","promises","u","miniCssF","g","globalThis","Function","hmd","children","prop","toStringTag","nmd","paths","location","installedChunks","importScripts","chunkLoadingGlobal","parentChunkLoadingFunction","moreModules"],"sourceRoot":""}